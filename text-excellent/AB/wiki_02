<doc id="54572207" url="https://en.wikipedia.org/wiki?curid=54572207" title="Mosaics of Delos">
Mosaics of Delos

The mosaics of Delos are a significant body of ancient Greek mosaic art. Most of the surviving mosaics from Delos, Greece, an island in the Cyclades, date to the last half of the 2nd century BC and early 1st century BC, during the Hellenistic period and beginning of the Roman period of Greece. Hellenistic mosaics were no longer produced after roughly 69 BC, due to warfare with the Kingdom of Pontus and subsequently abrupt decline of the island's population and position as a major trading center. Among Hellenistic Greek archaeological sites, Delos contains one of the highest concentrations of surviving mosaic artworks. Approximately half of all surviving tessellated Greek mosaics from the Hellenistic period come from Delos.

The paved walkways of Delos range from simple pebble or chip-pavement constructions to elaborate mosaic floors composed of tesserae. Most motifs contain simple geometric patterns, while only a handful utilize the "opus tessellatum" and "opus vermiculatum" techniques to create lucid, naturalistic, and richly colored scenes and figures. Mosaics have been found in places of worship, public buildings, and private homes, the latter usually containing either an irregular-shaped floor plan or peristyle central courtyard.

Although there are minor traces of Punic-Phoenician and Romano-Italian influence, the Delian mosaics generally conform to the major trends found in Hellenistic art. The same wealthy patrons who commissioned paintings and sculptures at Delos may have also been involved in hiring mosaic artists from abroad. Delian mosaics share characteristics with those in other parts of the Greek world, such as Macedonian mosaics in Pella. They also bear some attributes with Greek painting traditions and often employ a similar black-background technique found in red-figure pottery of the Classical period. Some of the styles and techniques found at Delos are evident in Roman art and mosaics, although contemporary Roman examples from Pompeii, for instance, reveal significant differences in the production and design of mosaics in the western versus eastern Mediterranean.

Precisely 354 mosaics from Delos survive and have been studied by French archaeologist . Most date to the late Hellenistic period, contiguous with the late Roman Republic (i.e. the last half of the 2nd century BC and early 1st century BC). A handful were dated to the Classical period, with one mosaic attributed to the Imperial Roman era. Bruneau believed that nominally undated pieces, on the basis of their styles, were produced within the same period as majority of examples, roughly between 133 and 88 BC. In 167 or 166 BC, after the Roman victory in the Third Macedonian War, Rome ceded the island of Delos to the Athenians, who expelled most of the original inhabitants.

The Roman destruction of Corinth in 146 BC allowed Delos to at least partially assume the former's role as the premier trading center of Greece. Delos' commercial prosperity, construction activity, and population waned significantly after the island was assaulted by the forces of Mithridates VI of Pontus in 88 and 69 BC, during the Mithridatic Wars with Rome. Despite the invasions by Pontus, the island was only gradually abandoned after Rome secured a more direct trading link with the Orient that marginalized Delos as a pivotal midway point for trade leading to the East.

The composition of the Delos mosaics and pavements include simple pebble constructions, chip-pavement made of white marble, ceramic fragments, and pieces of tesserae. The latter falls into two categories: the simpler, tessellated "opus tessellatum" using large pieces of tesserae, on average eight by eight millimeters, and the finer "opus vermiculatum" using pieces of tesserae smaller than four by four millimeters. Many Delian mosaics use a mixture of these materials, while chip pavement is the most common. The latter is found in 55 homes and usually reserved for the ground floor. The majority of Delian mosaics comprise broken pieces of marble set into cement floors; other flooring bases are composed of either rammed earth or gneiss flagstones. Pavements in kitchens and latrines were built with pottery, brick, and tile fragments for the purpose of waterproofing. Thin strips of lead set into the cement are often used to distinguish the contours of geometric-patterned mosaics, but are absent in the more complex tessellated, figured mosaics.
While some mosaics have been unearthed from religious sanctuaries and public buildings, most of them were found in residential buildings and private homes. The majority of these houses possess an irregular-shaped floor plan, while the second largest group were built with a peristyle central courtyard. Simple mosaics were usually relegated to normal walkways, whereas rooms designated for receiving guests featured more richly decorated mosaics. However, only 25 houses of Delos feature "opus tessellatum" mosaics and only eight houses possess the "opus vermiculatum"-style motifs and figured scenes. The vast majority of decorated floors feature only simple geometric patterns. It is also more common for "opus vermiculatum" and "opus tessellatum" mosaics to be found in upstairs rooms than on the ground floors of ancient Delian homes. With the exception of the House of Dionysos and House of the Dolphins, the courtyards of peristyle homes in Delos feature only floral and geometric motifs.
Among the various patterns and motifs found in Delian mosaics is the triple-colored lozenge that creates a three-dimensional illusion of cubes in perspective for the viewer. This pattern appears in fifteen different locations, making it one of the most common. Other motifs include waves and stepped triangles, while major themes include maritime, theatrical, natural, or mythological objects and figures. The single wave pattern, a common motif in Hellenistic art, is the most predominant type of border design for mosaics at Delos and can be found at other sites such as Arsameia (albeit arranged in the opposite direction). The rosette motif, which is found in the mosaics of various Hellenistic sites across the Mediterranean, is often coupled with single-wave borders in Delian mosaics. The typical Hellenistic palmette motif is used in a mosaic of Delos to fill the four corners around a central rosette motif. The illusion of three-dimensional relief in the figured scenes of Delian mosaics was usually achieved by the use of polychrome, with white, black, yellow, red, blue and green hues.

The origins of the composition, techniques, layout, and style of Delian mosaics can be found in 5th-century BC pebble mosaics of Olynthus in the Chalcidice of northern Greece, with mosaics positioned in the center of cement floors and utilizing garland, meander, and wave patterns around a centralized motif or figured scene. This design scheme is similar to that of 4th-century BC mosaics of Pella in Macedonia, although the pebble mosaics there employ a wider range of colors to create the effects of volume. The transition from pebble mosaics to more complex tessellated mosaics perhaps originated in Hellenistic-Greek Sicily during the 3rd century BC, developed at sites such as Morgantina and Syracuse. Much like Olynthus, mosaics of Morgantina contain the garland, wave, and meander patterns, although the latter was finally executed with perspective.

Aside from a symbol of the Punic-Phoenician goddess Tanit, all pavement motifs are typically Hellenistic Greek in origin, although some pavement mortars used with tesserae designs betray some Italian influence. The three major ethnic groups of Delos included Greeks (largely of Athenian origin), Syrians/Phoenicians, and Italians/Romans, but it is very likely that many of these Italians were Italiotes, Greek-speaking natives of Magna Graecia in what is now southern Italy. Delian inhabitants of either Greek, Italian, and Syrian origins owned mosaics in their private households, but Vincent J. Bruno asserts that the designs of their mosaic artworks were indebted entirely to Greek artistic traditions.

The French archaeologist François Chamoux considered the mosaics of Delos as the "high-water mark" and pinnacle of ancient Greek mosaic art utilizing tesserae to create rich, detailed and colorful scenes. This Hellenistic style of mosaic continued until the end of Antiquity and may have influenced the widespread use of mosaics in the Western world during the Middle Ages. In her study of the households and artworks of Mediterranean trading centers, Birgit Tang analyzed three archaeological sites: Delos in the Aegean, Carthage in what is now modern Tunisia, and Emporion, modern Empúries in Catalonia, Spain, which was once a Greek colony. The reasons for her choosing these sites in particular for investigation and comparison include their status as major maritime trading hubs as well as their relatively well-preserved ruins of urban households.

Ruth Westgate writes that Delos contains roughly half of all surviving tessellated Greek mosaics from the Hellenistic period. In her estimation the sites of Delos and Morgantina and Soluntum in Sicily contain the largest amount of surviving evidence for Hellenistic Greek mosaics. Hariclia Brecoulaki asserts that the Delos mosaics represent the single largest collection of Greek mosaics. She also states that only the Macedonian capital of Pella ranks as an equal in having private homes (as opposed to royal residences) decorated with elaborate wall paintings, signed mosaics, and freestanding marble sculptures. Katherine M. D. Dunbabin writes that while many Hellenistic mosaics have been found in mainland Greece, Asia Minor, and northeast Africa (i.e. Cyrene), it is only at the site of Delos where they occur in "sufficient numbers to allow general conclusions about their use and nature."

In her comparative analysis of mosaic art in the Greco-Roman world, Hetty Joyce chose the mosaics of Delos and Roman Pompeii as chief representative samples for determining distinctions in the form, function, and production techniques of mosaics in the Greek East and Latin West. Her reasoning for the selection of these two sites are their well-preserved pavements, the secure dating of the samples to the late 2nd and early 1st centuries BC, and, thanks to the extensive documentation of Delian mosaics by Bruneau, a sufficient amount of academic literature dedicated to each site to form comparisons. Ruth Westgate, in her survey and comparative study of Hellenistic Greek mosaics with mosaics of Pompeii, concludes that the Roman mosaics, dated to the Pompeian First Style of wall painting in the late 2nd and early 1st centuries BC, were derived from the Greek tradition. However, she stresses that Pompeian mosaics departed from their Greek counterparts by almost exclusively featuring figured scenes instead of abstract designs, in plain pavement most likely set by local craftsmen and produced separately from the figured panels, the latter of which were perhaps made by Greek artisans for their Roman patrons.

Due to the similarities between the Hellenistic wall paintings at Delos and the First Style of Pompeii, Joyce contends that the differences in Delian and Pompeian mosaics are the deliberate product of artistic preference rather than the result of ignorance of each other's traditions. These differences include the widespread use of "opus signinum" at Pompeii, with only four known examples at Delos; the use of "opus sectile" at Pompeii and its complete absence at Delos; the prevalent use of polychrome patterns and intricate, three-dimensional figured designs in Delian mosaics versus two-dimensional designs at Pompeii, which at best utilize two colors. Complex three-dimensional figured mosaics using polychrome designs to achieve the illusion of light and shadow were not produced at Pompeii until the Pompeian Second Style of wall painting (80-20 BC) and are considered an adoption from Hellenistic art trends. While lead strips were used in Hellenistic mosaics of Delos, Athens, and Pella (Greece), Pergamon (Turkey), Callatis (Romania), Alexandria (Egypt), and Chersonesus (the Crimean peninsula), they are absent in Western Mediterranean mosaics of Malta, Sicily, and the Italian peninsula. Westgate affirms that Hellenistic mosaics can be divided into two broad categories: eastern and western, based on their different styles and production techniques.

Red-figure pottery was no longer produced by the time the Delos mosaics were made. The black background technique of red-figure pottery was still appreciated in 4th-century-BC Macedonian pebble mosaics from Pella and in mosaics at Delos, such as the white-figured tritoness mosaic with tesserae. The black background technique was later used in glass art such as cameo glass, particularly Roman glass (e.g. Portland Vase, Gemma Augustea, Great Cameo of France, etc.).

The undulating garland motif against a black background from the masonry-style mural paintings at Delos were earlier featured in Greek works ranging from vases to 4th-century-BC Macedonian mosaics of Pella, particularly the Stag Hunt Mosaic. However, the painters of Delos arguably invented their own decorative genre using a combination of these older elements with new naturalistic coloring. Aside from the black background, mosaics like the Stag Hunt Mosaic were also inspired by the illusionist, three-dimensional qualities of Greek paintings. At Delos, paintings and mosaics inherited the same Classical Greek standards of craftsmanship, lighting, shading, and coloring. Sculptors, painters, and mosaic artists may have all been part of the same system of patronage at Delos, which in some instances would have necessitated the importation of foreign artists.

The northern quarter of Delos contains the Jewelry Quarter, where older structures such as workshops and other archaeological remains dating to the 3rd century BC and early 2nd century BC have been discovered. By the second half of the 2nd century BC these were replaced by private homes built in the most characteristic fashion for Delos: a narrow, rectangular floor plan with a central courtyard, a "vestibule" service room in the front, and a larger, main room in the rear. The quarter of the House of the Masks is the only area of Delos without this archetypal house plan. Some houses of the Northern Quarter feature mosaic decorations with mythological scenes, including Lycurgus of Thrace and Ambrosia in an upper-story mosaic, as well as Athena and Hermes together with a seated woman in a main-room mosaic.

Most houses in the crowded Theatre Quarter of Delos have irregular-shaped floor plans (such as trapezoidal designs), as opposed to square or rectangular designs. The narrow and irregular street grid is unlike that of other quarters, where streets usually meet at approximately right angles. Similar to the majority of excavated homes of Delos, those in the Theatre Quarter feature an open courtyard without porticoes, instead of the peristyle layout with columns. Some of the houses in the Theatre Quarter lack interior decoration altogether, with neither wall murals nor mosaics, which is unusual for most Delian homes.

The Delos mosaic of Dionysos riding a tiger in the House of Dionysos is one of the finest examples of "opus vermiculatum", according to Dunbabin, and is comparable to the Dionysos riding a leopard from the House of the Masks if not the simpler from the Macedonian capital at Pella. A key thematic difference, however, are the wings of Dionysos that suggest his incarnation as a daimon instead of a god. The tesserae materials, made of glass, faience, terracotta and natural stones, are fashioned into pieces measuring roughly one millimeter square, allowing for sharp detail and an elaborate color scheme.

The House of the Masks is named after the mosaic motif of theatre masks decorating ivy scrolls arranged in strips around a central surface area decorated in a cubic pattern. The more intricately decorated mosaics are found in four different rooms branching off from the peristyle courtyard paved with marble chips, with corridor mosaics utilizing amphora fragments. In the center of one mosaic is the figure of Dionysos riding a leopard against a similar black background to the mosaic in the House of Dionysos. Another central mosaic features a flute player and a dancing figure, the latter perhaps representing Silenus. Only the Dionysos figure employs the "vermiculatum" method. The other mosaics of the house fail to achieve the naturalism of finer figure scenes and motifs, but they nevertheless demonstrate an attempt at mimicking their illusionist qualities with the tesselated technique.

The House of the Dolphins contains a peristyle floor mosaic design that is unusual for Delos, with a circle enclosed by a square outline. In each corner of the square are pairs of dolphins ridden by tiny winged figures bearing the emblems of various Greek deities, namely the thyrsus, caduceus, trident, and one object that is missing due to damage. The circle contains a central rosette design surrounded by floral garlands and griffins. The mosaic, signed by a certain Asclepiades of Arados (in ancient Phoenicia, now western Syria), is one of only two examples from Delos that bear a signature of the original artist.

Similar to the design of the majority of Delian homes, the House of the Lake is an irregular-shaped home (as opposed to a rectangular or square floor plan), located near a sacred lake and inhabited from roughly 300 to 100 BC. The peristyle "impluvium" is decorated with a geometric-patterned motif around a central rosette, surrounded on all sides by monolithic columns in the Ionic style.

The House of the Trident contains peristyle panels with the motif of a black dolphin situated around a red anchor and black tridents against a white background. The theme suggests that the owners of the house were somehow connected to maritime pursuits. These simple, two-dimensional mosaics stand in contrast to the multicolored, finely detailed, three-dimensional Hellenistic mosaics of figures and motifs. They are perhaps comparable or even linked to the black-and-white pavement mosaics that appear in Roman Italy some decades later.





</doc>
<doc id="54638395" url="https://en.wikipedia.org/wiki?curid=54638395" title="Ethiopian historiography">
Ethiopian historiography

Ethiopian historiography embodies the ancient, medieval, early modern and modern disciplines of recording the history of Ethiopia, including both native and foreign sources. The roots of Ethiopian historical writing can be traced back to the ancient Kingdom of Aksum (c. 100 – c. 940 AD). These early texts were written in either the Ethiopian Ge'ez script or the Greek alphabet, and included a variety of mediums such as manuscripts and epigraphic inscriptions on monumental stelae and obelisks documenting contemporary events. The writing of history became an established genre in Ethiopian literature during the early Solomonic dynasty (1270–1974). In this period, written histories were usually in the form of royal biographies and dynastic chronicles, supplemented by hagiographic literature and universal histories in the form of annals. Christian mythology became a linchpin of medieval Ethiopian historiography due to works such as the Orthodox "Kebra Nagast". This reinforced the genealogical traditions of Ethiopia's Solomonic dynasty rulers, which asserted that they were descendants of Solomon, the legendary King of Israel.

Ethiopian historiographic literature has been traditionally dominated by Christian theology and the chronology of the Bible. There was also considerable influence from Muslim, pagan and foreign elements from within the Horn of Africa and beyond. Diplomatic ties with Christendom were established in the Roman era under Ethiopia's first Christian king, Ezana of Axum, in the 4th century AD, and were renewed in the Late Middle Ages with embassies traveling to and from medieval Europe. Building on the legacy of ancient Greek and Roman historical writings about Ethiopia, medieval European chroniclers made attempts to describe Ethiopia, its people, and religious faith in connection to the mythical Prester John, who was viewed as a potential ally against Islamic powers. Ethiopian history and its peoples were also mentioned in works of medieval Islamic historiography and even Chinese encyclopedias, travel literature, and official histories.

During the 16th century and onset of the early modern period, military alliances with the Portuguese Empire were made, the Jesuit Catholic missionaries arrived, and prolonged warfare with Islamic foes including the Adal Sultanate and Ottoman Empire, as well as with the polytheistic Oromo people, threatened the security of the Ethiopian Empire. These contacts and conflicts inspired works of ethnography, by authors such as the monk and historian Bahrey, which were embedded into the existing historiographic tradition and encouraged a broader view in historical chronicles for Ethiopia's place in the world. The Jesuit missionaries Pedro Páez (1564–1622) and Manuel de Almeida (1580–1646) also composed a history of Ethiopia, but it remained in manuscript form among Jesuit priests of Portuguese India and was not published in the West until modern times.

Modern Ethiopian historiography was developed locally by native Ethiopians as well as by foreign historians, most notably Hiob Ludolf (1624–1704), the German orientalist who British historian Edward Ullendorff (1920–2011) considered the founder of Ethiopian Studies. The late 19th and early 20th centuries marked a period where Western historiographic methods were introduced and synthesized with traditionalist practices, embodied by works such as those by Heruy Wolde Selassie (1878–1938). The discipline has since developed new approaches in studying the nation's past and offered criticism of some traditional Semitic-dominated views that have been prevalent, sometimes at the expense of Ethiopia's traditional ties with the Middle East. Marxist historiography and African studies have also played significant roles in developing the discipline. Since the 20th century, historians have given greater consideration to issues of class, gender, and ethnicity. Traditions pertaining mainly to other Afroasiatic-speaking populations have also been accorded more importance, with literary, linguistic, and archaeological analyses reshaping the perception of their roles in historical Ethiopian society. Historiography of the 20th century focused largely on the Abyssinian Crisis of 1935 and the Second Italo-Ethiopian War, whereas the Ethiopian victory over the Kingdom of Italy in the 1896 Battle of Adwa played a major role in the historiographic literature of these two countries immediately following the First Italo-Ethiopian War.

Writing was introduced to Ethiopia as far back as the 5th century BC with the ancient South Arabian script. This South Semitic script served as the basis for the creation of Ethiopia's Ge'ez script, the oldest evidence for it found in Matara, Eritrea, and dated to the 2nd century AD. However, the 1st-century AD Roman "Periplus of the Erythraean Sea" asserts that the local ruler of Adulis could speak and write in Greek. This embrace of Hellenism could also be found in the coinage of Aksumite currency, in which legends were usually written in Greek, much like ancient Greek coinage.

The roots of the historiographic tradition in Ethiopia date back to the Aksumite period (c. 100 – c. 940 AD) and are found in epigraphic texts commissioned by monarchs to recount the deeds of their reign and royal house. Written in a false autobiographical style, in either the native Ge'ez script, the Greek alphabet, or both, they are preserved on stelae, thrones, and obelisks found in a wide geographical span that includes Sudan, Eritrea, and Ethiopia. In commemorating the contemporary ruler or aristocrats and elite members of society, these documents record various historical events such as military campaigns, diplomatic missions, and acts of philanthropy. For instance, 4th-century stelae erected by Ezana of Axum memorialize his achievements in battle and expansion of the realm in the Horn of Africa, while the "Monumentum Adulitanum" inscribed on a throne in Adulis, Eritrea, contains descriptions of Kaleb of Axum's conquests in the Red Sea region during the 6th century, including parts of the Arabian peninsula. It is clear that such texts influenced the epigraphy of later Aksumite rulers who still considered their lost Arabian territories as part of their realm.

In Roman historiography, the ecclesiastical history of Tyrannius Rufinus, a Latin translation and extension of the work of Eusebius dated circa 402, offers an account of the Christian conversion of Ethiopia (labeled as "India ulterior") by the missionary Frumentius of Tyre. The text explains that Frumentius, in order to complete this task, was ordained a bishop by Athanasius of Alexandria (298–373), most likely after 346 during the latter's third tenure as Bishop of Alexandria. The mission certainly took place before 357, when Athanasius was deposed, replaced by George of Cappadocia, and forced into flight, during which he wrote an apologetic letter to Roman emperor Constantius II (r. 337–361) that coincidentally preserved an Imperial Roman letter to the royal court of Aksum. In this letter, Constantius II addresses two "tyrants" of Ethiopia, Aizanas and Sazanas, who are undoubtedly Ezana and his brother Saiazana, or Sazanan, a military commander. The letter also hints that the ruler of Aksum was already a Christian monarch. From the early inscriptions of Ezana's reign it is clear that he was once a polytheist, who erected bronze, silver, and gold statues to Ares, Greek god of war. But the dual Greek and Sabaean-style Ge'ez inscriptions on the Ezana Stone, commemorating Ezana's conquests of the Kingdom of Kush (located in Nubia, i.e. modern Sudan), mention his conversion to Christianity. This claim is supported by the Christian symbol of the cross decorating virtually all Aksumite coins minted after Ezana's reign, along with the sudden discontinuation of polytheistic writings.
Cosmas Indicopleustes, a 6th-century Eastern Roman monk and former merchant who wrote the "Christian Topography" (describing the Indian Ocean trade leading all the way to China), visited the Aksumite port city of Adulis and included eyewitness accounts of it in his book. He copied a Greek inscription detailing the reign of an early 3rd-century polytheistic ruler of Aksum who sent a naval fleet across the Red Sea to conquer the Sabaeans in what is now Yemen, along with other parts of western Arabia. Ancient Sabaean texts from Yemen confirm that this was the Aksumite ruler Gadara, who made alliances with Sabaean kings, leading to eventual Axumite control over western Yemen that would last until the Himyarite ruler Shammar Yahri'sh (r. c. 265 – c. 287) expelled the Aksumites from southwestern Arabia. It is only from Sabaean and Himyarite inscriptions that we know the names of several Aksumite kings and princes after Gadara, including the monarchs `DBH and DTWNS. Inscriptions of king Ezana mention stone-carved thrones near the Church of Our Lady Mary of Zion in Axum (the platforms of which still exist), and Cosmas described a white-marble throne and stele in Adulis that were both covered in Greek inscriptions.

Aside from epigraphy, Aksumite historiography also includes the manuscript textual tradition. Some of the earliest Ethiopian illuminated manuscripts include translations of the Bible into Ge'ez, such as the Garima Gospels that were written between the 4th and 7th centuries and imitated the Byzantine style of manuscript art. The Aksum Collection containing a Ge'ez codex that provides chronologies for the diocese and episcopal sees of the Coptic Orthodox Church of Alexandria in Roman Egypt was compiled between the 5th and 7th centuries. These texts reveal how the Aksumites viewed history through the narrow lens of Christian chronology, but their early historiography was perhaps also influenced by non-Christian works, such as those from the Kingdom of Kush, the Ptolemaic dynasty of Hellenistic Egypt, and the Yemenite Jews of the Himyarite Kingdom.

The power of the Aksumite Kingdom declined after the 6th century due to the rise of other regional states in the Horn of Africa. Modern scholars continue to debate the identity and provenance of the legendary or semi-legendary figure Gudit (fl. 10th century), a queen who is traditionally believed to have overthrown the Kingdom of Aksum. The legend is found in the 13th-century chronicle of the monk Tekle Haymanot, who compiled historical writings gathered from various Ethiopian churches and monasteries. The chronicle alleges that, after being exiled from Axum, she married a Jewish king of Syria and converted to Judaism. The Scottish travel writer James Bruce (1730–1794) was incredulous about the tale and believed she was simply a Jewish queen. Carlo Conti Rossini (1872–1949) hypothesized that she was an ethnic Sidamo from Damot, whereas Steven Kaplan argues she was a non-Christian invader and historian Knud Tage Andersen contends she was a regular member of the Aksumite royal house who shrewdly seized the throne. The latter is more in line with another legend that claims Dil Na'od, the last king of Aksum, kept his daughter Mesobe Werq in isolation out of fear of a prophecy that her son would overthrow him, yet she eloped with the nobleman Mara Takla Haymanot from Lasta who eventually killed the Aksumite king in a duel, took the throne and founded the Zagwe dynasty. The latter remains one of the most poorly understood periods of Ethiopia's recorded history. What is known is that the early Zagwe kings were polytheistic, eventually converted to Christianity, and ruled over the northern Highlands of Ethiopia, while Islamic sultanates inhabited the coastal Ethiopian Lowlands.

When the forces of Yekuno Amlak (r. 1270–1285) toppled the Zagwe dynasty in 1270 he became the first Emperor of Ethiopia, establishing a line of rulers in the Solomonic dynasty that would last into the 20th century. By this time the Greek language, once pivotal for translation in Ethiopian literature, had become marginalized and mixed with Coptic and Arabic translations. This contributed to a process by which medieval Ethiopian historians created a new historiographic tradition largely divorced from the ancient Aksumite textual corpus. The Solomonic kings professed a direct link to the kings of Aksum and a lineage traced back to Solomon and the Queen of Sheba in the Hebrew Bible. These genealogical traditions formed the basis of the "Kebra Nagast", a seminal work of Ethiopian literature and Ge'ez-language text originally compiled in Copto-Arabic sometime between the 10th and 13th centuries. Its current form dates to the 14th century, by which point it included detailed mythological and historical narratives relating to Ethiopia along with theological discourses on themes in the Old and New Testament. De Lorenzi compares the tome's mixture of Christian mythology with historical events to the legend of King Arthur that was greatly embellished by the Welsh cleric Geoffrey of Monmouth in his chronicle "Historia Regum Britanniae" of 1136. Although the "Kebra Nagast" indicates that the emperors of Rome or Constantinople and Ethiopia were descended from the Israelite king Solomon, there is an emphatically anti-Jewish sentiment expressed in several passages of the book.

The most common form of written history sponsored by the Solomonic royal court was the biography of contemporary rulers, who were often lauded by their biographers along with the Solomonic dynasty. The royal biographical genre was established during the reign of Amda Seyon I (r. 1314–1344), whose biography not only recounted the diplomatic exchanges and military conflicts with the rival Islamic powers of the Ifat Sultanate and Adal Sultanate, but also depicted the Ethiopian ruler as the Christian savior of his nation. The origins of the dynastic history ("tarika nagast") are perhaps found in the biographical chronicle of Baeda Maryam I (r. 1468–1478), which provides a narrative of his life and that of his children and was most likely written by the preceptor of the royal court. Teshale Tibebu asserts that Ethiopian court historians were "professional flatterers" of their ruling monarchs, akin to their Byzantine Greek and Imperial Chinese counterparts. For instance, the anonymously written biography of the emperor Gelawdewos (r. 1540–1549) speaks glowingly of the ruler, albeit in an elegiac tone, while attempting to place him and his deeds within a greater moral and historical context.

There are also hagiographies of previous Zagwe dynastic rulers composed during the Solomonic period. For instance, during the reign of Zara Yaqob (1434–1468) a chronicle focusing on Gebre Mesqel Lalibela (r. 1185–1225) portrayed him as a Christian saint who performed miracles. Conveniently for the legitimacy of the Solomonic dynasty, the chronicle stated that Lalibela did not desire for his heirs to inherit his throne.

In Greek historiography, Herodotus (484–425 BC) had written brief descriptions of ancient Ethiopians, who were also mentioned in the New Testament. Although the Byzantine Empire maintained regular relations with Ethiopia during the Early Middle Ages, the Early Muslim conquests of the 7th century severed the connection between Ethiopia and the rest of Christendom. Records of these contacts encouraged medieval Europeans to discover if Ethiopia was still Christian or had converted to Islam, an idea bolstered by the presence of Ethiopian pilgrims in the Holy Land and Jerusalem during the Crusades. During the High Middle Ages, the Mongol conquests of Genghis Khan (r. 1206–1227) led Europeans to speculate about the existence of a priestly, legendary warrior king named Prester John, who was thought to inhabit distant lands in Asia associated with Nestorian Christians and might help to defeat rival Islamic powers. The travel literature of Marco Polo and Odoric of Pordenone regarding their separate journeys to Yuan-dynasty China during the 13th and 14th centuries, respectively, and fruitless searches in southern India, helped to dispel the notion that Prester John's kingdom existed in Asia. An Ethiopian diplomatic mission sent to Christian Europe by Ethiopian emperor Wedem Arad (r. 1299–1314) in 1306 allowed the chance for the priest and cartographer Giovanni da Carignano (1250–1329) to question the embassy as they stayed in Genoa, after which he published a work describing the geographic location of their kingdom and the nature of their Christian faith.

In his 1324 "Book of Marvels" the Dominican missionary Jordanus, bishop of the Roman Catholic Diocese of Quilon along the Malabar Coast of India, was the first known author to suggest that Ethiopia was the location of Prester John's kingdom. The Florentine merchant Antonio Bartoli visited Ethiopia from the 1390s until about 1404 when he returned to Europe with Ethiopian diplomats. This was followed by the lengthy stay of Pietro Rombuldo in Ethiopia from 1404 to 1444 and Ethiopian diplomats attending the ecumenical Council of Florence in 1441, where they expressed some vexation with the European attendees who insisted on addressing their emperor as Prester John. Thanks to the legacy of European medieval historiography, this belief persisted beyond the Late Middle Ages. For instance, the Portuguese missionary Francisco Álvares set out for Ethiopia in 1520 believing that he was to visit the homeland of Prester John.

Ethiopia is mentioned in some works of Islamic historiography, usually in relation to the spread of Islam. Islamic sources state that in 615 the Aksumite king Armah (r. 614–631) provided refuge for the exiled followers of Muhammad in Axum, an event known as the First Hejira (i.e. Migration to Abyssinia). In his "History", the scholar ibn Wadîh al-Ya'qûbî (d. 897) of the Abbasid Caliphate identified Abyssinia ("al-Habasha") as being located to the north of the territory of the Berber (Somali) as well as the land of the Zanj (the "Blacks"). The Mamluk-Egyptian historian Shihab al-Umari (1300–1349) wrote that the historical state of Bale, neighboring the Hadiya Sultanate of southern Ethiopia, was part of an Islamic Zeila confederacy, although it fell under the control of the Ethiopian Empire in the 1330s, during the reign of Amda Seyon I. Al-Maqrizi (1364–1422), another Mamluk-Egyptian historian, wrote that the Ifat sultan Sa'ad ad-Din II (r. 1387–1415) won a crushing victory against the Christian Amhara in Bale, despite the latter's numerical superiority. He described other allegedly significant victories won by the Adal sultan Jamal ad-Din II (d. 1433) in Bale and Dawaro, where the Muslim leader was said to have taken enough war booty to provide his poorer subjects with multiple slaves. Historian Ulrich Braukämper states that these works of Islamic historiography, while demonstrating the influence and military presence of the Adal sultanate in southern Ethiopia, tend to overemphasize the importance of military victories that at best led to temporary territorial control in regions such as Bale. In his "Description of Africa" (1555), the historian Leo Africanus (c. 1494–1554) of Al-Andalus described Abassia (Abyssinia) as the realm of the "Prete Ianni" (i.e. Prester John), unto whom the Abassins (Abyssinians) were subject. He also identified Abassins as one of five main population groups on the continent alongside Africans (Moors), Egyptians, Arabians and Cafri (Cafates).

Contacts between the Ethiopian Empire and Imperial China seem to have been very limited, if not mostly indirect. There were some attempts in Chinese historiographic and encyclopedic literature to describe parts of Ethiopia or outside areas that it once controlled. Zhang Xiang, a scholar of Africa–China relations, asserts that the country of "Dou le" described in the "Xiyu juan" (i.e. Western Regions) chapter of the "Book of Later Han" was that of the Aksumite port city of Adulis. It was from this city that an envoy was sent to Luoyang, the capital of China's Han dynasty, in roughly 100 AD. The 11th-century "New Book of Tang" and 14th-century "Wenxian Tongkao" describe the country of Nubia (previously controlled by the Aksumite Kingdom) as a land of deserts south of the Byzantine Empire that was infested with malaria, where the natives of the local "Mo-lin" territory had black skin and consumed foods such as Persian dates. In his English translation of this document, Friedrich Hirth identified "Mo-lin" ("Molin") with the kingdom of 'Alwa and neighboring "Lao-p'o-sa" with the kingdom of Maqurra, both in Nubia. 

The "Wenxian Tongkao" describes the main religions of Nubia, including the "Da Qin" religion (i.e. Christianity, particularly Nestorian Christianity associated with the Eastern Roman Empire) and the day of rest occurring every seven days for those following the faith of the "Da shi" (i.e. the Muslim Arabs). These passages are ultimately derived from the "Jingxingji" of Du Huan (fl. 8th century), a travel writer during the Chinese Tang dynasty (618–907) who was captured by Abbasid forces in the 751 Battle of Talas, after which he visited parts of West Asia and northeast Africa. Historian Wolbert Smidt identified the territory of "Molin" in Du's "Jingxingji" (preserved in part by the "Tongdian" of Du You) as the Christian kingdom of Muqurra in Nubia. He also associated the territory of "Laobosa" ("Lao-p'o-sa") depicted therein with Abyssinia, thereby making this the first Chinese text to describe Ethiopia. When Du Huan left the region to return home, he did so through the Aksumite port of Adulis. Trade activity between Ethiopia and China during the latter's Song dynasty (960–1279) seems to be confirmed by Song-Chinese coinage found in the medieval village of Harla, near Dire Dawa, Ethiopia. The Chinese Ming dynasty (1368–1644) sent diplomats to Ethiopia, which was also frequented by Chinese merchants. Although only private and indirect trade was conducted with African countries during the early Manchu-led Qing dynasty (1644–1911), the Chinese were able to refer to Chinese-written travel literature and histories about East Africa before diplomatic relations were restored with African countries in the 19th century.

During the 16th century the Ethiopian biographical tradition became far more complex, intertextual, and broader in its view of the world given Ethiopia's direct involvement in the conflicts between the Ottoman and Portuguese empires in the Red Sea region. The annals of Dawit II (r. 1508–1540) describe the defensive war he waged against the Adal sultan Ahmad ibn Ibrahim al-Ghazi (r. 1527–1543), in an episodic format quite different from the earlier chronicling tradition. The chronicle of Gelawdewos, perhaps written by the Ethiopian Orthodox Church abbot Enbaqom (1470–1560), is far more detailed than any previous Ethiopian work of history. It explains the Ethiopian emperor's military alliance with Cristóvão da Gama (1516–1542), son of the Portuguese explorer Vasco da Gama, against the Adal Sultan al-Ghazi and his Ottoman allies, and later against the Ottoman governor of Yemen, Özdemir Pasha (d. 1560).

The biography of Galawdewos' brother and successor Menas of Ethiopia (r. 1559–1563) is divided into two parts, one dedicated to his life before taking the throne and the other to his troubled reign fighting against rebels. His chronicle was completed by the biographers of his successor Sarsa Dengel (r. 1563–1597). The latter's chronicle can be considered an epic cycle for its preface describing events in previous eras mixed with biblical allusions. It also describes conflicts against rebel nobility allied with the Ottomans as well as a military campaign against Ethiopian Jews.

By the 16th century Ethiopian works began to discuss the profound impact of foreign peoples in their own regional history. The chronicle of Gelawdewos explained the friction between the Ethiopian Orthodox Church and the Catholic missionaries from Spain and Portugal, after the arrival of the Jesuits in 1555. With the persuasion of Jesuits in his realm, emperor Susenyos I (r. 1607–1632) became the only Ethiopian ruler to convert from Orthodox Christianity to Catholicism, perhaps earlier than the accepted date of 1625, after which his attempts to convert his subjects and undermine the Orthodox church led to internal revolts. In 1593 the Ethiopian monk, historian, and ethnographer Bahrey published a work of ethnography that provided reasoning for the military success of the polytheistic Oromo people who fought against the Ethiopian Empire. Ethiopian histories of this period also included details of foreign Muslims, Jews, Christians (including those from Western Europe), Safavid Iranians, and even figures of the fallen Byzantine Empire.
Pedro Paez (1564–1622), a Spanish Jesuit at the court of Susenyos I, translated portions of the chronicles of Ethiopian emperors stretching back to the reign of Amda Seyon I in the 14th century AD, as well as the reign of king Kaleb of Axum in the 6th century AD. Some of these fragments were preserved in the "Historia de Ethiopia" by the Portuguese Jesuit Manuel de Almeida (1580–1646), but Paez's original manuscript was largely rewritten to remove polemical passages against the rival Dominican Order. Paez also translated a chapter from an Ethiopian hagiography that covered the life and works of the 13th-century ruler Gebre Mesqel Lalibela. The "Historia de Ethiopia", which arrived in Goa, India, by the end of 1624, was not published in Europe until the modern era and only remained in circulation among members of the Society of Jesus in Portuguese India, although Almeida's map of Ethiopia was published by Baltasar Teles in 1660. In his treatise "A Voyage to Abyssinia", the Portuguese Jesuit missionary Jerónimo Lobo (1595–1678) described Abyssinia and its denizens, indicating that the native inhabitants were of two distinct physical types: the Abyssinians proper, who were narrow-featured, near olive-skinned, and had long hair which they wore in various styles, and the Soudans, who were platyrrhine (i.e. flat-nosed), black-skinned, and had woolly hair. Following the abdication of Susenyos I, his son and successor Fasilides (r. 1632–1667) had the Jesuits expelled from Ethiopia.

At least as far back as the reign of Susenyos I the Ethiopian royal court employed an official court historian known as a "sahafe te'ezaz", who was usually also a senior scholar within the Ethiopian Orthodox Church. Susenyos I had his confessor Meherka Dengel and counselor Takla Sellase (d. 1638), nicknamed "Tino", compose his biography. Biographies were written for the emperors Yohannes I (r. 1667–1682), Iyasu I (1682–1706), and Bakaffa (r. 1721–1730), the latter employing four separate court historians: Sinoda, Demetros, Arse, and Hawaryat Krestos. The reigns of the emperors Iyasu II (r. 1730–1755) and Iyoas I (r. 1755–1769) were included in general dynastic histories, while the last known royal biography in chronicle format prior to the 19th century was written by the church scholar Gabru and covered the first reign of Tekle Giyorgis I (r. 1779–1784), the text ending abruptly just before his deposition.

The chaotic period known as the Era of the Princes (Ethiopian: "Zemene Mesafint") from the mid-18th to mid-19th centuries witnessed political fragmentation, civil war, loss of central authority, and, as a result of these, a complete shift away from the royal biography in favor of dynastic histories. A new genre of dynastic history, known as the "Short Chronicle" according to Lorenzi, was established by a church scholar named Takla Haymanot, whose work combined universal history with Solomonic dynastic history. The "Short Chronicle" genre of historiography continued well into the 20th century. Ge'ez became an extinct language by the 17th century, but it wasn't until the reign of Tewodros II (r. 1855–1868) that royal chronicles were written in the vernacular Semitic language of Amharic.

Another genre of history writing produced during the Era of the Princes was the terse Ethiopian annal known as "ya'alam tarik". These works attempted to list major world events from the time of the biblical Genesis until their present time in a universal history. For instance, the translated work of John of Nikiû explaining human history until the Muslim conquest of Egypt in 642 became a canonical text in Ethiopian historiography. There are also chronological and genealogical lists of rulers and Orthodox Church patriarchs that include some elements of historical narrative.

Various biographies of Ethiopian emperors have been compiled in the modern era. In 1975 the Oxford-educated historian Zewde Gebre-Sellassie (1926–2008) published a biography on the Emperor Yohannes II (r. 1699–1769), with whom he was distantly related. In 1973 and 1974, the Emperor Haile Selassie (r. 1930–1974) published his autobiography "My Life and Ethiopia's Progress"; in 1976 it was translated from Amharic into English and annotated by Edward Ullendorff in an Oxford University Press publication. Hanna Rubinkowska maintains that Emperor Selassie was an active proponent of "historiographic manipulation", especially when it came to concealing historical materials that seemingly contested or contradicted dynastic propaganda and official history. For instance, he removed certain chronicles and historical works from the public eye and placed them in his private library, such as "aleqa" Gabra Igziabiher Elyas' (1892–1969) biographical chronicle covering the reigns of Selassie's predecessors Iyasu V (r. 1913–1916), a late convert to Islam, and the Empress Zewditu (r. 1916–1930). The latter work was edited, translated into English and republished by Rudolf K. Molvaer in 1994.

Edward Ullendorff considered the German orientalist Hiob Ludolf (1624–1704) to be the founder of Ethiopian studies in Europe, thanks to his efforts in documenting the history of Ethiopia and the Ge'ez language, as well as Amharic. The Ethiopian monk Abba Gorgoryos (1595–1658), while lobbying the "Propaganda Fide" in Rome to become bishop of Ethiopia following his Catholic conversion and expulsion of the Jesuits by Ethiopian emperor Fasilides, collaborated with Ludolf – who never actually visited Ethiopia – and provided him with critical information for composing his "Historia Aethiopica" and its "Commentaries". The ethnically-Ethiopian Portuguese cleric António d'Andrade (1610–1670) aided them as a translator, since Abba Gorgoryos was not a fluent speaker of either Latin or Italian. After Ludolf, the 18th-century Scottish travel writer James Bruce, who visited Ethiopia, and German orientalist August Dillmann (1823–1894) are also considered pioneers in the field of early Ethiopian studies. After spending time at the Ethiopian royal court, Bruce was the first to systematically collect and deposit Ethiopian historical documents into libraries of Europe, in addition to composing a history of Ethiopia based on native Ethiopian sources. Dillmann cataloged a variety of Ethiopian manuscripts, including historical chronicles, and in 1865 published the "Lexicon Linguae Aethiopicae", the first such lexicon to be published on languages of Ethiopia since Ludolf's work.
Ethiopian historians such as Taddesse Tamrat (1935–2013) and Sergew Hable Sellassie have argued that modern Ethiopian studies were an invention of the 17th century and originated in Europe. Tamrat considered Carlo Conti Rossini's 1928 "Storia d'Etiopia" a groundbreaking work in Ethiopian studies. The philosopher Messay Kebede likewise acknowledged the genuine contributions of Western scholars to the understanding of Ethiopia's past. But he also criticized the perceived scientific and institutional bias that he found to be pervasive in Ethiopian-, African-, and Western-made historiographies on Ethiopia. Specifically, Kebede took umbrage at E. A. Wallis Budge's translation of the "Kebra Nagast", arguing that Budge had assigned a South Arabian origin to the Queen of Sheba although the "Kebra Nagast" itself did not indicate such a provenience for this fabled ruler. According to Kebede, a South Arabian extraction was contradicted by biblical exegetes and testimonies from ancient historians, which instead indicated that the Queen was of African origin. Additionally, he chided Budge and Ullendorff for their postulation that the Aksumite civilization was founded by Semitic immigrants from South Arabia. Kebede argued that there is little physical difference between the Semitic-speaking populations in Ethiopia and neighboring Cushitic-speaking groups to validate the notion that the former groups were essentially descendants of South Arabian settlers, with a separate ancestral origin from other local Afroasiatic-speaking populations. He also observed that these Afroasiatic-speaking populations were heterogeneous, having interbred with each other and also assimilated alien elements of both uncertain extraction and negroid origin.

During the late 19th and early 20th centuries, Ethiopian vernacular historiography became more heavily influenced by Western methods of historiography, but De Lorenzi contends that these were "indigenized" to suit the cultural sensibilities of traditionalist historians. Gabra Heywat Baykadan, a foreign-educated historian and reformist intellectual during the reign of Menelik II (r. 1889–1913), was unique among his peers for breaking almost entirely from the traditionalist approach to writing vernacular history and systematically adopting Western theoretical methods. Heruy Wolde Selassie (1878-1938), "blattengeta" and foreign minister of Ethiopia, used English scholarship and nominally adopted modern Western methods in writing vernacular history, but he was a firmly traditionalist historian. His innovative works include a 1922 historical dictionary that offered a prosopographic study of Ethiopia's historical figures and contemporary notables, a history of Ethiopian foreign relations, historiographic travel literature, and a traditionalist historical treatise combining narrative histories for the Zagwe and Solomonic dynasties with other parts on church history and biographies of church leaders.

Takla Sadeq Makuriya (1913–2000), historian and former head of the National Archives and Library of Ethiopia, wrote various works in Amharic as well as foreign languages, including a four-volume Amharic-language series on the history of Ethiopia from ancient times until the reign of Selassie, published in the 1950s. During the 1980s he published a three-volume tome exploring the reigns of 19th-century Ethiopian rulers and the theme of national unity. He also produced two English chapters on the history of the Horn of Africa for UNESCO's "General History of Africa" and several French-language works on Ethiopia's church history and royal genealogies. Some volumes from his vernacular survey on general Ethiopian history have been edited and circulated as school textbooks in Ethiopian classrooms by the Ministry of Education. Kebede Michael (1916–1998), a playwright, historian, editor, and director of archaeology at the National Library, wrote works of world history, histories of Western civilization, and histories of Ethiopia, which, unlike his previous works, formed the central focus of his 1955 world history written in Amharic.

The decisive victory of the Ethiopian Empire over the Kingdom of Italy in the 1896 Battle of Adwa, during the First Italo-Ethiopian War, made a profound impact on the historiography of Italy and Ethiopia. It was not lost to the collective memory of Italians, since the Italian capture of Adwa, Tigray Region, Ethiopia in 1935, during the Second Italo-Ethiopian War, was hailed as an act that avenged their previous humiliation and defeat. Historiography about Ethiopia throughout much of the 20th century focused primarily on this second invasion and the Abyssinian Crisis that preceded it, in which Ethiopia was depicted as being relegated to the role of a pawn in European diplomacy. The Ethiopian courtier (i.e. "blatta") and historian Marse Hazan Walda Qirqos (1899–1978) was commissioned by the Selassie regime to compile a documentary history of the Italian occupation entitled "A Short History of the Five Years of Hardship", composed concurrently with the submission of historical evidence to the United Nations War Crimes Commission for Fascist Italy's war crimes. Coauthored by Berhanu Denque, this work was one of the first vernacular Amharic histories to cover the Italian colonial period, documenting contemporary newspaper articles and propaganda pieces, events such as the 1936 fall of Addis Ababa and the 1941 British-Ethiopian reconquest of the country, and speeches by key figures such as Emperor Selassie and Rodolfo Graziani (1882–1955), Viceroy of Italian East Africa.

Modern historians have taken new approaches to analyzing both traditional and modern Ethiopian historiography. For instance, Donald Crummey (1941–2013) investigated instances in Ethiopian historiography dealing with class, ethnicity, and gender. He also criticized earlier approaches made by Sylvia Pankhurst (1882–1960) and Richard Pankhurst (1927–2017), who focused primarily on the Ethiopian ruling class while ignoring marginalized peoples and minority groups in Ethiopian historical works. Following the 1974 Ethiopian Revolution and overthrow of the Solomonic dynasty with the deposition of Haile Selassie, the historical materialism of Marxist historiography came to dominate the academic landscape and understanding of Northeast African history. In her 2001 article "Women in Ethiopian History: A Bibliographic Review", Belete Bizuneh remarks that the impact of social history on African historiography in the 20th century generated an unprecedented focus on the roles of women and gender in historical societies, but that Ethiopian historiography seems to have fallen outside the orbit of these historiographic trends.

By relying on the written works of both Christian and Muslim authors, oral traditions, and modern methods of anthropology, archaeology, and linguistics, Mohammed Hassen, Associate Professor of History at Georgia State University, asserts that the largely non-Christian Oromo people have interacted and lived among the Semitic-speaking Christian Amhara people since at least the 14th century, not the 16th century as is commonly accepted in both traditional and recent Ethiopian historiography. His work also stresses Ethiopia's need to properly integrate its Oromo population and the fact that the Cushitic-speaking Oromo, despite their traditional reputation as invaders, were significantly involved in maintaining the cultural, political, and military institutions of the Christian state.

In his 1992 review of Naguib Mahfouz's "The Search" (1964), the Ethiopian scholar Mulugeta Gudeta observed that Ethiopian and Egyptian societies bore striking historical resemblances. According to Haggai Erlich, these parallels culminated in the establishment of the Egyptian "abun" ecclesiastical office, which exemplified Ethiopia's traditional connection to Egypt and the Middle East. In the earlier part of the 20th century, Egyptian nationalists also propounded the idea of forming a "Unity of the Nile Valley", a territorial union that would include Ethiopia. This objective gradually ebbed due to political tension over control of the Nile waters. Consequently, after the 1950s, Egyptian scholars adopted a more distant if not apathetic approach to Ethiopian affairs and academic studies. For instance, the Fifth Nile 2002 Conference held in Addis Ababa in 1997 was attended by hundreds of scholars and officials, among whom were 163 Ethiopians and 16 Egyptians. By contrast, there were no Egyptian attendees at the Fourteenth International Conference of Ethiopian Studies later held in Addis Ababa in 2000, similar to all previous ICES conferences since the 1960s.

Erlich argues that, in deference to their training as Africanists, native and foreign Ethiopianists of the post-1950 generation focused more on historiographic matters pertaining to Ethiopia's place within the African continent. This trend had the effect of marginalizing Ethiopia's traditional bonds with the Middle East in historiographic works. In Bahru Zewde's retrospective on Ethiopian historiography published in 2000, he highlighted Ethiopia's ancient tradition of historiography, observing that it dates from at least the fourteenth century and distinguishes the territory from most other areas in Africa. He also noted a shift in emphasis in Ethiopian studies away from the field's traditional fixation on Ethiopia's northern Semitic-speaking groups, with an increasing focus on the territory's other Afroasiatic-speaking communities. Zewde suggested that this development was made possible by a greater critical usage of oral traditions. He offered no survey of Ethiopia's role in Middle Eastern studies and made no mention of Egyptian-Ethiopian historical relations. Zewde also observed that historiographic studies in Africa were centered on methods and schools that were primarily developed in Nigeria and Tanzania, and concluded that "the integration of Ethiopian historiography into the African mainstream, a perennial concern, is still far from being achieved to a satisfactory degree."




</doc>
<doc id="54741680" url="https://en.wikipedia.org/wiki?curid=54741680" title="The Boat Race 2018">
The Boat Race 2018

The Boat Race 2018 (also known as The Cancer Research UK Boat Race for the purposes of sponsorship) took place on 24 March 2018. Held annually, The Boat Race is a side-by-side rowing race between crews from the universities of Oxford and Cambridge along a tidal stretch of the River Thames in south-west London. For the third time in the history of the event, the men's, women's and both reserves' races were all held on the Tideway on the same day.

The women's race was the first event of the day, and saw Cambridge lead from the start, eventually winning by a considerable margin to record their second consecutive victory, and taking the overall record in the Women's Boat Race to 43–30 in their favour. The men's race was the final event of the day and completed a whitewash as Cambridge won, their second victory in three years, and taking the overall record to 83–80 in their favour. In the women's reserve race, Cambridge's "Blondie" defeated Oxford's "Osiris" by nine lengths, their third consecutive victory. The men's reserve race was won by Cambridge's "Goldie" who defeated Oxford's "Isis" by a margin of four lengths.

The races were watched by around a quarter of a million spectators live, and were broadcast around the world by a variety of broadcasters. The two main races were also available for the second time as a live stream using YouTube.

The Boat Race is a side-by-side rowing competition between the University of Oxford (sometimes referred to as the "Dark Blues") and the University of Cambridge (sometimes referred to as the "Light Blues"). First held in 1829, the race takes place on the Championship Course, between Putney and Mortlake on the River Thames in south-west London. The rivalry is a major point of honour between the two universities; it is followed throughout the United Kingdom and broadcast worldwide. Oxford went into the race as champions, having won the 2017 race by a margin of one and a quarter lengths, with Cambridge leading overall with 82 victories to Oxford's 80 (excluding the 1877 race, officially a dead heat though claimed as a victory by the Oxford crew).

It was the third time in the history of The Boat Race that all four senior races – the men's, women's, men's reserves' and women's reserves' – were held on the same day and on the same course along the Tideway. Prior to 2015, the women's race, which first took place in 1927, was usually held at the Henley Boat Races along the course. However, on at least two occasions in the interwar period, the women competed on the Thames between Chiswick and Kew. Cambridge's women went into the race as reigning champions, having won the 2017 race by 11 lengths, and led 42–30 overall.

For the sixth year, the men's race was sponsored by BNY Mellon while the women's race had BNY Mellon's subsidiary Newton Investment Management as sponsors. In January 2016, it was announced that the sponsors would be donating the title sponsorship to Cancer Research UK and that the 2016 event onwards would be retitled "The Cancer Research UK Boat Races". There is no monetary award for winning the race, as the journalist Roger Alton notes: "It's the last great amateur event: seven months of pain for no prize money".

The autumn reception was held at the Guildhall in London on 10 November 2017. As Cambridge's women had won the previous year's race, it was Oxford's responsibility to offer the traditional challenge to the Cambridge University Women's Boat Club (CUWBC). To that end, Katherine Erickson, President of Oxford University Women's Boat Club (OUWBC), challenged Daphne Martschenko, her Cambridge counterpart. Oxford's victory in the men's race meant that Hugo Ramambason, President of Cambridge University Boat Club (CUBC), challenged Iain Mandale, President of Oxford University Boat Club (OUBC).

The men's race was umpired by the former Light Blue rower John Garrett who represented Great Britain at the 1984, 1988 and 1992 Summer Olympics. He umpired men's race twice previously, in 2008 and 2012. He rowed for Lady Margaret Boat Club in The Boat Race in 1984 and 1985. The 73rd women's race was umpired by the multiple Olympic gold-medallist Matthew Pinsent. As well as rowing for Oxford in the 1990, 1991 and 1993 races, he was assistant umpire in the 2012 race before umpiring the 2013 race. The women's reserve race was presided over by former Dark Blue, Matt Smith, who rowed for Oxford in the 2001, 2002 and 2003 races. Richard Phelps, who rowed for Cambridge in the 1993, 1994 and 1995 races oversaw the men's reserve race.

The event was broadcast live in the United Kingdom on the BBC. Numerous broadcasters worldwide also showed the main races, including SuperSport across Africa and the EBU across Europe. It was also streamed live on BBC Online. For the second time, the men's and women's races were streamed live on YouTube.

The Cambridge men's crew coaching team was led by their chief coach Steve Trapmore, a gold medal-winning member of the men's eight at the 2000 Summer Olympics, who was appointed to the post in 2010. He was assisted by Richard Chambers, silver medallist in the men's lightweight coxless four at the 2012 Summer Olympics. Donald Legget, who rowed for the Light Blues in the 1963 and 1964 races acted as a supporting coach, along with coxing coach Henry Fieldman (who steered Cambridge in the 2013 race) and the medical officer Simon Owens. Sean Bowden was chief coach for Oxford, having been responsible for the senior men's crew since 1997, winning 12 from 18 races. He is a former Great Britain Olympic coach and coached the Light Blues in the 1993 and 1994 Boat Races.

OUWBC's chief coach was the former OUBC assistant coach Andy Nelder who previously worked with Bowden for eleven years. He was assisted by Jamie Kirkwood. Cambridge's women were coached by former Goldie coach Rob Baker who was assisted by Paddy Ryan.

Dates for the trials, where crews are able to simulate the race proper on the Championship Course, were announced on 17 November 2017.

Cambridge's women's trial took place on the Championship Course on 5 December, between the Harry Potter-themed boats "Expecto Patronum" and "Wingardium Leviosa". The CUWBC president Daphne Martschenko was unavailable through illness to participate in the race, which was umpired by Matthew Pinsent. "Wingardium Leviosa" took the early lead but the crews were level by Barn Elms boathouse, before "Leviosa" pulled half a length ahead by Craven Cottage. Level once again as the crews passed Harrods Furniture Depository, "Expecto Patronum" made a push at Hammersmith Bridge, handling the rough conditions better than their opponents. With a clear water advantage by Chiswick Eyot, "Expecto Patronum" passed the finish line two lengths ahead.

Oxford's trial race was conducted on 21 January 2018, delayed from December through ill health of the rowers. The race was held in windy and wet conditions on the Tideway between "Great Typhoon" and "Coursing River" umpired by Pinsent. "Coursing River" made the better start from the Surrey station before "Great Typhoon" drew level, before taking advantage of the curve of the river and pulling ahead. An oar clash followed but a series of pushes from "Great Typhoon" saw them take the lead and push away under Barnes Bridge, to win by half a length.

Cambridge's men's trial took place on the Championship Course on 5 December, between the boats "Goblins" and "Goons". "Goblins", starting from the Surrey station, took an early lead which they held until "Goons" drew level, and then began to pull away, as the crews passed below Hammersmith Bridge. "Goblins" responded, restored parity and then took the lead at the Bandstand. Despite each crew making a series of pushes, "Goblins" held a half-length lead under Barnes Bridge and maintained the advantage to the finish line.

The Oxford trial boats were named "Strong" and "Stable", in reference to the Tory manifesto for the 2017 general election. They raced against one another along the Championship Course on 6 December 2017, umpired by John Garrett. "Strong", starting from the Middlesex station, took an early lead and held a half-length advantage by the time the crews passed Craven Cottage. Garrett repeatedly warned both crews as they each infringed the racing line, and "Strong" capitalised on the advantage of the bend to be almost a length ahead. "Stable" fought back and were nearly level by the time they passed Harrods. "Strong" reacted to pull half a length ahead by Chiswick Eyot, extending to clear water by the Bandstand, and a final push at Barnes Bridge ensured them a two-length victory.

CUWBC faced a crew from University of London Boat Club (ULBC) in two races on the Tideway umpired by Judith Packer on 17 February 2018. The first segment, from Putney Bridge to Hammersmith Bridge, was an easy victory for the Light Blues, winning by around five lengths. The second segment, from Chiswick Steps to the finish line, saw Cambridge quickly overcome their starting one-length deficit to take a clear water advantage under Barnes Bridge before ending as "clear winners".

OUWBC went up against Oxford Brookes University Boat Club (OBUBC) in a two-piece race on the Championship Course on 24 February 2018. Despite a strong start from OBUBC in the first segment, OUWBC held a lead of around a length by Craven Cottage and continued to pull away to a three-length victory at Chiswick Eyot. In the second segment, OUWBC took a slight early lead but OBUBC remained in contention, taking advantage of Middlesex bend, but could not catch the Dark Blues who passed the finish line with a lead of a couple of seats.

On 4 March 2018, OUWBC took on a crew from Molesey Boat Club in a race along a section of the Championship Course from the start to Chiswick Steps, umpired by Sarah Winckless. The Dark Blues held a three-seat advantage by the time the crews had passed the boathouses, and despite under-rating Molesey, continued to pull away to hold a clear water advantage and a three-length lead by Hammersmith Bridge which they extended to a five-length lead by Chiswick Steps.

CUBC faced a ULBC crew in a three-piece race along the Tideway umpired by Rob Clegg on 18 February 2018. The first section of the race was strongly contested with clashes in the early stages, with ULBC taking the lead, only for the Light Blues to draw level and then lead past Craven Cottage. The race concluded as Cambridge passed Harrods with a three length lead. The second segment from Harrods to the Bandstand saw Cambridge lead all the way, to win by several lengths. The final section of the race from Chiswick Eyot to the finish line, saw further oar clashes, but Cambridge controlled the situation, winning by more than two lengths.

On 4 March 2018, CUBC took part in a two-piece race against OBUBC. The first section, from the start line to Chiswick Steps, was won by one length by CUBC who led from the start. The second race, from Chiswick Eyot to the finishing line, was more robustly contested. CUBC took an early lead in difficult conditions, only to be overhauled by OBUBC who took a lead of a length from Barnes Bridge to the finish.

OUBC took on an OBUBC crew in two stages along the Championship Course on 24 February 2018, umpired by John Garrett. OBUBC made the better start in the first section, but OUBC drew level at the Town Buoy. Oxford Brookes started to pull away and held a length's advantage as the crews passed the Mile Post and into the headwind. OUBC coped with the conditions well and were just ahead by Hammersmith Bridge but Oxford Brookes took advantage of the stream to win by a length as the crews passed St Paul's School. The second section of the race saw early clashes from which OBUBC emerged with a length advantage. Although OUBC pushed to close the gap, OBUBC responded and pulled away to win by just over one length.

On 3 March 2018, OUBC faced a ULBC crew along a section of the Championship Course from the start line to Chiswick Steps, in a race umpired by Richard Phelps. Starting from the Middlesex station, the Dark Blues took an early lead and held a length's advantage by the time the crews passed Harrods. In an early attempt to claim the racing line, OUBC moved into ULBC's water and both crews were warned by Clegg to return to their station. Despite this, OUBC extended their lead and were several lengths clear by Hammersmith Bridge and were able to take advantage of clear water, winning the race by at least four lengths.

The official weigh-in for the crews took place at City Hall, London, on 26 February 2018.
The Cambridge crew weighed an average of , per rower more than their opponents. The Light Blues averaged in height, more than Oxford. The Dark Blues featured one returning crew member, number four Alice Roberts, who rowed in the unsuccessful 2017 Oxford boat. The Cambridge crew included some experienced Boat Race rowers: Thea Zabell, Imogen Grant, Alice White and Myriam Goudet-Boukhatmi all rowed in the previous year's race. The Light Blues also featured the 2015 World Rowing Championships quad sculls gold medallist Olivia Coffey.
The Cambridge crew weighed an average of , per rower more than their opponents, and averaged in height, taller than Oxford. The Light Blue's number four, James Letten, at , was the tallest individual ever to have competed in The Boat Race. One member of the Oxford crew has previous Boat Race experience: stroke Vassilis Ragoussis featured in the successful 2017 Dark Blue boat. On 20 March 2018, it was announced that as a result of illness, number six Joshua Bugajski would withdraw from the race and be replaced by Isis rower Benedict Aldous. It was later revealed that Bugajski's departure was also related to disagreements with the Dark Blue coach Sean Bowden. Claas Mertens, the Oxford bow man, won gold at the 2015 World Rowing Championships with the German lightweight men's eight. Cambridge's crew contains four individuals who have featured in the Boat Race: Hugo Ramambason, Freddie Davidson and Letten participated in 2017, while Charles Fisher rowed in the 2016 race.

Two days before the race, both the Blues boats and the reserve boats practised their starts from the stakeboats on the Championship Course. On the same day, two BBC television cameras located on Putney Bridge and Barnes Bridge were targeted by thieves; their attempts at Putney were thwarted by an off-duty policeman and a Royal National Lifeboat Institution crew, but the gang escaped with one of the cameras on Barnes Bridge. 

Cambridge were pre-race favourites to win both the men's and women's senior races.

The Queen's barge "Gloriana" led a procession of traditional craft along the course. These included the waterman's cutters used for the Oxbridge Waterman's Challenge.

The races were held on 24 March 2018. Weather was overcast with light winds.

"Blondie" won the women's reserve boat race which was held after the conclusion of the Women's Boat Race by nine lengths in a time of 19 minutes 45 seconds. Already six seconds ahead at the Mile Post, "Blondie" continued to pull away to be twelve seconds ahead by Hammersmith Bridge before passing the finishing post in 19 minutes 45 seconds, 27 seconds ahead of "Osiris". It was "Blondies" third consecutive victory, and took the overall tally (since 1968) to 24–20 in Cambridge's favour. 

"Goldie" won the men's reserve race, which was held after the women's reserve race and before the men's race. Five seconds ahead at the Mile Post, the Light Blue reserves were warned after a clash of oars, and "Isis" reduced the gap to three seconds by Hammersmith Bridge. "Goldie" were clear of "Isis" by Barnes Bridge with a seven-second lead, and maintained that advantage as they crossed the finish line in a time of 18 minutes 12 seconds. It was "Goldies" first victory since 2010 and took the overall tally in the event to 30–24 in their favour.

The women's race started at 4:31 p.m. Greenwich Mean Time (GMT). CUWBC won the toss and elected to start from the Surrey side of the river, handing the Middlesex side to Oxford. Cambridge made the better start taking an early lead and were around a half of a length ahead after the first minute of the race. By Craven Cottage, and in spite of Oxford having the advantage of the bend in the river, the Light Blues were ahead by a length. At the Mile Post, Cambridge held a clear water advantage, two lengths ahead. The Light Blues passed under Hammersmith Bridge with a three-length lead. At Chiswick Steps, Oxford were fifteen seconds behind, and a further five down at Barnes Bridge. Cambridge passed the finishing post in a time of 19 minutes 6 seconds, around seven lengths ahead of Oxford. It was Cambridge's second consecutive victory but only their third win in eleven years, and took the overall record in the event to 43–30 in their favour.

The men's race started at 5.33 pm. GMT in very "gloomy conditions". The Light Blues won the toss and elected to start from the Surrey side of the river. Oxford made the better start and were quickly a canvas ahead, but Cambridge restored parity within 40 seconds, going on to take a third of a length lead themselves. Cambridge received several warnings from the umpire John Garrett for encroaching into Oxford's water, forcing them to move back towards their station, but were still over a length ahead by Craven Cottage. The Light Blues passed the Mile Post five seconds ahead, and shot Hammersmith Bridge with a lead of four lengths. They maintained their lead of 12 seconds as they passed Chiswick Steps. The Dark Blues reduced the Cambridge to eleven seconds by Barnes Bridge, but Cambridge passed the finishing line in 17 minutes 51 seconds, three lengths ahead of Oxford. It was Cambridge's second victory in the last three years, and took the overall record in the event to 83–80 in their favour.

CUWBC's cox Sophie Shapter said "We just knew we had to go out there and do a job" while OUWBC's president Katherine Erickson explained that she was proud of her crew, many of whom had learnt to row at Oxford. James Letten remarked that his Cambridge crew were "on the money" and had "stepped up and delivered". In CUBC's Steve Trapmore's final Boat Race before moving to Team GB Olympic Rowing as a high performance coach, he admitted that "the boys really stepped up and delivered".

As the men's senior crews passed below Hammersmith Bridge, a banner was unfurled by the Cambridge Zero Carbon Society and smoke flares were let off, to protest against investment in fossil fuel companies by the two universities. Although the banner was not clear to viewers during the live race coverage on the BBC, commentator Andrew Cotter remarked "flares at the boat race, whatever next?"


</doc>
<doc id="55125362" url="https://en.wikipedia.org/wiki?curid=55125362" title="Ice drilling">
Ice drilling

Ice drilling allows scientists studying glaciers and ice sheets to gain access to what is beneath the ice, to take measurements along the interior of the ice, and to retrieve samples. Instruments can be placed in the drilled holes to record temperature, pressure, speed, direction of movement, and for other scientific research, such as neutrino detection.

Many different methods have been used since 1840, when the first scientific ice drilling expedition attempted to drill through the Unteraargletscher in the Alps. Two early methods were percussion, in which the ice is fractured and pulverized, and rotary drilling, a method often used in mineral exploration for rock drilling. In the 1940s, thermal drills began to be used; these drills melt the ice by heating the drill. Drills that use jets of hot water or steam to bore through ice soon followed. A growing interest in ice cores, used for palaeoclimatological research, led to ice coring drills being developed in the 1950s and 1960s, and there are now many different coring drills in use. For obtaining ice cores from deep holes, most investigators use cable-suspended electromechanical drills, which use an armoured cable to carry electrical power to a mechanical drill at the bottom of the borehole.

In 1966, a US team successfully drilled through the Greenland ice sheet at Camp Century, at a depth of . Since then many other groups have succeeded in reaching bedrock through the two largest ice sheets, in Greenland and Antarctica. Recent projects have focused on finding drilling locations that will give scientists access to very old undisturbed ice at the bottom of the borehole, since an undisturbed stratigraphic sequence is required to accurately date the information obtained from the ice.

The first scientific ice drilling expeditions, led by Louis Agassiz from 1840 to 1842, had three goals: to prove that glaciers flowed, to measure the internal temperature of a glacier at different depths, and to measure the thickness of a glacier. Proof of glacier motion was achieved by placing stakes in holes drilled in a glacier and tracking their motion from the surrounding mountain. Drilling through glaciers to determine their thickness, and to test theories of glacier motion and structure, continued to be of interest for some time, but glacier thickness has been measured by seismographic techniques since the 1920s. Although it is no longer necessary to drill through a glacier to determine its thickness, scientists still drill shot holes in ice for these seismic studies. Temperature measurements continue to this day: modelling the behaviour of glaciers requires an understanding of their internal temperature, and in ice sheets, the borehole temperature at different depths can provide information about past climates. Other instruments may be lowered into the borehole, such as piezometers, to measure pressure within the ice, or cameras, to allow a visual review of the stratigraphy. IceCube, a large astrophysical project, required numerous optical sensors to be placed in holes 2.5 km deep, drilled at the South Pole.

Borehole inclination, and the change in inclination over time, can be measured in a cased hole, a hole in which a hollow pipe has been placed as a "liner" to keep the hole open. This allows the three-dimensional position of the borehole to be mapped periodically, revealing the movement of the glacier, not only at the surface, but throughout its thickness. To understand whether a glacier is shrinking or growing, its mass balance must be measured: this is the net effect of gains from fresh snow, minus losses from melting and sublimation. A straightforward way to determine these effects across the surface of a glacier is to plant stakes (known as ablation stakes) in holes drilled in the glacier's surface, and monitor them over time to see if more snow is accumulating, burying the stake, or if more and more of the stake is visible as the snow around it disappears. The discovery of layers of aqueous water, and of more than a hundred subglacial lakes, beneath the Antarctic ice sheet, led to speculation about the existence of unique microbial environments that had been isolated from the rest of the biosphere, potentially for millions of years. These environments can be investigated by drilling.

Ice cores are one of the most important motivations for drilling in ice. Since ice cores retain environmental information about the time the ice in them fell as snow, they are useful in reconstructing past climates, and ice core analysis includes studies of isotopic composition, mechanical properties, dissolved impurities and dust, trapped atmospheric samples, and trace radionuclides. Data from ice cores can be used to determine past variations in solar activity, and is important in the construction of marine isotope stages, one of the key palaeoclimatic dating tools. Ice cores can also provide information about glacier flow and accumulation rates. IPICS (International Partnership in Ice Core Sciences) maintains a list of key goals for ice core research. Currently these are to obtain a 1.5 million year old core; obtain a complete record of the last interglacial period; use ice cores to assist with the understanding of climate change over long time scales; obtain a detailed spatial array of ice core climate data for the last 2,000 years; and continue the development of advanced ice core drilling technology.

The constraints on ice drill designs can be divided into the following broad categories.

The ice must be cut through, broken up, or melted. Tools can be directly pushed into snow and firn (snow that is compressed, but not yet turned to ice, which typically happens at a depth of to ); this method is not effective in ice, but it is perfectly adequate for obtaining samples from the uppermost layers. For ice, two options are percussion drilling and rotary drilling. Percussion drilling uses a sharp tool such as a chisel, which strikes the ice to fracture and fragment it. More common are rotary cutting tools, which have a rotating blade or set of blades at the bottom of the borehole to cut away the ice. For small tools the rotation can be provided by hand, using a T-handle or a carpenter's brace. Some tools can also be set up to make use of ordinary household power drills, or they may include a motor to drive the rotation. If the torque is supplied from the surface, then the entire drill string must be rigid so that it can be rotated; but it is also possible to place a motor just above the bottom of the drill string, and have it supply power directly to the drill bit.

If the ice is to be melted instead of cut, then heat must be generated. An electrical heater built into the drill string can heat the ice directly, or it can heat the material it is embedded in, which in turn heats the ice. Heat can also be sent down the drill string; hot water or steam pumped down from the surface can be used to heat a metal drillhead, or the water or steam can be allowed to emerge from the drillhead and melt the ice directly. In at least one case a drilling project experimented with heating the drillhead on the surface, and then lowering it into the hole.

Many ice drilling locations are very difficult to access, and drills must be designed so that they can be transported to the drill site. The equipment should be as light and portable as possible. It is helpful if the equipment can be broken down so that the individual components can be carried separately, thus reducing the burden for hand-carrying, if required. Fuel, for steam or hot water drills, or for a generator to provide power, must also be transported, and this weight has to be taken into account as well.

Mechanical drilling produces pieces of ice, either as cuttings, or as granular fragments, which must be removed from the bottom of the hole to prevent them from interfering with the cutting or percussing action of the drill. An auger used as the cutting tool will naturally move ice cuttings up its helical flights. If the drill's action leaves the ice chips on top of the drill, they can be removed by simply raising the drill to the surface periodically. If not, they can be brought to the surface by lowering a tool to scoop them up, or the hole can be kept full of water, in which case the cuttings will naturally float to the top of the hole. If the chips are not removed, they must be compacted into the walls of the borehole, and into the core if a core is being retrieved.

Cuttings can also be moved to the surface by circulating compressed air through the hole, either by pumping the air through the drillpipe and out at the drillhead, forcing the chips up in the space between the drill string and the borehole wall, or by reverse air circulation, in which the air flows up through the drill string. Compressed air will be heated by the compression, and it must be cooled before being pumped downhole, or it will cause melting of the borehole walls and the core. If the air is circulated by creating a vacuum, rather than pumping air in, ambient air carries the cuttings, so no cooling is needed.

A fluid can be used to circulate the cuttings away from the bit, or the fluid may be able to dissolve the cuttings. Rotary mineral drilling (through rock) typically circulates fluid through the entire hole, and separates solids from the fluid at the surface before pumping the fluid back down. In deep ice drilling it is usual to circulate the fluid only at the bottom of the hole, collecting cuttings in a chamber that is part of the downhole assembly. For a coring drill, the cuttings chamber can be emptied each time the drill is brought to the surface to retrieve a core.

Thermal drills will produce water, so there are no cuttings to dispose of, but the drill must be capable of working while submerged in water, or else the drill must have a method of removing and storing the meltwater while drilling.

The drilling mechanism must be connected to the surface, and there must be a method of raising and lowering the drill. If the drill string consists of pipes or rods that have to be screwed together, or otherwise assembled, as the hole gets deeper and the drill string lengthens, then there must be a way to hold the drill string in place as each length of rod or pipe is added or removed. If the hole is only a few metres deep, no mechanical assistance may be necessary, but drill strings can get very heavy for deep holes, and a winch or other hoisting system must be in place that is capable of lifting and lowering it.

A "trip" in drilling refers to the task of pulling a drill string completely out of the hole (tripping out) and then reinserting it back into the hole (tripping in). Tripping time is the time taken to trip in and out of the hole; it is important for a drill design to minimize tripping time, particularly for coring drills, since they must complete a trip for each core.

The overburden pressure in a deep hole from the weight of the ice above will cause a borehole to slowly close up, unless something is done to counteract it, so deep holes are filled with a drilling fluid that is about the same density as the surrounding ice, such as jet fuel or kerosene. The fluid must have low viscosity to reduce tripping time. Since retrieval of each segment of core requires a trip, a slower speed of travel through the drilling fluid could add significant time to a project—a year or more for a deep hole. The fluid must contaminate the ice as little as possible; it must have low toxicity, for safety and to minimize the effect on the environment; it must be available at a reasonable cost; and it must be relatively easy to transport. The depth at which borehole closure prevents dry drilling is strongly dependent on the temperature of the ice; in a temperate glacier, the maximum depth might be , but in a very cold environment such as parts of East Antarctica, dry drilling to might be possible.

Snow and firn are permeable to air, water, and drilling fluids, so any drilling method that requires liquid or compressed air in the hole needs to prevent them from escaping into the surface layers of snow and firn. If the fluid is only used in the lower part of the hole, permeability is not an issue. Alternatively the hole can be cased down past the point where the firn turns to ice. If water is used as a drilling fluid, in cold enough temperatures, it will turn to ice in the surrounding snow and firn and seal the hole.

Tools can be designed to be rotated by hand, via a brace or T-handle, or a hand crank gearing, or attached to a hand drill. Drills with powered rotation require an electrical motor at the rig site, which generally must have fuel, though in at least one case a drilling project was set up near enough to a permanent research station to run a cable to the research building for power. The rotation can be applied at the surface, by a rotary table, using a kelly, or by a motor in the drillhead, for cable-suspended drills; in the latter case the cable must carry power to the drillhead as well as support its weight. For rotary drills, gearing is required to reduce the engine's rotation to a suitable speed for drilling.

If torque is supplied at the bottom of the hole, the motor supplying it to the drillbit beneath it will have a tendency to rotate around its own axis, rather than imparting the rotation to the drillbit. This is because the drillbit will have a strong resistance to rotation since it is cutting ice. To prevent this, an anti-torque mechanism of some kind must be provided, typically by giving the motor some grip against the walls of the borehole.

A thermal drill that uses electricity to heat the drill head so that it melts the ice must bring power down the hole, just as with rotary drills. If the drillhead is heated by pumping water or steam down to the bottom of the hole, then no downhole power is needed, but a pump at the surface is required for hot water. The water or steam can be heated at the surface by a fuel-powered boiler. Solar power can also be used.

Some drills which are designed to rest on their tip as they drill will lean to one side in the borehole, and the hole they drill will gradually drift towards the horizontal unless some method of counteracting this tendency is provided. For other drills, directional control can be useful in starting additional holes at depth, for example to retrieve additional ice cores.

Many glaciers are temperate, meaning that they contain "warm ice": ice that is at melting temperature (0 °C) throughout. Meltwater in boreholes in warm ice will not refreeze, but for colder ice, meltwater is likely to cause a problem, and may freeze the drill in place, so thermal drills that operate submerged in the meltwater they produce, and any drilling method that results in water in the borehole, are difficult to use in such conditions. Drilling fluids, or antifreeze additives to meltwater, must be chosen to keep the fluid liquid at the temperatures found in the borehole. In warm ice, ice tends to form on cutters and the drillhead, and to pack into spaces at the bottom of the hole, slowing down drilling.

To retrieve a core, an annulus of ice must be removed from around the cylindrical core. The core should be unbroken, which means that vibrations and mechanical shocks must be kept to a minimum, and changes in temperature which could cause thermal shock to the core must also be avoided. The core must be kept from melting caused by heat generated either mechanically from the drilling process, from the heat of compressed air if air is used as the drilling fluid, or from a thermal drill, and must not be contaminated by the drilling fluid. When the core is about to be retrieved, it is still connected to the ice beneath it, so some method of breaking it at the lower end must be provided, and of gripping it so it does not fall from the core barrel as it is brought to the surface, which must be done as quickly and safely as possible.

Most coring drills are designed to retrieve cores that are no longer than , so drilling must stop each time the hole depth is extended by that amount, so that the core can be retrieved. A drill string that must be assembled and disassembled in segments, such as pipe sections that must be screwed together, takes a long time to trip in and out; a cable which can be continuously winched up, or a drill string that is flexible enough to be coiled, significantly reduces tripping time. Wireline drills have a mechanism that allows the core barrel to be detached from the drill head and winched directly to the surface without having to trip out the drill string. Once the core is removed, the core barrel is lowered to the bottom of the hole and reattached to the drill.

Over a depth range known as the brittle ice zone, bubbles of air are trapped in the ice under great pressure. When a core is brought to the surface, the bubbles can exert a stress that exceeds the tensile strength of the ice, resulting in cracks and spall. At greater depths, the ice crystal structure changes from hexagonal to cubic, and the air molecules move inside the crystals, in a structure called a clathrate. The bubbles disappear, and the ice becomes stable again.

The brittle ice zone typically returns poorer quality samples than for the rest of the core. Some steps can be taken to alleviate the problem. Liners can be placed inside the drill barrel to enclose the core before it is brought to the surface, but this makes it difficult to clean off the drilling fluid. In mineral drilling, special machinery can bring core samples to the surface at bottom-hole pressure, but this is too expensive for the inaccessible locations of most drilling sites. Keeping the processing facilities at very low temperatures limits thermal shocks. Cores are most brittle at the surface, so another approach is to break them into 1 m lengths in the hole. Extruding the core from the drill barrel into a net helps keep it together if it shatters. Brittle cores are also often allowed to rest in storage at the drill site for some time, up to a full year between drilling seasons, to let the ice gradually relax. Core quality in the brittle ice zone is much improved when a drilling fluid is used, as opposed to dry hole drilling.

A percussion drill penetrates ice by repeatedly striking it to fracture and fragment it. The cutting tool is mounted at the bottom of the drill string (typically connected metal rods), and some means of giving it kinetic energy must be provided. A tripod erected over the hole allows a pulley to be set up, and a cable can then be used to repeatedly raise and drop the tool. This method is known as cable tool drilling. A weight repeatedly dropped on to a rigid drill string can also be used to provide the necessary impetus. The pulverized ice collects at the bottom of the borehole, and must be removed. It can be collected with a tool capable of scooping it from the bottom of the hole, or the hole can be kept full of water, so that the ice floats to the top of the hole, though this retards the momentum of the drill striking the ice, reducing its effectiveness. A percussion drilling tool that is not mechanically driven requires some method of raising the drill so it can be released, to fall on the ice. To do this efficiently with manual labour, it is usual to set up a tripod or other supporting scaffold, and a pulley to allow the drill string to be raised by a rope. This arrangement, known as a cable-tool rig, can also be used for mechanical drilling, with a motor raising the drill string and allowing it to fall. An alternative approach is to leave the drill string at the bottom of the borehole, and to raise and let fall a hammer weight onto the drill string.

The earliest scientific ice drilling expedition used percussion drilling; Louis Agassiz used iron rods to drill holes in the Unteraargletscher, in the Alps, in the summer of 1840. Cable-tool rigs have been used for ice drilling in more recent times; Soviet expeditions in the 1960s drilled with cable-tool rigs in the Caucasus and the Tien Shan range, and US projects have drilled on the Blue Glacier in Washington between 1969 and 1976, and on the Black Rapids Glacier in Alaska in 2002.

Two other percussion methods have been tried. Pneumatic drills have been used to drill shallow holes in ice in order to set blast charges, and rotary percussion drills, a type of drilling tool once in common use in the mining industry, have also been used for drilling blasting holes, but neither approach has been used for scientific investigations of ice. Percussion drilling is now rarely used for scientific ice drilling, having been overtaken by more effective techniques for both ice and mineral drilling.

A soil sampling auger contains a pair of blades at the bottom of an enclosed cylinder; it can be driven and rotated by hand to pick up soft soil. A similar design, called a spoon-borer, has been used for ice drilling, though it is not effective in hard ice. A version used by Erich von Drygalski in 1902 had two half-moon cutting blades set into the base of the cylinder in such a way as to allow the ice cuttings to accumulate in the cylinder, above the blades.

Augers have long been used for drilling through ice for ice fishing. Augers can be rotated by hand, using a mechanism such as a T handle or a brace bit, or by attaching them to powered hand drills. Scientific uses for non-coring augers include sensor installation and determining ice thickness. Augers have a helical screw blade around the main drilling axis; this blade, called the "flighting", carries the ice cuttings up from the bottom of the hole. For drilling deeper holes, extensions can be added to the auger, but as the auger gets longer it becomes more difficult to rotate. With a platform such as a stepladder, a longer auger can be rotated from higher off the ground.

Commercially available ice augers for winter fishing, powered by petrol, propane, or battery power, are available for hole diameters from 4.5 in to 10 in. For holes deeper than 2 m a tripod can be used to winch the auger from the hole. A folding brace handle with an offset design is common; this allows both hands to contribute to the torque.

Augers that are capable of retrieving ice cores are similar to noncoring augers, except that the flights are set around a hollow core barrel. Augers have been devised that consist of the helical cutting blades and a space for a core, without the central supporting cylinder, but they are difficult to make sufficiently rigid. Coring augers typically produce cores with diameters in the range 75–100 mm, and with lengths up to 1 m. Coring augers were originally designed to be manually rotated, but over time they have been adapted for use with handheld drills or small engines.

As with noncoring augers, extensions can be added to drill deeper. Drilling deeper than 6 m requires more than one person because of the weight of the drill string. A clamp placed at the surface is useful for supporting the string, and a tripod and block and tackle can also be used for support and to increase the weight of string that can be handled. As the drill string gets longer, it takes more time to complete a trip to extract a core, since each extension rod must be separated from the drill string when tripping out, and re-attached when tripping in.

Drilling with a tripod or other method of handling a long drill string considerably extends the depth limit for the use of a coring auger. The deepest hole drilled by hand with an auger was 55 m, in the Ward Hunt Ice Shelf on Ellesmere Island, in 1960. Usually a hole deeper than 30 m will be drilled with other methods, because of the weight of the drill string and the long trip time required.

Modern coring augers have changed little in decades: an ice coring auger patented in the US in 1932 closely resembles coring augers in use eighty years later. The US military's Frost Effects Laboratory (FEL) developed an ice mechanics testing kit that included a coring auger in the late 1940s; the Snow, Ice and Permafrost Research Establishment (SIPRE), a successor organization, refined the design in the early 1950s, and the resulting auger, known as the SIPRE auger, is still in wide use. It was modified slightly by the Cold Regions Research and Engineering Laboratory (CRREL), another successor organization, in the 1960s, and is sometimes known as the CRREL auger for that reason. An auger developed in the 1970s by the Polar Ice Core Office (PICO), then based in Lincoln, Nebraska, is also still widely used. A coring auger designed at the University of Copenhagen in the 1980s was used for the first time at Camp Century, and since then has been frequently used in Greenland. In 2009, the US Ice Drilling Design and Operations group (IDDO) began work on an improved hand auger design and a version was successfully tested in the field during the 2012–2013 field season at WAIS Divide. As of 2017 IDDO maintains both 3-inch and 4-inch diameter versions of the new auger for the use of US ice drilling research programs, and these are now the most-requested hand auger provided by IDDO.

The Prairie Dog auger, designed in 2007, adds an outer barrel to the basic coring auger design. Cuttings are captured between the auger flights and the outer barrel, which has an anti-torque section to prevent it from rotating in the hole. The goal of the outer barrel is to increase the efficiency of chip collection, since it is common to see chips from a hand auger run fall back into the hole from the auger flights, which means the next run has to redrill through these cuttings. The outer barrel also makes the auger effective in warm ice, which could easily cause an auger with no outer barrel to jam. The outside barrel of the Prairie Dog is the same as the diameter of the PICO auger, and since the Prairie Dog's anti-torque blades do not perform well in soft snow and firn, it is common to start a hole with the PICO auger and then continue it with the Prairie Dog once dense firn is reached. The Prairie Dog is relatively heavy, and can require two drillers to handle it as it is being removed from the hole. The IDDO maintains a Prairie Dog drill for the use of US ice drilling research programs.

IDDO also provides a lifting system for use with hand augers, known as the Sidewinder. It is driven by an electric hand drill, which can be powered by a generator or by solar cells. The Sidewinder winds a rope around the hand auger as it is lowered into the hole, and assists in raising the auger back out of the hole. This extends the maximum practical depth for hand augering to about 40 m. Sidewinders have proved popular with researchers.

A piston drill consists of a flat disc at the bottom of a long rod, with three or four radial slots in the disc, each of which has a cutting edge. The rod is rotated by hand, using a brace handle; the ice comes through the slots and piles up on top of the disc. Pulling the drill out of the borehole brings the cuttings up on the disc. In the 1940s some patents for piston drill designs were filed in Sweden and the U.S., but these drills are now rarely used. They are less efficient than auger drills, since the drill must be periodically removed from the hole to get rid of the cuttings.

Some hand drills have been designed to retrieve cores without using auger flights to transport the cuttings up the hole. These drills typically have a core barrel with teeth at the lower end, and are rotated by a brace or T-handle, or by a small engine. The barrel itself can be omitted, so that the drill consists only of a ring with a cutting slot to cut the annulus around the core, and a vertical rod to attach the ring to the surface. A couple of small hand-held drills, or mini drills, have been designed to quickly collect core samples up to 50 cm long. A difficulty with all these designs is that as soon as cuttings are generated, if they are not removed they will interfere with the cutting action of the drill, making these tools slow and inefficient in use. A very small drill, known as the Chipmunk Drill, was designed by IDDO for use by a project in West Greenland in 2003 and 2004, and was subsequently used at the South Pole in 2013.

Rotary rigs used in mineral drilling use a string of drillpipe connected to a drillbit at the bottom of the hole, and to a rotary mechanism at the top of the hole, such as a top drive or rotary table and kelly. As the borehole deepens, drilling is paused periodically to add a new length of drill pipe at the top of the drill string. These projects have usually been undertaken with commercially available rotary rigs originally designed for mineral drilling, with adaptations to suit the special needs of ice drilling.

When drilling in ice, the hole may be drilled dry, with no mechanism to dispose of the cuttings. In snow and firn this means that the cuttings simply compact into the walls of the borehole; and in coring drills they also compact into the core. In ice, the cuttings accumulate in the space between the drillpipe and the borehole wall, and eventually start to clog the drill bit, usually after no more than 1 m of progress. This increases the torque needed to drill, slows down progress, and can cause the loss of the drill. Dry core drilling generally produces a poor quality core that is broken into pieces.

In 1950, the French Expédition Polaires Françaises (EPF) drilled two dry holes in Greenland using a rotary rig, at Camp VI, on the west coast, and Station Centrale, inland, reaching 126 m and 151 m. Some shallow holes were also drilled that summer on Baffin Island, using a coring drill, and in the Antarctic, the Norwegian–British–Swedish Antarctic Expedition (NBSAE) drilled several holes between April 1950 and the following year, eventually reaching 100 m in one hole. The last expedition to try dry drilling in ice was the 2nd Soviet Antarctic Expedition (SAE), which drilled three holes between July 1957 and January 1958. Since that time dry drilling has been abandoned as other drilling methods have proved to be more effective.

Several holes have been drilled in ice using direct air circulation, in which compressed air is pumped down the drillpipe, to escape through holes in the drillbit, and return up the annular space between the drillbit and the borehole, carrying the cuttings with it. The technique was first tried by the 1st Soviet Antarctic Expedition, in October 1956. There were problems with poor cuttings removal, and ice forming in the borehole, but the drill succeeded in reaching a depth of 86.5 m. Further attempts were made to use air circulation with rotary rigs by US, Soviet and Belgian expeditions, with a maximum hole depth of 411 m reached by a US team at Site 2 in Greenland in 1957. The last time a project used a conventional rotary rig with air circulation was 1961.

In mineral exploration, the most common drilling method is a rotary rig with fluid circulated down the drillpipe and back up between the drillpipe and the borehole wall. The fluid carries the cuttings to the surface, where the cuttings are removed, and the recycled fluid, known as mud, is returned to the hole. The first ice drilling project to try this approach was an American Geographical Society expedition to the Taku Glacier in 1950. Fresh water, drawn from the glacier, was used as the drilling fluid, and three holes were drilled, to a maximum depth of 89 m. Cores were retrieved, but in poor condition. Seawater has also been tried as a drilling fluid. The first time a fluid other than water was used with a conventional rotary rig was in late 1958, at Little America V, where diesel fuel was used for the last few metres of a 254 m hole.

A wireline drill uses air or fluid circulation, but also has a tool that can be lowered into the drillpipe to retrieve a core without removing the drill string. The tool, called an overshot, latches onto the core barrel and pulls it up to the surface. When the core is removed, the core barrel is lowered back into the borehole and reattached to the drill. A wireline core drilling project was planned in the 1970s for the International Antarctic Glaciological Project, but was never completed, and the first wireline ice drilling project took place in 1976, as part of the Ross Ice Shelf Project (RISP). A hole was started in November of that year with a wireline drill, probably using air circulation, but problems with the overshot forced the project to switch to thermal drilling when the hole was 103 m deep. The RISP project reached over 170 m with another wireline drill the following season, and several 1980s Soviet expedition also used wireline drills, after starting the holes with an auger drill and casing the holes. The Agile Sub-Ice Geological (ASIG) drill, designed by IDDO to collect sub-glacial cores, is a recent wireline system; it was first used in the field in the 2016–2017 season, in West Antarctica.

There are many disadvantages to using conventional rotary rigs for ice drilling. When a conventional rotary rig is used for coring, the entire drill string must be hoisted out of the borehole each time the core is retrieved; each length of pipe in turn must be unscrewed and racked. As the hole gets deeper, this becomes very time-consuming. Conventional rigs are very heavy, and since many ice drilling sites are not easily accessible these rigs place a large logistical burden on an ice drilling project. For deep holes, a drilling fluid is required to maintain pressure in the borehole and prevent the hole from closing up because of the pressure the ice is under; a drilling fluid requires additional heavy equipment to circulate and store the fluid, and to separate the circulated material. Any circulation system also requires the upper part of the hole, through the snow and firn, to be cased, since circulated air or fluid would escape through anything more permeable than ice. Commercial rotary rigs are not designed for extremely cold temperatures, and in addition to problems with components such as the hydraulics and fluid management systems, they are designed to operate outdoors, which is impractical in extreme environments such as Antarctic drilling.

Commercial rotary rigs can be effective for large-diameter holes, and can also be used for subglacial drilling into rock. They have also been used with some success for rock glaciers, which are challenging to drill because they contain a heterogeneous mixture of ice and rock.

Flexible drillstem rigs use a drill string that is continuous, so that it does not have to be assembled or disassembled, rod by rod or pipe by pipe, when tripping in or out. The drill string is also flexible, so that when out of the borehole it can be stored on a reel. The drill string may be a reinforced hose, or it may be steel or composite pipe, in which case it is known as a coiled-tubing drill. Rigs designed along these lines began to appear in the 1960s and 1970s in mineral drilling, and became commercially viable in the 1990s.

Only one such rig, the rapid air movement (RAM) system developed at the University of Wisconsin-Madison by Ice Coring and Drilling Services (ICDS), has been used for ice drilling. The RAM drill was developed in the early 2000s, and was originally designed for drilling shot holes for seismic exploration. The drill stem is a hose through which air is pumped; the air drives a turbine that powers a downhole rotary drill bit. Ice cuttings are removed by the exhaust air and fountain out of the hole. The compressor increases the temperature of the air by about 50°, and it is cooled again before being pumped downhole, with a final temperature about 10° warmer than the ambient air. This means it cannot be used in ambient temperatures warmer than −10 °C. To avoid ice forming in the hose, ethanol is added to the compressed air. The system, which includes a winch to hold 100 m of hose, as well as two air compressors, is mounted on a sled. It has successfully drilling hundreds of holes in West Antarctica, and was easily able to drill to 90 m in only 25 minutes, making it the fastest ice drill. It was also used by the Askaryan Radio Array project in 2010–2011 at the South Pole, but was unable to drill below 63 m there because of variations in the local characteristics of the ice and firn. It cannot be used in a fluid-filled hole, which limits the maximum hole depth for this design. The main problem with the RAM drill is a loss of air circulation in firn and snow, which might be addressed by using reverse air circulation, via a vacuum pump drawing air up through the hose. As of 2017 IDDO is planning a revised design for the RAM drill to reduce the weight of the drill, which is currently 10.3 tonnes.

Other flexible drill stem designs have been considered, and in some cases tested, but as of 2016 none had been successfully used in the field. One design suggested using hot water to drill via a hose, and replacing the drillhead with a mechanical drill for coring once the depth of interest is reached, using the hot water both to hydraulically power the down hole motor, and to melt the resulting ice cuttings. Another design, the RADIX drill, produces a very narrow hole (20 mm) and is intended for rapid drilling access holes; it uses a small hydraulic motor on a narrow hose. It was tested in 2015 but found to have difficulty with cuttings transport, probably because of the very narrow space available between the hose and the borehole wall.

Coiled-tubing designs have never been successfully used for ice drilling. Coring operations would be particularly difficult, since a coring drill must trip out and in for each core, which would lead to fatigue; the tubing is typically rated for a lifetime of only 100 to 200 trips.

A cable-suspended drill has a downhole system, known as a sonde, to drill the hole. The sonde is connected to the surface by an armoured cable, which provides power and enables the drill to be winched in and out of the hole. Electromechanical (EM) cable-suspended drills have a cutting head, with blades that shave the ice as they rotate, like a carpenter's plane. The depth of penetration of the cut is adjusted by a device called a shoe, which is part of the cutting head. The ice cuttings are stored in a chamber in the sonde, either in the core barrel, above the core, or in a separate chamber, further up the drill.

The cuttings can be transported by auger flights or by fluid circulation. Drills that rely on auger flights and which are not designed to work in a fluid-filled hole are limited to depths at which borehole closure is not a problem, so these are known as shallow drills. Deeper holes have to be drilled with drilling fluid, but whereas circulation in a rotary drill takes the fluid all the way down and then up the borehole, cable-suspended drills only need to circulate the fluid from the drill head up to the cuttings chamber. This is known as bottom-hole circulation.

The upper part of the sonde has an antitorque system, which most commonly consists of three or four leaf-springs that press out against the borehole walls. Sharp edges on the leaf springs catch in the walls and provide the necessary resistance to prevent this part of the drill from rotating. At the point where the cable connects to the sonde, most drills include a slip ring, to allow the drill to rotate independently of the cable. This is to prevent torque damage to the cable if the anti-torque system fails. Coring drills may also have a weight that can be used as a hammer to assist in breaking the core, and a chamber for any instrumentation or sensors needed.

At the bottom of the sonde is the cutting head, and above this is the core barrel, with auger flights around it on shallow drills, and typically an outer barrel around that, usually with internal vertical ribs or some other way of providing additional impetus to the upward-bound cuttings on the flights. If there is a separate chip chamber it will be above the core barrel. The motor, with suitable gearing, is also above the core barrel.

Shallow drills can retrieve cores up to 300–350 m deep, but core quality is much improved if drilling fluid is present, so some shallow drills have been designed to work in wet holes. Tests reported in 2014 showed that wet drilling, with the top of the drilling fluid no deeper than 250 m, would maintain good core quality.

Drilling fluids are necessary for drilling deep holes, so the cable-suspended drills that are used for these projects use a pump to provide fluid circulation, in order to remove the cuttings from the bit. A few drills designed for use with drilling fluid also have auger flights on the inner barrel. As with shallow drills, the cuttings are stored in a chamber above the core. The circulation can be in either direction: down the inside of the drill string, and up between the core barrel and the borehole wall, or in the reverse direction, which has become the favoured approach in drill design as it gives better cuttings removal for a lower flow rate. Drills capable of reaching depths over 1500 m are known as deep drilling systems; they have generally similar designs to the intermediate systems that can drill from 400 m to 1500 m, but must have heavier and more robust systems such as winches, and have longer drills and larger drilling shelters. Core diameters for these drills have varied from 50 mm to 132 mm, and the core length from as short as 0.35 m up to 6 m. A common design feature of these deep drills is that they can be tipped to the horizontal to make it easier to remove the core and the cuttings. This reduces the required height of the mast, but requires a deep slot to be cut into the ice, to make room for the sonde to swing up.

The first cable-suspended electromechanical drill was invented by Armais Arutunoff for use in mineral drilling; it was tested in 1947 in Oklahoma, but did not perform well. CRREL acquired a reconditioned Arutunoff drill in 1963, modified it for drilling in ice, and in 1966 used it to extend a hole at Camp Century in Greenland to the base of the ice cap, at 1387 m, and 4 m further into the bedrock.

Many other drills have since been based on this basic design. A recent variation on the basic EM drill design is the Rapid Access Isotope Drill, designed by the British Antarctic Survey to drill dry holes to 600 m. This drill does not collect a complete ice core; instead it will collect ice cuttings, using a cutting head similar to a spoonborer. The resulting access hole will be used for temperature profiling, and along with the isotope results which will indicate the age of the ice, the data will be used for modeling the ice profile down to bedrock in order to determine the best place to drill to obtain the oldest possible undisturbed basal ice. The drill is expected to reach 600 m in 7 days of drilling, rather than the 2 months which would be needed to drill a core; the speed is because the cutters can be more aggressive as core quality is not an issue, and because the borehole is narrow which reduces power requirements for the winch.

Thermal drills work by applying heat to the ice at the bottom of the borehole to melt it. Thermal drills in general are able to drill successfully in temperate ice, where an electromechanical drill is at risk of jamming because of ice forming in the borehole. When used in colder ice, some form of antifreeze is likely to be introduced into the borehole to prevent the meltwater from freezing in the drill.

Hot water can be used to drill in ice by pumping it down a hose with a nozzle at the end; the jet of hot water will quickly produce a hole. Letting the hose dangle freely will produce a straight hole; as the hole gets deeper the weight of the hose makes this hard to manage manually, and at a depth of about 100 m it becomes necessary to run the hose over a pulley and use counterweights. Since the pressure in the hose is proportional to the square of the flow, hose diameter is one of the limiting factors for a hot-water drill. To increase flow rate beyond a certain point, the hose diameter must be increased, but this will require significant capacity increases elsewhere in the drill design. Hoses that are wrapped around a drum before being pressurized will exert constricting force on the drum, so the drums must be of robust design. Hoses must wrap neatly when spooling up, to avoid damage; this can be done manually for smaller systems, but for very large drills a level-wind system has to be implemented. The hose ideally should have the tensile strength to support its weight when spooling into the hole, but for very deep holes a supporting cable may need to be used to support the hose.

Steam can also be used in place of hot water, and does not need to be pumped. A handheld steam drill is able to rapidly drill short holes, for example for ablation stakes, and both steam and hotwater drills can be made light enough to be hand carried. A guide tube can be used to help keep the borehole straight.

In cold ice, a borehole drilled with hot water will close up as the water freezes. To avoid this, the drill can be run back down the hole, warming the water and hence the surrounding ice. This is a form of reaming. Repeated reamings will raise the temperature of the surrounding ice to the point where the borehole will stay open for longer periods. However, if the goal is to measure temperature in the borehole, then it is better to apply as little additional heat as possible to the surrounding ice, which means that a higher energy drill with a high water flow rate is desirable, since this will be more efficient. If there is a risk of the drill freezing in, a "back drill" can be included in the design. This is a mechanism which redirects the hot water jet upwards if the drill meets with resistance on tripping out. A separate hot water reamer can also be used, jetting hot water sideways onto the borehole walls as it passes.

Boreholes drilled with hot water are rather irregular, which makes them unsuitable for certain kinds of investigations, such as speed of borehole closure, or inclinometry measurements. The warm water from the nozzle will continue to melt the borehole walls as it rises, and this will tend to make the hole cone-shaped—if the hole is being drilled at a location with no surface snow or firn, such as an ablation zone in a glacier, then this effect will persist to the top of the borehole.

The water supply for a hot water drill can come from water at the surface, if available, or melted snow. The meltwater in the borehole can be reused, but this can only be done once the hole penetrates below the firn to the impermeable ice layer, because above this level the meltwater escapes. The pump to bring the meltwater back to the surface must be placed below this level, and in addition, if there is a chance that the borehole will penetrate to the base of the ice, the drilling project must plan for the likelihood that this will change the water level in the hole, and ensure that the pump is below the lowest likely level. Heating systems are usually adapted from the heaters used in the pressure washer industry.

When any thermal drilling method is used in dirty ice, the debris will accumulate at the bottom of the borehole, and start to impede the drill; enough debris, in the form of sand, pebbles, or a large rock, could completely stop progress. One way to avoid this is to have a nozzle angled at 45°; using this nozzle will create a side channel into which the obstructions will go. Vertical drilling can then start again, bypassing the debris. Another approach is to recirculate the water at the bottom of the hole, with an electrical heater embedded in the drill head and filters in the circulation. This can remove most of the small debris that impedes the drillhead.

A different problem with impure ice comes from contaminants brought in by the project, such as clothing and wood fibres, dust, and grit. Using snow from around the campsite to supply the drill with water is often necessary at the start of drilling, as the hole will not yet have reached the impermeable ice, so water cannot be pumped back up from the bottom of the hole; shoveling this snow into the drill's water supply will pass these contaminants through the drill mechanism, and can damage the pumps and valves. A fine filter is required to avoid these problems.

An early expedition using hot water drills was in 1955, to the Mer de Glace; Électricité de France used hot water to reach the base of the glacier, and also used equipment that sprayed multiple jets simultaneously to create a tunnel under the ice. More development work was done in the 1970s. Hot water drills are now capable of drilling very deep holes: for example, between 2004 and 2011, a large hot water drill at the South Pole was used to drill 86 holes to a depth of 2.5 km to set strings of sensors in the boreholes, for the IceCube project. Hot water coring drills have also been developed.

An early steam drill was developed by F. Howorka in the early 1960s for work in the Alps. Steam drills are not used for holes deeper than 30 m, as they are quite inefficient due to thermal losses along the hose, and pressure losses with increasing depth under water. They are primarily used for quickly drilling shallow holes.

Instead of using a jet of hot water or steam, thermal drills can also be constructed to provide heat to a durable drillhead, for example by pumping hot water down and back up again inside the drill string, and use that to melt the ice. Modern thermal drills use electrical power to heat the drillhead instead.

It is possible to drill with a hotpoint that consists of an electrical heating element, directly exposed to the ice; this means that the element must be able to work underwater. Some drills instead embed the heating element in a material such as silver or copper that will conduct the heat quickly to the hotpoint surface; these can be constructed so that the electrical connections are not exposed to water. Electrothermal drills require a cable to bring the power down the hole; the circuit can be completed via the drillpipe if one is present. A transformer is needed in the drill assembly since the cable must carry high voltage to avoid power dissipation. It is more difficult to arrange electrical power at a remote location than to generate heat via a gas boiler, so hotpoint drills are only used for boreholes up to a few hundred metres deep.

The earliest attempt to use heat to drill in ice was in 1904, when C. Bernard drilling at the Tête Rousse Glacier, tried using heated iron bars to drill with. The ends of the bars were heated until incandescent, and lowered into the borehole. The first true hotpoint was used by Mario Calciati in 1942 on the Hosand Glacier. Calciati pumped hot water from the surface down the drillstem, and back up after it had passed through the drillhead. Other hotpoint designs have used electrical heating to heat the drillhead; this was done in 1948 by a British expedition to the Jungfraujoch, and by many other drill designs since then. Hotpoints do not produce cores, so they are used primarily for creating access holes.

The development in the 1960s of thermal coring drills for intermediate depth holes was prompted by the problems associated with rotary coring drills, which were too costly to use for polar ice cores because of the logistical problems caused by their weight. The components of a thermal drill are generally the same as for a cable-suspended EM drill: both have a mast and winch, and an armoured cable to provide power downhole to a sonde, which includes a core barrel. No antitorque system is needed for a thermal drill, and instead of a motor that provides torque, the power is used to generate heat in the cutting head, which is ring shaped to melt an annulus of ice around the core. Some drills may also have a centralizer, to keep the sonde in the middle of the borehole.

The sonde of an electrothermal drill designed to run submerged in meltwater may consist almost entirely of the core barrel plus the heated cutting head (diagram (a) in the figure to the right). Alternative designs for use in colder ice (see diagram (b) at right) may have a compartment above the core barrel, and tubes that run down to just above the cutting head; a vacuum pump sucks up the meltwater. In these drills the meltwater must be emptied at the surface at the end of each coring run.

Another approach (see (c) at right) is to use a drilling fluid that is a mixture of ethanol and water, with the exact proportions determined by the ice temperature. In these drills there is a piston above the core barrel and at the start of a run the piston is at the bottom of the sonde, and the space above is filled with drilling fluid. As the drills cuts downwards, the core pushes the piston up, pumping the fluid down and out around the cutting head, where it mixes with the meltwater and prevents it from freezing. The piston is the only moving part, which simplifies the design; and the core barrel can take up much of the length of the sonde, whereas drills which suck out the meltwater in order to drill in a dry hole have to sacrifice a large section of the sonde for meltwater storage.

Thermal drills designed for temperate ice are light and straightforward to operate, which makes them suitable for use on high-altitude glaciers, though this also requires that the drill can be disassembled into components for human-powered transport to the most inaccessible locations, since helicopters may not be able to reach the highest glaciers.

Electrothermal drill designs date back to the 1940s. An electrothermal drill was patented in Switzerland in May 1946 by René Koechlin, and was used in Switzerland, and in 1948 a British expedition to the Jungfraujoch drilled to the bed of the glacier using an electrothermal design. Twenty electrothermal coring drills were designed between 1964 and 2005, though many designs were abandoned because of the higher performance of EM coring drills.

If the goal is to obtain instrument readings from within the ice, and there is no need to retrieve either the ice or the drill system, then a probe containing a long spool of cable and a hotpoint can be used. The hotpoint allows the probe to melt its way through the ice, unreeling the cable behind it. The meltwater will refreeze, so the probe cannot be recovered, but it can continue to penetrate the ice until it reaches the limit of the cable it carries, and send instrument readings back up through the cable to the surface. Known as Philberth probes, these devices were designed by Karl and Bernhard Philberth in the 1960s as a way to store nuclear waste in the Antarctic, but were never used for that purpose. Instead, they were adapted to use for glaciological research, reaching a depth of 1005 metres and sending temperature information back to the surface when tested in 1968 as part of the Expédition Glaciologique Internationale au Groenland (EGIG).

Because thermal probes support their weight on the ice at the bottom of the borehole, they lean slightly out of the vertical, and this means they have a natural tendency to stray away from a vertical borehole towards the horizontal. Various methods have been proposed to address this. A cone-shaped tip, with a layer of mercury above the tip, will cause additional heat transfer to the lower side of a slanting borehole, increasing the speed of melting on that side, and returning the borehole to the vertical. Alternatively the probe can be constructed to be supported by ice above its centre of gravity, by providing two heating rings, one of which is towards the top of the probe, and has a greater diameter than the rest of the probe. Giving this upper ring a slightly lower heating power will cause the probe to have more bearing pressure on the upper ring, which will give it a natural tendency to swing back to vertical if the borehole starts to deviate. The effect is called pendulum steering, by analogy with the tendency of a pendulum always to swing back towards a vertical position.

In the 1990s NASA combined the Philberth probe design with ideas drawn from hot-water drills, to design a cryobot probe that had hot water jets in addition to a hotpoint nose. Once the probe was submerged in a thin layer of meltwater, the water was drawn in and reheated, emerging at the nose as a jet. This design was intended to help move particulate matter away from the nose, as a hot-water drill tends to. A version with no analytical tools on board was built and field tested in Svalbard, Norway, in 2001. It penetrated to 23 m, successfully passing through layers of particulates.

Cryobots remain in good thermal contact with the surrounding ice throughout their descent, and in very cold ice this can drain a substantial fraction of their power budget, which is finite since they must carry their power source with them. This makes them unsuitable for investigating the Martian polar ice cap. Instead, NASA added a pump to the cryobot design, to raise meltwater to the surface, so that the probe, known as the SIPR (for Subsurface Ice Probe) descends in a dry hole. The lower gravity on Mars means that the overburden pressure on the ice cap is much less, and an open borehole is expected to be stable to a depth of 3 km, the expected depth of the ice cap. The meltwater can then be analyzed at the surface. Pumping through a vertical tube will cause mixing, so to ensure discrete samples for analysis at the surface, a large bore and a small bore tube are used; the small bore tube is used for sampling, and then its contents are allowed to return to the probe and are pumped back up the large bore tube for use in experiments that do not depend on stratigraphy, such as searches for living organisms. Leaving the analytical instruments on the surface reduces the necessary size of the probe, which helps make this design more efficient.

Along with the water transport tubes, a heated wire ensures that the water stays liquid all the way to the surface, and power and telemetry is also carried from the surface. To keep the hole vertical the probe can sense when it is deviating, and the jets of hot water are adjusted to compensate. The drill is expected to make use of solar power in operation, meaning it must be able to function on less than 100 W when in sunlight. A fully built version of the probe was successfully tested in Greenland in 2006, drilling to a depth of 50 m. NASA has proposed a similar designed for drilling in the ice on Europa, a moon of Jupiter. Any such probe would have to survive temperatures of 500 °C while being sterilized to avoid biological contamination of the target environment.

Snow samples are taken to measure the depth and density of the snow pack in a given area. Measurements of depth and density can be converted into a snow water equivalent (SWE) number, which is the depth of water that would result from converting the snow into water. Snow samplers are typically hollow cylinders, with toothed ends to help them penetrate the snow pack; they are used by pushing them into the snow, and then pulling them out along with the snow in the cylinder. Weighing the cylinder full of snow and subtracting the weight of the empty cylinder gives the snow weight; samplers usually have lengthwise slots to allow the depth of the snow to be recorded as well, though a sampler made of transparent material makes this unnecessary.

The sampler must grip the snow well enough to keep the snow inside the cylinder as it is removed from the snow, which is easier to accomplish with a smaller diameter cylinder; however, larger diameters give more accurate readings. Samples must avoid compacting the snow, so they have smooth inner surfaces (usually of anodized aluminium alloy, and sometimes waxed in addition) to prevent the snow from gripping the sides of the cylinder as it is pushed in. A sampler may penetrate light snow under its own weight; denser snow pack, firn, or ice, may require the user to rotate the sampler gently so that the cutting teeth are engaged. Pushing too hard without successfully cutting a dense layer may cause the sample to push the layer down; this situation can be identified because the snow level inside the sampler will be lower than the surrounding snow. Multiple readings are usually taken at each location of interest, and the results are averaged. Snow samplers are usually accurate to within about 5–10%.

The first snow sampler was developed by J.E. Church in the winter of 1908/1909, and the most common modern snow sampler, known as the Federal snow sampler, is based on Church's design, with some modifications by George D. Clyde and the U.S. Soil Conservation Service in the 1930s. It can be used for sampling snow up to 9 m in depth.

Penetration testing involves inserting a probe into snow to determine the snow's mechanical properties. Experienced snow surveyors can use an ordinary ski pole to test snow hardness by pushing it into the snow; the results are recorded based on the change in resistance felt as the pole is inserted. A more scientific tool, invented in the 1930s but still in widespread use, is a ram penetrometer. This takes the form of a rod with a cone at the lower end. The upper end of the rod passes through a weight that is used as a hammer; the weight is lifted and released, and hits an anvil—a ledge around the rod which it cannot pass—which drives the rod into the snow. To take a measurement, the rod is placed on the snow and the hammer is dropped one or more times; the resulting depth of penetration is recorded. In soft snow a lighter hammer can be used to obtain more precise results; hammer weights range from 2 kg down to 0.1 kg. Even with lighter hammers, ram penetrometers have difficulty distinguishing thin layers of snow, which limits their usefulness with regard to avalanche studies, since thin and soft layers are often involved in avalanche formation.

Two lightweight tools are in wide use that are more sensitive than ram penetrometers. A snow micro-penetrometer uses a motor to drive a rod into snow, measuring the force required; it is sensitive to 0.01–0.05 newtons, depending on the snow strength. A SABRE probe consists of a rod that is inserted manually into snow; accelerometer readings are then used to determine the penetrative force needed at each depth, and stored electronically.

For testing dense polar snow, a cone penetrometer test (CPT) is use, based on the equivalent devices used for soil testing. CPT measurements can be used in hard snow and firn to depths of 5–10 m.

Commercially available rotary rigs have been used with large augers to drill in ice, generally for construction or for holes to gain access below the ice. Although they are unable to produce cores, they have been intermittently used by US and Soviet scientific expeditions in the Antarctic. In 2012, a British Antarctic Survey expedition to drill down to Lake Ellsworth, two miles below the surface of the Antarctic ice, used an Australian earth auger driven by a truck-mounted top drive to help drill two 300 m holes as part of the project, though in the event the project was delayed.

Powered augers designed to drill large holes through ice for winter fishing may be mounted on a snow vehicle, or a tractor or sled; hole diameters can be as high as 350 mm. These rigs have been produced commercially in both the US and the USSR, but are no longer in common use.

A flame-jet drill, more usually used to drill through crystalline rocks, was used to drill through ice on the Ross Ice Shelf, in the 1970s. The drill burns fuel oil, and can be run under water as long as enough compressed air is available. It drills rapidly, but produces an irregular hole contaminated by soot and fuel oil.

A Soviet-designed drill used a motor to provide vertical vibration to the barrel of the drill at 50 Hz; the drill had an outer diameter of 0.4 m, and in tests at Vostok Station in the Antarctic drilled a 6.5 m hole, with a 1.2 m drilling run taking between 1 and 5 minutes to complete. The drill's steel edges compacted snow into the core, which helped it stick to the inside of the barrel when the drill was winched out of the hole.

Mechanical drills typically have three cutters, spaced evenly around the drill head. Two cutters leads to vibration and poorer ice core quality, and tests of drillheads with four cutters have produced unsatisfactory performance. Geometric design varies, but the relief angle, α, varies from 5–15°, with 8–10° the most common range in cold ice, and the cutting angle, , varies from 45° (the most common in cold ice) up to 90°. The safety angle, between the underside of the cutting blade and the ice, can be as low as 0.8° in successful drill designs. Different shapes for the end of the blade have been tried: flat (the most common design), pointed, rounded, and scoop shaped.

Cutters have to be made of extremely strong materials, and usually have to be sharpened after every 10–20 m of drilling. Tool steels containing carbon are not ideal because the carbon makes the steel brittle in temperatures below −20 °C. Sintered tungsten carbide has been suggested for use in cutters, since it is extremely hard, but the best tool steels are more cost effective: carbide cutters are fixed to the body of the cutting tool by cold pressing or brass soldering, and cannot easily be unmounted and sharpened in the field.

The cutting depth is controlled by mounting shoes on the bottom of the drill head; these ride on the ice surface and so limit how deep the cutter can penetrate in each revolution of the drill. They are most commonly mounted just behind the cutters, but this position can lead to ice accumulating in the gap between the cutter and the shoe. So far it has not proved possible to correct this by modifying the shoe design.

Drilling fluids are necessary for borehole stability in deep cores, and can also be used to circulate cuttings away from the bit. Fluids used include water, ethanol/water and water/ethylene glycol mixtures, petroleum fuels, non-aromatic hydrocarbons, and n-butyl acetate.
Densifiers are used in drilling fluids to adjust the density of the fluid to match the surrounding ice. Perchloroethylene and trichloroethylene were often used in early drilling programs, in combination with petroleum fuels. These have been phased out for health reasons. Freon was a temporary replacement, but has been banned by the Montreal Protocol, as has HCFC-141b, a hydrochlorfluorocarbon densifier used once Freon was abandoned. Future options for drilling fluids include low molecular weight esters, such as ethyl butyrate, n-propyl propionate, n-butyl butyrate, n-amyl butyrate and hexyl acetate; mixtures of various kinds of ESTISOL; and dimethyl siloxane oils.

The two main requirements of an anti-torque system are that it should prevent rotation of the sonde, and it should allow easy movement of the drill up and down the borehole. Attempts have been made to design drills with counter-rotating components so that overall torque is minimized, but these have had limited success. Five kinds of anti-torque systems have been devised for use with cable-suspended EM drills, though not all are in current use, and some drills have used a combination of more than one design. The first drill to require an anti-torque system was used at Camp Century by CRREL in 1966; the drill incorporated a set of hinged friction blades that swung out from the sonde when the drill motor was started. These were found to have very weak friction against the borehole wall, and were ineffective; the drill had to be controlled carefully to prevent twisting the cable. No other drills have attempted to use this approach.

For the next deployment of the drill leaf springs were installed, and this has proved to be a more durable design. These are mounted vertically, with a curve outwards so that they are easily compressed by the borehole wall, and can slide up and down with the movement of the drill. They pass easily through any areas of irregularity in the borehole, but the edges of the springs cut into the borehole wall and prevent rotation. Leaf springs are very simple mechanically, with the additional benefit of being easy to adjust by changing the spacing between the end points. They can be placed anywhere on the drill that does not rotate, so they do not add length to the sonde. The shape is usually a fourth-order parabola, since this has been determined to provide the most even loading against the borehole wall. Leaf springs have been found to be so effective that they can prevent rotation even in heavy drills running at full power.

Skate antitorque systems have blades attached to vertical bars which are pushed against the borehole wall; the blades dig into the wall and provide the anti-torque. Skates can be built with springs which allow them to keep the blades pressed against the wall in an irregular borehole, and to prevent problems in narrower parts of the borehole. Although skates are a popular design for anti-torque and have been used with success, they have difficulty preventing rotation in firn and at boundaries between layers of different densities, and can cause problems when drilling with high torque. When they fail, they act as reamers, removing chips from the wall which can fall to the drillbit and interfere with drilling.
In the 1970s, the Japanese Antarctic Research Expedition (JARE) group designed several drills using side-mill cutters. These are toothed gears that are driven from the rotation of the main drill motor via 45° spiral gears; their axis of rotation is horizontal, and they are placed so that the teeth cut four vertical slots in the borehole wall. Guide fins higher on the sonde travel in these slots and provide the antitorque. The design was effective at preventing rotation of the sonde, but it proved to be almost impossible to realign the guide fins with the existing slots when tripping in. Misalignment increased the chance of the drill getting stuck in the borehole; and there was also a risk of ice cuttings from the mill cutters jamming in between the drill and the borehole wall, causing the drill to get stuck. The system was used again in a drill developed in China in the 1980s and 1990s, but the problems inherent in the design are now considered insuperable and it is no longer in use.

The most recent anti-torque system design is the use of U-shaped blades, made of steel and fixed vertically to the sides of the sonde. Initial implementations ran into problems with thin blades bending too easily, and thick blades providing too much resistance to vertical movement of the sonde, but the final design can generate strong resistance to torque in both firn and ice.

Drills may be designed with more than one anti-torque system in order to take advantage of the different performance of the different designs in different kinds of snow and ice. For example, a drill may have skates to be used in hard firn or ice, but also have a leaf-spring system, which will be more effective in soft firn.

In ice core drilling, when an annulus has been drilled around the core to be retrieved, the core is still attached to the ice sheet at its lower end, and this connection has to be broken before the core can be retrieved. One option is to use a collet, which is a tapered ring inside the cutting head. When the drill is pulled up, the collet compresses the core and holds it, with loose ice chips wedged in it increasing the compression. This breaks the core and holds it in the barrel once it has broken. Collets are effective in firn but less so in ice, so core dogs, also known as core catchers, are often used for ice cores.

A typical ice drill core dog has a dog-leg shape, and will be built into the drill head with the ability to rotate, and with a spring supplying some pressure against the core. When the drill is lifted, the sharp point of the core dog engages and rotates around, causing the core to break. Some core dogs have a shoulder to stop them from over-rotating. Most drill heads have three core dogs, though having only two core dogs is possible; the asymmetric shearing force helps break the core. The angle, , between the core dog point and the core, has been the subject of some investigation; a study in 1984 concluded that the optimum angle was 55°, and a later study concluded that the angle should be closer to 80°. Core catchers are made from hardened steel, and need to be as sharp as possible. The force required to break the core varies with temperature and depth, and in warm ice the core dogs may gouge grooves up the core before they catch and it breaks. Some drills may also include a weight that can be used as a hammer, to provide an impact to help in breaking the core.

For snow and firn, where the core material may be at risk of falling out of the bottom of the core barrel, a basket catcher is a better choice. These catchers consist of spring wires or thin pieces of sheet metal, placed radially around the bottom of the core barrel and pressed against the side of the barrel by the core as the drill descends around it. When the drill is lifted, the ends of the catcher engage with the core and break it from the base, and act as a basket to hold it in place while it is brought to the surface.

Casing, or lining a hole with a tube, is necessary whenever drilling operations require that the borehole be isolated from the surrounding permeable snow and firn. Uncased holes can be drilled with fluid by using a hose lowered into the hole, but this is likely to lead to increased drilling fluid consumption and environmental contamination from leaks. Steel casing was used in the 1970s, but rust from the casing caused damage to the drills, and the casing was not sealed, leading to fluid leaks. There were also problems with the casing tubes not being centered, which caused damage to the drill bit as it was lowered through the casing. Fibreglass and HDPE casing has become more common, with junctions sealed with PTFE tape, but leaks are frequent. Heat fusion welding for HDPE casing is a possible solution. To seal the bottom of the casing, water can be pumped to the bottom of the hole once the casing is set, or a thermal head can be used to melt ice around the casing shoe, creating a seal when the water freezes again. Another approach is to use a hotpoint drill, saturating the snow and firn with melted water, which will then freeze and seal the borehole.

Low-temperature PVC tubing is not suitable for permanent casing, since it cannot be sealed at the bottom, but it can be used to pass drilling fluid through the permeable zone. Its advantage is that it requires no connections since it can be coiled on a reel for deployment.




</doc>
<doc id="55233455" url="https://en.wikipedia.org/wiki?curid=55233455" title="John Doubleday (restorer)">
John Doubleday (restorer)

John Doubleday (about 1798 – 25January 1856) was a British craftsperson, restorer, and dealer in antiquities who was employed by the British Museum for the last 20 years of his life. He undertook several duties for the museum, not least as a witness in criminal trials, but was primarily their specialist restorer, perhaps the first person to hold the position. He is best known for his 1845 restoration of the severely-damaged Roman Portland Vase, an accomplishment that places him at the forefront of his profession at the time.

While at the British Museum, Doubleday also dealt in copies of coins, medals, and ancient seals. His casts in coloured sulphur and in white metal of works in both national and private collections, allowed smaller collections to hold copies at a fraction of the price that the originals would command. Thousands of his copies entered the collections of institutions and individuals. Yet the accuracy he achieved led to confusion with the originals; after his death he was labelled a forger, but with the caveat that "[w]hether he did copies with the intention of deceiving collectors or not is open to doubt".

Little is known about Doubleday's upbringing or personal life. Several sources describe him as an American, including the 1851 United Kingdom census, which records him as a New York-born British subject. An obituary noted that he worked at a printer's shop for more than 20 years during his youth, which gave him the experience of casting type that he would employ in his later career as a copyist. Doubleday's early life, family, and education are otherwise unknown. He died in 1856, leaving a wife and five daughters, all English; the eldest child was born around 1833.

From 1836 to 1856 Doubleday worked in the Department of Antiquities at the British Museum. He appears to have been employed as a freelancer who also occasionally acted as an agent in sales to the museum. At times he presented the museum with items including coins, medals, and Egyptian objects. Among other donations, his 1830 gift of 2,433 casts of medieval seals was the only significant donation recorded by the museum that year, he offered several coins and another 750 casts the following year, and in 1836, he presented the museum with a Henry Corbould lithograph of himself. A further presentation in 1837 was still considered, in 1996, to be one of the museum's most important collections of casts of seals. He seems to have been the museum's primary, and perhaps its first, dedicated restorer; his death was described as leaving the post vacant. At his death, it was noted that he was "chiefly employed in the reparation of innumerable works of art, which could not have been intrusted to more skilful or more patient hands", and that he "was well known as one of the most valuable servants of that department".

The highlight of Doubleday's career came after 7 February 1845 when a young man, who later admitted having spent the prior week "indulging in intemperance", smashed the Portland Vase, an example of Roman cameo glass and among the most famous glass items in the world, into hundreds of pieces. After his selection for the restoration, Doubleday commissioned a watercolour painting of the fragments by Thomas H. Shepherd. No account of his restoration survives, but on 1 May he discussed it in front of the Society of Antiquaries of London, and by 10 September he had glued the vase whole again. Only 37 small splinters, most from the interior or thickness of the vase, were left out; the cameo base disc, which was found to be a modern replacement, was set aside for separate display. A new base disc of plain glass, with a polished exterior and matte interior, was diamond-engraved "Broke Feby 7th 1845 Restored Sept 10th 1845 By John Doubleday". The British Museum awarded Doubleday an additional £25 () for his work.

At the time the restoration was termed "masterly" and Doubleday was lauded by "The Gentleman's Magazine" for demonstrating "skilful ingenuity" and "cleverness ... sufficient to establish his immortality as the prince of restorers". In 2006 William Andrew Oddy, a former keeper of conservation at the museum, noted that the achievement "must rank him in the forefront of the craftsmen-restorers of his time." Doubleday's restoration would remain for more than 100 years until the adhesive grew increasingly discoloured. The vase was next restored by J. W. R. Axtell in 1948–1949, and then by Nigel Williams in 1988–1989.

Beyond his work on the Portland Vase, several other of Doubleday's responsibilities at the British Museum have been recorded. In 1851 he successfully undid damaging restoration work by William Thomas Brande of the Royal Mint, who in using acid to clean bronze bowls from Nimrud had caused extreme oxidation. Doubleday's method, described at the time only as "a very simple process and without employing acids", is unknown, but may have used warm water with soap.

Doubleday was again called upon when, between 1850 and 1855, the museum received clay tablets from excavations in Babylonia and Assyria. Some were poorly packaged and had developed crystalline deposits rendering the writing illegible. Under the direction of Samuel Birch, then the keeper of the Department of Oriental Antiquities, Doubleday attempted to remove the deposits. The results were described by E. A. Wallis Budge, former keeper of Egyptian and Assyrian antiquities at the museum, as "disastrous", but by modern reasoning as "prescient", for though unsuccessful, the underlying methods were subsequently refined by others. Doubleday first attempted to harden the tablets by firing them, but this resulted in the flaking of the surfaces, destroying the inscriptions. His second attempt, submerging the tablets in solutions, also resulted in disintegration, at which point Birch suspended the efforts entirely. Later attempts by other conservators in firing similar tablets were more successful; mainstream acceptance today is more tempered by concerns about reversibility than by concerns about efficacy. Doubleday is regarded as the inventor of this method, and his failures may have been caused by raising or lowering the temperature too quickly.

Doubleday twice served as a witness in criminal matters. In 1841 he testified about his analysis of a gold medal during a trial concerning its theft. Eight years later Doubleday again testified, in March and April 1849, in a matter concerning the theft of coins from the museum. Early in February, Timolean Vlasto, a fashionable twenty-four-year-old from Vienna whose late father, Count Vlasto, had been a diplomat, had been introduced to Charles Newton (later Sir Charles) by a friend, who described Vlasto as a person interested in coins. Vlasto was given unfettered access to the museum's collection. Suspicions were aroused on 24 March, and on Monday the 26th a label was found on the floor; the coin that it described was missing. Upon inspection many more coins could not be found, but some were recovered when a search warrant for Vlasto's lodgings was obtained on Thursday. Doubleday was called to testify on Thursday or Friday; he stated that some of the coins exactly matched sulphur casts which he had made before the theft, and that the market value was between £3,000 and £4,000. Vlasto, who was remanded without bail, claimed that the majority of the coins discovered were not the museum's. On 17 April Doubleday again testified, identifying two more coins as belonging to the museum. In early May Vlasto pleaded guilty to the theft of 266 coins from the museum, valued at £500, and another 71, valued at £150, from the house of General Charles Richard Fox. Vlasto's lawyer termed him a monomaniac who was only interested in collecting, not selling. The pleas met little sympathy. Vlasto was sentenced by the Central Criminal Court to seven years transportation to Australia, and in early 1851 was placed on board the "Lady Kennaway" for the journey.

Apart from his work at the British Museum, Doubleday was a dealer and a copyist of coins, medals, and ancient seals. He sold sulphur and white metal casts, the former coloured in different hues, at his establishment, which, located near the British Museum, may have helped facilitate his employment there. He also sold curiosities, such as cabinets, snuff boxes, and lead seals purportedly made from materials taken from the charred ruins of the Palace of Westminster with the consent of the Commissioners of Woods and Forests, and pieces of wood said to be from a tree planted by Shakespeare. In 1835 Doubleday advertised for sale copies of 6000 Greek coins, 2050 bronze, 1000 silver, and 500 gold Roman coins, and 300 Roman medallions, in addition to other antiquities and what Doubleday termed "the most extensive Collection of Casts in Sulphur of ancient seals ever formed". By 1851 he had casts of more than 10,000 seals, and at his death it was said that he "possessed the largest collection of casts of seals in England, probably in the world." This comprehensiveness led to his contribution to the 1848 "Monumenta Historica Britannica" of a descriptive catalogue of Roman coins relating to Britain. More unique pieces he sometimes exhibited, either himself or by loan to Sir Henry Ellis, to the Society of Antiquaries of London. Doubleday's casts came from a range of places; on good terms with a variety of institutions and collectors, he was permitted to take casts at will from the collections of the British Museum and the "Bibliothèque nationale" in Paris.

Doubleday's casts were inexpensive, and sold widely. He was well known among collectors, and also sold to lyceums; University College London filled out their collection with his casts, finding them cost-effective substitutes for study. This same appearance of realism saw some of Doubleday's copies passed off as real. Doubleday was cast as a forger in Leonard Forrer's 1904 "Biographical Dictionary of Medallists", though with the caveat that "[w]hether he did copies with the intention of deceiving collectors or not is open to doubt".

Little is known about Doubleday's personal life, and nothing about his upbringing or education. An 1859 edition of "The English Cyclopædia" described him as American, and the 1851 census as a New York-born "artist" who was nonetheless a British subject, married to one Elizabeth and father of five daughters, all Londoners. His eldest daughter, also an Elizabeth, was born around 1833, suggesting that Doubleday and his wife had married by then.

Doubleday worked at a printer's shop in his youth for more than 20 years, according to his obituary in "The Athenæum", giving him experience through making type in the casting of metal and other materials. Subsequently, he began copying medals, ancient seals, and coins, occasionally devising new methods of doing so; he also prepared castings for the Royal Mint, and become a founding member of the Royal Numismatic Society. By 1832 he was listed in directories under the heading "Curiosity, shell & picture dealers", and as a dealer in ancient seals. As well as his work at the British Museum, he may have been a collector.

According to the obituary in "The Athenæum", Doubleday died "after a long illness" on 25 January 1856, "in the fifty-seventh year of his age". The illness was termed "extreme" by colleagues, such that he was unreachable for months. Obituaries were published in "The Athenæum" and "The Gentleman's Magazine", and he was buried in Kensal Green Cemetery. His will was made only six days before his death. His entire estate was left to Elizabeth Bewsey, the daughter of a deceased bookkeeper; she was apparently not the Elizabeth to whom Doubleday was married, making it a bequest that seemingly left nothing for his wife or daughters. His library was sold by Sotheby's that April. The 322 lots combined to fetch £228 2s6d ().



</doc>
<doc id="55326272" url="https://en.wikipedia.org/wiki?curid=55326272" title="The Princesse de Broglie">
The Princesse de Broglie

The Princesse de Broglie (French La Princesse de Brogli) is an oil on canvas painting by the French Neoclassical artist Jean-Auguste-Dominique Ingres. Completed between 1851 and 1853, it shows Pauline de Broglie, styled Princesse, who, in 1845, married Albert de Broglie, the 28th Prime Minister of France.

Pauline was aged 28 at the time of its completion. She was highly intelligent, widely known for her beauty, and deeply religious, but suffered from profound shyness, and the painting captures her melancholia. Pauline contracted tuberculosis in her early 30s and died in 1860 aged 35. Although Albert lived until 1901, he was heartbroken and did not remarry.

In preparation for the commission, Ingres undertook a number of preparatory pencil sketches, each of which captures her personality and sense of taste. They show her in various poses, including standing, and in differently styled dresses. The eventual painting is considered one of Ingres' finest later-period portraits of women, along with the "Comtesse d'Haussonville", "Portrait of Baronne de Rothschild" and "Madame Moitessier". As with many of Ingres' female portraits, details of costume and setting are rendered with a chilly precision while her body seems to lack a solid bone structure. The painting is held in the collection of the Metropolitan Museum of Art, New York, and is signed and dated 1853.

Joséphine-Éléonore-Marie-Pauline de Galard de Brassac de Béarn (1825–1860) married Albert de Broglie on 18 June 1845, and they had five sons together. Although not high royalty, on the occasion of their marriage, they styled themselves Prince and Princesse respectively. Pauline was a highly intelligent and religious woman, who was well read and wrote a number of texts in her lifetime. Her shyness was well known; she was widely considered strikingly beautiful and charming, but those around her would often avoid eye contact so as not to embarrass her. Albert was devoted to his wife, and commissioned the painting after being impressed by Ingres' 1845 portrait of his sister, the Comtesse d'Haussonville.
Albert approached Ingres around 1850 to undertake the portrait. Ingres dined with the de Broglie family in January 1850, and according to one eye witness, "seemed to be very happy with his model."

Although Ingres' main source of income came from portraiture, it distracted from his main interest in history painting, which early in his career, was far less lucrative. He found acclaim in the 1840s, when he became successful enough to no longer depend on commissions. This painting was Ingres' second-last female portrait, and final society portrait.

Influenced by the working methods of Jacques-Louis David, Ingres began with a number of nude preparatory sketches, for which he employed professional models. He built up a picture of the sitter's underlying anatomical structure, as seen in the Musée Bonnat study, before deciding on how to build the lavish costume and accessories. Although there is no surviving record of the commissions, and the exact sequence of events is uncertain, the sketches can be dated from 1850, the year the style of her evening dress came into fashion. Ingres signed and dated the final picture at the left center "J. INGRES. pit 1853".

Pauline died in 1860 aged 35 from tuberculosis. After her death, Albert published three volumes of her essays on religious history. Albert (who in 1873 became the 28th Prime Minister of France) lived until 1901, but was heartbroken and did not remarry. He kept her portrait for the remainder of his life draped in fabric and hidden behind a velvet curtain, only lending it to select exhibitions. After his own death, the painting passed within the family until 1958 when it was sold to the Metropolitan Museum of Art via the banker and art collector Robert Lehman, and is today held in the Lehman Wing. The family kept most of the jewelry and accessories seen in the painting, although the marabou feathers were sold to the Costume Institute of the Metropolitan Museum.

There are comparatively few extant preparatory sketches for the de Broglie painting compared to other of his later period portraits. Ingres' usual technique was to use sketches both to plot the final work and to provide guidance for assistants on whom he relied to paint in the less important passages. Some others may have been lost or destroyed.

The extant sketches date from 1850 to 1853 and are drawn with graphite on paper or tracing paper. They vary in elaboration and detail, but show Ingres thinking through the eventual form and pose of the sitter. The earliest consists of a brief sketch of the princess in a seated pose. There is a full-length study of a nude standing in essentially the final pose, in which Ingres experimented with two different positions of the crossed arms. A second full-length study shows a clothed figure. Two others are focused on her hands. A highly finished drawing of the princess standing with her left hand at the neck and dressed in a simpler costume than in the painting, may be a study for the painting or an independent work. Besides these five or six extant sketches, about the same number are known to be lost.

The painting's central motifs were already established in the earliest studies, in which her oval face, arched eyebrows, and habit of folding her arms with one stuffed into the opposing sleeve appear. Ingres found the sittings difficult and agonised over every detail. He wrote to his friend and patron Charles Marcotte that he was "killing [his] eyes on the background of the Princesse de Broglie, which I am painting at her house, and that helps me advance a great deal; but, alas, how these portraits make me suffer, and this will surely be the last one, excepting, however, the portrait of [his second wife] Delphine."

The Princesse de Broglie is shown in three quarters view, her arms resting on a lavishly upholstered, pale gold damask easy chair. Her head is tilted to the viewer's left, black hair tightly pulled back and bound by blue satin ribbons. She is located in the family home at 90 rue de l'Université in Paris, in an evening dress that implies she is about to depart for the evening. She is dressed in the height of contemporary Parisian fashion, in particular the opulent Second Empire fashions then current in clothing, jewelry and furniture. She wears a gold embroidered evening shawl, and an off-shoulder pale blue satin hoop skirt ball gown, with lace and ribbon trim. Her hair is covered with a sheer frill trimmed with matching blue ribbon knots.

Her adornments include a necklace, tasseled earrings and bracelets on each wrist. Her pendant with cross pattée signifies her piety and was perhaps designed by Fortunato Pio Castellani or Mellerio dits Meller. Her earrings are made from cascades of small natural pearls. Her left wrist contains a bracelet of roped pearls; the one on her right is made of red enameled and diamond set gold links. The necklace is held by a double looped chain holding a gold pendant, which appears to be an original Roman bulla. 
As with all of Ingres' female portraits, her body seems to lack a solid bone structure. Her neck is unusually elongated, and her arms seem boneless or dislocated, while her left forearm appears to be under modeled and lacking in musculature. Her oval face and facial expression are idealised, lacking the level of detail given to other foreground elements, although she was widely known as a great beauty.

The painting is composed from grey, white, blue, yellow and gold hues. The costume and decor are painted with a supreme precision, crispness and realism that art historians have compared to Jan van Eyck. In many ways the painting is austere; art historian Robert Rosenblum describes a "glassy chill", and "astonishing chromatic harmonies that, for exquisite, silvery coolness, are perhaps only rivaled by Vermeer." Her facial features are statuesque and in passages display the quality of porcelain. The painting contains a number of pentimenti, including around the contours of her hair, and the yellow chair. The horizontal bands are about 2.5 cm wide, and composed from yellow paint on either side of her head near the earrings. They seem to have been used to plot the positioning of the moldings. The black hat on the chair seems to have been a late addition. There are visible passages of underdrawing where the artist seems to trace out shapes and positions, established in the preparatory sketches, onto the grounded canvas. These include squared lines around the left shoulder and chest areas. There are lines mapping out the throat and top edge of the bodice.
Compared to the "Portrait of Comtesse d'Haussonville", or most of Ingres' later portraits, the background is flat and featureless, probably to place emphasis on the coat of arms. It comprises a neutral soft pale grey and evenly textured wall, with a linear structured gilded wood mouldings, and a fictitious coat of arms combining the heraldics of the de Broglie and de Bearn families. The grey wall is underlined with a barely discernible deep blue pigment. This minimalist approach reflects the "ascetic elegance" of his early female portraits, where the sitter was often set against featureless backdrops. The precisely rendered details and geometric background create an impression of immobility, while subtle movement is implied by the tilt of her head and the shimmering folds of her dress.

The current frame measures 157 x 125.6 cm at the exterior and is made of pink-orange bole pine, lined with a garland of gilt-plastered ornament flowers. Its ornaments lie on ovolo molding. It was produced in the United States between 1950 and 1960 (around the time the Metropolitan acquired the work) in the French Louis XIII style fashionable in Ingres' period. It is similar to, and probably modeled on, the frame used for "Madame Moitessier", which is most likely an original and is dated 1856. The original de Broglie plaster frame was completed at latest c. 1860, and is thought to have been similar to the current.

The painting remained in Ingres' possession until 1854, when it was first exhibited that December in his studio, alongside his unfinished "Madame Moitessier" (c. 1844–56), "Portrait of Lorenzo Bartolini", and c. 1808 "Venus Anadyomene". One critic wrote that the painting showed Pauline as "refined, delicate, elegant to her finger tips...a marvelous incarnation of nobility". In general it is held in the same high regard as his "Comtesse d'Haussonville", and "Portrait of Baronne de Rothschild".

The work was an instant critical and popular success, and widely admired and written about. Most critics understood the artfulness of physical deformations, although one writer, writing under the byline A. de. G., and representing a minority, academic view, describes her as a "puny, wilted, sickly, woman; her thin arms rest on an armchair placed in front of her. M. Ingres has rendered in an unheard-of manner these large, veiled eyes, deprived of sight. He has given this face a negative expression that he must have seen in real life, and reproduced it with a sure touch."

The majority of critics noted Ingres' attention to detail in describing her clothes, accessories and decor, and saw an artist at the height of his creativity, with a few invoking the precision of van Eyck. Some writers detected a hint of melancholy in de Broglie's eyes and expression.



</doc>
<doc id="55459674" url="https://en.wikipedia.org/wiki?curid=55459674" title="2018 Pennsylvania's 18th congressional district special election">
2018 Pennsylvania's 18th congressional district special election

A special election for Pennsylvania's 18th congressional district was held on March 13, 2018, following the resignation of Republican Representative Tim Murphy. Murphy, who held the seat since January 3, 2003, declared his intent to resign on October 5, 2017, and vacated his seat on October 21 that year. Democrat Conor Lamb defeated Republican Rick Saccone 49.9% to 49.5%. Saccone conceded the race eight days after the election.

Eight-term incumbent Republican U.S. Representative Tim Murphy was involved in a sex scandal consisting of his allegedly having extramarital sexual relations with a woman and then asking her to have an abortion. This was particularly damaging because Murphy identifies as a pro-life politician. Murphy subsequently announced that he was resigning and that he would leave office on October 21.

Primary elections were not held in the race. Instead, nominees were chosen by each party. The Republican Party held a special convention on November 11, 2017, to choose a nominee through a conferee process involving 215 local Republican activists. The Democratic Party held their nominating convention on November 19, 2017. The Libertarian Party of Allegheny County and the Libertarian Party of Washington County nominated a candidate via party caucus.

Pennsylvania's 18th Congressional District is located in Western Pennsylvania and borders the state of West Virginia. It includes portions of Greene, Washington, Allegheny and Westmoreland counties. The district has a Cook PVI score of R+11. This was the last election for Pennsylvania's 18th congressional district held under its configuration made in 2011 by the Pennsylvania Legislature as new districts have been drawn in accordance with the ruling of the state supreme court in "League of Women Voters v. Commonwealth of Pennsylvania" and will be in effect for the main 2018 congressional elections in November. The bulk of the old 18th will become the 14th District.






The Libertarian candidate was Pittsburgh attorney Drew Gray Miller.

The major party candidates were Republican Rick Saccone and Democrat Conor Lamb, in a district that Donald Trump carried by almost 20 points in the 2016 presidential election. The special election attracted national attention and was seen by many political analysts and commentators as a bellwether on the popularity of Donald Trump, Trump's tariffs on steel and aluminum imports, and the Republican Party. Saccone said the special election was a referendum on Trump's presidency and called himself "Trump before Trump was Trump". Prominent Republicans including Donald Trump (twice), Donald Trump Jr., Ivanka Trump, and Mike Pence came to the state to campaign for Saccone. The district was contested by a third-party candidate (Libertarian Drew Gray Miller) for the first time since 1996.

Republicans and aligned groups spent more than twice as much as Democrats and aligned groups on the special election. Although Lamb's campaign fund raised more than Saccone's campaign fund, Saccone benefited from far more spending by outside groups than Lamb.

The Republican Party and outside pro-Republican "independent expenditure" groups spent almost $10.7 million to support Saccone or oppose Lamb in the campaign, while $2.6 million in independent expenditures was spent in support of Lamb. Fourteen outside groups (seven Republican, seven Democrat) comprised over 95% of the spending. For the Republican side, the major groups spending money on Saccone's behalf were Paul Ryan's Congressional Leadership Fund, the National Republican Congressional Committee, the Republican National Committee, and the pro-Trump groups America First Action and 45Committee. For the Democratic side, the major groups spending money on Lamb's behalf were the Democratic Congressional Campaign Committee, Patriot Majority PAC, and the VoteVets.org Action Fund.

KDKA-TV hosted the first debate on February 19, 2018. WTAE-TV hosted the second and final debate on March 3, 2018, at 7 PM EST. The debate was hosted live by WTAE-TV along with the League of Women Voters of Greater Pittsburgh. 



</doc>
<doc id="55460657" url="https://en.wikipedia.org/wiki?curid=55460657" title="Chains of Love (TV series)">
Chains of Love (TV series)

Chains of Love is an American dating game show that aired for six episodes in April–May 2001 on the United Paramount Network (UPN). Adapted from a Dutch television series, it revolves around a man or woman being chained to four members of the opposite sex over four days and nights. This person, identified as the "Picker", is given $10,000 and can remove three contestants one at a time. The Picker can give a portion of the money to each eliminated participant. When left with a single partner, the Picker can choose to either split the money or keep it. American television personality Madison Michele hosted each episode.

"Chains of Love" was originally ordered by NBC, before UPN began producing it. The program was produced as part of a campaign to have more unscripted programming in UPN's schedule to boost the network's ratings. Media outlets have identified "Chains of Love" as part of a renaissance in reality television. David Garfinkle, who had previously worked on the show "Blind Date", served as the executive producer.

Before its premiere, UPN had promoted the series through a month-long online campaign aimed at young women. Initially broadcast on Tuesday nights at 8:00 pm EST, the network envisioned the show as a companion piece to the simulated fugitive-chase series "Manhunt". Media outlets questioned whether the show's airing on network television had restricted its content. Critical feedback to "Chains of Love" was mixed, the show's premise dividing television critics. Its structure and tone were compared to other programs where contestants seek love partners, such as "Blind Date" and "The Dating Game". "Chains of Love" has never been released on DVD or Blu-ray, or through online-streaming services.

In each episode, five contestants are taken to a house in Palos Verdes Estates, California. Bound together by a six-foot chain, they are linked by their hands and feet in the "Ritual Room" by the "Lockmaster", played by a muscular man in a suit and sunglasses. The participants are kept less than 24 inches from each other during filming. The chaining of the contestants is not shown on-screen.

The group includes a "Picker" and four potential suitors, referred to as "playmates". The four contestants are selected based on how their answers to questions in an earlier interview matched the qualities the Picker sought in a suitor. Both men and women have been Pickers; the four participants are always the opposite sex of the Picker. The group remains chained together for four days and nights and is taken to complete day-to-day tasks, such as grocery shopping, preparing food, and ice skating. There are certain times when individuals are allowed their privacy, including using the bathroom, bathing, and changing clothing.

At the start of each episode, the Picker is provided with $10,000 to give to the other participants based on his or her personal preferences. The Picker eliminates potential matches, who are then unchained by the Lockmaster. As each contestant leaves, the Picker can decide to give a portion of the money to them. When one contestant remains, the Picker can choose to split the remaining cash with them if they feel that a "love connection" has been formed. The Picker can also choose to keep the money for themselves. The final contestant can refuse to be involved any further in a relationship.

"Chains of Love" was inspired by a television program from the Netherlands. The Dutch company Endemol handled the production, and David Garfinkle acted as the executive producer. Garfinkle had previously worked on the American dating game show "Blind Date". It was also produced by people involved in the development of the reality game show franchise "Big Brother". Garfinkle said that he was not certain how the series would unfold during its broadcast, explaining that the competition among the contestants began to resemble soap operas such as "Melrose Place". American television personality Madison Michele hosted each episode.

"Chains of Love" was initially picked up by NBC, in an agreement with Endemol where the network would produce nine episodes of an unidentified program. NBC eventually decided not to pursue the series due to creative differences with its producer. "Newsweek"'s Marc Peyser believed that NBC dropped out of the project on "moral grounds" due to its premise. The network chose to adapt the Dutch show "Now or Neverland" into "Fear Factor" in place of "Chains of Love". UPN began producing "Chains of Love", along with three other reality television shows, as part of its extensive campaign to air more unscripted content on the network.

"The New York Times"' Bill Carter identified the series as part of a "second wave of reality shows" that started from the success of the reality competition television franchise "Survivor". Carter interpreted new reality programming as "designed to push the envelope of prime-time broadcasting" content. In her 2007 chapter "Models of (Im)perfection", writer Kimberly K. Bell wrote that "Chains of Love" contained a similar production style to "Survivor"; she commented that both shows are set in "elaborately structured playing fields", in which producers edit the contestant's identity to better appeal to an audience.

UPN executives associated the rise in interest in unscripted content as connected to its low production costs compared to scripted programming and its appeal to a younger demographic. Network president Dean Valentine explained: <nowiki>"</nowiki>From a societal view, audiences, especially young people, are finding it harder and harder to relate to fictional storytelling – it just seems fake to them.<nowiki>"</nowiki> The network's entertainment chief Tom Nunan said the series was intended to improve the network's ratings, explaining: "There's a wave of television viewer right now that is very clear to us – it shows that audiences seem to be responding to these event programs that don't feel like cookie-cutter TV."

Interpreting the titular chains as "metaphors for the bonds of human affection", Valentine found the series was primarily an example of physical comedy. He emphasized that the show was not directly related to sadomasochism. Valentine said that the show's appeal would extend beyond sexuality, identifying the contestants as "kind of morph[ing] together [as] they have to deal with acceptance, rejection, fear, need". He explained that the process was "so emotional and stripped down [that] you almost feel like you're watching an est session".

"Chains of Love" was one of three series that debuted on UPN during the 2000–01 US television schedule as a mid-season replacement; the other two were "Special Unit 2" and "All Souls". "Chains of Love" was broadcast initially on Tuesday nights at 8:00 pm EST, premiering on April 17, 2001. UPN picked it up originally as a companion piece to the 2001 "Manhunt", in which pretend fugitives ran away from actors posing as bounty hunters. During its broadcast, "Chains of Love" was briefly paired with "All Souls", which UPN placed on a hiatus after two episodes were aired.

Before the show's debut, the network had conducted a month-long online promotional campaign. The advertisements, specifically aimed at women from the ages of 12 to 34 years old, appeared on the websites Targetmatch.com, Madhive.com, and Ecrush.com. The Marina del Rey-based marketing firm L90, who had previously done work for UPN for the series "Gary & Mike", created the campaign. Lauren Kay, the company's vice president of marketing, said that for the show they put together a "clean simple branding program" using pop-up ads, flash animation, and a sweepstake done through a microsite.

Media outlets questioned whether the show's broadcast on network television had a limited impact on its more mature content. Carman identified a scene as confusing in which the Picker suggested skinny-dipping in a hot tub to the four women, followed by a shot of them appearing in swimsuits. Peyser equated the sexual content with that of "Dawson's Creek", and wrote that it was not as explicit as he had first imagined.

UPN canceled "Chains of Love" after its six-episode season. It has never been released on DVD or Blu-ray, or made available through online-streaming services. Following the show's cancellation, William Shatner and Valentine parodied it during an event at Madison Square Garden, where they appeared chained together.

On its debut, "Chains of Love" received primarily negative feedback from television critics. Equating the concept to "televised prostitution", the "Pittsburgh Post-Gazette"s Rob Owen wrote that the program represented the worst aspects of television, and did not believe it would have even a guilty-pleasure appeal for viewers. A writer for "The Augusta Chronicle" described the series as an embarrassment for the network. In his 2005 book "From Daytime to Primetime: The History of American Television Programs", scholar James W. Brown felt the concept fostered an environment for emotional and physical abuse.

Some commentators had more positive comments for the series. Even though he heavily panned the show for lowering contestants' dignity to attract viewers, the "San Francisco Chronicle" John Carman wrote that he was interested in watching how each of the men would approach the situation differently. "Entertainment Weekly" Dan Snierson described it as "the most bizarre and captivating reality series in TV history" due primarily to its premise.

"Chains of Love" was often compared to other television shows. Due to the series' emphasis on gender relations as part of its game theory, Brown identified "Chains of Love" as a descendant of the programs "The Dating Game" and "Anything Goes". Marc Peyser of "Newsweek" wrote that the producers of the series maintained the same irreverent tone from "Blind Date". Later dating shows, "Elimidate Deluxe" and "Tethered", were described as borrowing elements from "Chains of Love", specifically the concept of having a contestant go on a date with four individuals and binding people together to complete specific tasks.




</doc>
<doc id="55705165" url="https://en.wikipedia.org/wiki?curid=55705165" title="Helicopter 66">
Helicopter 66

Helicopter 66 is a United States Navy Sikorsky Sea King helicopter used during the late 1960s for the water recovery of astronauts during the Apollo program. It has been called "one of the most famous, or at least most iconic, helicopters in history", was the subject of a 1969 song by Manuela and was made into a die-cast model by Dinky Toys. In addition to its work in support of NASA, Helicopter 66 also transported the Shah of Iran during his 1973 visit to the aircraft carrier .

Helicopter 66 was delivered to the U.S. Navy in 1967 and formed part of the inventory of U.S. Navy Helicopter Anti-Submarine Squadron Four for the duration of its active life. Among its pilots during this period was Donald S. Jones, who would go on to command the United States Third Fleet. Later re-numbered Helicopter 740, the aircraft crashed in the Pacific Ocean in 1975 during a training exercise. At the time of its crash, it had logged more than 3,200 hours of service.

Helicopter 66 was a Sikorsky Sea King SH-3D. The SH-3D model Sea Kings were designed for anti-submarine warfare (ASW) and were typically configured to carry a crew of four and up to three passengers. Powered by two General Electric T58-GE-10 turboshaft engines producing up to each, SH-3Ds had a maximum airspeed of and a mission endurance averaging 4.5 hours. They had a maximum allowable weight of with the ability to carry an external payload of up to .

During ASW missions, the Sea King SH-3D was typically armed with MK-46/44 torpedoes.

Helicopter 66 was delivered to the U.S. Navy on March 4, 1967, and, in 1968, was added to the inventory of U.S. Navy Helicopter Anti-Submarine Squadron Four (HS-4). Its original tail number was NT-66/2711.

Activated on June 30, 1952, Squadron Four – "the Black Knights" – was the first anti-submarine warfare helicopter squadron of the U.S. Navy to deploy aboard an aircraft carrier when, in 1953, it operated from . It began using the Sea King SH-3D in 1968, transitioning from the SH-3A model. That year, the squadron was assigned to Carrier Anti-Submarine Air Group 59 and deployed aboard to the Sea of Japan ("East Sea") in response to the capture of by the Korean People's Navy. Later that year, "Yorktown"—and Squadron Four—was tasked to support the National Aeronautics and Space Administration (NASA) in the oceanic recovery of returning astronauts.

During the Apollo 8, Apollo 10, and Apollo 11 missions, Helicopter 66 was the primary recovery vehicle which hoisted returning astronauts from the spacecraft command modules. As a result, it was featured prominently in television news coverage and still photography, achieving—in the words of space historian Dwayne A. Day—the status of "one of the most famous, or at least most iconic, helicopters in history". Commander Donald S. Jones, who would later command the United States Third Fleet, piloted Helicopter 66 during its inaugural astronaut recovery mission following Apollo 8, and again during the Apollo 11 recovery.

Following the Apollo 11 mission, the Navy switched to a three-digit designation system and Helicopter 66 was retagged Helicopter 740. Recognizing the fame Helicopter 66 had achieved, the Navy began the practice of repainting Helicopter 740 as Helicopter 66 for the later recovery missions in which it participated, Apollo 12 and Apollo 13, painting it back as Helicopter 740 at the conclusion of each mission. During the period of its use for astronaut recovery, Helicopter 66 bore victory markings on its fuselage showing a space capsule silhouette, with one being added for each recovery in which it participated. For the recovery of the Apollo 11 astronauts, the underside of the fuselage was emblazoned with the words "Hail, Columbia".

By 1973 Helicopter Squadron Four, and Helicopter 66 with it, were embarked aboard . That year, Helicopter 66 transported the Shah of Iran, Mohammad Reza Pahlavi, to "Kitty Hawk" for a shipboard visit while it transited the Indian Ocean.

At 7:00 p.m. on June 4, 1975, Helicopter 66, renumbered as '740', departed Naval Outlying Landing Field Imperial Beach near San Diego, California, en route to the U.S. Navy's Helo Offshore Training Area to conduct a regularly scheduled, three-hour nighttime anti-submarine training exercise. During the operation, in which it was carrying a full complement of four crew, the helicopter crashed. Though the crew was subsequently rescued by the U.S. Coast Guard, pilot Leo Rolek was critically injured and later died of the wounds he sustained in the crash. The exact cause of the downing of Helicopter 66 is unknown; as of 2017 the U.S. Navy incident report remains largely classified. The broken fuselage of the helicopter later sank in of water. At the time of its crash, Helicopter 66 had flown 3,245.2 flight hours since being brought into service, and 183.6 hours since its last overhaul.

The submerged helicopter remains the property of the U.S. Navy, and a 2004 effort by private interests to recover it for preservation was not realized.

A painting of Helicopter 66 was commissioned in 1969 from artist Tom O'Hara as part of a NASA art initiative. It was subsequently placed in the custody of the National Air and Space Museum.

In September 1969 German singer Manuela released a single titled "Helicopter U.S. Navy 66" which features the sound of helicopter rotors. The song was covered the next year by the Belgian pop singer Samantha, and was credited with helping launch her career. In a 2007 interview, the popularity of "Helicopter U.S. Navy 66" as a closing song at dance clubs in 1970s Belgium was cited by the Belgian Schlager vocalist Laura Lynn as the inspiration for her hit "Goud".

During the early 1970s Dinky Toys released a die-cast model of a Sea King helicopter in Helicopter 66 livery. The model included a working winch which could lift a plastic space capsule toy.

Replicas of Helicopter 66 are on display at the Evergreen Aviation & Space Museum in Oregon, the USS "Midway" Museum in San Diego, and the USS "Hornet" Museum in Alameda, California. The helicopter at the USS "Hornet" Museum is a retired Navy Sikorsky Sea King that was used in filming the motion picture "Apollo 13".



</doc>
<doc id="55706953" url="https://en.wikipedia.org/wiki?curid=55706953" title="Second Australian Imperial Force in the United Kingdom">
Second Australian Imperial Force in the United Kingdom

Elements of the Second Australian Imperial Force (AIF) were located in the United Kingdom (UK) throughout World War II. For most of the war these comprised only a small number of liaison officers. However, between June and December 1940 around 8,000 Australian soldiers organised into two infantry brigades and supporting units were stationed in the country. Several small engineer units were also sent to the UK, and up to 600 forestry troops were active there between July 1940 and mid-1943. A prisoner of war (POW) repatriation unit arrived in the UK in August 1944, and over 5,600 released AIF prisoners eventually passed through the country. Following the war small numbers of Australian soldiers formed part of a military cricket team which toured England, and the Army contributed most members of the Australian contingent to the June 1946 victory parade in London.

Although the UK had accommodated the main rear base for the First Australian Imperial Force during most of World War I, the deployment during 1940 was the only time significant numbers of Australian combat soldiers were stationed in the country during World War II. These soldiers arrived in mid-June on a convoy which had been diverted from its original destination in the Middle East. During the Battle of Britain the Australian force formed part of the mobile reserve which would have counter-attacked any German amphibious or airborne landings in southern England. The Australians were moved to eastern England in October 1940, and departed the country in several convoys between mid-November 1940 and January 1941 to concentrate the AIF in the Middle East.

In August 1944, AIF personnel arrived in the UK and established facilities to accommodate and support the thousands of Australian POWs held by Germany once they were released. Significant numbers of released AIF POWs arrived in the UK during the last weeks of the war in Europe and after the German surrender in May 1945. After being granted a period of leave, these men were accommodated at reception camps located in and around the coastal town of Eastbourne until they could be repatriated to Australia. Almost all of the released prisoners had departed the UK by August 1945.

At the time of the two world wars, Australia and the United Kingdom (UK) had a very close relationship. Australia was a self-governing dominion within the British Empire, and its foreign and defence policies were strongly influenced by those of the British Government. Most Australians were either descended from Britons or had been born in the UK, leading to a widespread perception that Britain was the "Mother Country".

The UK accommodated the rear base for the First Australian Imperial Force for most of World War I. The AIF had been formed in 1914 and comprised soldiers who had volunteered for service outside Australia. From 1916 until the end of the war in 1918, the majority of the AIF fought on the Western Front in France. The force's administrative headquarters was in London, and large numbers of Australian training, medical and other support facilities were in the UK. Australian soldiers also frequently took leave in the country. As a result, virtually all members of the AIF who served in France passed through the UK. Historian Roger Beckett has written that "from summer 1916 to the end of the war there were never fewer than 50,000 Australian troops in Britain". Following the war's conclusion in November 1918, all AIF personnel in France were gradually transferred to the UK, from where they embarked on ships bound for Australia. In May 1919, the remaining elements of the AIF in France were moved to the UK, leading to the number of Australians there peaking at 70,000. By the end of 1919, almost all AIF personnel had departed the UK, and the force was formally disbanded on 1 April 1921.

Following the outbreak of World War II, the Australian Government established a Second Australian Imperial Force on 15 September 1939. Volunteers for the AIF were liable for service in Australia or overseas. Four AIF infantry divisions were eventually established, of which three were deployed to the Middle East and one to Malaya between 1940 and 1941. 

Elements of the Royal Australian Air Force (RAAF) and Royal Australian Navy (RAN) were stationed in the UK during World War II. Several flying squadrons and more than 10,000 RAAF airmen who had been trained under the Empire Air Training Scheme served in the country. A small number of RAN warships operated from the UK at various times between 1940 and 1942, and hundreds of Australians who had been posted to the Royal Navy were based in the country.

The Australian Army, RAN and RAAF had liaison teams in London throughout the war. These officers were based at the High Commission of Australia in London. This continued pre-war arrangements, which included an Australian representative to the British War Office and the Imperial General Staff as well as advisers to the High Commissioner. Lieutenant General Edward Smart was appointed the head of the Australian Military Mission to the UK in October 1942. His role included representing Australia on senior decision-making bodies located in the UK. An Army officer also served on the staff of Australia's representative to the British War Cabinet. In March 1945 they were joined by RAN and RAAF officers. Smart remained the head of the Australian Army Staff (UK) until his retirement in July 1946.

The Army's medical branch also maintained specialist liaison staff in the UK. Major General Rupert Downes, who as the Director General of Medical Services was the Army's senior medical officer, arrived in the country shortly before the outbreak of war in 1939. During his visit to London, he secured the services of two eminent Australian physicians, the surgeon Sir Thomas Dunhill and Neil Hamilton Fairley, to serve as technical advisers to the Army. These two men provided valuable advice to the Australian Army during the early period of the war, including by passing on copies of medical literature and personal accounts of surgery and treatments. Dunhill also mentored Australian surgeons who were either working in the UK or were sent there during the war to gain experience. Shortly after the outbreak of war in September 1939, a medical liaison officer was posted to the High Commission. This officer reported on new developments in medical treatments, and helped to facilitate orders of medical equipment for the Army.

Other Australian soldiers served in specialist and staff roles in the UK. For example, Major General Sydney Rowell was the Director of Tactical Investigation at the War Office from March 1944 until the end of the war. In April 1944, two majors from the Army's Directorate of Research, W.E.H Stanner and John Kerr, were also temporarily attached to the Australian Army Staff in London. These officers remained in the UK for eight months, during which time they reported on British civil affairs practices, post-war planning and the War Office's financial and research arrangements.

In addition to their military functions, the AIF personnel in the UK publicised the force's achievements. In April 1944, the Department of Information and the Australian Army Staff in London jointly arranged for a series of pamphlets describing the AIF's campaigns to be distributed in the UK by the British Army Bureau of Current Affairs. In November that year, 250,000 copies of the booklet "The Australian Army at War" were printed by His Majesty's Stationery Office on behalf of the Australian Army Staff and distributed in the UK.

In November 1939, the Australian War Cabinet agreed to a request from the British Government for the 6th Division, the AIF's first combat formation, to be sent from Australia to the Middle East to complete its training. If the security situation in the Middle East permitted, it was intended to transfer the AIF to either the UK or France once its training was complete to receive its equipment. When fully trained and equipped, the AIF was to become part of the British Expeditionary Force (BEF) in France.

The first of the division's three brigades to deploy, the 16th, sailed from Fremantle, Western Australia on 20 January 1940 and arrived in Egypt on 12 February. In late April, the British Government became concerned that Italy was about to enter the war alongside Germany following the successful German invasion of Norway. At this time, the two convoys transporting the 6th Division's other brigades to the Middle East were en route. Convoy US 2, which was transporting the 17th Brigade, was in the Indian Ocean and Convoy US 3 was off the Australian coast. Convoy US 3 was carrying 8,000 Australian soldiers assigned to the 18th Brigade and other units as well as 6,000 New Zealanders. 
On 1 May, Anthony Eden, the British Secretary of State for Dominion Affairs, sent the Australian Government a cable proposing that both convoys be diverted to the UK. Eden justified his proposal on the grounds of the undesirability of sending convoys past Italian naval bases in the Red Sea due to the risk of attack, and the possibility of it becoming difficult to send supplies from the UK to equip units in the Middle East. In response, the Australian Government directed that Convoy US 2 be held at Colombo and US 3 at Fremantle until it received advice on the war situation from the British and Australian Chiefs of Staff. The government had a strong preference for the AIF to be concentrated in a single location, so it could fight under Australian command. The Australian Chiefs of Staff initially supported the proposal to send both convoys to the UK on the grounds that it would result in the brigades being deployed to the main theatre of war, would ease the problems associated with equipping these units and would encourage volunteers for another AIF division which was being formed at the time. However, on 4 May the British Chiefs of Staff recommended that both convoys proceed to the Middle East. The Australian Chiefs of Staff subsequently endorsed this position. As a result, on 8 May the Australian Government directed the convoys to continue to the Middle East. Convoy US 2 arrived in Egypt on 18 May.

Convoy US 3's destination changed following the German invasion of France on 10 May 1940, which further increased the probability that Italy would enter the war. On 15 May, the British Government proposed again that Convoy US 3 be diverted to the UK. The Australian Chiefs of Staff endorsed this proposal, but the War Cabinet remained reluctant to split the AIF. Instead, the War Cabinet asked whether the troops could be sent to either South Africa or North-West India to complete their training before joining the 6th Division in Egypt. Several ministers believed that the British Government was seeking to divide the AIF into separate elements which would operate under direct British command. After being advised by the British Government that accommodation and equipment for the troops were not likely to be available in India or South Africa, the War Cabinet agreed on 19 May for Convoy US 3 to proceed to the UK. At this time the Allied forces in France were rapidly retreating. The New Zealand Government had agreed in April for its soldiers on Convoy US 3 to be sent to the UK in the event that the ships were unable to enter the Red Sea.

On 4 June, the Chief of the Australian General Staff, General Brudenell White, recommended to the War Cabinet that the 6th Division be immediately transferred from the Middle East to Marseilles in France to complete its preparations for combat before entering the fighting there. Most elements of the BEF were being evacuated from Dunkirk at this time, but the British Government intended to reconstitute it in France. The 6th Division was not combat-ready, as it still had not completed training or received key equipment. The War Cabinet rejected White's proposal, and agreed to remind the British Government of its previous assurances that the AIF would be concentrated in a single location. A proposal made in late June by the British Medical Directorate for Australian soldiers who were wounded in the Middle East to be sent to the UK to recover for as long as the Mediterranean remained open to Allied shipping was also rejected. 

Convoy US 3 arrived at Gourock in Scotland on 17 June. The reception party which greeted the troops was led by the First Lord of the Admiralty, Albert Alexander, and Under Secretary of State for the Dominions, Geoffrey Shakespeare. Shakespeare delivered a welcome message from King George VI. The combat units which arrived with the convoy included the 18th Brigade, which was made up of the 2/9th, 2/10th, and 2/12th Battalions. The force also included some of the 6th Division's combat support units, including the 2/3rd Field Regiment, 2/1st Anti-Tank Regiment, 2/1st Machine Gun Battalion, 2/3rd Field Company and the 2/1st Field Park Company. Most of the 6th Division's signals, Army Service Corps and ordnance personnel as well as 459 infantrymen who had been dispatched as reinforcements had travelled on the convoy. As well as medical personnel assigned to individual units, the force included the medical staff of the 2/3rd Field Ambulance and 3rd Australian Special Hospital and 77 female members of the Australian Army Nursing Service. 

The force was commanded by Major General Henry Wynter. This officer had embarked on the convoy to take up a position with the I Corps headquarters, and been both promoted and appointed to the role on 14 June. The commander of the 18th Brigade, Brigadier Leslie Morshead, had previously been the senior officer for the AIF units on Convoy US 3. Wynter's command was designated "Australforce" after its arrival in the UK.

Immediately after disembarking, Australforce proceeded to Salisbury Plain in southern England to prepare to resist what seemed like an imminent German invasion. Its lead elements arrived there on 18 June. Most elements of Australforce were housed in tents near Tidworth. Wynter's AIF Administrative Headquarters was located at Amesbury Abbey and the 18th Brigade's headquarters was at Lopcombe Corner. On the night of 19/20 June, the Australian soldiers observed an air raid on the nearby city of Southampton from their accommodation.

Reorganising Australforce was an early priority for Wynter. Official historian Gavin Long described the force which arrived on Convoy US 3 as a "hodge-podge" of units, and noted that none of its elements had their full allocation of equipment. The infantry was armed only with rifles and machine guns, the artillery had no guns and the numbers of vehicles and supplies of technical equipment fell short of requirements. After considering options developed by Morshead to form the force into a strong brigade group with supporting units, Wynter instead chose to organise it into two infantry brigades (the 18th and the new 25th), each with three battalions. These brigades were to be supported by a machine gun battalion, two field batteries, two anti-tank batteries, two companies of engineers and support units. The 25th Brigade's three new infantry battalions were to be manned by the 459 infantry replacements and 1,300 soldiers transferred from the artillery, machine gun and support units. A small number of officers and enlisted men were also to be transferred from the 18th Brigade. Due to manpower shortages, the new battalions would be organised into three rifle companies rather than the standard four. Wynter selected this organisation as he expected that artillery guns and technical equipment would take a long time to be delivered, and there was a high probability that his force would need to go into combat at short notice with what it had. The British War Office approved this structure by 22 June. 

Wynter believed that it would take about a month for the 25th Brigade to be "in reasonable shape". He appointed Colonel William Bridgeford, who had previously been a liaison officer at the High Commission of Australia, to command the formation. The new infantry battalions were initially numbered the 2/28th, 2/29th and 2/30th Battalions, but were redesignated the 70th to 72nd Battalions after it was learned that these names had already been allocated to units which were to be formed in Australia. The three battalions' titles were selected to continue the numbering assigned to infantry battalions of the First AIF, which had reached the 69th Battalion. Many of the technical personnel who were transferred to the 25th Brigade's infantry battalions were unhappy, as they believed this misused their specialist skills.

Australforce's medical units were also reorganised to provide adequate support for the soldiers. The two brigades required greater medical services than the initial structure of medical units could provide, leading to the 2/3rd Field Ambulance being split in half to establish the 2/11th Field Ambulance. One of these units was allocated to each brigade. Establishing a hospital to provide medical care for the force was a priority, but it proved difficult to find a suitable facility in which an Australian hospital could operate independently of British units. Eventually a wing containing 360 beds at the King George V Sanitorium in Godalming was allocated to the AIF, and the 2/3rd Australian General Hospital was established on 30 July to operate it. This unit absorbed the 3rd Special Hospital, and several medical officers from other units were transferred to it. The 2/3rd Australian General Hospital provided a wide range of services, but lacked specialists in X-rays, eye conditions and anaesthesia. Australforce had more than enough nurses to staff the facility, and those who were not needed there were posted to nearby British hospitals. Due to shortages of Australian orderlies, local women were hired to provide domestic services.

Detention facilities were established in the UK. Prior to Australforce's arrival, it had been intended to use British Army facilities to hold Australian offenders. However, these facilities were found to be overcrowded and unable to accommodate many Australians. After a period in which offenders other than those serving long sentences were held by their units, the AIF (UK) Detention Camp was established on 7 August near Salisbury. It soon proved to be too small, and at times soldiers under sentence had to wait their turn to be imprisoned. Offenders serving lengthy sentences, including three soldiers convicted of mutiny on 29 October, were held in civil prisons. 

Throughout its time in the UK, Australforce undertook training and sought to improve its equipment holdings. An infantry leaders school was established in late June, and several British non-commissioned officers were posted to it as instructors. An important function of this school was to train the artillerymen, engineers and other specialists who had been transferred into the new infantry battalions. Australian gunners also undertook courses at a British artillery school at Larkhill. Equipment was slow to arrive, as Britain had used most of its stocks to equip the BEF and now needed to re-equip its surviving formations. The infantry battalions only reached their training stocks of weapons in late July, and these fell well short of the full combat allocations. On 4 July, King George VI inspected Australforce and observed training exercises. British Prime Minister Winston Churchill also visited the force on 4 September.

Despite the poor state of Australforce's equipment, on 26 July it was assigned a key role in British counter-invasion plans. The 18th Brigade and the force's machine gun and artillery units were selected as the Southern Command Striking Force. In the event of invasion, the Australian units would be augmented by a British field artillery battery. Two or three other mobile striking columns were also to be formed from British troops located in the Salisbury Plain area. The Australforce's headquarters would have commanded all of these units. If a mobile striking force was not considered necessary, the 18th Brigade and other units were to form the reserve for V Corps. To ensure that they could rapidly respond to an invasion, trucks carrying live ammunition accompanied all elements of the brigade when they left camp to undertake training exercises. As of July, Wynter believed that the 25th Brigade was capable of only local defence duties in the Salisbury Plain area.

Australforce remained on alert throughout the peak of the Battle of Britain. On 13 July, a German bomber machine gunned the 2/9th and 2/10th Battalions' accommodation, wounding a soldier. Long states that this man was probably the AIF's first combat casualty. The encampments on Salisbury Plain were repeatedly bombed during August, and Australian positions were struck on three consecutive days. These attacks did little damage, however. The Australian soldiers also often observed British and German aircraft dogfighting. They fired on German aircraft which came within range of their guns, though as little training in aircraft recognition had been provided some Royal Air Force machines were mistakenly attacked. On 7 September, the Germans mounted the first major air raid on London, leading the British command to issue warnings that an invasion was imminent. As a result, the Australian mobile units were placed on one hour's notice to move. The 18th Brigade was assigned the role of countering a German paratrooper landing on Salisbury Plain throughout the crisis period in September. Morshead believed that a German invasion was inevitable. Australforce and the other mobile units in the UK were directed to stand down on 23 September, after the threat of invasion had passed.

While the Australian units were mainly focused on preparing for combat, the soldiers were able to take leave. Soldiers began to be granted 36 hour periods of leave from 27 June, but no more than 10 percent of each infantry battalion could be absent at any time. Many took the opportunity to visit London, where accommodation was made available in Westminster Hall. From 21 August, the maximum allowable period of leave was extended to six days, and up to 15 percent of battalions could be absent. Those taking leave were issued with free rail passes for travel within England. The Australian High Commission did not provide any facilities for soldiers on leave at Australia House in London, its main offices. The Agents-General for individual Australian states in London offered hospitality to the troops, and facilities for them were established at the Strand Theatre and the Royal Empire Society. After being criticised for not assisting soldiers, the High Commission established the Australian Forces Centre at Australia House. This was expanded to the Boomerang Club in 1942 which provided a comprehensive range of services to members of the Australian military. These services included recreation facilities, cheap meals, advice on accommodation, a bookstall and stocks of military kit.

The Australian Government remained committed to concentrating the AIF in the Middle East, and discussions of options to transfer Australforce there commenced in August 1940. On 23 August, Wynter directed that the 2/3rd Field Regiment and 2/1st Anti-Tank Regiment be re-established as part of preparations for this move. At about this time, the Army Headquarters in Australia decided to retain the 25th Brigade as a permanent element of the AIF. All unit commanding officers were advised on 28 August that Australforce was under orders to transfer to the Middle East, and were directed to have their units ready to depart within 24 hours by 15 September. On 15 September, the British Government advised the Australian Government that Australforce would be dispatched to the Middle East, but British units were substituted for the Australians on 21 September. The Australian Government did not object to this delay in transferring the AIF. On 29 September, Army Headquarters advised Wynter that Australforce was to be used as the nucleus for a new 9th Division. 
On 16 October, Australforce was transferred to Colchester to the east of London where it became part of Eastern Command. The 18th Brigade was assigned to the town's garrison, and the 25th Brigade became part of XI Corps mobile reserves. Due to the Australian soldiers' reputation for drunkenness and disorder, the Colchester town council considered introducing restricted trading arrangements for the town's hotel. Morshead was concerned about the possible loss of recreational opportunities for his men, and informed Colchester's mayor that if this occurred "he wouldn't answer for the consequences". However, Morshead also expected the 18th Brigade's soldiers to behave responsibly while off-duty. As part of the redeployment of Australforce, the AIF (UK) Detention Camp moved to Colchester on 23 October. At its peak size on 5 December 1940, 107 soldiers were held there. Between 8 August and 7 December, 542 soldiers served sentences at the AIF (UK) Detention Camp. The average daily number of prisoners was 75.

Wynter was appointed to command the 9th Division on 23 October. At this time he was on two weeks' sick leave, and Morshead was the acting commander. On 30 October, King George VI inspected Australian troops again at Colchester. The 25th Brigade's infantry battalions were also renumbered again during October, with the 70th Battalion becoming the 2/31st, the 71st the 2/32nd and the 72nd the 2/33rd.

The force finally left the UK between mid-November 1940 and early January 1941. The 18th Brigade embarked onto ships at Glasgow on 15 November, and reached Alexandria in Egypt on 31 December. The 25th Brigade remained at Colchester for several more weeks, and embarked on 3 January 1941. It reached Palestine on 10 March. The 2/3rd Australian General Hospital remained in the UK until mid-March to enable all of the sick or chronically unwell soldiers to be collected; it departed on 17 March. As part of a reorganisation of the AIF, the 18th and 25th Brigades were re-assigned to the 7th Division following their arrival in the Middle East, and remained with this formation for the rest of the war.

Following the departure of Australforce, a small AIF administrative section remained in the UK as part of the military liaison staff at the High Commission of Australia. The small numbers of soldiers who were serving lengthy periods of imprisonment in British Army detention facilities or civil prisons also remained in the country until they completed their sentence. 

In 1944, 13 members of the AIF were posted to the UK to gain experience in planning and conducting large-scale amphibious operations as part of an effort to improve the army's procedures ahead of Australian landings in the Pacific. The team was made up of officers representing each of the Army's corps, and included some of the most talented and experienced members of the service. Most of these men subsequently served in combat with British units in Western Europe.

In 1939, the British Government requested that Australia raise three 200-man strong companies of foresters as part of an intended force of thirty such units drawn from Canada, New Zealand and the UK which was to support the BEF in France. The Australian Government agreed. The three forestry companies were classed as engineer units. In line with a request from the French Government, all of their officers were members of either the Commonwealth or State government forest services or employed in the sawmilling industry. The enlisted soldiers were also highly skilled forestry workers.
The 1st and 2nd Forestry Companies were formed in February 1940, and arrived in the UK during July that year. Upon arrival, they undertook military training in southern England. Both units began cutting timber in Northumberland during September 1940. The Australians experienced difficulty working in the cold climate, and many fell sick over the winter of 1940/41. They also needed to adapt their methods to those used in the UK. The 3rd Forestry Company arrived in early 1941. In July that year, the Australian Forestry Group UK was established to command the three units. It was led by Lieutenant-Colonel C.R. Cole throughout its existence. A medical officer was attached to the group to supervise the foresters' medical treatment. All three companies were relocated to Dumfriesshire in Scotland during 1941.

The foresters had closer and more frequent interactions with British civilians than the AIF infantry units. They were initially billeted with civilians in private homes, and later usually accommodated in camps located near villages. The foresters were granted local leave most days, which allowed them to drink in pubs after completing their work. By the time the Forestry Group returned to Australia, 120 of its men had married British women and 40 children had been born. 

To maximise the Australian foresters' productivity, less skilled forestry workers from Honduras and Italian prisoners of war (POWs) were placed under their control to undertake unskilled work. As well as working with timber, the forestry companies also maintained their military skills. They undertook military training for one day each week and a fortnight every six months. The companies were allocated roles in British counter-invasion plans. Despite this training, the foresters were not involved in any combat.

The size of the Forestry Group decreased over time due to illness and a shortage of reinforcements. While 30 foresters had been scheduled to be sent to the UK as reinforcements every six months, only two such parties were dispatched. The Forestry Group was informed in August 1942 that no further reinforcements would be sent. By June the next year the Forestry Group was 29 men under-strength. Due to the lack of reinforcements, in April 1943 Smart proposed to Army Headquarters that one of the forestry companies be disbanded and its personnel used to reinforce the others. This proposal was rejected.

In mid-1943 the Australian Government asked the British Government to release the Forestry Group so that it could be redeployed to New Guinea. The British Government agreed, but asked that the 1st Forestry Company remain in Scotland for a further two to three months; the Australian Government endorsed this request. The Australian Forestry Group moved to Sussex at around the same time. The Group departed the UK for Australia on 22 September 1943, but the wives of the men who had married in the UK were unable to accompany them until August 1944 due to a lack of shipping. By the time they left the UK, the Australians had produced 30 million super feet of sawn timber. The foresters paraded through New York City in September 1943 while en route to Australia. Following their return to Australia in November, the forestry companies were deployed to the Northern Territory and New Guinea where they operated sawmills.

An Australian Railway Construction and Maintenance Group also served in the UK. This formation arrived at Liverpool on 17 July 1940 and comprised a company and half of personnel as well as a group headquarters. It was based at a camp in the Woolmer Forest in southern England, where it built several large storage sidings and conducted other work. The Railway Construction and Maintenance Group departed the UK in January 1941 and arrived in the Middle East during March that year.

Between 1940 and 1942, 7,115 members of the AIF were taken prisoner by Axis forces in North Africa, Greece and Crete. The great majority of these men remained prisoners of war in Germany until the end of the war, and the conditions in which they were held deteriorated considerably over 1944 and early 1945. In January 1945, approximately 5,300 Australian soldiers remained in German custody.

The first former POWs to arrive in the UK were escapees from Axis custody. In November 1942 several such men were attached to the Forestry Group to recuperate. When the Forestry Group was preparing to leave the UK, volunteers were called for to establish a POW reception team. Three officers and twelve enlisted men remained in the UK for this purpose.

In October 1943, 28 Australian soldiers were repatriated from German captivity via the UK as part of a POW exchange. The men disembarked in Scotland on 28 October, where they were greeted by two members of the Australian Army Staff in London and Australian Red Cross representatives. The British Army was responsible for assisting these repatriated Australian POWs in the days immediately after they arrived in the UK. The soldiers then received support from the Australian High Commission until they departed for Australia.

As the war in Europe neared its conclusion, the Allies began preparations to repatriate prisoners of war held in German custody. A multi-national Prisoner of War Executive (PWX) was established within the Supreme Headquarters Allied Expeditionary Force to coordinate these efforts, and contact officers were selected to take charge of POWs after they were liberated. Following a request from the British War Office, Australia posted 18 military personnel to Europe to assist with POW repatriation. Of these, 13 were AIF members and the remainder RAAF airmen. The senior liaison officer, Lieutenant-Colonel J. S. Smith, and an RAAF officer were assigned to the PWX. All of the other personnel were posted to units in the field.

Facilities were also established in the UK to accommodate and support released POWs until they could be repatriated to Australia. The 1st AIF Reception Group (United Kingdom) was established in Australia under the command of Brigadier Eugene Gorman during June 1944. It arrived in the UK in August. The Reception Group was located at the resort town of Eastbourne on the coast of Sussex, which had comfortable accommodation and good recreation facilities. A transit camp and four reception camps for AIF personnel were established in the town. Several specialist services were attached to the 1st AIF Reception Group; these included a dental unit, a pay office, a provost platoon and a postal unit. Detachments from the Australian Canteen Services and the Australian Red Cross were also attached. Official war artist Stella Bowen was assigned to one of the reception camps to record the former POWs' experiences. The RAN and RAAF established similar facilities for their personnel; the RAAF's was located at Brighton..
Due to the poor health of most POWs, Allied policy was to rapidly transport liberated POWs to the UK, usually by air. Few prisoners were freed until April 1945, however. By this time, the 1st AIF Reception Group had handled 209 Australians released during two POW exchanges, as well as small numbers of soldiers who arrived via Italy, Switzerland and the Soviet Union. From 4 April, the AIF Reception Group generally received 30 released POWs per day. This rose to about 100 per day from 20 April, and then gradually increased. After the German surrender on 8 May large numbers of released POWs arrived. The AIF Reception Group handled over 1,000 during the week ending 15 May. The number of released POWs arriving in the UK decreased over the remainder of May, and the last arrived in June. Overall, the AIF Reception Group handled 5,668 released POWs.

Upon arrival in the UK, the released AIF prisoners of war were either sent to a hospital if they required medical attention or proceeded directly to the AIF Transit Camp at Eastbourne. Hospitalised soldiers were also posted to Eastbourne upon release. Upon arrival at Eastbourne, the soldiers were provided with a meal, accommodation, a uniform and an advance on their pay, as well as a "welcome package" from the Red Cross. They were also allowed to send a cable to Australia free of charge. The soldiers received medical checks and more than double the usual rations. After completing the reception process at Eastbourne, the former POWs were granted 14 days' leave and given free rail passes. When they returned from leave the soldiers were allocated to a reception camp, generally with others from their home state. The former POWs were provided with entertainment, and many also undertook education courses.

The released POWs were repatriated to Australia as quickly as was possible. While it had been feared that the former POWs would have to remain in the UK for up to six months due to a shortage of shipping, it proved possible to dispatch them to Australia at regular intervals. A total of 1,600 departed the UK in May 1945, followed by an equivalent number the next month, almost 2,000 in July and approximately 600 in August. By this time, only a small number of released POWs remained in the UK, comprising men undertaking training and those who had chosen to be discharged there. On average, released POWs needed to wait for three weeks until they embarked for Australia, though many endured waits of two to three months. Most elements of the 1st AIF Reception Group departed the UK in August 1945, and its remaining elements were absorbed into the Australian Army Staff (UK) the next month.

A small number of AIF personnel were involved in the Victory Tests series of cricket matches played between the Australian Services cricket team and an English team from 19 May to 22 August 1945. Several members of the team, including its captain Warrant Officer Lindsay Hassett, were seconded to the AIF Reception Group (United Kingdom) and released POWs attended matches.

In March 1946, the Australian Government decided to dispatch 250 members of the military to the UK to take part in the London Victory Celebrations. Of these places, 159 were allocated to the Army, and Major General Kenneth Eather was appointed the contingent's commanding officer. Volunteers were called for, and the selection criteria favoured those who were in very good health, had a distinguished service record and had served outside Australia during the war. Those selected included three soldiers who had been awarded the Victoria Cross and several members of the Australian Women's Army Service. 

The Australian Victory Contingent reached the UK on 30 May 1946. The male members of the contingent were accommodated alongside most of the other national contingents at a tented camp in Kensington Gardens in central London. The Australians took part in the victory parade through London on 8 June which was viewed by more than 5 million civilians. Similar parades were held in the Australian state and territory capital cities on 10 June. During their time in the UK, members of the Victory Contingent were encouraged to interact with civilians and be prepared to answer questions about Australia from Britons who were considering emigrating. 

Following the victory parade, the contingent was granted an extensive period of leave. All personnel were provided with rail passes entitling them to free travel in England and Scotland. Many members of the contingent accepted offers made by British civilians to billet them in their homes, and others toured Europe. Most of the Victory Contingent departed the UK in late June, though two of the Victoria Cross recipients—Private Richard Kelliher and Sergeant Reginald Rattey—remained in London to receive their medals from King George VI during an investiture ceremony on 9 July.

A total of 33 members of the Second AIF are buried in or commemorated at graveyards administered by the Commonwealth War Graves Commission in the United Kingdom. The majority of these men, 18 soldiers, were members of Australforce who died during 1940. The Australian Forestry Group UK suffered five fatalities, and the other soldiers were members of a range of units.




</doc>
<doc id="55760582" url="https://en.wikipedia.org/wiki?curid=55760582" title="History of aluminium">
History of aluminium

Aluminium or aluminum is a chemical element with symbol Al and atomic number 13. At standard conditions, aluminium forms a bright silvery metal; this metal is unusually light and resistant against corrosion. Chemically, aluminium is a main-group element that normally assumes the +3 oxidation state. Aluminium is the third most abundant element in the Earth's crust; as such, it is widespread in human-related activities. Aluminium is produced in tens of millions of metric tons; the metal is commonly alloyed to improve some characteristics, such as hardness. Aluminium has no biological role but is not particularly toxic.

The aluminium compound alum has been known since the 5th century BCE and was extensively used by the ancients for dyeing and city defense; during the Middle Ages, the former use made alum a subject of international commerce. Scientists of the Renaissance believed alum was a salt of a new earth; during the Age of Enlightenment, it was established that the earth was an oxide of a new metal. Discovery of this metal was announced in 1825 by Danish physicist Hans Christian Ørsted, whose work was extended by German chemist Friedrich Wöhler.

Aluminium was difficult to refine and thus uncommon in actual usage. Soon after its discovery, the price of aluminium exceeded that of gold and was only reduced after the initiation of the first industrial production by French chemist Henri Étienne Sainte-Claire Deville in 1856. Aluminium became much more available to the general public with the Hall–Héroult process independently developed by French engineer Paul Héroult and American engineer Charles Martin Hall in 1886 and the Bayer process developed by Austrian chemist Carl Joseph Bayer in 1889. These processes have been used for aluminium production up to the present.

Introduction of these methods to mass production of aluminium led to the extensive use of the metal in industry and everyday lives. Aluminium has been used in aviation, engineering, construction, and packaging thanks to its lightness and resistance against corrosion. Its production grew exponentially in the 20th century and it became an exchange commodity in the 1970s. In 1900, production was 6,800 metric tons; in 2015, it was 57,500,000 tons.

The history of aluminium was shaped by the usage of its compound alum. The first written record of alum was made in the 5th century BCE by Greek historian Herodotus. The ancients used alum as a dyeing mordant, in medicine, as a fire-resistant coating for wood, and in chemical milling. Aluminium metal was unknown. Roman writer Petronius mentioned in his novel "Satiricon" that an unusual glass had been presented to the emperor: after it was thrown on the pavement, it did not break but only deformed and it was brought to the former shape with a hammer. After learning from the inventor that nobody else knew how to produce this material, the emperor had the inventor executed so that it does not diminish the price of gold. Variations of this story were briefly mentioned in "Natural History" by Roman historian Pliny the Elder (who noted the story had "been current through frequent repetition rather than authentic") and "Roman History" by Roman historian Cassius Dio. Some sources suggest this glass could be aluminium. It is possible that aluminium-containing alloys were produced in China during the reign of the first Jin dynasty (265–420).

After the Crusades, alum was a subject of international commerce; it was indispensable in the European fabric industry. Small alum mines were worked in Catholic Europe but most alum came from the Middle East. Alum continued to be traded through the Mediterranean Sea until the mid-15th century, when the Ottomans greatly raised export taxes. In a few years, alum was discovered in great abundance in Italy and the Pope forbade all imports from the east in two years and used profits from alum trade to start a war with the Ottomans. This newly found alum long played an important role in European pharmacy but the high prices set by the papal government eventually made other states start their own production; large-scale alum mining came to other regions of Europe in the 16th century.

At the start of the Renaissance, the nature of alum remained unknown. Around 1530, Swiss physician Paracelsus recognized alum as separate from "vitriole" (sulfates) and suggested that it was a salt of an earth. In 1595, German doctor and chemist Andreas Libavius demonstrated alum and green and blue "vitriole" were formed by the same acid but different earths; for the undiscovered earth that formed alum, he proposed the name "alumina". In 1702, German chemist Georg Ernst Stahl stated the unknown base of alum was akin to lime or chalk; this mistaken view was shared by many scientists for half a century. In 1722, German chemist Friedrich Hoffmann suggested the base of alum was a distinct earth. In 1728, French chemist Étienne Geoffroy Saint-Hilaire claimed alum was formed by an unknown earth and sulfuric acid; he mistakenly believed burning of that earth yielded silica. In 1739, French chemist Jean Gello proved the earth in clay and the earth resulting from the reaction of an alkali on alum were identical. In 1746, German chemist Johann Heinrich Pott showed the precipitate obtained from pouring an alkali into a solution of alum was different from lime and chalk.

In 1754, German chemist Andreas Sigismund Marggraf synthesized the earth of alum by boiling clay in sulfuric acid and adding potash. He realized that adding soda, potash, or an alkali to a solution of the new earth in sulfuric acid yielded alum. He described the earth as alkaline, as he had discovered it dissolved in acids when dried. Marggraf also described salts of this earth: the chloride, the nitrate, and the acetate. In 1758, French chemist Pierre Macquer wrote that alumina resembled a metallic earth. In 1760, French chemist Theodor Baron de Henouville expressed his confidence that alumina was a metallic earth.

In 1767, Swedish chemist Torbern Bergman synthesized alum by boiling alunite in sulfuric acid and adding potash to the solution. He also synthesized alum as a reaction product between sulfates of potassium and earth of alum, demonstrating that alum was a double salt. In 1776, German pharmaceutical chemist Carl Wilhelm Scheele demonstrated that both alum and silica originated from clay and alum did not contain silicon. In 1782, French chemist Antoine Lavoisier wrote he considered alumina was an oxide of a metal with an affinity for oxygen so strong that no known reducing agents could overcome it. Geoffroy's mistake was only corrected in 1785 by German chemist and pharmacist Johann Christian Wiegleb who determined the earth of alum could not be synthesized from silica and alkalis, contrary to contemporary belief.

Swedish chemist Jöns Jacob Berzelius suggested in 1815 the formula AlO for alumina. The correct formula, AlO, was established by German chemist Eilhard Mitscherlich in 1821; this helped Berzelius determine the correct atomic weight of the metal, 27.

In 1760, de Henouville attempted to reduce alumina to its metal, but with no success. He claimed he had tried every method of reduction known at the time, though his methods were not published. It is probable that he mixed alum with carbon or some organic substance, with salt or soda for flux, and heated it in a charcoal fire. In 1790, Austrian chemists Anton Leopold Ruprecht and Matteo Tondi repeated Baron's experiments, significantly increasing the temperatures. They found small metallic particles they believed were the sought-after metal; but later experiments by other chemists showed these were iron phosphide from impurities in charcoal and bone ash. German chemist Martin Heinrich Klaproth commented in an aftermath, "if there exists an earth which has been put in conditions where its metallic nature should be disclosed, if it had such, an earth exposed to experiments suitable for reducing it, tested in the hottest fires by all sorts of methods, on a large as well as on a small scale, that earth is certainly alumina, yet no one has yet perceived its metallization." Lavoisier in 1794 and French chemist Louis-Bernard Guyton de Morveau in 1795 melted alumina to a white enamel in a charcoal fire fed by pure oxygen but found no metal. American chemist Robert Hare in 1802 melted alumina with an oxyhydrogen blowpipe, also obtaining the enamel, but still found no metal.

In 1807, British chemist Humphry Davy successfully electrolyzed alumina with alkaline batteries, but the resulting alloy contained potassium and sodium, and Davy had no means to separate the desired metal from these. He then heated alumina with potassium, forming potassium oxide, but was unable to produce the sought-after metal. In 1808, Davy set up a different experiment on electrolysis of alumina, establishing that alumina decomposed in the electric arc, but formed metal alloyed with iron and he was unable to separate the two. Finally he tried yet another electrolysis experiment, seeking to collect the metal on iron, but was again unable to separate the coveted metal from it. Davy suggested the metal be named "alumium" in 1808 and "aluminum" in 1812, thus producing the modern name. Other scientists used the spelling "aluminium"; the former spelling regained usage in the United States in the following decades.

In 1813, American chemist Benjamin Silliman repeated Hare's experiment and obtained small granules of the sought-after metal, which almost immediately burned.

In 1824, Danish physicist Hans Christian Ørsted attempted the production of the metal. He reacted anhydrous aluminium chloride with potassium amalgam, yielding a lump of metal that looked similar to tin. He presented his results and demonstrated a sample of the new metal in 1825. In 1826, he wrote, "aluminium has a metallic luster and somewhat grayish color and breaks down water very slowly"; this suggests that he had obtained an aluminium–potassium alloy rather than pure aluminium. Ørsted gave little importance to his discovery. He did not notify either Davy or Berzelius, both of whom he knew, and published his work in a Danish magazine unknown to the general European public. As a result, he is often not credited as the discoverer of the element; some earlier sources claimed Ørsted had not isolated aluminium.

Berzelius tried to isolate the metal in 1825 by carefully washing the potassium analog of the base salt in cryolite in a crucible. Prior to the experiment, he had correctly identified the formula of this salt as KAlF. He found no metal, but his experiment came very close to succeeding and was successfully reproduced many times later. Berzelius's mistake was in using an excess of potassium, which made the solution too alkaline; the alkaline solution dissolved all the newly formed aluminium.

In 1827, German chemist Friedrich Wöhler visited Ørsted and received explicit permission to continue the aluminium research, which Ørsted "did not have time" for. Wöhler repeated Ørsted's experiments but did not identify any aluminium. (Wöhler later wrote to Berzelius, "what Oersted assumed to be a lump of aluminium was certainly nothing but aluminium-containing potassium".) He conducted a similar experiment, mixing anhydrous aluminium chloride with potassium, and produced a powder of aluminium. After hearing about this, Ørsted suggested his own aluminium may have contained potassium. Wöhler continued his research and in 1845 was able to produce small pieces of the metal and described some of its physical properties. Wöhler's description of the properties indicates that he obtained impure aluminium. Other scientists also failed to reproduce Ørsted's experiment, and Wöhler was credited as the discoverer. While Ørsted was not concerned with the priority of the discovery, some Danes tried to demonstrate he had obtained aluminium, and in 1921, the reason for the inconsistency between Ørsted's and Wöhler's experiments was discovered by Danish chemist Johan Fogh, who demonstrated that Ørsted's experiment was successful thanks to use of a large amount of excess aluminium chloride and an amalgam with low potassium content. In 1936, scientists from American aluminium producing company Alcoa successfully recreated that experiment. However, many later sources still referred to Wöhler as the discoverer.

Since Wöhler's method could not yield large amounts of aluminium, the metal remained uncommon; its cost had exceeded that of gold before a new method was devised. In 1852, aluminium cost US$545 per pound.

French chemist Henri Étienne Sainte-Claire Deville announced an industrial method of aluminium production in 1854 at the Paris Academy of Sciences. Aluminium chloride could be reduced by sodium, a metal more convenient and less expensive than potassium used by Wöhler. Deville was able to produce an ingot of the metal. Napoleon III of France promised Deville an unlimited subsidy for aluminium research; in total, Deville used 20 times the annual income of an ordinary family. Napoleon's interest for aluminium lied in its potential military use: he wished weapons, helmets, armor, and other equipment for the French army could be made of the new light shiny metal. While the metal was still not displayed to the public, Napoleon is reputed to have held a banquet where the most honored guests were given aluminium utensils while others made do with gold.

Twelve small ingots of aluminium were subsequently exhibited for the first time to the general public at the Exposition Universelle of 1855. The metal was presented as "the silver from clay" (aluminium is very similar to silver visually), and this name was soon widely used. It caught wide attention, but not all of it was favorable. Newspapers wrote, "The Parisian expo put an end to the fairy tale of the silver from clay", saying that much of what had been said about the metal was exaggerated if not untrue and that the amount of the presented metal—about a kilogram—contrasted with what had been expected and was "not a lot for a discovery that was said to turn the world upside down". However, the metal was noticed by the avantgarde writers of the time—Charles Dickens, Nikolay Chernyshevsky, and Jules Verne—who envisioned its usage in the future. Overall, the fair led to eventual commercialization of the metal. The price of aluminium fell to US$115 per pound in 1855 and to $17 in 1859. At the next fair in Paris in 1867, the visitors were presented with aluminium wire and foil.

Manufacturers did not wish to devote resources from producing well-known metals, such as iron and bronze, to experiment with a new one; moreover, produced aluminium was still not of great purity and differed in properties by sample. This led to the initial general reluctance to produce the new metal. The world's first industrial production of aluminium was established at a smelter in Rouen in 1856 by Deville and partners. Deville's smelter moved that year to La Glacière and then Nanterre, and in 1857 to Salindres. For the factory in Nanterre, an output of 2 kilograms of aluminium per day was recorded; purity of the produced aluminium was 98%. In 1860, Deville sold his aluminium interests to Henri Merle, a founder of Compagnie d'Alais et de la Camargue; this company will dominate the aluminium market in France decades later. The factory in Salindres used bauxite as the primary aluminium ore; some chemists, including Deville, sought to use cryolite, but with little success. British engineer William Gerhard set up a plant with cryolite as the primary raw material in Battersea, London, in 1856, but technical and financial difficulties forced the closure of the plant in three years. British ironmaster Isaac Lowthian Bell produced aluminium from 1860 to 1874. During the opening of his factory, he waved to the crowd with a unique and costly aluminium top hat. No statistics about this production can be recovered, but it "cannot be very high". Deville's output grew to 1 metric ton per year in 1860; 1.7 tons in 1867; 1.8 tons in 1872. At the time, demand for aluminium was low: for example, sales of Deville's aluminium by his British agents equaled 15 kilograms in 1872. Aluminium at the time was often compared with silver; like silver, it was found to be suitable for making jewelry and objéts d'art.

Other production sites began to appear in the 1880s. British engineer James Fern Webster launched the industrial production of aluminium by reduction with sodium in 1882; his aluminium was much purer than Deville's (it contained 0.8% impurities whereas Deville's typically contained 2%). World production of aluminium in 1884 equaled 3.6 tons. In 1884, American architect William Frishmuth combined production of sodium, alumina, and aluminium into a single technological process; this contrasted with the previous need to collect sodium, which combusts in water and sometimes air. In 1886, American engineer Hamilton Castner devised a method of cheaper production of sodium, which decreased the cost of aluminium to $8 per pound, but he did not have enough capital to construct a large factory like Deville's. In 1887, he constructed a factory in Oldbury; Webster constructed a plant nearby and bought Castner's sodium to employ it in his own production of aluminium. In 1887, Aluminium- und Magnesiumfabrik started production in Hemelingen. In 1889, German metallurgist Curt Netto launched a method of reduction of cryolite with sodium that produced aluminium containing 0.5–1% of impurities. The United States Department of the Interior estimated in 1890 that the total amount of unalloyed aluminium produced from 1860 to 1889 in France, the United Kingdom, the United States, and Germany equaled , suggesting that "the indications are that the manufacture will be so largely increased from now on that this amount will soon be exceeded from now on by the annual production".

Aluminium was first synthesized electrolytically in 1854 independently by the German chemist Robert Wilhelm Bunsen and Deville. Their electrolysis methods did not become the basis for industrial production of aluminium because electrical supplies were inefficient at the time; this only changed with the invention of the dynamo, which made creation of large amounts of electricity possible, by Belgian engineer Zénobe-Théophile Gramme in 1870 and the three-phase current, which made transmission of this electricity over large distances possible, by Russian engineer Mikhail Dolivo-Dobrovolsky in 1889. Soon after the discovery, Bunsen moved on to other areas of interest while Deville's work was noticed by Napoleon III; this was the reason why Deville's Napoleon-funded research on aluminium production had been started. Deville quickly realized electrolytic production was impractical at the time and moved on to the chemical methods.

The first large-scale production method was independently developed by French engineer Paul Héroult and American engineer Charles Martin Hall in 1886; it is now known as the Hall–Héroult process. Electrolysis of pure alumina is impractical given its very high melting point; both Héroult and Hall realized its melting point could be significantly lowered by presence of molten cryolite. Héroult could not find enough interest in his invention as demand for aluminium was still small and Deville's factory in Salindres did not wish to improve their process. In 1888, Héroult and his companions founded Aluminium Industrie Aktien Gesellschaft and started industrial production of aluminium bronze in Neuhausen am Rheinfall. This production was only active for a year, but during that time, Société électrométallurgique française was founded in Paris. The society purchased Héroult's patents and appointed him as the director of a smelter in Isère, which would produce aluminium bronze on a large scale at first and pure aluminium in a few months.

At the same time, Hall produced aluminium by the same process in his home at Oberlin and successfully tested it at the smelter in Lockport. He then sought to employ it for a large-scale production. The smelter owners did not wish to change their production methods because they feared a mass production of aluminium would immediately drop the price of the metal. The president of the company considered purchasing Hall's patent to ensure that the competitors would not make use of it. Hall founded the Pittsburgh Reduction Company in 1888 and initiated production of aluminium. In the coming years, this technology was improved and new factories were constructed.

The Hall–Héroult process converts alumina into the metal; Austrian chemist Carl Joseph Bayer discovered a way of purifying bauxite to yield alumina in 1889, now known as the Bayer process. Bayer sintered bauxite with alkali and leached it with water; after stirring the solution and introducing a seeding agent to it, he found a precipitate of pure aluminium hydroxide, which decomposed to alumina on heating. In a few years, he discovered that the aluminium contents of bauxite dissolved in the alkaline leftover from isolation of alumina solids; this was crucial for the industrial employment of this method.

By the end of 1889, a consistently high purity of aluminium produced via electrolysis had been achieved. In 1890, Webster's factory went obsolete after an electrolysis factory had been opened in England. Netto's main advantage, high purity of the resulting aluminium, was outmatched by electrolytic aluminium and his company closed next year. Compagnie d'Alais et de la Camargue also decided to switch to electrolytic production, and their first plant using this method was opened in 1895.

Modern production of the aluminium metal is based around the Bayer and Hall–Héroult processes. The Hall–Héroult process was further improved in 1920 by a team led by Swedish chemist Carl Wilhelm Söderberg. Previously, anode cells had been made from pre-baked coal blocks, which quickly corrupted and required replacement; the team introduced continuous electrodes made from a coke and tar paste in a reduction chamber. This greatly increased the world output of aluminium.

The prices for aluminium declined, and by the early 1890s, the metal had become widely used in jewelry, eyeglass frames, optical instruments, and many everyday items. Aluminium tableware began to be produced in the late 19th century and gradually supplanted copper and cast iron tableware in the first decades of the 20th century. Aluminium foil was popularized at that time. Aluminium is soft and light, but it was soon discovered that alloying it with other metals could increase its hardness while preserving low density. Aluminium alloys found many uses in the late 19th and early 20th centuries. For instance, aluminium bronze is applied to make flexible bands, sheets, and wire, and is widely employed in the shipbuilding and aviation industries. Aviation used a new aluminium alloy, duralumin, invented in 1903. Aluminium recycling started in the early 1900s and has been used extensively since as aluminium is not impaired by recycling and thus can be recycled repeatedly. At this point, only the metal that had not been used by end-consumers was recycled. During World War I, major governments demanded large shipments of aluminium for light strong airframes. They often subsidized factories and the necessary electrical supply systems. Overall production of aluminium peaked during the war: world production of aluminium in 1900 was 6,800 metric tons; in 1916, annual production exceeded 100,000 tons. The war created a greater demand for aluminium, which the growing primary production was unable to fully satisfy, and recycling grew intensely as well. The peak in production was followed by a decline, then a swift growth.

During the first half of the 20th century, the real price for aluminium continuously fell from $14,000 per metric ton in 1900 to $2,340 in 1948 (in 1998 United States dollars) with some exceptions such as the sharp price rise during World War I. Aluminium was plentiful and in 1919, Germany began to replace its silver coins with aluminium ones; more and more denominations were switched to aluminium coins as hyperinflation progressed in the country. By the mid-20th century, aluminium had become a part of everyday lives, becoming an essential component of houseware. Aluminium freight cars first appeared in 1931. Their lower mass allowed them to carry more cargo. During the 1930s, aluminium emerged as a civil engineering material, being used in both basic construction and building interiors, and advanced its use in military engineering for both airplanes and tank engines.

Aluminium obtained from recycling was considered inferior to primary aluminium because of poorer chemistry control as well as poor removal of dross and slags. Recycling grew overall but largely depended on the output of primary production: for instance, as electric energy prices went down in the United States in the late 1930s, more primary aluminium could be produced in the energy-expensive Hall–Héroult process, rendering recycling less needed, and thus aluminium recycling rates went down. By 1940, mass recycling of post-consumer aluminium had begun.

During World War II, production peaked again, first exceeding 1,000,000 metric tons in 1941. Aluminium was heavily used in aircraft production and thus a strategic material of extreme importance; so much so that when Alcoa (successor of Hall's Pittsburgh Reduction Company and the aluminium production monopolist in the United States at the time) did not expand its production, the United States Secretary of the Interior proclaimed in 1941, "If America loses the war, it can thank the Aluminum Corporation of America". In 1939, Germany was world's leading producer of aluminium; the Germans thus saw aluminium as their edge in the war. Aluminium coins continued to be used but while they symbolized decline on introduction, by 1939, they had come to represent power. (In 1941, they began to be withdrawn from circulation.) After the United Kingdom was attacked in 1940, it started an ambitious program of aluminium recycling; the newly appointed Minister of Aircraft Production appealed to the public to donate any household aluminium for airplane building. The Soviet Union received 328,100 metric tons of aluminium from its co-combatants from 1941 to 1945; this aluminium would be used in aircraft and tank engines. Without these shipments, the output of the Soviet aircraft industry would have fallen by over a half. Production fell after the war but then rose again.

Earth's first artificial satellite, launched in 1957, consisted of two joined aluminium hemispheres, and all subsequent spacecraft have used aluminium to some extent. The aluminium can was first manufactured in 1956 and employed as a container for drinks in 1958. In the 1960s, aluminium was employed for production of wires and cables. Since the 1970s, high-speed trains have commonly used aluminium for its lightness. For the same reason, the aluminium content of cars is growing.

By 1955, the world market had been mostly divided by the Six Majors: Alcoa, Alcan (originated as a part of Alcoa), Reynolds, Kaiser, Pechiney (merger of Compagnie d'Alais et de la Camargue that bought Deville's smelter and Société électrométallurgique française that hired Héroult), and Alusuisse (successor of Héroult's Aluminium Industrie Aktien Gesellschaft); their combined share of the market equaled 86%. From 1945, aluminium consumption grew by almost 10% each year for nearly three decades, gaining ground in building applications, electric cables, basic foils, and the aircraft industry. In the early 1970s, an additional boost came from the development of aluminium beverage cans. The real price declined until the early 1970s; in 1973, the real price equaled $2,130 (in 1998 United States dollars). The main drivers of the decline of prices were the decline of extraction and processing costs over technological progress as well as the growth of aluminium production, which first exceeded 10,000,000 metric tons in 1971.

In the late 1960s, governments became aware of waste from the industrial production; they enforced a series of regulations favoring recycling and waste disposal. Söderberg anodes, which save capital and labor to bake the anodes but are more harmful to the environment (because of a greater difficulty in collecting and disposing of the baking fumes), fell in disfavor, and production began to shift back to the pre-baked anodes. The aluminium industry started to promote recycling of aluminium cans in an attempt to avoid restrictions on them. This sparked recycling of aluminium previously used by end-consumers: for example, in the United States, levels of recycling of such aluminium increased 3.5 times from 1970 to 1980 and 7.5 times to 1990. Production costs for primary aluminium grew in the 1970s and 1980s, and this also contributed to the rise of aluminium recycling.

In the 1970s, the increased demand for aluminium made it an exchange commodity; it entered the London Metal Exchange, world's oldest industrial metal exchange, in 1978. Since then, aluminium has been traded for United States dollars and its price fluctuated along with the exchange rates of the currency. The need to exploit lower-grade poorer quality deposits and the use of fast increasing input costs (above all, energy, but also bauxite) as well as changes in exchange
rates and greenhouse gas regulation increased the net cost of aluminium; the real price grew in the 1970s.

The increase of the real price and changes of tariffs and taxes started redistribution of the world producers' shares: the United States, the Soviet Union, and Japan accounted for nearly 60% of world's primary production in 1972 (and their combined share of consumption of primary aluminium was also close to 60%), but their combined share only slightly exceeded 10% in 2012. The production shift started in the 1970s; production started to move from the United States, Japan, and Western Europe to Australia, Canada, the Middle East, Russia, and China, where it was cheaper due to lower electricity prices and favorable regulation from states, such as low taxes or subsidies. Production costs in the 1980s and 1990s declined because of advances in technology, lower energy and alumina prices, and high exchange rates of the United States dollar.

In the 2000s, the BRIC countries' combined share grew from 32.6% to 56.5% in primary production and 21.4% to 47.8% in primary consumption. China has accumulated an especially large share of world production, thanks to abundance of resources, cheap energy, and governmental stimuli; it also increased its share of consumption from 2% in 1972 to 40% in 2010. The only other country with a two-digit percentage was the United States with 11%; no other country exceeded 5%. In the United States, Western Europe, and Japan, most aluminium was consumed in transportation, engineering, construction, and packaging.

In the mid-2000s, increasing energy, alumina, and carbon (used in anodes) prices caused an increase in production costs. This was amplified by a shift in currency exchange rates: not only a weakening of the United States dollar, but also a strengthening of the Chinese yuan. The latter became important as most Chinese aluminium was relatively cheap.

The world output continued to grow: in 2013, annual production of aluminium exceeded 50,000,000 metric tons. In 2015, it was a record 57,500,000 tons. Its real price (in 1998 United States dollars) in 2015 was $1,340 per metric ton ($1,940 per ton in contemporary dollars).




</doc>
<doc id="55858225" url="https://en.wikipedia.org/wiki?curid=55858225" title="Construction of Rockefeller Center">
Construction of Rockefeller Center

The construction of the Rockefeller Center complex in New York City was conceived as an urban renewal project in the late 1920s, spearheaded by John D. Rockefeller Jr. to help revitalize Midtown Manhattan. Rockefeller Center is located on one of Columbia University's former campuses and is bounded by Fifth Avenue to the east, Sixth Avenue (Avenue of the Americas) to the west, 48th Street to the south, and 51st Street to the north. The center occupies in total, with some of office space.

Columbia University had acquired the site in the early 19th century but had moved to Morningside Heights in Upper Manhattan in the early 1900s. By the 1920s, Fifth Avenue in Midtown Manhattan was a prime site for development. Around that time, the Metropolitan Opera (Met) was looking for a new site for their opera house, and architect Benjamin Wistar Morris decided on the former Columbia site.

Rockefeller eventually became involved in the project and leased the Columbia site in 1928 for 87 years. The lease excluded land along the east side of Sixth Avenue to the west of the Rockefeller property, as well as at the site's southeast corner. He hired Todd, Robertson, and Todd as design consultants and selected the architectural firms of Corbett, Harrison & MacMurray, Hood, Godley & Fouilhoux, and Reinhard & Hofmeister for the opera complex. However, the Met was unsure about moving there, and the Wall Street Crash of 1929 put an end to the plans. Rockefeller instead entered into negotiations with the Radio Corporation of America (RCA) to create a mass-media complex on the site. A new plan was released in January 1930, and an update to the plan was presented after Rockefeller obtained a lease for the land along Sixth Avenue. Revisions continued until March 1931, when the current site design was unveiled. A late change to the proposal included a complex of internationally themed structures along Fifth Avenue.

All structures in the original complex were designed in the Art Deco architectural style. Excavation of the site started in April 1931, and construction began that September on the first buildings. The first edifice was opened in September 1932, and most of the complex was completed by 1935. The final three buildings were built between 1936 and 1940, although Rockefeller Center was officially completed by November 2, 1939. The construction project employed more than 40,000 people and was considered the largest private construction project at the time. It had cost the equivalent of $ billion in dollars to construct. Since then, there have been several modifications to the complex. An additional building at 75 Rockefeller Plaza was constructed in 1947, while another at 600 Fifth Avenue was constructed in 1952. Four towers were built along the west side of Sixth Avenue in the 1960s and 1970s, which was the most recent expansion of Rockefeller Center. The Center Theatre from the original complex was demolished in 1954.

In 1686, much of Manhattan, including the future Rockefeller Center site, was established as a "common land" of the city of New York. The land remained in city ownership until 1801, when the physician David Hosack, a member of the New York elite, purchased a parcel of land in what is now Midtown for $5,000, equivalent to $ in dollars. In terms of the present-day street grid, Hosack's land was bounded by 47th Street on the south, 51st Street on the north, and Fifth Avenue on the east, while the western boundary was slightly east of Sixth Avenue (also known as Avenue of the Americas). At the time, the land was sparsely occupied and consisted mostly of forest. Hosack opened the Elgin Botanic Garden, the country's first public botanical garden, on the site in 1804. The garden would operate until 1811, when Hosack put the land on sale for $100,000 (). As no one was willing to buy the land, the New York State Legislature eventually bought the land for $75,000 ().

In 1814, the trustees of Columbia University (then Columbia College) were looking to the state legislature for funds when the legislature unexpectedly gave Hosack's former land to the college instead. The gardens became part of Columbia's "Upper Estate" (as opposed to the "Lower Estate" in Lower Manhattan), on the condition that the college move its entire campus to the Upper Estate by 1827. Although the relocation requirement was repealed in 1819, Columbia's trustees did not see the land as "an attractive or helpful gift", so the college let the gardens deteriorate. The area would not become developed until the 1830s, and the land's value did not increase to any meaningful amount until the late 1850s, when St. Patrick's Cathedral was built nearby, spurring a wave of development in the area. Ironically, the cathedral was built in that location because it faced the Upper Estate gardens. By 1860, the Upper Estate contained four row houses below 49th Street as well as a wooden building across from the cathedral. The surrounding area was underdeveloped, with a potter's field and the railroad lines from Grand Central Depot located to the east.

Columbia built a new campus near its Upper Estate in 1856, selling a plot at Fifth Avenue and 48th Street to the St. Nicholas Church to pay for construction. Shortly afterward, Columbia implemented height restrictions that prevented any taller buildings, such as apartment blocks or commercial and industrial buildings, from being built on its property. Narrow brownstone houses and expensive "Vanderbilt Colony" mansions were built on nearby streets, and the area became synonymous with wealth. By 1879, there were brownstones on every one of the 203 plots in the Upper Estate, which were all owned by Columbia.

The construction of the Sixth Avenue elevated line in the 1870s made it easy to access commercial and industrial areas elsewhere in Manhattan. However, the line also drastically reduced property values because the elevated structure obstructed views from adjacent properties, and because the trains on the structure caused noise pollution. Columbia sold the southernmost block of its Midtown property in 1904, using the $3 million from the sale (equivalent to $ million in ) to pay for newly acquired land in Morningside Heights even further uptown, to replace its Lower Estate. Simultaneously, the low-lying houses along Fifth Avenue were being replaced with taller commercial developments, and the widening of the avenue between 1909 and 1914 contributed to this transition. Columbia also stopped enforcing its height restriction, which Okrent describes as a tactical mistake for the college because the wave of development along Fifth Avenue caused the Upper Estate to become available for such redevelopment. Since the leases on the Upper Estate row houses were being allowed to expire without renewal, the university's real estate adviser John Tonnele was tasked with finding suitable tenants, who could net the university more profit than what the row houses' occupants were currently paying.

In 1926, the Metropolitan Opera (Met) started looking for locations to build a new opera house to replace its existing building at 39th Street and Broadway. This was not a new effort, as Otto Kahn, the Met's president, had been seeking to erect a new opera house since he assumed the position in 1908. However, the Met did not itself have money to fund the new facility, and Kahn's efforts to solicit funding from R. Fulton Cutting, the wealthy and influential Cooper Union president, were unsuccessful. Cutting did support the 1926 proposal for a new building, as did William Kissam Vanderbilt. By mid-1927, Kahn had hired architect Benjamin Wistar Morris and designer Joseph Urban to come up with designs for the opera house. They created three designs, all of which the Met rejected.

Kahn wanted the opera house to be located on a relatively cheap piece of land near a busy theater and retail area, with access to transportation. In January 1928, Tonnele approached Cutting to propose the Upper Estate as a possible location for the Met. Cutting told of Tonnele's idea to Morris, who thought that the Columbia grounds in Midtown were ideal for the new opera house. By early 1928, Morris had created blueprints for an opera house and a surrounding retail complex at the Upper Estate. However, the new building was too expensive for the Met to fund by itself, and it needed an endowment. On May 21, 1928, Morris presented the project during a dinner for potential investors, at which the Rockefeller family's public relations adviser Ivy Lee was a guest. Lee later informed his boss, John D. Rockefeller Jr., about the proposal, in which the latter showed interest. Rockefeller wished to give the site serious consideration before he invested, and he did not want to fund the entire project on his own. As a result, in August 1928, Rockefeller contacted several firms for advice. Rockefeller ended up hiring the Todd, Robertson and Todd Engineering Corporation as design consultants to determine the project's viability, with one of the firm's partners, John R. Todd, to serve as the principal builder and managing agent for the project. Todd submitted a plan for the opera house site in September 1928, in which he proposed constructing office buildings, department stores, apartment buildings, and hotels around the new opera house.

In June 1928, Charles O. Heydt, one of Rockefeller's business associates, met with Kahn to discuss the proposed opera house. After the meeting, Heydt purchased land just north of the proposed opera house site as per Rockefeller's request, but for a different reason: Rockefeller was afraid that many of the landmarks of his childhood, located in the area, were going to be demolished by the 1920s wave of development. The writer Daniel Okrent states that Rockefeller's acquisition of the land might have been more for the family name rather than for the Met proposal itself, as Rockefeller never mentioned the details of the proposal in his daily communications with his father, John D. Rockefeller Sr.

In mid-1928, the Met and Rockefeller were named as prospective buyers for the Columbia site. A lease was agreed to in late 1928 and signed on December 31 of that year. Columbia leased the parcel to Rockefeller for 87 years at a cost of over $3 million per year (equivalent to $ million in ), thereby allowing all the existing leases on the site to expire by November 1931 so Rockefeller could purchase them. Rockefeller would pay $3.6 million per year (equivalent to $ million in ). In return, he would be entitled to the income from the property, which at the time was about $300,000 annually (equivalent to $ million in ). This consisted of a 27-year lease for the site from Columbia, with the option for three 21-year renewals, such that the lease could theoretically last until 2015. (The lease was renewed in 1953 and 1973, and Columbia sold the land to Rockefeller Center Inc. in 1985.) Moreover, Rockefeller could avoid any rent increases for forty-five years, adjusted for inflation. The lease did not include the strip of land bordering Sixth Avenue on the west side of the parcel, as well as St. Nicholas Church's property on Fifth Avenue between 48th and 49th Streets, and so these were excluded from the plans. Simultaneous with the signing of the lease, the Metropolitan Square Corporation was created to oversee construction.

In October 1928, before the lease had been finalized, Rockefeller started asking for recommendations for architects. By the end of the year, Rockefeller hosted a "symposium" of architectural firms so he could solicit plans for the new complex. He invited seven firms, six of which specialized in Beaux-Arts architecture, and asked the noted Beaux-Arts architects John Russell Pope, Cass Gilbert, and Milton B. Medary to judge the proposals. Rockefeller requested that all the plans be submitted by February 1929, and the blueprints were presented in May of that year. All of the proposals called for rentable space of , but these plans were so eccentric that the board rejected all of them. For the time being, Morris was retained until a more suitable architect could be found. However, by the end of 1928, Morris had been fired without pay. He was eventually paid $50,000 for his contributions to the project, and many of his ideas were included in the final design.

Rockefeller retained Todd, Robertson and Todd as the developers of the proposed opera house site; and, as a result, Todd stepped down from the Metropolitan Square Corporation's board of directors. In October 1929, Todd appointed Corbett, Harrison & MacMurray; Hood, Godley & Fouilhoux; and Reinhard & Hofmeister to design the buildings. They worked under the umbrella of "Associated Architects" so none of the buildings could be attributed to any specific firm.The lead architect and foremost member of the Associated Architects was Raymond Hood, who would become memorable for his creative designs. Hood, along with Harvey Wiley Corbett, were retained as consultants. The team also included Wallace Harrison, who would later become the family's principal architect and adviser to John Rockefeller Jr.'s son Nelson. Hood, Morris, and Corbett were the consultants on "architectural style and grouping", although Morris was in the process of being fired. Todd also hired L. Andrew Reinhard and Henry Hofmeister as rental or tenant architects, to design the floor plans for the complex, and who were known as the most pragmatic of the Associated Architects. Hood, Corbett, Harrison, Reinhard, and Hofmeister were generally considered to be the principal architects. They mostly communicated with Todd, Robertson and Todd, rather than with Rockefeller himself.

Hugh Ferriss and John Wenrich were hired as "architectural renderers", to produce drawings of the proposed buildings based on the Associated Architects' blueprints. Rene Paul Chambellan was commissioned to sculpt the models of the buildings, and he would later also create some of the art for the center.

The Metropolitan Opera was reluctant to commit to the development, and they refused to take up the site's existing leases until they were certain that they had enough money to do so. In January 1929, Cutting unsuccessfully asked Rockefeller for assistance in buying the leases. Since the Met would not have any funds until after they sold their existing building by April, Heydt suggested that the Metropolitan Square Corporation buy the leases instead, in case the Met ultimately did not have money to relocate. The Met felt that the cost of the new opera house would far exceed the potential profits. They wanted to sell their existing facility and move into the proposed new opera house by 1931, which meant that all existing leases would need to be resolved by May 1930. Otherwise, the new opera house could not be mortgaged, and Columbia would regain possession of the land, which would be a disadvantage for both the Met and Rockefeller.

In August 1929, Rockefeller created a holding company to purchase the strip of land on Sixth Avenue that he did not already lease, so he could construct a larger building on the site and maximize his profits. The company was called the Underel Holding Corporation because the land in question was located under the Sixth Avenue Elevated.

By October 1929, Todd announced a new proposal that differed greatly from Morris's original plan. Todd suggested that the site's three blocks be further subdivided into eight lots, with an "Opera Plaza" in the middle of the center block. The complex would contain the Metropolitan Opera facility; a retail area with two 25-story buildings; department stores; two apartment buildings; and two hotels, with one rising 37 stories and the other being 35 stories. The Met's opera house would have been located on the center block between 49th and 50th Streets east of Sixth Avenue. The retail center would be built around or to the west of the opera building. According to one "New York Times" account, the shops in the Metropolitan Opera development would be laid out in a way similar to that of the retail center in the English city of Chester.

The main hurdle to the plan was the Met's reluctance to commit to the development by buying leases from existing tenants and handing the leases over to Columbia. Rockefeller stood to lose $100,000 per year (equivalent to $ million in ) if he leased the new opera house. Todd had objected that the opera house would be a blight rather than a benefit for the neighborhood, as it would be closed most of the time. After the stock market crash of 1929, these concerns were mooted: the opera could no longer afford to move. The Met instead suggested that Rockefeller finance their relocation by purchasing a half-interest in the old opera house and the land under it, an offer that he refused. On December 6, 1929, the plans for the new opera house were abandoned completely.

With the lease still in effect, Rockefeller had to quickly devise new plans so that the three-block Columbia site could become profitable. Hood came up with the idea to negotiate with the Radio Corporation of America (RCA) and its subsidiaries, National Broadcasting Company (NBC) and Radio-Keith-Orpheum (RKO), to build a mass media entertainment complex on the site. This was achievable because Wallace Harrison was good friends with Julian Street, a "Chicago Tribune" reporter whose wife's great-uncle Edward Harden was part of the RCA board of directors. At Hood's request, Harrison made a lunch date to tell Street about the mass-media complex proposal. Harden, in turn, described the idea to RCA's founder Owen D. Young, who was amenable to the suggestion. It turned out that Young, a longtime friend of Rockefeller's, had been thinking of constructing a "Radio City" for RCA for several years. Rockefeller later stated, "It was clear that there were only two courses open to me. One was to abandon the entire development. The other to go forward with it in the definite knowledge that I myself would have to build it and finance it alone."

In January 1930, Rockefeller tasked Raymond Fosdick, the Rockefeller Foundation's future head, with scheduling a meeting with Young. The RCA founder was enthusiastic about the project, expressing his vision for a complex that, according to Daniel Okrent, contained "an opera house, a concert hall, a Shakespeare theater—and both broadcast studios and office space for RCA and its affiliated companies". RCA president David Sarnoff would join the negotiations in early 1930. Sarnoff immediately recognized Radio City's potential to affect the fledgling television and radio industries. By May, RCA and its affiliates had made an agreement with Rockefeller Center managers. RCA would lease of studio space; get naming rights to the development's largest tower; and develop four theaters, at a cost of $4.25 million per year (equivalent to $ million in ). NBC was assured exclusive broadcasting rights at Rockefeller Center as part of the deal.

In January, Todd released a new plan called G-3. Like the previous Metropolitan Opera proposal, this plan subdivided the complex into eight blocks with a plaza in the middle of the center block. It was similar to his October 1929 plan for the Met, with one major change: the opera house was replaced with a 50-story building. The 50-story tower was included because its larger floor area would provide large profits, and its central location was chosen because Todd believed that the center of the complex was too valuable to waste on low-rise buildings. Plans for a new Metropolitan Opera building on the site were still being devised, including one proposal to place an office building over the opera facility. However, this was seen as increasingly unlikely due to the Met's reluctance to move to the complex. A proposal to create roads that crossed the complex diagonally was briefly considered, but it was dropped because it involved decommissioning city streets, which could only be done after lengthy discussions with city officials. Plan G-3 was presented to the Metropolitan Square Corporation's managers in February. At the time, Todd thought that G-3 was the most viable proposal for the complex.

Another plan called H-1 was revealed in March 1930, after the Underel Corporation had acquired the land near Sixth Avenue. The leases for the newly acquired land contained specific stipulations on how it was to be used. Under this new proposal, there would be facilities for "television, music, radio, talking pictures and plays". RCA planned to build theaters on the north and south blocks near Sixth Avenue, with office buildings above the theaters' Sixth Avenue sides. The theater entrances would be built to the west along Sixth Avenue, and the auditoriums would be located to the east, since the city's building code prohibited the erection of structures over the auditoriums of theaters. The delivery lane was eliminated in this plan, as it was perceived as unnecessary: the theaters would only see occasional deliveries. The complex would also contain three tall buildings in the center of each block, including a 60-story building in the center block for RCA (the current 30 Rockefeller Plaza). Todd suggested that this large tower be placed on Sixth Avenue because the Sixth Avenue Elevated would reduce the value of any other properties at the west end of the complex. On Fifth Avenue there would be a short oval-shaped retail building, whose top floors would be occupied by Chase National Bank offices. There would be a plaza between the oval building and RCA's building, and a restaurant atop the former. A transcontinental bus terminal would be built underground to entice tenants who might otherwise rent near Penn Station or Grand Central.

Plan H-1 was approved in June 1930. In the middle of the month, "The New York Times" announced the plans for the "Radio City" project between 48th Street, 51st Street, Fifth Avenue, and Sixth Avenue. Additional details were released: for example, the $200 million cost projection for the three skyscrapers (equivalent to $ billion in ). To provide space for the plaza, 49th and 50th streets would be covered over, and there would be parking structures both above- and underground, while the streets surrounding the project would be widened to accommodate the heavy traffic. Four theaters would also be built: two small theaters for television, comedy, and drama; a larger one for movies; and another theater, larger still, for vaudeville. Under the plan, the demolition of the site's existing structures would start later in the year, and the complex would be complete by 1933. This plan had not been disclosed to the general public prior to the announcement, and even John Rockefeller Jr. was surprised by the $350 million cost estimate (equivalent to $ billion in ), since a private project of this size was unprecedented.

Since Rockefeller had invested large sums of money in the stock market, his wealth declined sharply as a result of the 1929 stock market crash. In September 1930, Rockefeller and Todd started looking for funding to construct the buildings; and by November they secured a tentative funding agreement with the Metropolitan Life Insurance Company. In March 1931, this agreement was made official, with Metropolitan Life agreeing to lend $65 million (equivalent to $ million in ) to the Rockefeller Center Development Corporation. Metropolitan Life's president Frederick H. Ecker granted the money on two conditions: that no other entity would grant a loan to the complex, and that Rockefeller co-sign the loan so that he would be responsible for paying it off if the development corporation defaulted. Rockefeller covered ongoing expenses through the sale of oil company stock. Other estimates placed Radio City's cost at $120 million (equivalent to $ billion in ) based on plan H-16, released in August 1930, or $116.3 million (equivalent to $ billion in ) based on plan F-18, released in November 1930.

The design of the complex was affected greatly by the 1916 Zoning Resolution, which restricted the height that street-side exterior walls of New York City buildings could rise before they needed to incorporate setbacks that recessed the buildings' exterior walls from the streets. Although the 66-story RCA Building was located so far from the street that it could have simply risen as a slab without any setbacks, Hood decided to include setbacks anyway because they represented "a sense of future, a sense of energy, a sense of purpose", according to architecture expert Alan Balfour. A subsequent change to the zoning resolution in 1931, shortly after the Empire State Building opened, increased the maximum speed of New York City buildings' elevators from to , ostensibly as a result of lobbying from the Rockefeller family. This allowed Rockefeller Center's designers to reduce the number of elevators in the complex's buildings, especially the RCA Building. Originally, the elevators were supposed to be installed by the Otis Elevator Company, but Westinghouse Electric Corporation got the contract after agreeing to move into the RCA Building. This turned out to be a financially sound decision for Rockefeller Center, since Westinghouse's elevators functioned better than Otis's.

Hood and Corbett suggested some plans that were eccentric by the standards of the time. One plan entailed the construction of a massive pyramid spanning all three blocks; this was later downsized to a small retail pyramid, which evolved into the oval retail building. Another plan included a system of vehicular ramps and bridges across the complex. In July 1930, Hood and Corbett briefly discussed the possibility of constructing the entire complex as a superblock with promenades leading from the RCA Building. This suggestion was not considered further because, as with the diagonal-streets plan, it would have involved decommissioning streets. Eventually, all of the plans were streamlined into a more traditional design, with narrow rectangular slabs, set back from the street, on all of the blocks. Hood created a guideline that all of the office space in the complex would be no more than from a window, which was the maximum distance that sunlight could directly penetrate the interior of a building at New York City's latitude. At the time, air-conditioned office buildings were rare, and the sunlit offices provided comfortable accommodations, as opposed to offices in many older buildings, which were dark and cramped.

During late 1930 and early 1931, the plans were revised and streamlined. March 1931 saw the announcement of Plan F-18, which called for an International Music Hall (now Radio City Music Hall) and its 31-story office building annex to occupy the northernmost of the three blocks, located between 50th and 51st streets. The 66-story RCA Building would be located on the central block's western half, between 49th and 50th streets, and would house RCA and NBC offices as well as broadcasting studios. The oval-shaped retail complex would occupy the block's eastern half, with a rooftop garden. A RKO-operated sound theater would be located in the southernmost block between 48th and 49th streets. In the center of Radio City would be a new three-block-long private street running between Fifth and Sixth avenues, with a concave plaza at the midpoint. The complex would also include space for a future Metropolitan Opera venue on the northernmost block. An underground pedestrian shopping mall, which would be located above the underground bus terminal, was also added in this plan. The complex would include 28,000 windows and more than of structural steel, according to the builders. It would cost $250 million.

For the first time, a scale rendering of the proposed complex was presented to the public. The rendering was much criticized, with some taking issue with details or general dimensions of the as-yet-unconfirmed proposal, and others lambasting the location of the tall skyscrapers around the plaza. Daniel Okrent writes that "almost everyone" hated the updated plans. The renowned architectural scholar Lewis Mumford went into exile in upstate New York specifically because the "weakly conceived, reckless, romantic chaos" of the plans for Rockefeller Center had violated his sense of style. Mumford's commentary provoked a wave of blunt, negative criticism from private citizens; newspapers, such as the "New York Herald Tribune"; and architects, including both Frank Lloyd Wright and Ralph Adams Cram, whose styles were diametrically opposed to each other. "The New York Times" took note of the "universal condemnation" of the proposal, and after the complex's architects changed their plans in response to the criticism, the "Times" stated, "It is cheering to learn that the architects and builders of Radio City have been stirred by the public criticism of their plans." Despite the controversy over the complex's design, Rockefeller retained the Associated Architects for his project.

The oval-shaped retail building on Fifth Avenue between 49th and 50th streets was criticized for not fitting in with the rest of Fifth Avenue's architecture, with critics referring to the proposed building as an "oilcan". The original plan had been for two retail buildings, but was changed to one in response to Chase National Bank's request for a single building. However, the oval building was scrapped in early 1931, after Chase could not win exclusive banking rights at the building's location. An updated plan, F-19, restored two smaller 6-story retail buildings to the site of the oval building, and proposed a new 40-story tower for a nearby site. These buildings not only would provide retail space but also would fit in with Fifth Avenue's architecture.

In the new plan, an oval-shaped central plaza would be recessed below ground level, with plantings surrounding a large central fountain. A wide, planted esplanade, between 50th and 51st streets, would lead pedestrians from Fifth Avenue, on the east, to the plaza and to the RCA Building to the west, with steps leading down to the plaza. The western side of the plaza would lead directly to the underground pedestrian mall. A replica of Niagara Falls' Horseshoe Falls would also be built above a reflecting pool, while ivy would be planted on the outside of some of the buildings. In a June 1932 revision to the plan, the proposed plaza's shape was changed to a rectangle, and the fountain was moved to the western side of the plaza. The sculptor Paul Manship was then hired to create a sculpture to place on top of the fountain; his bronze "Prometheus" statue was installed on the site in 1934.

As a concession for the loss of the rooftop gardens planned for the oval building, Hood suggested that all of the complex's setbacks below the 16th floor be covered with gardens. Hood thought this was the cheapest way to make the buildings look attractive, with a cost estimate of $250,000 to $500,000 (about $ to $ million in ) that could pay for itself if the gardens were made into botanical gardens. Hood proposed a three-tiered arrangement, inspired by a similar plan of Le Corbusier. The lowest tier would be at ground level; the middle tier would be atop the retail buildings' low-lying roofs and the skyscrapers' setbacks; and the highest tier would be at the tops of the skyscrapers. A March 1932 update to the rooftop garden proposal also included two ornately decorated bridges that would connect the complex's three blocks, though the bridge plan was later dismissed due to its high cost. Ultimately, only seven disconnected gardens would be built.

Since American tenants were reluctant to rent in the retail buildings, Rockefeller Center's manager Hugh Robertson, formerly of Todd, Robertson and Todd, suggested foreign tenants for the buildings. They held talks with prospective Czech, German, Italian, and Swedish lessees who could potentially occupy the six-story internationally themed buildings on Fifth Avenue, although it was reported that Dutch, Chinese, Japanese, and Russian tenancies were also considered. The first themed building that was agreed on was the British Empire Building, the more southerly of the two buildings, which would host governmental and commercial ventures of the United Kingdom. In February 1932, French tenants agreed to occupy the British Empire Building's twin to the north, La Maison Française. A department store and 30-story building (later changed to 45 stories) were planned for the block to the north of the twin buildings, between 50th and 51st streets, with the department store portion facing Fifth Avenue. A "final" layout change to that block occurred in June, when the department store was replaced by the tower's two retail wings, which would be nearly identical to the twin retail buildings to their south. The two new retail buildings, connected to each other, and to the main tower, with a galleria, were proposed to serve Italian, and possibly also German, interests upon completion.

Out of the four theaters included in plan , of March 1930, the city only approved the construction of two; and thus, only these two theaters were constructed. Samuel "Roxy" Rothafel, a successful theater operator who was renowned for his dominance of the city's theater industry, joined the center's advisory board in 1930. He offered to build two theaters: a large, vaudeville "International Music Hall", on the northernmost block, with more than 6,200 seats; and the smaller, 3,500-seat "RKO Roxy" movie theater on the southernmost block. The idea for these theaters was inspired by Roxy's failed expansion of the 5,920-seat Roxy Theatre on 50th Street, one-and-a-half blocks away. Roxy also envisioned an elevated promenade between the two theaters, but this was never published in any of the official blueprints. Meanwhile, proposals for a Metropolitan Opera House on the site persisted. Official plans for a facility to the east of the RKO Roxy were filed in April 1932; the projected 4,042-seat opera facility would contain features such as a second-floor esplanade extending across 50th Street. However, the Met was unable to fund such a move, so the proposed new opera house was relegated to tentative status.

In September 1931, a group of NBC managers and architects toured Europe to find performers and look at theater designs. However, the group did not find any significant architectural details that they could use in the Radio City theaters. In any case, Roxy's friend Peter B. Clark turned out to have much more innovative designs for the proposed theaters than the Europeans did. The Music Hall was designed by architect Edward Durell Stone and interior designer Donald Deskey in the Art Deco style. Eugene Schoen was selected to design the RKO Roxy.

In December 1931, the Rockefeller Center Corporation put forth its expanded plans for the underground pedestrian mall. It would now include a series of people mover tunnels, similar to the U.S. Capitol subway, which would link the complex to locations such as Grand Central Terminal and Penn Station. A smaller, scaled-down version of the plan was submitted to the New York City Board of Estimate in October 1933. The plan included two vehicular tunnels to carry 49th and 50th streets underneath the entire complex, as well as a subterranean pedestrian mall connecting the buildings in the complex. Additionally, a system of pedestrian passages would be located underground, and a sunken lower plaza would connect to the mall via a wide concourse under the RCA Building. The complex-wide vehicular tunnels were not built; instead, a truck ramp from ground level to the underground delivery rooms was built at 50th Street.

An unfulfilled revision to the plan was submitted in May 1931, when Benjamin Wistar Morris, the architect of the original Opera proposal, proposed enhancing the complex's private passageway into a public "Metropolitan Avenue", which would run from 42nd to 59th streets. The avenue would break up the distance between Fifth and Sixth avenues, which was the longest gap between two numbered avenues in Manhattan. This was not a new proposal, as Mayor William Jay Gaynor had posited a similar avenue from 34th to 59th Streets in 1910, and Wistar himself had proposed the avenue in 1928 or 1929. If built, Metropolitan Avenue would have facilitated traffic through Rockefeller Center, in a manner similar to how Vanderbilt Avenue between Madison and Park Avenues had assisted traffic flow around nearby Grand Central Terminal. Ultimately, only the section between 48th and 51st streets was built; it now comprises Rockefeller Plaza, a pedestrian street.

Both Raymond Hood and John Todd believed that the center needed visual enhancement besides the rooftop gardens. Initially, Todd had only planned to allocate about $150,000 toward the building's art program, but Rockefeller wanted artworks that had meaningful purposes rather than purely aesthetic ones. In November 1931, Todd suggested the creation of a program for placing distinctive artworks within each of the buildings. Hartley Burr Alexander, a noted mythology and symbology professor, was tasked with planning the complex's arts installations. Alexander submitted his plan for the site's artwork in December 1932. As part of the proposal, the complex would have a variety of sculptures, statues, murals, friezes, decorative fountains, and mosaics. Expanding upon Hood's setback-garden plan, Alexander's proposal also included rooftop gardens atop all the buildings, which would create a "Babylonian garden" when viewed from above.

At first, Alexander suggested "Homo faber, Man the Maker" as the complex's overarching theme, representing satisfaction with one's occupation rather than with the wage. However, that theme was not particularly well received by the architects, so Alexander proposed another theme, the "New Frontiers"; this theme dealt with social and scientific innovations and represented the challenges that humanity faced "after the conquest of the physical world". In theory, this was considered a fitting theme, but Alexander had been so specific about the details of the necessary artworks that it limited the creative license for any artists who would execute such works. Alexander had created a 32-page paper that explained exactly what needed to be done for each artwork, with some of the key themes underlined in all caps, giving the paper "a tone more Martian than human", according to Okrent. In March 1932, Alexander was fired and replaced with a panel of five artists. The panel agreed on the current theme, "The March of Civilization", but by that point some of the art of previous themes had already been commissioned, including the works that Alexander had proposed.

The process of commissioning art for Rockefeller Center was complicated. Each building's architects would suggest some artwork. Todd would eliminate all of the unconventional proposals, and Rockefeller had the final say on many of the works. There were many locations that needed art commissioned, which prevented any specific artistic style from dominating the complex. Specialists from around the world were retained for the art program: for instance, Edward Trumbull coordinated the colors of the works located inside the buildings, and Léon-Victor Solon did the same job for the exterior pieces.

Gaston Lachaise, a renowned painter of female nudes, executed the commission for six uncontroversial bas-reliefs for Rockefeller Center, four at the front of 1230 Avenue of the Americas (RCA Building West) and two at the back of the International Building. The "Prometheus", "Youth", and "Maiden" sculptures that Paul Manship had created for the complex were prominently situated in the complex's lower plaza. Barry Faulkner had only one commission for the entirety of Rockefeller Center: a mosaic mural located above the entrance of 1230 Avenue of the Americas. Alfred Janniot also created a single work for Rockefeller Center, the bronze panel outside La Maison Francaise's entrance. Lee Lawrie was by far the complex's most prolific artist, with 12 works. Most of Lawrie's commissions were limestone screens above the main entrances of buildings, but he had also created two of Rockefeller Center's best-known artworks: the "Atlas" statue in the International Building's courtyard, and the "Wisdom" screen above the RCA Building's main entrance. Ezra Winter, who created the "Quest for the Fountains of Eternal Youth" mural in Radio City Music Hall's lobby, largely adhered to Alexander's original specifications for the mural.

One of the center's more controversial works was created by Diego Rivera, whom Nelson Rockefeller had hired to create a color fresco for the wall in the RCA Building's lobby. His painting, "Man at the Crossroads", became controversial, as it contained Moscow May Day scenes and a clear portrait of Lenin, which had not been apparent in initial sketches (see .) Nelson issued a written warning to Rivera to replace the offending figure with an anonymous face, but Rivera refused, so in mid-1933, Rivera was paid for his commission and workers covered the mural with paper. The fresco was demolished completely in February 1934, and it was subsequently replaced by Josep Maria Sert's "American Progress" mural. As a result of the "Man at the Crossroads" controversy, Nelson scaled back his involvement with the complex's art, and his father began scrutinizing all of the artworks thereafter commissioned for the center.

One of the sculptor Attilio Piccirilli's works at Rockefeller Center would also be contested, albeit not until after the complex was finished. He had created bas-relief carvings above the entrances of Palazzo d'Italia and the International Building North. Piccirilli's relief on the Palazzo d'Italia was removed in 1941 because the panels were seen as an overt celebration of fascism, but his International Building North panels were allowed to remain.

During early planning, the development was often referred to as "Radio City". Before the announcement that the development would include a mass media complex, there were also other appellations such as "Rockefeller City" and "Metropolitan Square" (after the Metropolitan Square Corporation). Ivy Lee suggested changing the name to "Rockefeller Center". John Rockefeller Jr. initially did not want the Rockefeller family name associated with the commercial project, but was persuaded on the grounds that the name would attract far more tenants. The name was formally changed in December 1931. Rockefeller Jr. and "The New York Times" originally spelled the complex as "Rockefeller Centre", which was the British way of spelling "Center". After consultation with the famed lexicographer Frank H. Vizetelly, "Centre" was changed to "Center". Over time, the appellation of "Radio City" devolved from describing the entire complex to just the complex's western section; and by 1937, only the Radio City Music Hall contained the "Radio City" name.

According to Daniel Okrent, most sources estimated that between 40,000 and 60,000 people were hired during construction. One estimate by Raymond Fosdick, the Rockefeller Foundation head, placed the figure at 225,000 people, including workers who created materials for the complex elsewhere. When construction started, the city was feeling the full effects of the Depression, with over 750,000 people unemployed and 64% of all construction workers without a job. At the Depression's peak in the mid-1930s, John Rockefeller Jr. was praised as a job creator and a "patriot" for jump-starting the city's economy with the construction project. Rockefeller made an effort to form amicable relationships with Rockefeller Center's workers. Even when Rockefeller had to reduce wages for his union workers, he was praised for not reducing wages as severely as did other construction firms, many of which were either struggling or going bankrupt. At the time, the complex was the largest private building project ever undertaken. Carol Herselle Krinsky, in her 1978 book, describes the center as "the only large private permanent construction project planned and executed between the start of the Depression and the end of the Second World War".

For the project, 228 buildings on the site were razed and some 4,000 tenants relocated, with the estimated aggregate worth of the property exceeding $7 billion (equivalent to $ billion in ). Rockefeller achieved this by buying existing leases from the tenants. In January 1929, William A. White & Sons was hired to conduct the eviction proceedings. They worked with the law firm of Murray, Aldrich & Webb to give checks to tenants in exchange for property, sometimes for over $1 million. The area was mostly occupied by illegal speakeasy bars, as the Prohibition Era had banned all sales of alcoholic beverages. Although the more tenuous of these speakeasies quickly moved elsewhere at the mere mention of formal eviction proceedings, other tenants, including some of the brothels, were harder to evict. Many tenants only moved on certain conditions; and in one case, the firms acquired a lease from the estate of the late gambler Arnold Rothstein, who was murdered two months before he was set to be forcefully evicted from his Upper Estate building in January 1929. Demolition of the structures started in early 1930, and all of the buildings' leases had been bought by August 1931.

The center's managers then set to acquire the remaining lots along Sixth Avenue, and at the southeast corner of the site, so that they could create a larger complex, which led to the formation of the Underel Corporation. The negotiations for the Sixth Avenue properties were conducted by different brokers and law firms so as to conceal the Rockefeller family's involvement in the Underel Corporation's acquisitions. Even so, there were several tenants along Sixth Avenue who initially refused to give up their buildings. In total, Charles Heydt spent $10 million (equivalent to $ million in ) on acquiring the Sixth Avenue parcels, as compared to the $6 million (equivalent to $ million in dollars) budgeted for the task.

The tenants of two Sixth Avenue properties were ultimately allowed to stay. One lessee, who occupied a plot on the southeast corner of Sixth Avenue and 50th Street, never received a sale offer due to a misunderstanding. The owners of the other parcel, located on the northeast corner of Sixth Avenue and 49th Street, demanded an exorbitant price for their property. 30 Rockefeller Plaza was ultimately built around both parcels.

On the southeast corner of the site, several property owners also refused to sell. Columbia University was willing to give Rockefeller Center Inc. control of all leases in the former Upper Estate that were no longer held by a third party. However, William Nelson Cromwell, a prominent lawyer and Columbia alumnus, who owned three adjacent row houses at 10–14 West 49th Street, would not move out of his house when his lease expired in 1927. The disagreement continued until 1936, during which time Cromwell refused to pay rent on 14 West 49th Street, while Rockefeller Center Inc. withheld $400,000 of Cromwell's rent payments to Columbia. Rockefeller Center Inc. would later buy 8 West 49th, thus boxing Cromwell's land in between the two Rockefeller Center parcels. The company allowed Robert Goelet to keep the neighboring lot at 2–6 West 49th Street because the company considered his "interest and concern" to be a "large concern". However, he could not develop the land because Cromwell controlled an easement over part of Goelet's land. The St. Nicholas Church, located on 48th Street behind Goelet's lot, also refused to sell its property despite an offer of up to $7 million for the parcel.

Excavation of the Sixth Avenue side of the plot began in late July 1931, commencing a seven-year period of excavation during which of the underlying Manhattan schist would be removed from the site. By late 1931, the empty blocks were pits up to , with a few buildings still standing at the edges of each block. 49th and 50th streets resembled "causeways skimming the surface of a lake". A field office for the project was erected on Fifth Avenue. It served as the headquarters for the main construction contractors Todd & Brown, which was composed of John Todd's son Webster as well as Joseph O. Brown. Brown was especially involved in eliminating unnecessary costs and selecting firms for supplies.

Designs for the RCA Building and International Music Hall, to the north, were submitted to the New York City Department of Buildings in August 1931, with both buildings scheduled to open in 1932. The contracts for the music hall and 66-story skyscraper were awarded two months later. Ultimately, the project's managers would submit 1,860 contracts to the Department of Buildings. Rockefeller Center's construction progressed quickly; by October 1931, sixty percent of the digging was complete and the first contracts for the buildings had been let. The foundations had been dug up to below ground, and the first of the RCA Building's 86 piers, descending a maximum of , had been set. Of the brownstones on site, 177 had been demolished by that October, with the majority of the remaining buildings located near the avenues. Work on the new Roxy Theatre, to the south of the RCA Building, started that November.

The architects wanted a uniform color for the facades of all 14 of the new buildings. To that end, Raymond Hood, in December 1931, awarded a contract for Indiana Limestone that would make up the facades. At the time, it was the largest order of stone in history, with about of limestone being shipped. Rockefeller Center's managers also ordered of structural steel, the largest such order in history, which cost one-eighth of the projected $250 million total construction cost. The steel order involved a bidding war between Bethlehem Steel and U.S. Steel. The order ultimately went to U.S. Steel, providing 8,000 jobs for its workers, but resulting in a financial loss, as the bidding war resulted in a price that was too low to cover the cost of making the steel. Rockefeller Center also required nearly 23 acres () of glass for its windows, 25,000 doors, and of granite.

As a result of the Depression, building costs were cheaper than projected. Although Metropolitan Life's loan of $126 million remained the same, the final cost of the first ten buildings came to $102 million (equivalent to $ billion in dollars) by the time these structures were completed in 1935. John Todd used the surplus to install extra features in the buildings, such as wider-than-normal utility pipes, a subterranean boiler for the complex in case the steam system malfunctioned, and the complex's limestone facades. Todd even installed sprinklers on the exteriors of the Fifth Avenue retail buildings in case they needed to be converted into factories, since sprinklers were required on industrial buildings at the time. However, not all of the effects were positive: the construction boom of the late 1920s and early 1930s had almost doubled the total amount of real estate in Manhattan, and the construction of Rockefeller Center and the Empire State Building would increase the amount of space by another 56%. As a result, there was a lot of undervalued, vacant space. After RKO's bankruptcy in 1931, Sarnoff convinced John Rockefeller Jr. to buy RKO common stock and RCA preferred stock worth a total of $4 million (equivalent to $ million in ), in return for RCA downsizing its lease by .

Work on the steel structure of the RCA Building started in March 1932. Meanwhile, the British and French governments had already agreed to occupy the first two internationally themed buildings, and John Rockefeller Jr. started signing tenants from the respective countries. The cornerstone of the British Empire Building was laid in June, when Francis Hopwood, 1st Baron Southborough, placed the symbolic first stone in a ceremony. Significant progress on the theaters had been made by then: RKO Roxy's brickwork had been completed and the limestone-and-granite facade was almost ready to be installed, while the Music Hall's steelwork was complete. By September, both theaters were almost finished, as was the RCA Building, whose structural steel was up to the 64th floor. That month also saw the opening of the RKO Building, the first structure in the complex to be opened. The British Empire Building's structural steel started construction in October.

The Music Hall was the second site to open, on December 27, 1932, although it had topped out in August. This was followed by the RKO Roxy's opening two days later. Roxy originally intended to use the Music Hall as a vaudeville theater, but the opening of the Music Hall was widely regarded as a flop, and both theaters ended up being used for films and performing arts. Radio City's Roxy Theatre had to be renamed the Center Theatre in May 1933 after a lawsuit by William Fox, who owned the original Roxy Theatre on 50th Street. The failure of the vaudeville theater ended up ruining Roxy's enterprise, and he was forced to resign from the center's management in January 1934.

The cornerstone of La Maison Francaise was laid on April 29, 1933, by former French prime minister Édouard Herriot. The British Empire Building was open less than a week later. The RCA Building was slated to be open by May 1, but was delayed because of controversy over the "Man at the Crossroads" mural in the lobby. In July 1933, the managers opened a 70th-story observation deck atop the RCA Building, It was a great success: the 40-cents-per-head observation deck saw 1,300 daily visitors by late 1935.

Work on the rooftop gardens started in October 1933, and La Maison Francaise opened the same month. In December 1933, workers erected the complex's famed Christmas tree in the center of the plaza for the first time. Since then, it has been a yearly tradition to display a large Christmas tree at the plaza between November and January.

Simultaneously, the city built the part of the canceled "Metropolitan Avenue" that ran through Rockefeller Center. The new street, called "Rockefeller Plaza", was projected to carry an estimated 7,000 vehicles per day upon opening. The first segment, between 49th and 50th streets, opened in 1933, and a northern extension opened in 1934. The new street measured over wide and ran through the complex, with four vehicular levels.

From 1931 until 1944, Rockefeller Center Inc. employed Merle Crowell, the former editor of "American Magazine", as the complex's publicist. His first press release, published on July 25, 1931, extolled Rockefeller Center as "the largest building project ever undertaken by private capital". Thereafter, Crowell supplanted Ivy Lee as the complex's official publicity manager, and his subsequent releases employed a variety of superlatives, massive amounts of statistics and calculations, and the occasional bit of hyperbole. Crowell published many new press releases every day, and by the midpoint of the complex's construction in 1935, he also started staging celebrity appearances, news stories, and exhibitions at Rockefeller Center. The goal was for 34,500 people to work at Rockefeller Center once it was completed, as well as for 180,700 daily visitors.

Rockefeller hired Hugh Sterling Robertson to solicit tenants and secure lease agreements. It was hard to lease the complex in the wake of the Great Depression, but Robertson managed to identify 1,700 potential tenants, and had held meetings with 1,200 of them by the end of 1933. Rockefeller and his partners were also able to entice some prominent tenants to the center. The Rockefeller family's Standard Oil Company moved into the RCA Building in 1934. Over the next two years, several other major oil companies followed suit and took up leases in Midtown buildings, including Sinclair Oil and Royal Dutch Shell, which moved into Rockefeller Center. The United States Post Office Department opened a facility in the complex in early 1934, and would later rent space in the as-yet-incomplete International Building. The New York Museum of Science and Industry leased some of the less-sought-after space on the RCA Building's lower floors after Nelson Rockefeller became a trustee of the museum in late 1935. Westinghouse moved into the 14th through 17th floors of the RCA Building.

However, Rockefeller Center's managers had a hard time leasing the buildings past 60% occupancy during the earliest years of its existence, which coincided with the middle of the Depression. The Rockefeller family moved into various floors and suites throughout the same building in order to give potential tenants the impression of occupancy. In particular, the family's office took up the entire 56th floor, while the family's Rockefeller Foundation took up the entire floor below, and two other organizations supported by the Rockefellers also moved into the building. Because the sunken central plaza was mostly leased by luxury stores, the complex's managers opened an outdoor restaurant in the plaza in early 1934 to attract other customers. The complex's willingness to gain leases at almost any cost had repercussions of its own. In January 1934, August Heckscher filed a $10,000,000 lawsuit against Rockefeller Center Inc. for convincing tenants to abandon their ongoing leases within his properties in order to take up cheaper leases at Rockefeller Center. The lawsuit stalled in courts until Heckscher's death in 1941, when it was dismissed.

The managers of Rockefeller Center Inc. also wanted the complex to have convenient, nearby mass transit to attract potential lessees. The city-operated Independent Subway System (IND) had opened a subway station at Fifth Avenue and 53rd Street in 1933, drawing workers from Queens. The managers, seeing the success of the business districts around Penn Station and Grand Central, proposed a large rail terminal for trains from Bergen County, New Jersey, so workers from northern New Jersey would be drawn to the complex. Although the managers did decide on a possible location for the terminal on 50th Street, this plan did not work because the IND subway still did not have any stops at the complex itself. The consultants then offered a subway shuttle under 50th Street that would connect to the IND subway station at Eighth Avenue, or a rail line connecting to Penn Station and Grand Central. This plan did not work because the city was uninterested in building the new rail line. The plan was formally dropped in 1934, but proposals for similar ideas persisted until 1939. The city also had plans to construct a line under Sixth Avenue to supplant the elevated railway there, but did not start construction on the Sixth Avenue subway until 1936. Since the IND would be constructing a station at 47th–50th Streets, near the complex, Rockefeller Center's managers also wished to build their own connections to Penn Station and Grand Central using the subway tunnels that were being constructed. However, this proposal was declined because it would require extensive rezoning of the surrounding residential area.

An extension of Rockefeller Plaza northward to the Rockefeller Apartments at 53rd Street was also envisioned in early 1934, with Rockefeller Center's managers, between October 1934 and late 1937, acquiring land for the proposed street. Rockefeller legally condemned some of the buildings he acquired for the planned street expansion. The street was never extended for various reasons.

By July 1934, the complex had leased 80% of the available space in the six buildings that were already opened. The lower plaza's large "Prometheus" statue had been installed in January that year. The complex's underground delivery ramps, located on 50th Street under the present-day Associated Press Building, were completed in May. The ramps, a vestige of the tunnels originally planned for 49th and 50th streets, traveled underground and stretched for . By the end of the year, Wallace Harrison was the lead architect; Andrew Reinhard was in charge of floor plans for tenants; and Henry Hofmeister was tasked with planning the locations of the remaining unbuilt buildings' utilities and structural framework. Raymond Hood had died, while Harvey Corbett had moved on to other projects. Frederick A. Godley and J. André Fouilhoux of Hood, Godley & Fouilhoux, as well as William H. MacMurray of Corbett, Harrison & MacMurray, never had much to do with Rockefeller Center's development.

In May 1934, plans were officially filed for the remaining two International-themed buildings, as well as the larger 38-story, International Building at 45 Rockefeller Center. Work on the buildings started in September 1934. The more southerly of the retail buildings was dubbed "Palazzo d'Italia" and was to serve Italian interests. The Italian government later reneged on its sponsorship of the building, and the task of finding tenants went to Italian-American businesses. The more northerly small building was originally proposed for German occupation under the name "Deutsches Haus" before Adolf Hitler's rise to power in 1932. Rockefeller ruled this out in September 1933, after being advised of Hitler's Nazi march toward totalitarianism. Russia had also entered into negotiations to lease the final building in 1934; but by 1935, the Russians were no longer actively seeking a lease. With no definite tenant for the other building, the Rockefeller Center's managers reduced the proposed nine-story buildings to six stories, enlarged and realigned the main building from a north–south to a west–east axis, and replaced the proposed galleria between the two retail buildings with an expansion of the International Building's lobby. The empty office site thus became "International Building North", rented by various international tenants. In April 1935, developers opened the International Building and its two wings, which had been built in a record 136 days, from groundbreaking to completion. Aside from the averted controversy with the potential German tenants, the internationally themed complex was seen as a symbol of solidarity during the interwar period, when the United States' entry in the League of Nations was obstructed by American isolationists.

By late April 1935, the "Gardens of the Nations" on the RCA Building's 11th-story roof was complete. Upon opening, its collection of exotic flora attracted many visitors, and it became the most popular garden in Rockefeller Center. However, this novelty soon faded, and the gardens started running a $45,000-per-year deficit by 1937 ($ in dollars) due to the massive expense involved in hoisting plants, trees, and water to the roofs, as well as a lack of interest among tourists. Gardens on the roofs of the two theaters would also be installed in 1937, but they were not open to the public.

The underground pedestrian mall and ramp system, connecting the three blocks between 48th and 51st streets, was finished in early May. At the time of the mall's opening, 22 of the 25 retail spaces had been leased, and three more buildings were ready for occupancy that month. The underground concourse contained a post office, payphones, and several public restrooms. The complex was starting to attract large crowds of visitors, especially to Radio City Music Hall or one of the other exhibition and performance spaces. Despite this seeming success in the face of the Depression, construction was considered to be behind schedule: all the buildings had originally been set for completion by mid-1935, yet the central parts of the northern and southern blocks were still undeveloped.

Around this time, Rockefeller Center's managers were selling the lots for their failed Rockefeller Plaza extension. In January 1935, newly elected mayor Fiorello H. La Guardia proposed that a Municipal Art Center be built in or near the Rockefeller complex. It would have contained the Museum of Modern Art; the Guggenheim Collection; a costume museum; or broadcasting facilities for the Columbia Broadcasting System (CBS). Initially, the project was supposed to be located in Central Park. However, due to legal challenges, the site for the planned art center was moved several blocks south to a site between 51st and 53rd streets between Fifth and Sixth avenues, immediately north of Rockefeller Center. In October 1936, the Museum of Modern Art acquired a site on 53rd Street, across the street from the Municipal Art Center site. Several plans for an art center were discussed, but none were executed because of the same complications that befell the aborted Rockefeller Plaza extension.

Also in 1935, plans were filed for a 16-story western extension of the RCA Building, made of the same material but with extensive links to the pedestrian tunnel system and an elaborate entrance from the under-construction IND station at 47th–50th streets. The subway connection started construction in 1936 but would not open until 1940. Until the subway connection opened, the underground shopping mall was an elaborate catacomb that dead-ended on all sides. The retail space on the lower plaza was not profitable because the stores in the plaza were hidden underneath the rest of the buildings and behind the "Prometheus" statue, which made the shops hard for tourists to find. By 1935, there were ten times as many workers entering the RCA Building every day as there were visitors to the lower plaza. After several rejected suggestions to beautify the plaza, the managers finally decided on building the Rink at Rockefeller Center for $2,000 after Nelson Rockefeller found that a new system had been invented that allowed artificial outdoor ice skating, enabling him to bring the pastime to Midtown Manhattan. The new rink was open by Christmas 1936. The rink was originally intended as a "temporary" measure, but it became popular, and so it was kept.

By 1936, ten buildings had been built and about 80% of the existing space had been rented. The buildings, constituting the first phase of construction, were the International Building; the four small retail buildings; the RKO Building; the Center Theatre; the Music Hall; the RCA Building; and the RCA Building's western extension. The total investment in the center up to that point had been about $104.6 million (about $ billion in dollars), which was composed of $60 million of John Rockefeller Jr.'s money and $44.6 million from Metropolitan Life.

Rockefeller Center Inc. needed to develop the remaining empty lots of the northern and southern blocks. Notably, the southern plot was being used as a parking lot, and at the time, it was the city's largest parking facility. In 1936, Time Inc. expressed interest in moving out of their Chrysler Building offices into a larger headquarters, having just launched their "Life" magazine. Rockefeller Center's managers persuaded Time to move to a proposed skyscraper on part of the southern empty lot, located on Rockefeller Plaza between 48th and 49th streets. The steelwork for that building was begun on September 25, 1936, and was complete by November 28, forty-three working days later. The 36-story Time & Life Building, as it was known, opened on April 1, 1937, along with the final block of Rockefeller Plaza abutting the building, between 48th and 49th streets.

Rockefeller Center's executives had talks with the Associated Press for a building on the northern empty lot, which was occupied by the complex's truck delivery ramp. The lot had been reserved for the Metropolitan Opera house, but the managers could not wait anymore to develop the lot, and in 1937, the opera plans were formally scrapped. The lot had also been planned as a hotel site, but this was also deemed economically infeasible. In January 1938, the Associated Press agreed to rent four floors within the structure at 50 Rockefeller Plaza. In exchange, the building would be renamed for the company. Construction of the steelwork started in April 1938; after 29 working days, the 15-story structure was topped out on June 16. The Associated Press moved into 50 Rockefeller Plaza in December. The presence of the Associated Press and Time Inc. expanded Rockefeller Center's scope from strictly a radio-communications complex to a hub of both radio and print media. In 1938, the Associated Press opened the Guild, a newsreel theater, along the curve of the truck ramp below the building.

It was impossible to build any smaller buildings in the remainder of the complex, since the demands of tenants stipulated larger buildings. Additionally, it was no longer viable to build a system of rooftop gardens because the 15-story Associated Press Building was much taller than the 7- to 11-story-high gardens on the rest of the buildings, making it extremely hard to create a system of gardens without the use of extraordinarily steep bridges. The final plot on the southernmost block needed to be developed, and several tenants were being considered. In early 1937, the center's managers approached the Dutch government for a possible 16-story "Holland House" on the eastern part of the plot. The Dutch government did not enter into the agreement because of troubles domestically, most notably the social unrest that preceded Hitler's 1940 invasion of the Netherlands. However, the Rockefeller Center's managers were already in negotiations with Eastern Air Lines. Despite the lack of a definite tenant, the excavation of the 16-story structure at 10 Rockefeller Plaza started in October 1938, and the building was topped out by April 1939. Eastern Air Lines' CEO, Eddie Rickenbacker, did not sign a lease until June 1940. At that time, 10 Rockefeller Plaza was renamed the "Eastern Air Lines Building".

The management of Rockefeller Center shifted around this time. In November 1936, John Todd was featured in two "New Yorker" articles that emphasized his role in the complex's construction. At the same time, Nelson was gaining clout within Rockefeller Center Inc., and he disagreed with nearly all of Todd's suggestions. Nelson's father, John, was relinquishing his responsibilities, since the Rockefeller family's youngest son David had moved out of the family home at 10 West 54th Street, and John was now focusing on his own personal life. By April 1937, Todd regretted his decision to be featured in "The New Yorker". In March 1938, Nelson became the president of Rockefeller Center Inc. He then fired Todd as the complex's manager and appointed Hugh Robertson in his place. Nelson and Robertson wanted to avoid workers' strikes, which would delay the completion of construction. Nelson, Robertson, and the workers' unions agreed to a contract in which the unions would not strike, Robertson would not lock out union workers, and both would agree to arbitration if a labor dispute arose. Rockefeller Center was one of Nelson's primary business ventures until 1958, when he was elected Governor of New York.

Public relations officials were hired to advertise the different parts of the complex, such as the gardens and the plaza. Merle Crowell set up a viewing platform on the east side of Rockefeller Center and founded the facetious "Sidewalk Superintendents' Club" so the public could view construction.

The western half of the southern plot was still undeveloped due to perceived negative effects of the Sixth Avenue elevated. (The elevated line was closed in early December 1938, to be replaced by the IND Sixth Avenue subway, and was razed the next year.) Ultimately, the United States Rubber Company was convinced to move from their headquarters at Columbus Circle to the proposed 1230 Avenue of the Americas building at Rockefeller Center. The company leased eleven floors in the new building, a decrease of from the 12 stories that they leased at 1790 Broadway. The U.S. Rubber Company Building had been planned as a mirror of the RKO Building, but this was not possible because the symmetrical structure would have entailed constructing an expensive cantilever over the Center Theatre. Excavation of the U.S. Rubber Company Building site commenced in May 1939.

The complex was deemed complete by the end of October 1939. John Rockefeller Jr. installed the building's ceremonial final rivet on November 1, 1939, marking the completion of the original Rockefeller Center complex. The installation of the last rivet was accompanied by a celebratory speech by Rockefeller and many news accounts about the event. 10 Rockefeller Plaza, though, was not officially complete until its dedication in October 1940. Although the Dutch government had initially declined to enter an agreement to occupy 10 Rockefeller Plaza, it moved its offices-in-exile into the building once it opened.

After the original complex was finished, Rockefeller started to look for ways to expand, even though the outbreak of World War II stopped almost all civilian construction projects. In 1943, the complex's managers bought land and buildings on three street corners near the complex. Rockefeller Center unveiled plans for expansion to the southwest and north in 1944. At the time, the complex's existing rentable area totaled , with 99.7% of the space being leased.

Esso was one of the tenants that wanted to expand, and the company signaled that it would build its own office tower if Rockefeller Center's managers did not construct a building for them. In 1944, John Rockefeller Jr. officially approved Esso's proposal to construct a building on land that Rockefeller controlled, located at the north end of Rockefeller Plaza. At first, the managers of the property wanted to build a 16-story, $2 million structure on that property, but Hugh Robertson, the original complex's sole remaining architect, stated that the tower needed to be 36 floors high in order to be profitable. In February 1947, the under-construction Esso Building, at the north end of the existing property, became part of Rockefeller Center after ownership of the building was transferred from the Haswin Corporation to Rockefeller Center, Inc. The 33-story Esso Building was topped out the next month.

In 1949, in the face of a shrinking congregation, St. Nicholas Church leased the church building to the Massachusetts Mutual Life Insurance Company, who then leased three contiguous lots from Rockefeller Center for a proposed 28-story building. The Fifth Avenue facade of the new building would have a setback at the 11th floor, and the midblock facade on 49th Street would not block the view from La Maison Francaise across the street, as stipulated by an agreement with Rockefeller Center managers. The church moved out of its plot on Fifth Avenue and 48th Street, and the old church building was subsequently demolished. Construction commenced on 600 Fifth Avenue in 1950, and the tower was completed by 1952. The building was named after the Sinclair Oil Company, which leased eight floors. Although 600 Fifth Avenue was not developed by Rockefeller Center Inc., that company was allowed to dictate the general Art Deco design of the building as part of an agreement with Massachusetts Mutual. In return, Massachusetts Mutual stipulated that the building contain an entrance to Rockefeller Center's underground concourse, and that the leases for William Cromwell's remaining lots be transferred to Rockefeller Center.

The small Center Theatre was deemed redundant to Radio City Music Hall, and in its final years, had been used as an NBC and RCA broadcasting space. In 1953, NBC and RCA expanded into the office space in 30 Rockefeller Center that Sinclair had just vacated. After the broadcasting studio was abandoned, the U.S. Rubber Company indicated that it wanted to expand its office building into the space that was occupied by the underused theater. In October 1953, it was announced that the theater would be demolished. During the demolition process, the U.S. Rubber Building was put on temporary stilts, with the offices above the former theater still being occupied during the demolition process. No vestige of the former theater remains, since 1230 Avenue of the Americas' annex occupies the same space as the original building.
Time-Life also wanted to expand, as its existing space in 1 Rockefeller Plaza had become insufficient by 1953. In August of that year, Rockefeller Center Inc. bought a tract of land on the west side of Sixth Avenue between 50th and 51st streets. Rockefeller Center's managers originally wanted to build an extra NBC studio or a Ford vehicle showroom on the site. However, they changed their minds once they saw Time Inc.'s expansion needs: the company wanted to have its headquarters in a single building. As they would outgrow their existing space in 1 Rockefeller Plaza by 1954, the company would have to move elsewhere. Not wanting to lose Time Inc.'s tenancy, the complex's managers hired Harrison & Abramovitz, composed of Wallace Harrison and Max Abramovitz, to create plans for a building on the newly acquired plot that could house both NBC and Time. NBC later dropped out of the deal because its CEO, David Sarnoff, dissented.

In 1956, two years after the demolition of the Center Theatre, officials announced the construction of a new tower, the Time-Life Building, on the western side of Sixth Avenue between 50th and 51st Streets. The , $7 million building ($ in dollars) would include connections to the existing passageway system and to the Roxy Theater directly to its west. The tower would rise as a 48-story slab, with a plaza to the east and an eight-story annex along its western and northern sides. One of Rockefeller Center's subsidiaries, Westprop Inc., bought the air rights to the original Roxy Theater located next door so that the new tower could conform to the Zoning Resolution of 1916. Time Inc. and Rockefeller Center formed a joint venture, Rock-Time Inc., which would share the tower's rent income between them. Construction on the Time-Life Building's steelwork started in April 1958, and the structure topped out in November of that year. The cornerstone of the building was laid in June 1959, after the building's structure had been completed, and the first tenants began moving into the tower in December 1959.

During this time, plans called for Rockefeller Center to expand northward. Rockefeller Center, Uris Buildings Corporation, and Webb and Knapp formed another joint venture, Rock-Uris Corp., to construct a hotel to the west of 75 Rockefeller Center. However, Webb and Knapp faced monetary shortages, and the joint venture found that a hotel was not the most profitable use of the land. The joint venture instead decided to construct a glass-and-concrete 43-story office building on the site, with connections to the complex's underground concourse. In 1961, the building was named after Sperry Corporation, which leased eight floors in the future building. The planned hotel was moved to another site two blocks north, on the west side of Sixth Avenue between 53rd Streets. This became the New York Hilton at Rockefeller Center, which opened in 1963. The hotel's name was misleading because it was located outside the complex and not connected to the underground mall. The mall could not be extended anyway because the 53rd Street subway was in the way. Further expansion of Rockefeller Center on the west side of Sixth Avenue, between the Hilton and the new Time-Life Building, was not possible because the Equitable Life Assurance Society had built a tower in between the two properties.

In 1963, officials from Esso approached Gustav Eyssell, who had been Rockefeller Center Inc.'s president since Hugh Robertson had stepped down in 1948. Esso proposed that Eyssell approve another building for the company, which had outgrown the space that it already occupied in Rockefeller Center. Eyssell seriously considered the proposal because the complex did not want to lose Esso's tenancy, and because the complex's existing tenants were requesting a combined of extra space. Rockefeller Center's managers re-hired Harrison & Abramovitz to design three new towers on the west side of Sixth Avenue, with one tower on each of the blocks between 47th and 50th Streets. The managers purchased the land for the three proposed buildings in private. Simultaneously, they consulted with large potential tenants, and eventually managed to sign Esso, McGraw-Hill, and Celanese as the main tenants for the buildings. Under the plan, Esso (later renamed Exxon) would move into the northernmost tower at 1251 Avenue of the Americas, while McGraw-Hill would occupy the center tower at 1221, and Celanese would have the southernmost tower at 1211. These were called the "XYZ Buildings" because the three towers were so similar that their placements could be interchanged.

Harrison & Abramowitz's plans were influenced by several design elements. Most importantly, the firm wished to include plazas in front of each of the new Rockefeller Center buildings, drawing from their recent design for the Lincoln Center for the Performing Arts on the Upper West Side, which contained several buildings around a central plaza. The new Rockefeller Center buildings' front plazas would serve as large gathering spaces, similar to the lower plaza in the original complex. The original Rockefeller Center did not include plazas along Sixth Avenue because the elevated line would have overshadowed these spaces, but now that the Sixth Avenue elevated had been demolished, the new buildings' plazas would add open space to the Sixth Avenue side of the complex. As a bonus, the plazas canceled out the imposing visual effects of the buildings on the avenue's east side, which rose straight up from their property lines and made for a cliff-like effect. Finally, per a 1961 revision to the 1916 Zoning Resolution, the inclusion of public plazas would allow the towers' builders to include more office space in each building.

Unlike the old complex, which had to satisfy John D. Rockefeller Jr.'s aesthetic desires, the new towers did not need to be excessively beautiful: the present executives of Rockefeller Center were more concerned with the buildings' functionality. However, the proposed design of the new towers strained relations between Harrison and Nelson Rockefeller. This arose from the media's harsh reviews of the proposed expansion, which was described by one critic as "the sinister Stonehenge of economic man".

There were several impediments to the start of construction. Foremost among them was the fact that the proposed floor area of the new buildings was greater than the amount allowed under the 1961 resolution. The Urban Design Group, a division of the New York City Department of City Planning, suggested that the towers include an enclosed walkway with retail space, as well as a movie theater in one of the buildings. The McGraw-Hill Building was to include a basement planetarium with a dome above ground level, which would be operated by a subsidiary, while the Celanese Building was to contain the theater. Neither of these attractions were actually built: the planetarium space was occupied by a small theater after McGraw-Hill had sold its planetarium subsidiary, while the theater plan was scrapped due to a lack of funding and a decline in the area's theater industry. The start of construction was delayed for several years due to these bureaucratic requests. The city finally approved the project after Rockefeller Center Inc. promised to build the enclosed walkway west of the Celanese Building as well as two parks west of the Exxon and McGraw-Hill Buildings.

Plans for the new buildings were announced on a staggered schedule from 1967 to 1970. The Exxon Building was announced in August 1967, followed by the McGraw-Hill Building in November 1967 and the Celanese Building in 1970. Complications arose when William A. Ruben, a resident of 132 West 48th Street who lived on the planned Celanese Building site west of Sixth Avenue, refused to move from his home. He finally agreed to move, in July 1968, when he received compensation of over $22,000 (equivalent to $ in ). The construction process was further hampered by labor strikes. During mid-1969, workers on the Exxon and McGraw-Hill Buildings went on strike. Three years later in July 1972, construction workers at several projects across the city, including the Celanese Building, went on strike for over a month.

The Exxon Building, the northernmost of the three towers, was the first building to be completed, in 1971. The 54-story tower had of office space. This was followed by the McGraw-Hill Building, the central tower, in 1973. This 51-story building had of office space. The Celanese Building, the southernmost tower, was the last to open, in 1974. The 45-story building had of office space. After the completion of the final building, the center was spread out across of land and contained around of office space across 19 buildings.




</doc>
<doc id="55979268" url="https://en.wikipedia.org/wiki?curid=55979268" title="Emesa helmet">
Emesa helmet

The Emesa helmet (also known as the Homs helmet) is a Roman cavalry helmet from the early first century AD. It consists of an iron head piece and face mask, the latter of which is covered in a sheet of silver and presents the individualised portrait of a face, likely its owner. Decorations, some of which are gilded, adorn the head piece. Confiscated by Syrian police soon after looters discovered it amidst a complex of tombs in the modern-day city of Homs in 1936, the helmet was eventually thoroughly restored at the British Museum, and is now in the collection of the National Museum of Damascus. It has been exhibited internationally, although as of 2017, due to the Syrian Civil War, the more valuable items owned by the National Museum are hidden in underground storage.

Ornately designed yet highly functional, the helmet was probably intended for both parades and battle. Its delicate covering is too fragile to have been put to use during cavalry tournaments, but the thick iron core would have defended against blows and arrows. Narrow slits for the eyes, with three small holes underneath to allow downward sight, sacrificed vision for protection; roughly cut notches below each eye suggest a hastily made modification of necessity.

The helmet was found in a tomb near a monument to a former ruler of Emesa and, considering the lavishness of the silver and gold design, likely belonged to a member of the élite. As it is modelled after those helmets used in Roman tournaments, even if unlikely to have ever been worn in one, it may have been gifted by a Roman official to a Syrian general or, more likely, manufactured in Syria after the Roman style. The acanthus scroll ornamentation seen on the neck guard recalls that used on Syrian temples, suggesting that the helmet may have been made in the luxury workshops of Antioch.

The Emesa helmet is made of iron and consists of two parts: a head piece and a face mask. The head piece, which includes a neck guard, is made of one piece of iron and attached decorations. Attached to it are silver decorations, some of which are gilded in whole or in part: a diadem, a circular forehead rosette, a strip of metal serving as a crest, two ear guards, and a decorative plate over the neck guard. The ear guards are each attached by three rivets, the top and bottom of which help hold the diadem and decorative plate, respectively, to the head piece; the edges of the diadem and plate are folded over the iron core for additional support. The face mask hangs from the head piece by a central hinge, and would be fastened with straps connecting a loop under each ear with corresponding holes in the neck guard. The entire helmet, the iron core of which is between 1 and 6 millimetres thick, weighs , of which the face mask comprises .

The head piece is made of iron, now rusted. The top contains a dent, and shows the rusted impression of what once was a woven and likely colourful or patterned fabric. From ear to ear around the forehead runs a gilded diadem in the image of a laurel wreath, a traditional symbol of victory. Each side contains thirteen elements, each of three leaves and two berries. The leaves are worked in repoussé, and stand out in strong relief with nearly straight walls. Above the centre of the diadem is a rosette; it shows a flower with two rows, each of six petals, and an outer beaded border. The beading and the outer row of petals are in white silver, contrasting with the gilding of the inner row, the background, and the central rivet anchoring the rosette to the head piece. A narrow fluted strip serving as a crest, smooth silver with beaded edges, runs down the middle of the head piece from the rosette to the neck guard. The relative simplicity and inferiority of artisanship expressed by the crest and rosette may reflect repairs made locally, away from the luxury workshops of Antioch; unlike with the diadem, for example, the background of the rosette was not carefully punched down, but was flattened with a tubular instrument and now presents as a series of rings.
The neck guard, flared outward to protect the shoulders, is covered with a decorative plate consisting of three horizontal designs. At the top, over the base of the skull, a large torus of ivy leaves is bordered by cords; the ivy is gilded, though the cords are not. In the middle, a smooth and concave transitional zone corresponds to the hollow of the neck. At the bottom, an acanthus rinceau, or scroll, is interspersed with birds and butterflies. Portions of the bottom ornamentation are gilded, giving the helmet, with all its silver, gold, and iron components, a polychrome appearance. The ear guards encroach slightly on the bottom design, suggesting that it was not created specifically for the helmet.

The face mask is made of iron, and covered with a sheet of silver. The central hinge from which it hangs is made of three parts: an iron tube welded to the interior head piece with an exterior silver tube, a notched silver tube fixed to the face mask that envelops the first part, and a pin which passes through both and has a silver knob at each end. The mask is shaped in the form of a human face. Holes are drilled between the lips and as nostrils; the eyes each have a narrow slit, with three holes in a trefoil design, two round holes outside and a heart-shaped hole in the middle, underneath each eye to allow for a greater range of vision. These apparently were not enough, for a small and rudimentary notch was carved into each of the heart-shaped holes to increase the wearer's vision. The mask is approximately 2 millimetres thick, of which the silver, which is folded around both the edges and each hole to hold it to the iron, accounts for between .25 and .5 millimetres.

Distinctive features cover the face mask. The nose is long and fleshy with a prominent bump, and extends high between the eyes. The cheekbones are low yet prominent, and the small mouth, which droops toward the sinister side, shows a thick lower lip. Other features—the eyes and eyebrows, and the chin—are more conventional. The distinctive features suggest that the maker of the Emesa helmet attempted to translate some of the individual characteristics of the wearer's face into the helmet.

The Emesa helmet is highly functional, and was likely made for both parades and battle. It is thick and heavy, which would have offered protection against heavy blows or arrows, the former of which may have caused the dent on the head piece. Exceptionally narrow eye-slits also indicate care taken to increase protection; the rough manner in which the holes underneath were enlarged is likely the consequence of an emergency requiring a better field of vision. Although classified as a cavalry sports helmet, the type worn in equestrian displays and tournaments known as the "hippika gymnasia", it was unlikely to have been used in such events. Tournament helmets were robust and manufactured without finesse, to withstand the rigours of contest unscathed. The delicate ornamentation of the Emesa helmet, by contrast, would have been damaged easily, and thus suggests that it would have only been subjected to such risks in the exceptional circumstance of battle.

The helmet was discovered by looters in August 1936, in the modern-day city of Homs. Known as Emesa at the start of the first century AD, the city was at the eastern edge of the Roman Empire, and ruled by the Emesene dynasty, a client kingdom of the Romans. Nearly 2,000 years later, the looters—digging near the former site of a monument to Sampsigeramus—found a complex of rich tombs, and removed the grave goods. Their looting was uncovered because small golden plaques, adorning the burial shroud of the body in tomb 11, flaked off when disturbed. The next morning, children noticed these gold flakes mixed in with the earth and brought them to a bazaar, where it came to the attention of the police; it ultimately led to the arrest of the looters, and the confiscation of the grave objects. The objects, including the helmet, were then secured for the state collection by Emir Djaafar Abd el-Kader, curator of the National Museum of Damascus—even as merchants, eager to capitalize on the stories, hawked modern forgeries and unrelated ancient objects said to come from the tombs of Emesa.

The prompt intervention of el-Kader, who investigated the finds and interrogated the looters, allowed the finds to be recovered and well-understood. He also led further excavations, as did the French archaeologists Daniel Schlumberger and Henri Seyrig. The tomb in which the helmet was found—labelled tomb number 1, of the 22 in the complex—was a pitted grave with two chambers, one upper and one lower. The lower chamber, constituting the proper tomb, had soil for a floor and rock for walls; it measured , and was high. Between five and seven eroded basalt beams were placed over the opening connecting the lower chamber with the upper, which was then backfilled to surface level.

Tomb 1 included a rich assortment of objects. As well as the helmet, it contained a gold funerary mask; a gold and turquoise bracelet; an ornate gold ring with a royal bust in relief; a gold ring with carnelian intaglio; a gold appliqué with a sheep's head and a bird's head; a star-shaped fibula; a gold hook; a small tongue of gold; a spearhead decorated with gold; a silver vase; and a triangle of glass. The looters may have been incorrect in also attributing 19 gold plaques to the tomb, as these were seemingly identical to those from tomb 11. Decorations from the sarcophagus included fragmentary silver rings; 22 gold leaves in repoussé; six masks of Medusa; four rectangles adorned with a lion; four Victories; and eight busts of Apollo. According to Mohammed Moghrabi, who looted tomb 1, the helmet was found next to the skull.

After its discovery, the Emesa helmet underwent several unsuccessful restorations. The primary problem was the oxidisation of the iron core, which created blisters and cracks in the silver covering. Immediately after discovery, the helmet was sent to Paris for restoration by "MM. André père et fils"; only light work was carried out due to the hope that the Syrian climate would help slow the rate of oxidisation, and due to limited funds, but the helmet continued to deteriorate. By 1952, the helmet was described as being significantly more damaged than it had been when excavated, and in urgent need of further work. It was eventually taken to the British Museum, where a final restoration was finished in 1955. This was done by Herbert Maryon, who, in 1946, had reconstructed the Anglo-Saxon Sutton Hoo helmet. In 1956, an account of the process was published by Harold Plenderleith, keeper of the museum's research laboratory. Examination found the silver to be brittle, with cracks that had been filled in by a dark stopping substance. The iron behind the face mask had rusted, putting further stress on the silver and forcing the cracks open by as much as . The face mask—held on by wire, as the hinge had detached—was therefore removed from the helmet to be worked on.

The rusted iron was cut out from the back of the face mask around the mouth and jaw, where the distortions were greatest. To strengthen the silver enough that it could be manipulated, the mask was placed in an electric furnace, and the temperature raised to over three hours; blackened rust was then removed by brushing the mask with 9% oxalic acid, before heating it again, for eighteen hours at and at for thirteen. The silver was then cleaned again, on both sides, with silver gauze temporarily soldered over the cracks in the back to allow the front to be wiped down. The gauze was removed, the silver manipulated to close the cracks, and new gauze installed, permanently, using soft solder. Thin lines of solder showing through the closed cracks were concealed with a surface coating of applied silver. Finally, the iron that had been removed to expose the back of the silver was cleaned and placed back in position. Although a few cracks remained visible higher up on the face mask, they were closed, as the iron behind them was sound and not exerting pressure, yet would have to be removed for restoration to occur.

After restoration at the British Museum, the helmet was displayed in the museum's King Edward Gallery as a month-long loan from 25 April 1955, then returned to Damascus. From 1999 to 2002 the helmet was part of a travelling exhibition, "Syria: Land of Civilizations", with stops in Switzerland, Canada, and the United States. In 2017 the National Museum reopened after closing during the Syrian Civil War, but with the more valuable objects still hidden in underground storage.

The helmet is dated to the first half of the first century AD, based on the style of the acanthus scroll on the back of the helmet, and other objects found with the helmet and in the tombs nearby. It is the earliest known Roman helmet with a face mask, and is broadly classified as a cavalry sports helmet—type D, according to the typology put forward by H. Russell Robinson. Type D helmets are characterised by a single horizontal hinge attaching the face mask to the head piece, and by head pieces that are decorated to represent helmets. Several type D examples exist, such as the Nijmegen helmet, but unlike these, the Emesa helmet was probably never intended for sporting use. It may instead have been given as a gift by a Roman official to a general of the ruling family of Emesa, or manufactured in Syria to the likeness of helmets seen during Roman tournaments. The latter circumstance is thought more likely, for the acanthus ornamentation resembles that seen on Syrian temples. The helmet may therefore have been commissioned from the workshops of Antioch, known for their luxury.



</doc>
<doc id="55998544" url="https://en.wikipedia.org/wiki?curid=55998544" title="IFF Mark II">
IFF Mark II

IFF Mark II was the first operational identification friend or foe system. It was developed by the Royal Air Force just before the start of World War II. After a short run of prototype Mark I's, used experimentally in 1939, the Mark II began widespread deployment at the end of the Battle of Britain in late 1940. It remained in use until 1943, when it began to be replaced by the standardized IFF Mark III, which was used by all Allied aircraft until long after the war ended.

The Mark I was a simple system that amplified the signals of the British Chain Home radar systems, causing the aircraft's "blip" to extend on the radar display, identifying the aircraft as friendly. Mk. I had the problem that the gain had to be adjusted in flight to keep it working; in the field, it was correct only 50% of the time. Another issue was that it was only sensitive to a single frequency and had to be manually tuned to different radar stations. In 1939, Chain Home was the only radar of interest and operated on a limited set of frequencies, but new radars were already entering service and the number of frequencies was beginning to multiply.

Mark II addressed both of these problems. An automatic gain control eliminated the need to adjust the gain, making it much more likely to be working properly when interrogated. To work with many types of radar, a complex system of motorized gears and cams constantly shifted the frequency through three wide bands, scanning each every few seconds. These changes completely automated operation and made it truly useful for the first time; previously, operators could not be sure if a blip was an enemy aircraft or a friendly one with a maladjusted IFF. Originally ordered in 1939, installations were delayed during the Battle of Britain period and the system became widely used from the end of 1940.

Although the Mk. II's selection of frequencies covered the early war period, by 1942 so many different radars were in use that a whole series of sub-versions had been introduced to cover particular combinations of radars. The introduction of new radars based on the cavity magnetron required entirely different frequencies to which the system was not easily adapted. This led to the introduction of the Mark III, which operated on a single frequency that could be used with any radar; it also eliminated the complex gear and cam system. Mk. III began entering service in 1943 and quickly replaced the Mk. II.

Before Chain Home (CH) systems began deployment, Robert Watt had considered the problem of identifying friendly aircraft on a radar display. He filed initial patents on such systems in 1935 and 1936.

In 1938, researchers at the Bawdsey Manor radar research establishment began working with the first of Watt's concepts. This was a simple "reflector" system consisting of a set of dipole antennas that were tuned to resonate at the frequency of the CH radars. When a pulse from the radar hit them, they would resonate for a short period and cause an additional signal to be received by the station. The antennas were connected to a motorized switch that periodically shorted the antenna out and cancelled the broadcast, causing the signal to turn on and off. On the CH display, this caused the "blip" to periodically lengthen and contract. The system proved highly unreliable; it only worked when the aircraft was at certain locations and flying in certain directions.

It was always suspected that this system would be of little use in practice, and when that turned out to be the case the Royal Air Force (RAF) decided on an entirely different system that was also being planned. This consisted of a set of tracking stations using HF/DF radio direction finders. The standard aircraft radios were modified to send out a 1 kHz tone for 14 seconds every minute, allowing the tracking stations ample time to measure the aircraft's bearing. Several such stations were assigned to each sector of the air defence system, and sent their measurements to a plotting station at sector headquarters. There they used triangulation to determine the aircraft's location.

Known as "pip-squeak", the system worked but was very labour-intensive, requiring operators at several stations and at plotting boards in sector HQs. More operators were needed to merge the information from the pip-squeak system with that from the radar systems to provide a single view of the airspace. It also meant the pilots were constantly being interrupted when trying to talk their ground controllers. A system that worked directly with the radar was desired.

Seeking a system that would be as simple as possible, the Bawdsey researchers began work with a regenerative receiver. The idea behind regeneration is to amplify the radio signal and send it into an LC circuit, or "tank", that resonates at a selected frequency. A small part of the tank's output is sent back into the amplifier's input, causing feedback which greatly amplifies the signal. As long as the input signal is relatively constant, like Morse code signals, a single vacuum tube can provide significant amplification.

One problem with regeneration is that if the feedback is too strong, the signal will grow to the point where it begins to broadcast back out of the antenna and cause interference on other receivers. In the case of the IFF system, this is precisely what was desired. When the radar signal was received, and the gain was properly adjusted, the signal grew until it turned the system from a receiver to a broadcaster. The signal levels were still small, but the receivers in the radar systems were extremely sensitive and the signal from the transceiver was larger than what would normally be received from the reflection of the original radar pulse alone.

This extra signal would cause the aircraft's blip on the radar screen to suddenly grow to be much larger. Since it might be difficult to distinguish the resulting larger signal from IFF from the return of a larger aircraft or formation without IFF, the circuit was connected to a motorized switch that rapidly disconnected and reconnected the receiver, causing the blip to oscillate on the radar display. A switch on the front panel allowed the pattern to be controlled; one setting sent back 15 microsecond (μs) pulses, the second setting sent 40 μs pulses and the final setting switched between the two with every received pulse.

There were two major disadvantages of the design. One was that the pilot had to carefully set the feedback control; if it was too low the signal would not be received by the radar station, and if it was too high the circuit would amplify its own electronic noise and give off random signals known as "squitter" across a wide range of frequencies. This caused significant interference over a large area and was a major problem for radar operators. It was too easy to forget to adjust the gain during flight, especially in single-seat fighters, and it was estimated a usable signal was only returned about 50% of the time.

The other problem was that the CH stations operated on a small but distinct set of frequencies, and the system only worked at the single tuned frequency. An aircraft on a typical mission profile might only be visible to a single CH station, or perhaps two or three over their operational area. To address this, the IIF had a card with the frequencies of local CH stations on it, which the pilot had to tune as they moved about. Pilots often to forgot to do this, and if they were lost or off-course, they would not know which frequency to tune to, or the nearest station might not be on the card at all.

The Mark I was used only experimentally. Thirty sets were hand-made at AMES and an order for 1,000 was placed with Ferranti in September 1939.

Beyond the operational problems with the Mark I, a more serious issue was the ever-growing number of new radar systems being deployed through this period. Even as the Mk. I was being tested, the RAF, Royal Navy and British Army were all introducing new systems, spanning a wide range of frequencies from the RAF's 200 MHz systems used on night fighters and Chain Home Low through the Army's 75 MHz gun-laying radars and on to the CH at 20 to 30 MHz. Attempting to manually tune among these would be impractical, and completely impossible if the aircraft were visible to more than one radar, which was increasingly the case.

A solution was already under development in early 1939. This was similar in general terms to the Mark I, but employed tuned circuits sensitive to many radar sets. It used a "complicated system of cams and cogs and Geneva mechanisms" to switch among the different bands by connecting to different oscillators covering a given band, and then used a motorized tuning capacitor to sweep through the frequency range within that band. To ensure that the signal was the right strength and did not cause squitter, an automatic gain control was added. These changes eliminated the need for tuning or gain adjustments in flight, greatly improving the chance it would respond correctly to a radar's query. Only periodic adjustments on the ground were needed to keep it working properly.

An order for 1000 sets was sent to Ferranti in October 1939, and they had completed the first 100 sets by November. The rapid expansion of the RAF precluded a significant proportion of its force being equipped by the time of the Battle of Britain in mid-1940. In any case, the action took place mostly over southern England, where IFF would not be very useful as the CH stations were positioned along the coastline and could only see the fighters if they were out over the English Channel. There was no pressing need to install the systems, and pip-squeak continued to be used through the Battle of Britain.

The lack of IFF did lead to problems. Friendly fire was one; the Battle of Barking Creek in September 1939 would not have occurred if IFF had been installed. It also meant that enemy aircraft could not be identified if they were close to known RAF planes. In July 1940 the Germans began to take advantage of this by inserting their bombers into formations of RAF bombers returning from night missions over Europe. To the ground operators these appeared to be more RAF aircraft, and once they crossed the coast there was no way to track them. Even if one of the rare Mark I sets was available, the unreliability of their signals made it difficult for controllers to trust it.

As the Battle of Britain ended, Mk. II was rapidly installed in RAF aircraft. Its installation on the Supermarine Spitfire required two wire antennas on the tail that slowed the top speed by and added of weight. Pip-squeak was still used for areas over land where CH did not cover, as well as an emergency guidance system. Mark II also found a use on Royal Navy ships, where it was produced as the Type 252 so that ships could identify each other by radar alone. 

A Mark II set was taken to the US as part of the Tizard Mission in November 1940. US researchers were already working on their own IFF system of some complexity. They realized the importance of using a common IFF system, and in early 1941 they decided to install Mark II in their own aircraft. Production was taken up by Philco with an order for 18,000 sets as the SCR-535 in July 1942. The system was never entirely reliable.

The profusion of radars that led to the Mk. II continued, and by 1942 there were almost a dozen sub-types of the Mk. II covering different sets of frequencies. The cavity magnetron had matured and an entirely new set of radars operating in the microwave region was about to enter service. These worked on frequencies that the IFF receivers could not operate on.

In 1940, English engineer Freddie Williams had considered this problem and suggested that all IFF operations move to a single frequency. Instead of responding on the radar's frequency and thus mixing with their signal in the receiver, a completely separate unit would transmit "interrogation" pulses in synchronicity with the radar's pulses, and the received signals would be amplified independently and then mixed with the radar's signals on the display. This greatly simplified the airborne equipment because it operated on a single frequency, eliminating the complex multi-band system. The only disadvantage was that a second transmitter was needed at the radar stations.

Production of the IFF Mark III began at Ferranti and was quickly taken up in the US as well by Hazeltine. It remained the Allies' primary IFF system for the rest of the war, and the 176 MHz common frequency was used for many years after.






</doc>
<doc id="56009989" url="https://en.wikipedia.org/wiki?curid=56009989" title="Cadaver Tomb of René of Chalon">
Cadaver Tomb of René of Chalon

The Cadaver Tomb of René of Chalon (, also known as the Memorial to the Heart of René de Chalon or The Skeleton) is a late Gothic period funerary monument, known as a "transi", in the church of Saint-Étienne at Bar-le-Duc, in northeastern France. Consisting of an altarpiece and a limestone statue of a putrefied and skinless corpse which stands upright and extends his left hand outwards. Completed sometime between 1544 and 1557, the majority of its construction is attributed to the French sculptor Ligier Richier. Other elements, including the coat of arms and funeral drapery, were added in the 16th and 18th centuries respectively.

The tomb dates from a period of societal anxiety over death, as plague, war and religious conflicts ravaged Europe. It was commissioned as the resting place of René of Chalon, Prince of Orange, brother-in-law of Duke Antoine of Lorraine. René was killed aged 25 at the siege of St. Dizier on 15 July 1544, from a wound sustained the previous day. Richier presents him as an "écorché", with his skin and muscles decayed, leaving him reduced to a skeleton. This apparently fulfilled his deathbed wish that his tomb depict his body as it would be three years after his death. His left arm is raised as if gesturing towards heaven. Supposedly, at one time his heart was held in a reliquary placed in the hand of the figure's raised arm. Unusually for contemporary objects of this type, his skeleton is standing, making it a "living corpse", an innovation that was to become highly influential. The tomb effigy is positioned above the carved marble and limestone altarpiece.

Designated a "Monument historique" on June 18, 1898, the tomb was moved for safekeeping to the Panthéon in Paris during the First World War, before being returned to Bar-le-Duc in 1920. Both the statue and altarpiece underwent extensive restoration between 1998 and 2003. Replicas of the statue are in the Musée Barrois in Bar-le-Duc and the Palais de Chaillot, Paris.

René of Chalon, Prince of Orange and stadtholder of Holland, Zeeland, Utrecht and Gelre, died on 15 July 1544, aged 25, during the siege of St. Dizier where he fought for Emperor Charles V. René had been mortally wounded in battle the previous day, and died with the Emperor in attendance at his bedside. He died without leaving any direct descendants. Charles wrote soon after to René's wife, Anna of Lorraine (d. 1568), setting out in detail the circumstances of René's last hours and death. The monument apparently fulfills his wish that he be represented above this tomb as an écorché, that is a body without skin, and "as he would be three years after his death". Cadaver tombs had been built for other members of the family, including his father Henry III of Nassau-Breda, his uncle Philibert of Chalon, his grandmother, and the uncle of his wife. René requested that his tomb present him "not as a standard figure but a life-size skeleton with strips of dried skin flapping over a hollow carcass, whose right hand clutches at the empty rib cage while the left hand holds high his heart in a grand gesture". 
René's intention has never been definitively attributed, and there is no mention of it in either Charles' letter or René's will. Given this lack of record and that, at only 25 years, René was unlikely to have previously thought closely about his own burial and memorial, it seems most likely that the idea behind the design came from Anna. She is known to have commissioned the piece from Ligier Richier, who was then little known outside his local area of Saint-Mihiel in north-eastern France; is today considered one of the most important sculptors of the late Gothic period. Although the precise dating is uncertain, it is known to have begun after 1544 and was completed before 1557. The tomb has become his most well known and influential work.
In accordance with contemporary funeral rites, René's heart, bowels and bones were separated. His heart and bowels were kept at Bar-le-Duc and placed in the Collegiate Church of St. Maxe, which was demolished during the French Revolution and abandoned in 1782, while the rest were transferred to Breda to be interred with his father and his daughter, who died in early infancy. His widow commissioned Richier to construct a transi to hold some of the remains of her husband. The monument, along with other remains and relics of members of his family, were reinterred at the church of Saint-Étienne in June 1790.

Ann commissioned the tomb as a memento mori, but the level of detail she may have specified is uncertain. It is perhaps Richier's best known work, remarkable for its original presentation of a "living corpse", a motif unparalleled in earlier funerary art. He produced one more work in a similar vein, his "Death", now in the Musée des Beaux-Arts de Dijon. Both works are comparable in form and intent to the 1520s "La Mort Saint-Innocent" originally from the Holy Innocents' Cemetery in Paris, now in the Musee du Louvre. In that work, a realistically depicted and severely emaciated corpse raises his right hand upwards while holding a shield in his left hand.

The limestone statue is composed from three blocks of stone making up his head and torso, his left arm, and his legs and pelvis; each of which slot into each other. Both the statue and it's frame are supported by an iron stud located at the figure's pelvis. 

The life-sized figure represents a putrefied and emaciated, skinless corpse, and is positioned above an altarpiece. It's left arm reaches out, while its other rests on its chest. The hand of the outstretched arm may have once have held his preserved heart, and extends in a gesture that may be either pleading or in tribute to a higher being. It is 177 cm (5.8 in) in height, and made from black marble and limestone. The skeleton is sculpted with forensic and unflinching realism. It is placed on a stylobate which supports two black marble columns with Corinthian capitals. A coat of arms is placed underneath the figure, while the escutcheon is empty. The figure has been described as a "rotting corpse with shredded muscles falling from the bones and skin hanging in flaps over a hollow carcass". 

His left hand reaches upwards as if pleading to heaven or God. The gesture may be in reference to the biblical passage from Job 19:26: "And though after my skin, worms destroy my body, yet in my flesh shall I see God". The gesture may represent contrite pleading or supplication, or the ability of the spirit to overcome mortality. The art historian Kathleen Cohen writes that the monument may be an illustration of the "doctrine of corruption as a necessary step toward regeneration".

The hand holding the heart was broken off and stolen by a French soldier in 1793. It was later replaced, but shown holding either a clepsydra or hourglass, obvious symbolic objects for a memento mori. However, that placement changed the meaning of the sculpture, from a representation of René to a depiction of the personification of death or as a danse macabre.

The frame consists of black marble octagonal panels set in white stone, between which were twelve small corbel statuettes measuring between 38 and 40 cm (1.25–1.3 in) in height. None remain today; six are known to have destroyed in November 1793 during the French Revolution. The escutcheon above the statue is missing its emblem. 

The altarpiece is made from black carved marble and limestone and measures 267 x 592 cm (105 x 233 in). Its top-slab is taken from the former tomb of Henry IV, Count of Bar (d. 1344) and Yolande of Flanders (d. 1395). The black slab contains two series of inscriptions which are also later additions. The coat of arms of Bar and Lorraine were added to the front face in 1810 at the request of the then vicar of Saint-Étienne, Claude Rollet. The funeral drapery is also a later addition.
The altar holds a glass-covered reliquary for the bones of other royals and nobles of the Duchy of Bar, and includes the remains of Henry IV and his wife Yolande, Robert, Duke of Bar (d. 1411) and his wife Marie of France (d. 1404), as well as those of their son, Edward III, Duke of Bar (d. 1415). Other possible internees include Frederick I, Duke of Upper Lorraine, Edward I, Count of Bar (d. 1336) and Mary of Burgundy (b. 1298). The mural on the wall behind the statue was painted by Varembel Barber in 1790.

Cadaver tombs, in France known as transis, were intended to show the human body's "transition" from life to decomposition. Art historians debate this particular example's meaning, specifically the symbolism of the raised hand and what it originally held. At one time, the raised hand is supposed to have contained the prince's actual dried heart. 

The effigy is viewed by art historians in two distinct ways. The more literal interpretation is that the tomb is a dedication commissioned by a loving and pious wife. Other scholars, including Bernard Noël and Paulette Choné, read deeper meaning, and invoking a sense of the "spirituality of death", view the work as a comment on both the inevitability and effect of death. These opposing interpretations were juxtaposed in 1922 by the novelist Louis Bertrand when he wrote that the tomb may represent either despair or a romantic ideal of the eternal spirit. A further interpretation is that the work represents a mark of penance or repentance of past sins.

A copy of the cadaver for the Palais de Chaillot was produced in 1894. François Pompon made a further copy in 1922 for the tomb of the playwright and poet Henry Bataille at Moux, while another replica is in the Musée Barrois in Bar-le-Duc. "Death", an unattributed 16th century sculpture realistically depicting a corpse wrapped in a shroud, now in the Musée des Beaux-Arts de Dijon (catalog number 743), is very similar, but much smaller.

The first literary reference to the Transi appears in Louis Des Masures' 1557 "Epitaph on the Heart of René de Chalon, Prince of Orange", and a photograph of the statue appears on the cover of the 1992 Faber edition of the book. The French poet Louis Aragon evoked the tomb in "Le Crève-cœur", published in 1941. It inspired the titular poem in Thom Gunn's 1992 collection "The Man with Night Sweats"; elegies written in the aftermath of the deaths of friends from AIDS. The poems includes the lines "My flesh was its own shield:/Where it was gashed, it healed. / Stopped upright where I am / Hugging my body to me / As if to shield it from / The pains that will go through me". Simone de Beauvoir details her first encounter the tomb in her 1974 autobiography "All Said and Done", describing it as a "masterpiece" of a "living man...already mummified".

The tomb was designated as a "Monument historique" on 18 June 1898.

The tomb was originally placed in the collegiate church of Saint-Maxe in Bar-le-Duc, where it was positioned over a vault which may have held the hearts of Antoine de Lorraine, René and other members of his family. It was moved to the church of St Ėtienne in 1782 when the former site was abandoned. It was moved to the Panthéon in Paris during the First World War, before it was returned to Bar-le-Duc in 1920. 

Due to humidity and contact with water, the tomb has suffered damage over the centuries. It was restored in 1969 by Maxime Chiquet d'Allancancelles. Both the statue and altarpiece underwent further restoration between 1998 and 2003. In 1993 both the retable and the tomb were classified as historic monuments, and underwent restoration. An extensive assessment and historical study commissioned by the Direction régionale des affaires culturelles in 1998 was followed by a condition assessment and recommendations in 2001.

The 2003 restoration was conducted in stages, beginning with the dismantling of the statue which was painstakingly cleaned with cotton buds, before the altar was dismantled to clean its back wall. Microcrystalline cellulose wax was used to polish both the back wall and side columns. The restorer Françoise Joseph cleaned the mural, brightening the colours, and during the process discovered decorations at each of its four corners. Because the church's basement is often water-logged in winter, the mural had been damaged by humidity. Repairs to the statue included the removal of wrinkles, splinters, cracks and graffiti; much of the work centered on areas around the groin, knee and pelvis. The iron fasteners were removed and replaced with stainless steel studs, removing future risk of oxidation.




</doc>
<doc id="56065235" url="https://en.wikipedia.org/wiki?curid=56065235" title="2007 AT&amp;T 250">
2007 AT&amp;T 250

The 2007 AT&T 250 was a NASCAR Busch Series stock car race that took place on June 23, 2007. Held at the Milwaukee Mile in West Allis, Wisconsin, the race was the 17th of 35 in the 2007 NASCAR Busch Series season. Aric Almirola of Joe Gibbs Racing (JGR) was the listed winner of the race, Richard Childress Racing's Scott Wimmer finished second, and Braun Racing's Jason Leffler finished third.

The race became controversial because of a driver change made by the No. 20 JGR team. Almirola qualified the car on pole position, though Gibbs intended to have NASCAR Nextel Cup Series regular Denny Hamlin run the race. The Cup Series was racing that weekend at Sonoma Raceway in Sonoma, California, and Hamlin's helicopter could not find a landing spot at the track in time for the start of the race. Almirola was thus forced to start the race and ran the first 59 laps before he was pulled out of the car under caution; Hamlin finished the race and came from behind to win after losing a lap to the leaders during the driver change. NASCAR rules state that the driver who starts the race gets credit for the result, making Almirola the official race winner. The driver change frustrated Almirola, who proceeded to leave the track before the race ended, and it was further criticized by ESPN writer Terry Blount, who called the substitution "a Busch-league move."

The win for which Almirola was given credit was the first of his Busch Series career. Carl Edwards, who led nearly half of the race for Roush Fenway Racing (RFR), recovered from a flat tire to finish eighth, maintaining a significant lead in the Drivers' Championship. Edwards's No. 60 RFR team also maintained their Owners' Championship lead, and Chevrolet continued to lead the Manufacturers' Championship.

The Busch Series first came to the Milwaukee Mile in the 1984 and 1985 seasons before taking a seven-year absence from visiting the track. It returned to the schedule in 1993 and had been on the series calendar every year since then leading up to the 2007 edition of the race. The track itself was originally built as a horse racing track, and it later held its first automobile race in 1903, making it the oldest motor racing track in the United States.

Entering the race, the last two Busch Series races at Milwaukee had been won by Johnny Sauter and Paul Menard, both from Wisconsin. Four Wisconsin-born drivers entered the race hoping to continue the trend: Scott Wimmer, Todd Kluever, Kelly Bires, and Frank Kreyer. Wimmer, who entered the race with three consecutive top-five finishes and five straight top-tens, stated, "It's really exciting anytime I go back to Wisconsin for racing, especially The Milwaukee Mile. I've been going there since I can remember and watching a lot of great drivers racing out there." He also expressed excitement at the prospect of winning the upcoming race, saying, "It would really be neat to win a race there. I think that any Wisconsin driver, no matter what series, wants to win there."

Carl Edwards led the Drivers' Championship entering the race with 2,534 points. Dave Blaney followed in second with 1,833, while Kevin Harvick was third with 1,798. David Reutimann, Regan Smith, David Ragan, Greg Biffle, Marcos Ambrose, Bobby Hamilton Jr., and Jason Leffler rounded out the top ten. Jack Roush, owner of Edwards' No. 60 car, led the Owners' Championship, also with 2,534 points. Richard Childress's No. 29 team, shared by Wimmer and Jeff Burton, followed in second with 2,323, while Joe Gibbs's No. 20, Childress's No. 21, and DeLana Harvick's No. 33 teams completed the top-five. Chevrolet led the Manufacturers' Championship with 115 points; Ford, Dodge, and Toyota followed with 109, 69, and 59 respectively. Menard, the defending race winner, did not participate.

Two practice sessions were held the morning and afternoon before the evening race. With a time of 29.981 seconds, Wimmer was the quickest in the opening session ahead of Aric Almirola, Stephen Leicht, Reutimann, and Shane Huffman. Positions six through ten were occupied by Hamilton, Erik Darnell, Travis Kvapil, Todd Bodine, and Kelly Bires. In the second practice session, Wimmer was once again quickest with a lap time of 29.821 seconds, followed by Almirola, Johnny Benson Jr., Brad Coleman, Jason Keller, Leffler, Darnell, Huffman, Scott Lagasse Jr., and Leicht.

Forty-four cars entered qualifying; due to NASCAR's qualifying procedure, only forty-three could race. Almirola qualified his No. 20 car on pole position with a time of 29.608 seconds. Almirola was set to step aside for the race, however, as Nextel Cup Series regular Denny Hamlin was scheduled to travel from Sonoma Raceway to compete in the Saturday night event in Milwaukee. Almirola, who also qualified Hamlin's car on the pole the year before, commented, "Man, two poles in a row at Milwaukee and I don't get to race. Something's got to be set for that. I'll sit on the pit box and watch. I've got a lot to learn about racing these Busch cars and Denny is really, really good, so I'll just sit there and listen and learn all I can from Denny."

Almirola was joined on the front row by Leffler, while Wimmer, Coleman, and Huffman rounded out the top-five qualifiers. Bodine, Benson, Reutimann, Edwards, and Lagasse made up positions six through ten. Danny Efland was the only driver who failed to qualify as he did not set a qualifying time. Edwards replaced Kvapil in the No. 60 car after practice, qualifying in ninth. Like Hamlin, Edwards was also traveling from Sonoma and nearly missed qualifying, later remarking, "One minute later, we wouldn't have made it."

The 250-lap race began at 8:00 p.m. EDT, and was televised live in the United States on ESPN2. Hamlin's helicopter could not find a place to land in the infield; the helicopter pad was blocked by parked cars, forcing Hamlin to land elsewhere and arrive late via ground transportation. Hamlin was thus unable to start the race in Almirola's car, forcing Almirola to start the race himself. Ragan, the third Nextel Cup regular traveling to the track from Sonoma along with Hamlin and Edwards, replaced Darnell in Roush's No. 6 car; Ragan was forced to move to the rear of the field because of the driver swap and an engine change, as was Chase Miller who went to a backup car.

Almirola maintained his lead from pole position for the first 43 laps before being passed by Edwards. The caution had been displayed on lap 30 due to oil on the track in turn four, and shortly after the lap 43 restart, Edwards took over the lead of the race. On lap 57, Ron Hornaday Jr. was involved in an accident, prompting another caution period. It was under this caution period that the Gibbs team elected to make the driver change, and Hamlin took over driving the car for the remainder of the race.

Hamlin lost a lap and fell to 34th place, remaining a lap down until lap 149 when he received the free pass, allowing him to return to the lead lap. Edwards, meanwhile, continued to lead the race for a total of 123 laps, before Mike Wallace assumed the race lead. Six laps later, on lap 173, Hamlin completed his comeback drive to retake the lead for the No. 20 team. Edwards, meanwhile, suffered misfortune in the form of a flat right rear tire around the same period in the race, forcing him to pit with 77 laps remaining.

On lap 223, Kreyer was involved in an accident, causing the caution to be displayed again. Wimmer assumed the lead after pit stops, holding it until the caution came out again for Kevin Hamlin's accident. On the restart, Wimmer battled for the lead with Leffler, while Hamlin made it three-wide to retake the lead with 13 laps to go. Marc Mitchell, Richard Johns, and Brent Sherman crashed on lap 244, requiring another caution to be displayed. The race restarted with four laps remaining, with Hamlin retaining the lead to the finish. Since NASCAR rules credit the finishing position to the starting driver, Almirola was awarded the win. Wimmer, Leffler, and Coleman followed in second through fourth, while Keller, Bodine, Reutimann, Edwards, Benson, and Huffman completed the top-ten. Hamlin's margin of victory over Wimmer was .502 seconds.

Hamlin appeared in Victory Lane to represent Almirola's first career win in front of a crowd of 41,900 attendants, earning $66,823 for the victory; both the win and prize money were credited to Almirola, while team president J. D. Gibbs confirmed that Almirola would receive the winner's check. Hamlin credited Almirola for putting the team in a good position prior to the race, saying Almirola "did all the hard work." Runner-up Wimmer was taken aback that Gibbs elected to make the driver change: "I was surprised they did it, because Aric was running a good race." He also believed that fewer caution periods may have given him a better chance to win the race, saying, "I just drove as hard as I could, and unfortunately we weren't as good on the short runs. We'd get going after 20 laps, and I didn't need those cautions. Maybe we'll get a win one day. Maybe we won't. I don't know." Gibbs explained that Almirola was frustrated after being taken out of the car: "He's upset. I left a message for him [Saturday] night. I know he's upset. I would be too if I'm in his shoes." He also expressed relief that Hamlin was able to win the race, arguing, "Thank goodness he won. It would have looked bad if he didn't."

When explaining why the team made the decision to put Hamlin in the car, Gibbs said, "I told those guys as a group, if you think Denny can get in the car and win the race, let's go. Let's do that. If you don't think he can do that, let Aric run it out. Our guys kind of thought about it as a group and said, 'OK, we think Denny can run well and we're fast enough to win the race.' That was a huge discouragement of course to Aric." Other reasons included sponsorship obligations with Rockwell Automation, which sponsored the No. 20 car. Coleman, a Joe Gibbs Racing teammate, believed, "That might have had something to do with it." Almirola also expressed his belief that Rockwell, who are headquartered in Wisconsin, wanted Hamlin to drive during the race. "I totally understand the Gibbs side of the situation. You need that Cup superstar to sell sponsorship. It's not easy to sell sponsorship for somebody who hasn't proven themselves yet, and I understand that. At the time, in the heat of the moment, I was deep in the battle of the race. I didn't totally agree and understand the situation. But looking back on it now, I understand it. Rockwell's invested a lot in Denny Hamlin and Joe Gibbs Racing, so they deserved everything they got there at Milwaukee. They deserved to have their racecar in the spotlight and I was happy that I got the pole for them and that Denny won the race." He also stated that he did not consider himself to have won his first career race. "I feel like I was a part of it, but by no way, shape or form do I feel like that was my first victory. I feel like my first victory is still to come and I'll actually be in the car when it crosses the start-finish line for that one."

The driver change also attracted criticism from ESPN journalist Terry Blount, who called it "a Busch-league move," writing, "As if we don't have enough Cup dominance in the Busch Series, now they're replacing Busch drivers after a race starts." He continued, "Almirola was furious. Good for him. He should be furious. If a driver isn't angry about getting pulled from the car in the middle of a race, then he needs to take up another profession."

Edwards, who led the most laps and recovered to finish eighth after dominating the early stages of the race, insisted the night was "still fun," saying, "It was pretty frustrating. But you know what's cool? We raced hard and we had a lot of fun racing here at Milwaukee. Congratulations to Denny Hamlin. I can't believe they did a driver switch and he still won the race; that's pretty awesome...we just kind of had a bad luck night."

The result kept Edwards in the lead in the Drivers' Championship with a new total of 2,686 points. Reutimann and Ragan improved their positions to second and third (albeit 776 and 846 points behind) respectively, while Blaney and Kevin Harvick fell to fourth and fifth. Leffler, Ambrose, Hamilton, Smith, and Leicht rounded out the top ten. Roush's No. 60 team also maintained the lead in a much closer Owners' Championship with 2,686 points; Wimmer's strong second-place finish left Childress's No. 29 team only 188 points behind Roush, while Gibbs's No. 20, Childress's No. 21, and DeLana Harvick's No. 33 remained third, fourth, and fifth. Chevrolet maintained their lead in the Manufacturers' Championship with 124 points; Ford, Dodge, and Toyota followed with 113, 72, and 65 respective points.


</doc>
<doc id="56139974" url="https://en.wikipedia.org/wiki?curid=56139974" title="Herman Vandenburg Ames">
Herman Vandenburg Ames

Herman Vandenburg Ames (; August 7, 1865 – February 7, 1935) was an American legal historian, educator, and archivist long associated with the University of Pennsylvania, where he was a professor of United States constitutional history and, from 1907 to 1928, dean of its graduate school. His 1897 monograph, "The Proposed Amendments to the Constitution of the United States During the First Century of Its History", was a landmark work in American constitutional history. Other works by Ames included "John C. Calhoun and the Secession Movement of 1850", "Slavery and the Union 1845–1861", and "The X.Y.Z. Letters", the latter of which he authored with John Bach McMaster. Among his notable students were Ezra Pound, John Musser, and Herbert Eugene Bolton.

A member of the Ames family, Herman Ames was born in Massachusetts and educated at Amherst College. He received his doctorate from Harvard University, where he was the Ozias Goodwin Memorial Fellow in Constitutional and International Law, and studied under Albert Bushnell Hart. Like Hart, Ames spent time in Europe learning German historical methodology and was influenced in his own research by its approach. He was a driving force behind the establishment of the Pennsylvania State Archives and helped guide the widespread establishment of government archives throughout the United States. His papers are housed at the University of Pennsylvania's University Archives.

Herman Ames was born in Lancaster, Massachusetts, in 1865 to Marcus Ames and Jane Angeline Ames (née Vandenburg).

Ames' father, Marcus, was educated at Philips Andover Academy, where he graduated as valedictorian before studying medicine at Harvard Medical School. He was ordained in 1854, becoming in the words of David Ford a "brilliant, fervent, and impressive" Congregational preacher who ministered throughout Massachusetts, later serving as superintendent of the Lancaster Industrial School for Girls and chaplain of Rhode Island's asylum, prison, workhouse, and almshouse. Marcus Ames' parents—Herman Ames' paternal grandparents—were Azel Ames and Mercy Ames (née Hatch). Herman Ames' great grandfather, Job Ames, served in the Massachusetts Militia during the American Revolution. The Ames family descended from William Ames, who immigrated to the Province of Massachusetts Bay from the town of Bruton, England, in 1641.

The family's surname may have been a corruption of the name "Amyas" (meaning "merchant of Amiens"). In the 16th century "Amyas" was frequently confused with "Ames".

Ames was educated at the Mowry and Goff School in Providence, Rhode Island. After graduating, he enrolled at Brown University before transferring to Amherst College, where he was initiated into the Delta Upsilon fraternity and later became the chapter president. During the Delta Upsilon convention of 1887, Ames played a central role in resolving an intra-fraternity dispute concerning the authority by which the Executive Council of Delta Upsilon had admitted the DePauw University chapter some months earlier. As a student at Amherst, he was particularly influenced by Anson D. Morse, whom he credited with cultivating in him "a judicial attitude in the study of history", years later recalling that he had "never come in contact with a teacher who was so judicially minded".

Ames graduated from Amherst with an A.B. degree in 1888, and thereafter entered Harvard University. At Harvard he received an A.M. in 1890 and a Ph.D. the following year for his dissertation "The Proposed Amendments to the Constitution of the United States", which was done under the supervision of Albert Bushnell Hart. During his time at Harvard, he was
the Ozias Goodwin Memorial Fellow in Constitutional and International Law.

Between 1891 and 1894, Ames lectured in history at the University of Michigan under an appointment as acting assistant professor to fill a vacancy created by the resignation of J.H.T. McPherson. Though his interest was in U.S. history, at Michigan Ames was charged with teaching courses covering a variety of periods of world history, an assignment to which he would admit he was "not particularly prepared", but he resigned himself to the idea that "one must make a beginning somewhere, and this was the opening offered". He later recalled this first teaching experience "was a valuable one to me, far more so, I fear, than to the students taught. I was afforded the opportunity to become acquainted with the life and work of the leading state university of the time".

Ames spent 1895 abroad, taking advanced studies in history at Leipzig University and Heidelberg University. During his fifteen months in Europe, he also undertook a grand tour.

The year following his return from Germany, Ames was hired as an assistant professor in the history department at Ohio State University. In 1897, he moved to the University of Pennsylvania to continue teaching history, and by 1908 had become a full professor. From 1907 to 1928, he served as dean of Pennsylvania's graduate school. As dean, Ames made his office in room 105 of College Hall, a room which he shared with the rest of the graduate school staff. Under his administration, the number of graduate students at Pennsylvania increased five-fold and, in 1923, he consulted with U.S. Secretary of State Charles Evans Hughes on implementing a scholarship to fund the study of diplomacy at Pennsylvania using an $80,000 endowment from the late Frederic Courtland Penfield, which was among the largest university scholarship funds in existence at that time.

At Pennsylvania, Ames established a reputation for "tact, firmness and high ideals of scholarship". John Musser, one of his former students and graduate assistants, recalled that Ames' relationship with his students was accessible, courteous, and helpful and that he was known for welcoming students to his home and keeping notes on their careers after they had graduated. University provost Josiah Penniman would echo Musser's assessment, stating that he knew "of no dean who was more deeply interested in the graduate students of the University, not only those who were studying American History, but all who were studying in any courses".

During the 1901–02 academic year, Ames was one of Ezra Pound's instructors at Pennsylvania; Pound scholar David Ten Eyck credits him with stimulating the poet's interest in American history. Pound would later recall that Ames' "courses had a vitality outlasting the mere time of his lectures" and would humorously note that Ames was "undisturbed and undistracted" by students playing ping pong outside his office. According to Ten Eyck, studies of Pound's notes from Ames' courses indicate his "deep interest in the subject that must have provided a foundation for [Pound's] later reflections on American history". After Ames' death, Pound would note that—though they had no more contact than "perhaps two or three letters" in the ensuing decades—he continued to harbor a "strong, personal affection" towards Ames, citing this as proof of "humanity overcoming all systems of invented partition". Another notable student of Ames included Herbert Eugene Bolton.

As a scholar of legal history, his view of the United States Constitution was at once both liberal in its outlook while also guarded at attempts to meddle with its basic framework. When the eighteenth amendment, introducing prohibition of alcohol, was enacted he immediately and correctly predicted its eventual repeal.

In his view on history, Ames was both presentist and relativist; during a public lecture given at Muhlenberg College in 1909 he characterized the daily customs and behavior of early American settlers as "barbarous" compared to contemporary standards, and credited the growth of democracy with the development of more liberal social norms. At the same time, however, he cautioned about passing moral judgments on leaders of the past based on modern expectations. As an instructor of history, Ames was described by contemporary historian Wayne Journell as "unabashedly" supportive of its use to invigorate support for the government's policies during World War I, quoting him that "it is the duty of the teacher of history and civics to seize the wonderful opportunity afforded by the war to aid in promoting an intelligent and patriotic public opinion in support of the government in these critical times". Musser offered contradictory recollections of Ames' academic approach, describing him as having a completely objective and impartial view towards both history and current affairs and viewing with skepticism the idea that concerns such as politics or national interest should influence his teaching or research.

In the summer of 1908, Ames was a visiting lecturer at the University of Wisconsin–Madison where he taught the course "Political and Constitutional History of the United States, 1786–1837". He also held visiting lectureships at Columbia University and the University of California at Berkeley. Active in scholarly exchanges, Ames served on the Administrative Board of the Institute of International Education, led the Council of National Defense Philadelphia host committee during the 1920 visit of a British educational mission to the city, and represented the University of Pennsylvania to the 1909 convention of the American Association of Universities. He was the commencement speaker during the 1923 graduation exercises at New Brunswick High School in New Brunswick, New Jersey, delivering an address titled "Preparing for Citizenship". During the 1925 University of Pennsylvania commencement exercises, Ames and John Carew Rolfe both received honorary Doctor of Letters degrees.

After 21 years as head of Pennsylvania's graduate school, Ames resigned his post in 1928 and was succeeded by the classicist H. Lamar Crosby. He continued his teaching duties until his death.

Ames' administrative and teaching duties were an ongoing encumbrance on his research activity, limiting him to a small, albeit influential, body of work.

Ames' 1897 monograph "The Proposed Amendments to the Constitution of the United States During the First Century of Its History", which indexed 1,736 amendments proposed to the United States Constitution, was an expansion of his doctoral dissertation. According to Ames, he had returned from his travels in Europe too late in the year to find a teaching assignment and decided to spend the ensuing months writing and researching instead.

Ames' 400-page opus marked the first exhaustive catalog of proposed amendments to the U.S. Constitution ever compiled. Ames personally visited a large number of state and federal offices to record the details of the thousands of amending resolutions that had been proposed during the preceding hundred years. In discussing his research, Ames concluded that many of the amendments had failed as they were "cures for temporary evils ... were trivial or impracticable ... [or] found a place in that unwritten constitution that has grown up side by side with the written document". He also opined that the majorities required for ratification of amendments were so large as to create "insurmountable constitutional obstacles" to amendment, a frequent criticism leveled during the Progressive Era.

A review of the volume in the "Annals of the American Academy of Political and Social Science" concluded it was "a laborious and painstaking piece of work". "The Proposed Amendments to the Constitution of the United States During the First Century of Its History" earned Ames the American Historical Association's Justin Winsor Prize. It was reprinted in 1970.

Other works by Ames included "John C. Calhoun and the Secession Movement of 1850", "Slavery and the Union 1845–1861", and "The X.Y.Z. Letters", the latter of which he authored with John Bach McMaster. Ames also edited a volume of "State Documents on Federal Relations", a multi-issue compendium of state legislative documents pertaining to the U.S. government.

At the turn of the 20th century, the American Historical Association (AHA) undertook a nationwide effort to examine repositories of manuscripts and archival documents, and make specific recommendations for their future preservation. The initiative was partly influenced by Ames' advocacy of German historical methodology, learned during his year in Europe, which placed special emphasis on primary documentary sources. At the behest of the AHA, Ames spent several weeks in Harrisburg in 1899, examining Pennsylvania's state records, which were poorly organized and largely scattered across various state offices. He co-authored, with historian Lewis Slifer Shimmel, a report on their status, and in 1900 filed a separate report on the state of the Philadelphia municipal archives. Their report concluded that over the years Pennsylvania's public records had been partially plundered by government officials with some state documents known to be held by libraries in New York and Boston, and others probably once bearing the original signature of William Penn had since had their signature lines cut out, perhaps for souvenir keeping. Ames continued his attempts to inventory Pennsylvania public records in tandem with his teaching duties at the University of Pennsylvania, although his efforts were hampered by the tradition that all state government offices closed promptly at 3 o'clock in the afternoon.

Ames and Shimmel ended their work with several recommendations. First, they advised that original manuscripts, where they could be found, be printed and bound to guarantee the preservation of their contents even if the original records became destroyed, lost, or stolen. Second, they called for storing documents in steel—rather than wooden—filing cabinets as a fireproofing measure. Finally, they called for the cataloging and centralization of important historical documents. In 1903, at the behest of Pennsylvania governor Samuel W. Pennypacker, himself a noted historian, Ames and Shimmel's recommendations were realized and the Pennsylvania State Archives formally established.

Ames continued his advocacy of archival preservation as a member of the AHA's Public Archives Commission, serving as its chair from 1903 to 1913, and continuing as a member for many years thereafter. By 1904, the commission had secured the services of historians in 32 states to study the status of public records and in 1907 Ames authored a detailed status report regarding archival preservation legislation throughout the United States, which would later be credited as the first report of its kind. Two years later, in 1909, Ames organized the first national conference of American government archivists.

With his well-established expertise on archival issues, J. Franklin Jameson called on Ames to give testimony to the United States Senate Committee on Public Buildings and Grounds as he lobbied for the creation of the National Archives of the United States. In February 1912, Jameson wrote Ames to ask him to "lay before the committee whatever there has been in the practice or experience of states that deserves attention by persons who are planning a national building". Ames' prior commitment to attend the quasquicentennial celebrations of the University of Pittsburgh ultimately prevented him from traveling to Washington for the hearing. Still, Jameson continued to correspond with Ames seeking advice relating to the politics and impediments he had experienced in advocating for the creation of state archives.

Ames served as corresponding secretary of the Historical Society of Pennsylvania, and was elected to the American Antiquarian Society. He also served as president of the History Teachers' Association of the Middle States and Maryland. In 1918 he was appointed—along with John Bach McMaster, Hampton Carson, William Cameron Sproul, and others—to the Pennsylvania War History Commission, which was formed to preserve records related to Pennsylvania's participation in World War I.

Ames had planned his retirement for 1936, and was intending to spend his final year at Pennsylvania laying the groundwork for two new research projects: a biography of Robert J. Walker, and a study of the presidential veto power.

Neither of Ames' two planned projects bore fruit; he died at his home at 203 St. Mark's Square in Philadelphia on February 7, 1935, from a cerebral hemorrhage following a stroke. Funeral services were held at the Second Presbyterian Church of Philadelphia, with Thomas Sovereign Gates, Roland Sletor Morris, Emory Richard Johnson, Conyers Read, Roy Franklin Nichols, Lewis M. Stevens, Julian P. Boyd, H. Lamar Crosby, and Edward Cheyney serving as honorary pallbearers.

On May 7, 1935, three months after his death, a memorial meeting was held at Houston Hall in which Ames was eulogized by his colleagues and former students. A record of these speeches, along with letters contributed by those who could not attend—including his doctoral supervisor Albert Bushnell Hart—was compiled and edited by Edward Cheyney and Roy Franklin Nichols. The compilation was published by the University of Pennsylvania Press in 1936 as the 31-page "Memorial: Herman Vandenburg Ames".

Ames was unmarried. He had an older sister, Ella Elizabeth, who resided with him at the time of his death. His older brother, Marcus Judson, died in childhood.

In his personal mannerisms, it was said that Ames had a keen sense of humor and a relaxed disposition.

Ames' personal interests included music and travel. He was a member of the Order of the Founders and Patriots of America, serving as the society's Governor-General from 1919 to 1921, and the Society of Mayflower Descendants. He served, for a time, as president of the Delta Upsilon Club of Philadelphia and as the international historian of Delta Upsilon. Ames was also a member of the Presbyterian polity.


Ames' portrait, by Alice L. Emmong, is cataloged in the United States National Portrait Collection. The Ella E. and Herman V. Ames Fund, established in 1951 through a bequest by Ames' sister, who left most of her estate to the University of Pennsylvania, supports the acquisition of materials in American History by the University of Pennsylvania library system. His papers are housed at the University of Pennsylvania's University Archives.








</doc>
<doc id="56229000" url="https://en.wikipedia.org/wiki?curid=56229000" title="Idol Producer">
Idol Producer

Idol Producer () is the first season of the Chinese reality television show, "Idol Producer", premiered on January 19, 2018, on iQiyi. The show is presented by Lay Zhang, with Li Ronghao, Jackson Wang, MC Jin, Cheng Xiao, and Zhou Jieqiong serving as the coaches. On April 6, 2018, the last nine contestants debuted as "Nine Percent".

"Idol Producer" (season 1) brought 100 trainees who either from 31 entertainment companies or didn't sign under any companies together. The trainees had a closed training session lasted for four months, and nine of the trainees were finally chosen to debut as a boy group through viewers' vote; The voting mechanism is similar to those of South Korean reality television shows, "Produce 101" and "Mix Nine".


Color key

Chief producer Jiang Bin has previously produced many well known reality talent shows including "S-style Show, I Supermodel" and "Road to the Runway". The show recruited Cheng Gang, the chief director of "Super Boy" and Tan Yan, the visual director of "I Am a Singer".

The coaches for the series were introduced through posters which were uploaded online. On December 17, a press conference was held where coaches Lay Zhang, Jackson Wang, Cheng Xiao, Zhou Jieqiong and MC Jin attended. On February 3, a total of 21 contestants were invited by Lay Zhang to guest on Chinese variety show, "Happy Camp".

The first episode of the show garnered over 100 million viewers within the first hour of broadcast on iQiyi. Lay Zhang was remarked for his scrutiny towards the contestants during their performances.

The show has been accused of plagiarizing the format of South Korean talent show "Produce 101 Season 2". The head of formats and development at Mnet, Jin Woo Hwang claimed that “People in our company were shocked because it wasn’t just a similar show – it was almost a duplicate show.” 

The Format Recognition and Protection Association revealed results of its comparative analysis of “Idol Producer,” launched by China’s iQIYI in January 2018, and Korean company CJ’s “Produce 101,” which launched in 2016. The analysis concluded that the Chinese show scored 88% on FRAPA’s scale of infringement when compared with the Korean show – it was the highest score ever recorded for an alleged infringement. 

Individual trainee, Song Shuijiao, was forced to leave "Idol Producer" due to verbal misconduct widespread over the internet. The trainee participated in the filming of the first episode, but was subsequently edited out prior to the broadcast.

Young Culture's Qian Zhenghao was found to have violated dormitory regulations by eating hot pot in the dormitory late at night. A handwritten letter of apology was posted on the official "Idol Producer" dormitory "Sina Weibo" account.

Two of MERCURY NATION'S trainees, GIGEL and Wang Youchen, were removed from the show as disciplinary action for ground fighting in the dormitory.

On December 20, 2018, iQiyi announced the sequel, named Youth Has You (青春有你) is scheduled in 2019 with Lay Zhang, MC Jin, Li Ronghao retained, with Taiwanese singer Jolin Tsai and Xu Minghao from K-Pop boy group Seventeen promoted as dance instructors, replacing Cheng Xiao and Zhou Jieqiong. The aim remains to form another new male group.


</doc>
<doc id="56279481" url="https://en.wikipedia.org/wiki?curid=56279481" title="Cooperative pulling paradigm">
Cooperative pulling paradigm

The cooperative pulling paradigm is an experimental design in which two or more animals pull rewards toward themselves via an apparatus that they cannot successfully operate alone. Researchers (ethologists, comparative psychologists, and evolutionary psychologists) use cooperative pulling experiments to try to understand how cooperation works and how and when it may have evolved.

The type of apparatus used in cooperative pulling experiments can vary. Researcher Meredith Crawford, who invented the experimental paradigm in 1937, used a mechanism consisting of two ropes attached to a rolling platform that was too heavy to be pulled by a single chimpanzee. The standard apparatus is one in which a single string or rope is threaded through loops on a movable platform. If only one participant pulls the string, it comes loose and the platform can no longer be retrieved. Only by pulling together in coordination can the participants be successful; success by chance is highly unlikely. Some researchers have designed apparatus that involve handles instead of ropes.

Although many animals retrieve rewards in their cooperative pulling tasks, the conclusions regarding cooperation are mixed and complex. Chimpanzees, bonobos, orangutans, capuchins, tamarins, wolves, elephants, ravens, and keas appear to understand the requirements of the task. For example, in a delay condition, the first animal has access to the apparatus before the other. If the animal waits for its partner before pulling, this suggests an understanding of cooperation. Chimpanzees, elephants, wolves, dogs, ravens, and keas wait; grey parrots, rooks, and otters fail to wait. Chimpanzees actively solicit help when needed. They appear to recall previous outcomes to recruit the most effective partner. In a group setting, chimpanzees punish initial competitive behavior (taking food without pulling, displacing animals) such that eventually successful cooperation becomes the norm.

As for the evolution of cooperation, evidence from cooperative pulling experiments provides support for the theory that cooperation evolved multiple times independently. The fact that basic characteristics of cooperation are present in some mammals and some birds points to a case of convergent evolution. Within social animals, cooperation is suspected to be a cognitive adaptation.

Many species of animals cooperate in the wild. Collaborative hunting has been observed in the air (e.g., among Aplomado falcons), on land (e.g., among lions), in the water (e.g., among killer whales), and under the ground (e.g., among driver ants). Further examples of cooperation include parents and others working together to raise young (e.g., among African elephants), and groups defending their territory, which has been studied in primates and other social species such as bottlenose dolphins, spotted hyenas, and common ravens.

Researchers from various disciplines have been interested in cooperation in animals. Ethologists study animal behavior in general. Comparative psychologists are interested in the origins, differences, and commonalities in psychological capacities across animal species. Evolutionary psychologists investigate the origin of human behavior and cognition, and cooperation is of great interest to them, as human societies are built on collaborative activities.

For animals to be considered cooperating, partners must take account of each other’s behavior to pursue their common goal. There are various levels of cooperation. These increase in temporal and spatial complexity from performing similar actions, to synchrony (similar actions performed in unison), then coordination (similar actions performed at the same time and place), and finally collaboration (complementary actions performed at the same time and place). Researchers use controlled experiments to analyze the strategies applied by cooperating animals, and to investigate the underlying mechanisms that lead species to develop cooperative behavior.

The cooperative pulling paradigm is an experimental design in which two or more individuals, typically but not necessarily animals, can pull rewards towards themselves via an apparatus they can not successfully operate alone. The cooperative pulling paradigm is the most popular paradigm for testing cooperation in animals.

The type of apparatus used in cooperative pulling experiments can vary. Researcher Meredith Crawford, who invented the experimental paradigm in 1937 while at the Yerkes National Primate Research Center, used an apparatus consisting of two ropes attached to a box that was too heavy to be pulled by a single chimpanzee. The standard apparatus is used in the loose-string task, designed by Hirata in 2003, in which a single string or rope is threaded through loops on a movable platform. If only one participant pulls the string, it comes loose and the platform can no longer be retrieved. Only by pulling together in coordination can the participants be successful; success by chance is highly unlikely. Some researchers have designed apparatus that involve handles instead of ropes. De Waal and Brosnan have argued that complex electronically-mediated devices are not conducive to arrive at findings regarding cooperation. This is in contrast to mechanical pulling devices, in which the animals can see and feel their pull having immediate effect. String-pulling tasks have advantages in terms of ecological validity for animals that pull branches with food towards themselves. Tasks in which participants have different roles in collaboration, such as for example, one pulls a handle and the other one needs to insert a stick, are considered outside the cooperative pulling paradigm.

So far, fewer than twenty species have participated in cooperative pulling experiments: chimpanzees, bonobos, orangutans, capuchin monkeys, tamarins, macaques, humans, hyenas, wolves, dogs, elephants, otters, dolphins, rooks, ravens, parrots, and keas. Researchers have picked species that cooperate in the wild (e.g., capuchins), live in social structures (e.g., wolves), or have known cognitive abilities (e.g., orangutans). Most of the participating animals have been in human care at an animal research center; some lived semi-free at a sanctuary in their natural habitat. One study involved free animals (Barbary macaques) in the wild.

To arrive at conclusions regarding cooperation, researchers have designed experiments with various conditions.
The first animal has access to the apparatus before the other one. If the animal does not wait for its partner this suggests a lack of understanding of the requirements for successful cooperation.
The first animal gets to choose which animal from a pair it wants as a partner. In some cases individual animals from within a group can decide to join an animal already at the apparatus.
Instead of just one apparatus in the test area there are two identical ones. Animals can decide to work on the same one (which can lead to success) or on different ones (which will lead to failure). A further design involves two different apparatus. The first animal can decide whether to use an apparatus that can be operated alone or one that requires and has a partner waiting. A 'no rope' version involves an apparatus where everything is the same except for the rope on the partner's side being coiled up and not accessible to the partner.

Rewards can be food split equally over two bowls in front of each animal, or in one bowl only. The type of food can vary from lots of small pieces to one big lump (e.g., slices of an apple vs. a whole apple). In combination with the apparatus choice, the reward for the joint-task apparatus is often twice as big as the reward for the solo apparatus. Another variation is a modified apparatus where one partner gets food before the other, requiring the first one to keep pulling despite already having received the reward.
Typically the animals can see each other, all rewards, and all parts of the apparatus. To assess the role of visual communication, sometimes an opaque divider is placed such that the animals can no longer see each other, but can still see both rewards.
Animals are often first trained with an apparatus that can be operated by one individual. For example, the two ends of a string are on top of each other and a single animal can pull both ends. A technique called shaping can be used by gradually extending the distance between the string ends, or by gradually extending the length of delay between the arrival of the first and second animal at the apparatus.

Although many animals retrieve rewards in their cooperative pulling tasks, the conclusions regarding cooperation are mixed and complex. Some researchers have attributed successful cooperation to random simultaneous action, or to the simple reactive behavior of pulling the rope when it moves. Many trials with capuchins, hyenas, parrots and rooks led to failure because one partner pulled without the other present, suggesting a lack of understanding of cooperation. A few researchers have offered the possible explanation that animals may understand cooperation to some extent but simply can not suppress the desire to have food they see.

But there is evidence that some species do have an understanding of cooperation and perform intentional coordination to achieve a goal.
Specifically, chimpanzees, bonobos, orangutans, tamarins, capuchins, elephants, wolves, ravens, and keas appear to understand how cooperation works. Chimpanzees not only wait for a partner, but will actively solicit help when needed. They appear to recall previous outcomes to recruit the most effective partner. In a group setting, chimpanzees punish initial competitive behavior (taking food without pulling, displacing animals) such that eventually, after many trials, successful cooperation becomes the norm. Bonobos, which are social animals with higher tolerance levels, can outperform chimpanzees on some cooperative tasks. Elephants will wait for 45 seconds for a partner to arrive before they start a cooperative pulling task; wolves do the same for 10 seconds. Dogs raised as pets are also able to wait for a partner, albeit only for a few seconds; pack dogs on the other hand rarely succeed in cooperative pulling in any condition. Among birds, ravens are able to learn to wait after many trials, while keas have set the record in waiting for a partner, 65 seconds. Mere knowledge of the presence of a partner is not enough for success: when a barrier with a small hole was placed between two capuchins, obstructing the view of the partner's actions, the success rate dropped. Of those species tested in the delay condition, parrots, rooks, and otters failed.

In 2008, Seed, Clayton and Emery said the study of the proximate mechanisms underpinning cooperation in animals was in its infancy, due in part to the poor performances of animals such as chimpanzees in early tests that did not take factors such as inter-individual tolerance into account. Several studies since have highlighted the fact that tolerance has a direct impact on cooperation success, as the more tolerant an animal is around food the better it performs. Subordinate animals seem simply not willing to risk being attacked by intolerant dominant animals, even if it means they will not obtain food either. In general, cooperation will not emerge if individuals can not share the spoils obtained through their joint effort. Temperament, whether an animal is bold or shy, has also been found to predict success.

As for the evolution of cooperation, evidence from cooperative pulling experiments appears to support the theory that cooperation evolved multiple times independently. The fact that basic characteristics of cooperation are present in some mammals and some birds points to a case of convergent evolution. Within social animals, cooperation is suspected to be a cognitive adaptation. The ability of humans to cooperate is likely to have been inherited from an ancestor shared with at least chimpanzees and bonobos. The superior scale and range of human cooperation comes mainly from the ability to use language to exchange social information.

Chimpanzees ("Pan troglodytes") are smart, social animals. In the wild they cooperate to hunt, dominate rival groups, and defend their territory. They have participated in many cooperative pulling experiments. The first ever cooperative pulling experiment involved captive chimpanzees. In the 1930s Crawford was a student and researcher at the Yerkes National Primate Research Center. In 1937 he published a study of two young chimpanzees named Bula and Bimba pulling ropes attached to a box. The box was too heavy to be pulled in by just one ape. On top of the box was food. The two participants synchronized their pulling and were able to get the food reward in four to five short pulls. In a second part of the study, Crawford fed Bula so much prior to the test that she was no longer interested in the food reward. By poking her and pushing her hand towards the rope, Bimba tried to enlist her help in the task, with success. In a follow-up experiment with seven pairs of chimpanzees Crawford found none of the apes spontaneously cooperated. Only after extensive training were they able to work together to obtain food. They also failed to transfer this new skill to a slightly different task, in which the ropes were hanging from the ceiling.

Similar mixed results, not matching the cooperative abilities observed in chimpanzees in the wild, were obtained in later studies by other researchers using a variety of experimental set-ups, including the loose-string task pioneered by Hirata. Povinelli and O’Neill, for example, found that trained chimpanzees were unable to teach naive chimpanzees to cooperate on a Crawford-like box-pulling task. The naive animals did not imitate the experts. Chalmeau and Gallo found only two chimpanzees consistently cooperating in their handle-pulling task, and this involved one ape holding his own handle and waiting for the other to pull his. They concluded that social factors and not limited cognitive abilities were the reason for lack of widespread success, as they observed dominant chimpanzees controlling the apparatus and preventing others from interacting.

Melis, Hare, and Tomasello set up an experiment to control for such social factors. In a loose-string cooperative task without training they compared the ability of pairs of captive chimpanzees who in a non-cooperative setting were willing to share food with each other to pairs who were less inclined to do so. The results showed that food sharing was a good predictor for success in the cooperative pulling task. Melis, Hare, and Tomasello concluded that mixed results in the past could at least partially be explained by a failure to control for such social constraints. In a follow-up study with semi–free-ranging chimpanzees, again using the loose-string task, the researchers found that the apes only recruited a partner (by unlocking a door) if the task required it. When given the choice between partners, the apes chose the more effective one, based on their experience with each of them previously.

Suchak, Eppley, Campbell, Feldman, Quarles, and de Waal argued that even when experiments take social relationships into account, the results still do not match the cooperation capabilities observed in the wild. They set out to increase the ecological validity of their experiments by placing a handle-pulling apparatus in an open-group setting, allowing the captive chimpanzees themselves to choose to interact with it or not, and with whom. They also refrained from any training, offered as little human intervention as possible, and extended the duration to much longer than any test had ever done, to 47 days of 1 hour tests. The chimpanzees first discovered that cooperation could lead to success, but as more individuals became aware of this new way to obtain food, competition increased, taking the form of dominant apes displacing others, monopolizing the apparatus, and freeloading: taking the food others worked for. This competition led to fewer successful cooperative acts. The group did manage to restore and increase levels of cooperative behavior by various enforcement techniques: dominant individuals were unable to recruit partners and abandoned the apparatus, displacement was met with aggressive protest, and freeloaders were punished by third-party arbiters. When the researchers repeated this experiment with a brand new group of chimpanzees who not yet had established a social hierarchy, they again found that cooperation overcame competition in the long run. In a later study with a mix of novices and experts, Suchak, Watzek, Quarles, and de Waal found that novices learned rapidly in the presence of experts, although likely with limited understanding of the task.

Greenberg, Hamann, Warneken, and Tomasello used a modified apparatus that required two captive chimpanzees to pull, but delivered food to one ape first. They found that in many trials the apes who already had received a reward from joint effort kept pulling to help their partner obtain their food.
These partners did not need to gesture to solicit help, suggesting there was an understanding of what was wanted and needed.

Bonobos ("Pan paniscus") are social animals that live in less hierarchical structures than chimpanzees. Hare, Melis, Woods, Hastings, and Wrangham set out to compare cooperation in chimpanzees and bonobos. They first ran a cofeeding experiment for each species. Pairs of bonobos were given two food dishes. In some trials both dishes had sliced fruit; in some one dish was empty and the other had sliced fruit; and in some one dish was empty and the other contained just two slices of fruit. The same set-up was then used for pairs of chimpanzees. When both dishes had food, there was no difference in behavior between bonobos and chimpanzees. But when only one dish contained food, bonobos were more than twice as likely to share food than chimpanzees. Bonobos were more tolerant of each other than chimpanzees. The researchers then ran a loose-string cooperation task with both dishes filled with sharable food. The results showed similar success rates for bonobos and chimpanzees, 69% of chimpanzee pairs and 50% of bonobo pairs spontaneously solving the task at least once within the six-trial test session.

In a third experiment, a year later, the same cooperation task was administered but now with different food distributions. The bonobos outperformed the chimpanzees in the condition where one dish only had food and the food was clumped making it easier to monopolize the food reward. Bonobos cooperated more often in this condition. On average a single chimpanzee partner monopolized food rewards more often than a single bonobo did. In the condition where both dishes were filled with food, chimpanzees and bonobos performed similarly, as they had done the year before. The researchers concluded that the differences in performance between species were not due to differences in age, relationships, or experience. It was the bonobos' higher social tolerance level that enabled them to outperform their relatives.

Orangutans ("Pongo pygmaeus") are tool-using apes that are mostly solitary. Chalmeau, Lardeux, Brandibas, and Gallo tested the cooperative capabilities of a pair of orangutans, using a device with handles. Only through simultaneous pulling could the pair retrieve a food reward. Without any training the orangutans succeeded in the first session. Over the course of 30 sessions, the apes succeeded more quickly, having learned to coordinate. Across trials the researchers found an increase in a sequence of actions that suggested understanding of cooperation: first looking at the partner; then if the partner holds or pulls the handle, starting to pull.

The researchers also concluded that the orangutans learned a partner had to be present for success. For example, they observed that time spent alone at the apparatus decreased as the trials progressed. In some instances one orangutan pushed the other towards the free handle, soliciting cooperation. The researchers observed an asymmetry: one ape did all the monitoring and coordinating, the other one seemed to simply pull if the first one was present. Rewards did not have to be shared equally for success to appear, as one orangutan took 92% of all food. This ape anticipated the falling of food and stuck his hand out first, before recruiting help from his partner. Chalmeau, Lardeux, Brandibas, and Gallo concluded the apes appeared to understand the requirements of the cooperative task.

Capuchins ("Sapajus apella") are large-brained monkeys that sometimes hunt cooperatively in the wild and
show, for nonhuman primates, unusually high levels of social tolerance around food. Early experiments to prove their ability to cooperate were unsuccessful. These tests involved capuchins having to pull handles or press levers in complex devices that the animals did not understand. They did not pull the handle more often when a partner was pulling; both novices and experienced participants kept pulling even in situations where success was impossible. Visalberghi, Quarantotti, and Tranchida concluded that there was no evidence of an appreciation of the role played by the partner.

The first test with evidence of cooperation in capuchins happened when de Waal and Brosnan adopted Crawford's pulling paradigm. Two captive monkeys were situated in adjacent sections of a test chamber, with a mesh partition between them. In front of them was an apparatus consisting of a counter-weighted tray with two pull bars and two food cups. Each monkey had access to only one bar and one food cup, but could see both, and only one cup was filled with food. The tray was too heavy for one monkey to pull it in, with weights established over trials lasting three years. Only when they worked together and both pulled could they move the tray, enabling one of them to grab the food. Trained monkeys were much more successful if they both obtained rewards after pulling than if only one of them received rewards. The pull rate dropped significantly when monkeys were alone at the apparatus, suggesting an understanding of the need for a partner. In later tests, researchers replaced the mesh partition with an opaque barrier with a small hole, so that the monkeys could see the other one was there but not their actions. This dramatically reduced success in cooperation.
De Waal and Berger used the cooperative pulling paradigm to investigate animal economics. They compared the behavior when both transparent bowls were loaded with food to when just one was loaded, and with a solo task where the partner was only an observer and unable to help. They found that captive capuchin monkeys were willing to pull even if their bowl was empty and it was uncertain if their partner would share food. In 90% of cases the owner of the food did indeed share the food. Food was shared more often if the partner actually worked for it than just being an observer.

Brosnan, Freeman, and de Waal tested captive capuchin monkeys on a bar-pulling apparatus with unequal rewards. Contrary to their expectations, rewards did not have to be distributed equally to achieve success. What mattered was the behavior in an unequal situation: pairs that tended to alternate which monkey received the higher-value food were more than twice as successful in obtaining rewards than pairs in which one monkey dominated the higher-value food.

Cottontop tamarins ("Saguinus oedipus") are small monkeys who take care of their young cooperatively in the wild. Cronin, Kurian, and Snowdon tested eight captive cottontop tamarins in a series of cooperative pulling experiments. Two monkeys were put on opposite sides of a transparent apparatus containing food. Only if both monkeys pulled a handle on their side of the apparatus towards themselves at the same time would food drop down for them to obtain.
The tamarins were first trained, through shaping techniques, to use the handles successfully by themselves. In the joint pulling test pairs were successful in 96% of trials.

The researchers then ran a second study in which a tamarin was tested alone. The results showed that tamarins pulled the handles at a lower rate when alone with the apparatus than when in the presence of a partner. Cronin, Kurian, and Snowdon concluded from this that cottontop tamarins have a good understanding of cooperation. They suggest that cottontop tamarins have developed cooperative behavior as a cognitive adaptation.

Molesti and Majolo tested a group of wild Barbary macaques ("Macaca sylvanus") in Morocco to see if they would cooperate, and if so, what determined their partner choice. Macaques live in complex social environments and are relatively tolerant socially. After solo training, the researchers presented a loose-string apparatus for the cooperative task, which the animals were free to use.
Most animals that passed solo training were successful in spontaneously cooperating to obtain food (22 out of 26). More than half the pairs that chose to cooperate were juvenile-adult pairs. More than two monkeys pulling was never observed; stealing food from a partner was rare. After a first successful cooperation, they were more likely to pull when a partner was directly available, but this was not always the case. Molesti and Majolo did not rule out that pulling while no one held or pulled the other end of the rope was simply a signal to actively recruit a potential partner.
The researchers randomly introduced control trials in which the solo apparatus was set up as well. The macaques preferred to get the food alone when a partner was not needed during the control.

The extent to which a monkey tolerated another was a good predictor for initiating cooperation.
An individual was also found to be more successful with partners with whom they had a strong social bond.
Pairs sharing a similar temperament were more likely to initiate cooperation.
The quality of the relationship seemed to play an important role in the maintenance of cooperation over time.

Rekers, Haun, and Tomasello tested the cooperation abilities and preferences of humans ("Homo sapiens") and compared them to chimpanzees.
The researchers provided 24 three-year-old children with some basic training in pulling food rewards towards themselves; in pairs using a loose-string setup, and solo training in which the two ends of a rope were tied together. They then tested the children in an apparatus choice set-up. On one side was the loose end of a rope that threaded through the apparatus to the other child. On the other side were two ends of a rope that when pulled would pull a platform towards both the child and its partner. Both the joint-operator platform and the solo-operated platform were holding two food dishes, all containing the same amounts of food. That is, from a partner's perspective, on one side the child had to pull to get food; on the other the partner could get food without any effort. The children chose the joint-operated board in 78% of trials.

The researchers then changed the design to ascertain if this choice preference was due to wishing to avoid freeloading and it may be that the children did not like their partner getting food without making any effort. In the modified set-up the partners never received any reward, not from the joint-operated apparatus and not from the solo-operated apparatus. Children again chose the joint-operated platform significantly more often, in 81% of trials. As in the first study, there was no significant difference in the time taken to obtain the food reward between using one side or the other. These results suggest that to obtain food, children prefer to work together with a partner as opposed to working alone. The chimpanzees in their study appeared to choose between the two platforms randomly, indicating no preference to work collaboratively.

Captive spotted hyenas ("Crocuta crocuta"), social carnivores that hunt in groups, have cooperated to obtain food rewards by pulling ropes in an experimental setting. Mimicking the natural choice hunting hyenas face when deciding which of many prey to jointly attack, researchers Drea and Carter set up two devices instead of one, as previously used in all cooperative pulling tasks with other species. With four ropes to pull from, the animals had to pick the two belonging to the same device to be successful. If two vertically suspended ropes were simultaneously tugged, a spring-controlled trap door of an elevated platform was opened and previously hidden food dropped to the floor.
Another innovation was the introduction of more than two animals. One of the many factors the researchers controlled for was the Clever Hans effect (an effect in which humans unwittingly provide cues to animals), which they did by removing all humans from the test and by recording experiments on video.

After extensive solo trials, all hyenas were successful in cooperating, displaying remarkable efficiency even on their first try. On average, hyenas pulled on ropes more often when their companion was nearby and available to fulfil its partnership role. With only a few solo trials, the success rate of the cooperation task was very low for pairs. In groups of four hyenas, all trials were successful, regardless of the number of reward platforms. Thereafter, group exposure to a cooperation task had enhancing effects on pairwise performance. Social factors such as group size and hierarchy played a role. For instance, groups with a dominant member were far less successful than groups without, and lower-ranking animals were faster and consistently successful. When pairing experienced cooperators with animals new to the cooperation task, the researchers found that experienced animals monitored the novices and modified their behavior to achieve success. Despite initial accommodation, the pattern of rank-related social influences on partner performance also appeared in these tests with novices.

Ostojić and Clayton administered the loose-string cooperation task to domestic dogs ("Canis familiaris"). Pet dogs first were given a solo task in which the string ends were close enough for one dog to pull at both. Then they were given a transfer test to assess if they could generalize their newly learned rule to novel situations. Finally, the joint task was administered. Dog pairs always came from the same household. In half of the joint tasks one of the pair of dogs was shortly delayed by an obstacle course. All dogs that learned to master the solo task solved the joint task within 60 trials. In the delayed condition, the not-delayed dog waited before pulling most of the time, but only for a few seconds. The researchers also tested dog–human pairs, again in delayed and not-delayed conditions. Dogs were equally successful when working with humans in the non-delayed condition, but far less successful when they had to wait for the human, who on average arrived with a 13-seconds longer delay than the delayed dog in the dog–dog trials. Ostojić and Clayton concluded that inhibiting the necessary action was not easy for dogs. They ruled out that dogs simply went for any moving string, as in the dog–human trials the humans did not pull hard enough to make the other end move. They attributed success to the dogs' ability to read the social cue of their partner's behavior, but could not rule out that visual feedback of seeing rewards incrementally move closer also played a role.

These results with pet dogs stand in stark contrast to the results with pack dogs, which in a study by Marshall-Pescini, Schwarz, Kostelnik, Virányi, and Range rarely succeeded in obtaining food. The researchers theorized that pet dogs are trained not to engage in conflicts over resources, promoting a level of tolerance, which may facilitate cooperation. The pack dogs were used to competition over resources and thus were likely to have conflict avoidance strategies, which constrain cooperation.

Marshall-Pescini, Schwarz, Kostelnik, Virányi, and Range set out to test two competing hypotheses regarding cooperation in wolves ("Canis lupus") and dogs. On the one hand, it could be theorized that dogs have been selected, during domestication, for tame temperaments and an inclination to cooperate and therefore should outperform wolves on a cooperative pulling task. On the other hand, it could be argued that dogs have evolved to become less able to work jointly with other dogs because of their reliance on humans. Wolves rely on each other for hunting, raising young and defending their territory; dogs rarely rely on other dogs. The researchers set up a cooperative pulling task for captive wolves and pack dogs. Without any training on this task, five of the seven wolf pairs were successful at least once, but only one dog pair out of eight managed to obtain food, and only once. After solo training, again the wolves far outperformed the dogs on the joint task. The researchers concluded that the difference does not stem from a difference in understanding of the task (their cognitive capabilities are largely the same), nor from a difference in social aspects (for both species, aggressive behavior by dominant animals was rare, as was submissive behavior by lower ranked ones). More likely is that dogs avoid potential conflict over a resource more than wolves do, something which has been observed in other studies as well.

The wolves, but not the dogs, were then tested in pairs in a set-up with two identical apparatus 10 meters (39 ft) apart, requiring them to coordinate in time and space. In 74% of the trials they succeeded. The stronger the bond between the partners and the smaller the distance in rank, the better they performed. In a subsequent delay condition, with the second wolf released 10 seconds after the first, most wolves did well, one being successful in 94% of trials.

Elephants have a complex social structure and large brains that enable them to solve many problems. Their size and strength do not make them easy candidates for experiments. Researchers Plotnik, Lair, Suphachoksahakun, and de Waal adapted the apparatus and task to elephant requirements. They trained captive Asian elephants ("Elephas maximus") to use a rope to pull a sliding platform with food on it towards themselves. Once the elephants managed this solo task, the researchers introduced a loose-string apparatus by threading the rope around the platform. At first, two elephants were released simultaneously to walk side by side in two lanes to the two loose ends of the rope. Using their trunks the animals coordinated their actions and retrieved the food.

At this stage they could simply be applying a 'see the rope, pull the rope' strategy. To see whether they understood the requirements of the task the researchers introduced a delay for one elephant, initially of 5 seconds and ultimately of 45 seconds. At first the lead elephant failed to retrieve the food but was soon seen to wait for a partner. Across 60 trials the first elephant waited for the second one before pulling in most cases. In a further control the researchers prevented the second elephant from being able to access its end of the rope. In almost all of these cases the first elephant did not pull the rope, and four of the six returned when they saw the other rope end was not going to be accessible to their partner. The researchers concluded that this suggested the elephants understood they needed their partner to be present and to have access to the rope to succeed. One elephant never pulled the rope but simply put her foot on the rope and let the partner do all the pulling. Another one waited for his partner's release at the starting line rather than waiting at the rope. Plotnik, Lair, Suphachoksahakun, and de Waal conceded that it is difficult to distinguish learning from understanding. They did prove that elephants show a propensity towards deliberate cooperation. The speed with which they learned the critical ingredients of successful cooperation puts them on par with chimpanzees and bonobos.

Schmelz, Duguid, Bohn, and Völter presented two species of captive otters, giant otters ("Pteronura brasiliensis") and Asian small-clawed otters ("Aonyx cinerea"), with the loose-string task. Both species raise young cooperatively and live in small groups. Because giant otters forage together but small-clawed otters do not, the researchers expected the giant otters to do better in the cooperative pulling experiment. After solo training, they tested both species in a group setting, to maintain ecological validity. The results showed that most pairs of otters were successful in pulling food rewards to themselves. Contrary to expectation, there was no difference between the species in success rate. In a subsequent experiment the researchers first lured the group away from the apparatus into the opposite corner of the enclosure. Then they put food on the apparatus and observed what happened when the first otter arrived at the nearest end of the rope, as there was no partner yet at the other end. Very few trials led to success in this condition as otters pulled the rope as soon as they could. The researchers concluded from this that the otters did not understand the necessary elements of successful cooperation, or, alternatively, they understood but were unable to inhibit the desire to reach for the food. When the same task was repeated with a longer rope, success rate did go up, but the otters appeared unable to learn from this and be successful in the next task with the rope length restored to the original length. Schmelz, Duguid, Bohn, and Völter suggested that an understanding of cooperation may not be required for successful cooperation in the wild. Cooperative hunting may be possible through situational coordination and mutualism, without any complex social cognitive abilities.

Two groups of researchers (first Kuczaj, Winship, and Eskelinen, and then Eskelinen, Winship, and Jones) adapted the cooperative pulling paradigm for captive bottlenose dolphins ("Tursiops truncatus"). As apparatus they used a container which could only be opened at one end if two dolphins each pulled a rope on either end. That is, the dolphins would have to face each other and pull in opposite directions. They first attached the container to a stationary dock so a single dolphin could learn to open it and get the food reward. Then they ran trials in which the container was free floating in a large test area with six dolphins. In Kuczaj, Winship, and Eskelinen's study, only two dolphins interacted with the container. In eight of the twelve trials they pulled simultaneously and obtained food. Once, they also managed to open the container through asynchronous pulling, and once a single male dolphin managed to open it by himself. Kuczaj, Winship, and Eskelinen admitted that this behavior may appear to be cooperation but could possibly be competition. They conceded it is possible that the dolphins did not understand the role of the other dolphin, but instead simply tolerated it pulling on the other side. King, Allen, Connor, and Jaakkola later argued that this design makes for a competitive ‘tug-of-war’, not cooperation, and any conclusions regarding cooperation should therefore be invalid.

Rooks ("Corvus frugilegus") are large-brained members of the bird family Corvidae. They live in big groups and have a high level of social tolerance. Researchers Seed, Clayton, and Emery set up a loose-string experiment with eight captive rooks. They were first trained in a solo task, with the string ends placed at 1 cm, 3 cm and ultimately 6 cm apart (0.4, 1.2, and 2.4 inch respectively). A pair's willingness to share food was then tested, and was found to differ somewhat between pairs, although food was rarely monopolized by a dominant bird. In the cooperative task, all pairs were able to solve the cooperation problem and retrieve food; two pairs managed this in their first session. Food sharing was a good predictor for successful cooperation.

In a subsequent delay test, where one partner had access to the apparatus first, all rooks pulled the string without waiting for their partner to enter the test area in the majority of trials. In a second variant, birds were given a choice between a platform they could operate successfully alone and one that required a pulling partner. When tested alone, four of the six rooks showed no significant preference for either platform. Seed, Clayton, and Emery concluded that although successful at the cooperation task, it seemed unlikely that the rooks had an understanding of when cooperation was necessary.

Researchers Scheid and Noë subsequently found that successful cooperation in rooks depended to a large extent on their temperament. In their loose-string experiment with 13 captive rooks they distinguished between bold and shy animals. The results were mixed, ranging from some pairs cooperating successfully every time to some pairs never cooperating. In 81% of cases a rook should have waited for a partner, but it did not and started pulling. Scheid and Noë concluded their experiment provided no evidence for or against rooks having an understanding of the task. They attributed any cooperation success to common external cues and not coordination of actions. But all subjects did better when they were paired with a bolder partner. The researchers suggested that in evolution, cooperation can emerge because bolder individuals encourage a risk-averse one to engage.

Massen, Ritter, and Bugnyar investigated the cooperative capabilities of captive common ravens ("Corvus corax"), a species that frequently cooperates in the wild. They found that without training ravens cooperated in the loose-string task. The animals did not seem to pay attention to the behavior of their partners while cooperating, and, like rooks, did not seem to understand the need for a partner to be successful. Tolerance of their partner was a critical factor for success. In one condition the researchers let ravens choose a partner from a group to cooperate with. Overall success was higher in this condition, and again, individuals that tolerated each other more had more success. The ravens also paid attention to reward distribution: they stopped cooperating when being cheated upon.

Asakawa-Haas, Schiestl, Bugnyar, and Massen subsequently ran an open-choice experiment with eleven captive ravens in a group setting, using nine ravens from one group and two newcomers. They found that the ravens' decision which partner to cooperate with was based on tolerance of proximity and not on whether they were part of the group or not. The ravens in this experiment learned to wait for their partner and inhibit pulling the string too soon.

Researchers Péron, Rat-Fischer, Lalot, Nagle and Bovet had captive grey parrots ("Psittacus erithacus") try to cooperate in a loose-string experimental set-up. The grey parrots were able to act simultaneously but, like the rooks, largely failed to wait for a partner in the delay task. They did not make any attempts to recruit a helping partner. The parrots did take the presence of a partner into account, since they all pulled more when a partner was present, but this could be explained by instrumental learning rather than a real understanding of the task. The researchers also gave the parrots a choice between two apparatus, one from the solo task and one from the loose-string task, now stacked with double the food per bird. Two of the three parrots chose the solo apparatus when alone, and two of the three parrots preferred the joint-task apparatus when tested with a partner. When paired up, social preferences and tolerance affected the likelihood a pair cooperated.

Keas ("Nestor notabilis"), parrots native to New Zealand, are a distant relative of the grey parrot. They live in complex social groups and do well on cognitive tests. Heaney, Gray, and Taylor gave four captive keas a series of cooperative loose-string tasks. After solo training and shaping with string ends increasingly further apart, two birds were released simultaneously in a joint loose-string task. Both pairs did very well, one pair failing only 5 in 60 trials. Shaping was then used in a delay task, with the partner released after one second, then two, and gradually up to 25 seconds later than the first bird.
The birds managed to wait for a partner between 74% to 91% of test trials, including success at 65 seconds delay, longer than any other animal of any species had been tested for.
To assess if this success could be explained by the learning of a combination of cues, such as seeing a partner while feeling tension on the string, or by a proper understanding of cooperation, the researchers randomly gave the keas a set-up they could solve alone or one in which they needed to cooperate with a delayed partner.
Three of the four keas were successful at a significant rate: they chose to wait when they had to and immediately pulled when the task could be done alone. However, when the researchers modified the set-up and coiled up the string end of the delayed partner, no bird was successful at discriminating between a duo platform with both ends of string available to both keas and a duo platform with the partner’s string coiled out of reach.
The researchers were not able to determine the reason for this result. They speculated it could be that keas do have an understanding of when they need a partner but do not have a clear idea of the role their partner plays in relation to the string, or they may lack of a full causal understanding of how the string works. Finally, the researchers attempted to ascertain if keas have a preference for working alone or together. No preference was found in three of the four keas, but one kea preferred the duo platform significantly more. Heaney, Gray, and Taylor concluded that these results put keas on a par with elephants and chimpanzees in terms of cooperative pulling.

These conclusions are in sharp contrast to those of Schwing, Jocteur, Wein, Noë, and Massen, who tested ten captive keas in a loose-string task on an apparatus that provided limited visibility to follow the trajectory of the string. After training with a human partner (no solo training was done), only 19% of trials led to the birds obtaining food in the joint task. The researchers found that the closer the birds were affiliated, the more successful they were in the cooperation task. The keas did not seem to understand either the mechanics of the loose-string apparatus or the need of a partner, as in training with humans they still pulled the string even when the human was too far away or facing the wrong way. The way rewards were distributed had a small effect on the likelihood of cooperation attempts. The difference in social rank or dominance did not seem to matter.



</doc>
<doc id="56378386" url="https://en.wikipedia.org/wiki?curid=56378386" title="Parliament of 1327">
Parliament of 1327

The Parliament of 1327, which sat at the Palace of Westminster between 7 January and 9 March 1327, was instrumental in the transfer of the English crown from King Edward II to his son, Edward III. Edward II had become increasingly unpopular with the English nobility, predominantly because of the excessive influence of unpopular court favourites, the patronage he devoted to them, and his perceived ill-treatment of the nobility. By 1325, even his wife, Queen Isabella, despised him. Towards the end of the year, she took the young Edward to her native France, where she joined and probably entered into a relationship with the powerful and wealthy nobleman Roger Mortimer, whom her husband had exiled. The following year, they invaded England to depose Edward II. Almost immediately, the King's resistance was beset by betrayal, and he eventually abandoned London and fled west, probably to raise an army in Wales or Ireland. He was soon captured and imprisoned.

Isabella and Mortimer summoned a parliament to confer legitimacy on their regime. The meeting began gathering at Westminster on 7 January, but little could be done in the absence of the King. The fourteen-year-old Edward was proclaimed "Keeper of the Realm" (but not yet king), and a parliamentary deputation was sent to Edward II asking him to allow himself to be brought to parliament. He refused, and the parliament continued without him. The King was accused of offences ranging from the promotion of favourites to the destruction of the church, resulting in a betrayal of his coronation oath to the people. These were known as the "Articles of Accusation". The City of London was particularly aggressive in its attacks on Edward II, and its citizens may have helped intimidate those attending parliament into agreeing to the King's deposition, which occurred on the afternoon of 13 January.

On or around 21 January, the Lords Temporal sent another delegation to the King to inform him of his deposition, effectively giving Edward an ultimatum: if he did not agree to hand over the crown to his son, then the lords in parliament would give it to somebody outside the royal family. King Edward wept but agreed to their conditions. The delegation returned to London, and Edward III was proclaimed king immediately. He was crowned on 1 February 1327. In the aftermath of the parliamentary session, his father remained imprisoned, being moved around to prevent attempted rescues; he died—presumed killed, probably on Mortimer's orders—that September. Crises continued for Mortimer and Isabella, who were "de facto" rulers of the country, partly because of Mortimer's own greed, mismanagement, and mishandling of the new king. Edward III led a coup d'état against Mortimer in 1330, overthrew him, and began his personal rule.

King Edward II of England had court favourites who were unpopular with his nobility, such as Piers Gaveston and Hugh Despenser the Younger. Gaveston was killed during an earlier noble rebellion against Edward in 1312, and Despenser was hated by the English nobility. Edward was also unpopular with the common people due to his repeated demands from them for unpaid military service in Scotland. None of his campaigns there were successful, and this led to a further decline in his popularity, particularly with the nobility. His image was further diminished in 1322 when he executed his cousin, Thomas, Earl of Lancaster and confiscated the Lancaster estates. Historian Chris Given-Wilson has written how, by 1325 the nobility believed that "no landholder could feel safe" under the regime. This distrust of Edward was shared by his wife, Isabella of France, who believed Despenser responsible for poisoning the King's mind against her. In September 1324 Queen Isabella had been publicly humiliated when the government declared her an enemy alien, and the King had immediately repossessed her estates, probably at the urging of Despenser. Edward also disbanded her retinue. Edward had already been threatened with deposition on two previous occasions (in 1310 and 1321). Historians agree that hostility towards Edward was universal. W. H. Dunham and C. T. Wood ascribed this to Edward's "cruelty and personal faults", suggesting that "very few, not even his half-brothers or his son, seemed to care about the wretched man" and that none would fight for him. A contemporary chronicler described Edward as "rex inutilis", or a "useless king".
France had recently invaded the Duchy of Aquitaine, then an English royal possession. In response, King Edward sent Isabella to Paris, accompanied by their thirteen-year-old son, Edward, to negotiate a settlement. Contemporaries believed she had sworn, on leaving, never to return to England with the Despensers in power. Soon after her arrival, correspondence between Isabella and her husband, as well between them and her brother King Charles IV of France and Pope John XXII, effectively disclosed the royal couple's increasing estrangement to the world. A contemporary chronicler reports how Isabella and Edward became increasingly scathing of each other, worsening relations. By December 1325 she had entered into a possibly sexual relationship in Paris with the wealthy exiled nobleman Roger Mortimer. This was public knowledge in England by March 1326, and the King openly considered a divorce. He demanded that Isabella and Edward return to England, which they refused to do: "she sent back many of her retinue but gave trivial excuses for not returning herself" noted her biographer, John Parsons. Their son's failure to break with his mother angered the King further. Isabella became more strident in her criticisms of Edward's government, particularly against Walter de Stapledon, Bishop of Exeter, a close associate of the King and Despenser. King Edward alienated his son by putting the prince's estates under royal administration in January 1326, and the following month the King ordered that both he and his mother be arrested on landing in England.

While in Paris, the Queen became the head of King Edward's exiled opposition. Along with Mortimer, this group included Edmund of Woodstock, Earl of Kent, Henry de Beaumont, John de Botetourt, John Maltravers and William Trussell. All were united by hatred of the Despensers. Isabella portrayed her and Prince Edward as seeking refuge from her husband and his court, both of whom she claimed were hostile to her, and claimed protection from Edward II. King Charles refused to countenance an invasion of England; instead, the rebels gained the Count of Hainaut's backing. In return, Isabella agreed that her son would marry the Count's daughter Philippa. This was a further insult to Edward II, who had intended to use his eldest son's marriage as a bargaining tool against France, probably intending a marriage alliance with Spain.

From February 1326 it was clear in England that Isabella and Mortimer intended to invade. Despite false alarms, large ships, as a defensive measure, were forbidden from leaving English ports, and some were pressed into royal service. King Edward declared war on France in July; Isabella and Mortimer invaded England in September, landing in Suffolk on the 24th. The commander of the royal fleet assisted the rebels: the first of many betrayals Edward II suffered. Isabella and Mortimer soon found they had significant support among the English political class. They were quickly joined by Thomas, Earl of Norfolk, the King's brother, accompanied by Henry, Earl of Leicester (brother of the executed Earl of Lancaster), and soon afterwards arrived the Archbishop of Canterbury and the Bishops of Hereford and Lincoln. Within the week, support for the King had dissolved, and, accompanied by Despenser, he deserted London and travelled west. Edward's flight to the west precipitated his downfall. Historian Michael Prestwich describes the King's support as collapsing "like a building hit by an earthquake". Edward's rule was already weak, and "even before the invasion, along with preparation, there had been panic. Now there was simply panic". Ormrod notes how

King Edward's attempt to raise an army in South Wales was to no avail, and he and Despenser were captured on 16 November 1326 near Llantrisant. This, along with the unexpected swiftness with which the entire regime had collapsed, forced Isabella and Mortimer to wield executive power until they made arrangements for a successor to the throne. The King was incarcerated by the Earl of Leicester, while those suspected of being Despenser spies or supporters of the King—particularly in London, which was aggressively loyal to the Queen—were murdered by mobs.

Isabella spent the last months of 1326 in the West Country, and while in Bristol witnessed the hanging of Despenser's father, the Earl of Winchester on 27 October. Despenser himself was captured in Hereford and executed there within the month. In Bristol Isabella, Mortimer and the accompanying lords discussed strategy. Not yet possessing the Great Seal, on 26 October they proclaimed the young Edward guardian of the realm, declaring that "by the assent of the whole community of the said kingdom present there, they unanimously chose [Edward III] as keeper of the said kingdom". He was not yet officially declared king. The rebels' description of themselves as a community deliberately harked back to the reform movement of Simon de Montfort and the baronial league, which had described its reform programme as being of the community of the realm against Henry III. Claire Valente has pointed out how, in reality, the most common phrase heard "was not 'the community of the realm', but 'the quarrel of the earl of Lancaster'", illustrating how the struggle was still a factional one within baronial politics, whatever cloak it may have appeared to possess as a reform movement.

By 20 November 1326 the Bishop of Hereford had retrieved the Great Seal from the King, and delivered it to the King's son. He could now be announced as his father's heir apparent. Although, at this stage, it might still have been possible for Edward II to remain king, says Ormrod, "the writing was on the wall". A document issued by Isabella and her son at this time described their respective positions thus:

Isabella, Mortimer and the lords arrived in London on 4 January 1327. In response to the previous year's spate of murders, Londoners had been forbidden to bear arms, and two days later all citizens had sworn an oath to keep the peace. Parliament met on 7 January to consider the state of the realm now the King was incarcerated. It had originally been summoned by Isabella and the Prince, in the name of the King, on 28 October the previous year. Parliament had been intended to assemble on 14 December 1326, but on 3 December—still in the name of the King—further writs were issued deferring the sitting until early the next year. This, it was implied, was due to the King being abroad, rather than imprisoned. Because of this, parliament would have to be held before the Queen and Prince Edward. "The History of Parliament Trust" has described the legality of the writs as being "highly questionable", and C. T. Wood called the sitting "a show of pseudo-parliamentary regularity", "stage-managed" by Mortimer and Thomas, Lord Wake. For Isabella and Mortimer, governing through parliament was only a temporary solution to a constitutional problem, because at some point their positions would likely be challenged legally. Thus, suggests Ormrod, they had to enforce a solution favourable to Mortimer and the Queen, by any means they could.

Contemporaries were uncertain as to the legality of Isabella's parliament. Edward II was still king, although in official documents, this was only alongside his "most beloved consort Isabella queen of England" and his "firstborn son keeper of the kingdom", in what Phil Bradford called as a "nominal presidency". King Edward was said to be abroad when in reality he was imprisoned in Kenilworth Castle. It was maintained that he desired a ""colloquium"" and a ""tractatum"" (conference and consultation) with his lords "upon various affairs touching himself and the state of his kingdom", hence the holding of parliament. Supposedly it was Edward II himself who postponed the first sitting until January, "for certain necessary causes and utilities", presumably at the behest of the Queen and Mortimer.

A priority for the new regime was deciding what to do with Edward II. Mortimer considered holding a state trial for treason, in the expectation of a guilty verdict and a death sentence. He and other lords discussed the matter at Isabella's Wallingford Castle just after Christmas, but with no agreement. The Lords Temporal affirmed that Edward had failed his country so gravely that only his death could heal it; the attending bishops, on the other hand, held that whatever his faults, he had been anointed king by God. This presented Isabella and Mortimer with two problems. First, the bishops' argument would be popularly understood as risking the wrath of God. Second, public trials always bring the danger of an unintended verdict, particularly as it seems likely a broad body of public opinion doubted whether an anointed king could even commit treason. Such a result would mean not only Edward's release but his restoration to the throne. Mortimer and Isabella sought to avoid a trial and yet keep Edward II imprisoned for life. The King's imprisonment (officially by his son) had become public knowledge, and Isabella's and Mortimer's hand was forced as the arguments for the young Edward being named keeper of the kingdom were now groundless (as the King had clearly returned to his realm—one way or another).

No parliament had sat since November 1325. Only 26 of the 46 barons who had been summoned in October 1326 for the December parliament were then also summoned to that of January 1327, and six of those had never received summonses under Edward II at all. Officially, the instigators of the parliament were the Bishops of Hereford and Winchester, Roger Mortimer and Thomas Wake; Isabella almost certainly played a background role. They summoned, as Lords Spiritual, the Archbishop of Canterbury and fifteen English and four Welsh bishops as well as nineteen abbots. The Lords Temporal were represented by the Earls of Norfolk, Kent, Lancaster, Surrey, Oxford, Atholl and Hereford. Forty-seven barons, twenty-three royal justices, and several knights and burgesses were summoned from the shires and the Cinque Ports. They may well have been encouraged, suggests Maddicott, by the wages to be paid to those attending: the "handsome sum" of four shillings a day for a knight and two for a burgess. The knights provided the bulk of Isabella's and the Prince's vocal support; they included Mortimer's sons, Edward, Roger and John. Sir William Trussell was appointed procurator, or Speaker, despite his not being an elected member of parliament. Although the office of procurator was not new, the purpose of Trussell's role set a constitutional precedent, as he was authorised to speak on behalf of parliament as a body. A chronicle describes Trussell as one "who cannot disagree with himself and, [therefore], shall ordain for all". There were fewer lords present than were traditionally summoned, which increased the influence of the Commons. This may have been a deliberate strategy on behalf of Isabella and Mortimer, who, suggests Dodd, would have known well that in the occasionally tumultuous parliaments of earlier reigns, "the trouble that had been caused in parliament had emanated almost exclusively from the barons". The Archbishop of York, who had been summoned to the December parliament, was "conspicuous by his absence" from the January sitting. Some Welsh MPs also received summonses, but these had deliberately been despatched too late for those elected to attend; others, such as the sheriff of Meirionnydd, Gruffudd Llwyd, refused to attend, out of loyalty to Edward II and also hatred of Roger Mortimer.

Although a radical gathering, the parliament was to some degree consistent with previous assemblies, being dominated by lords reliant on a supportive Commons. It differed, though, in the greater-than-usual influence that outsiders and commoners had, such as those from London. The January–February parliament was geographically broader too, as it contained unelected members from Bury St Edmunds and St Albans: says Maddicott, "those who planned the deposition reached out in parliament to those who had no right to be there". And, says Dodd, the rebels deliberately made parliament "centre stage" to their plans.

Before parliament met, the lords had sent Adam Orleton (the Bishop of Hereford) and William Trussell to Kenilworth to see the King, with the intention of persuading Edward to return with them and attend parliament. They failed in this mission: Edward flatly refused and roundly cursed them. The envoys returned to Westminster on 12 January; by which time parliament had been sitting five days. It was felt that nothing could be done until the King had arrived: historically a parliament could only pass statutes with the monarch present. On hearing from Orleton and Trussell how Edward had denounced them, the King's opponents were no longer willing to let his absence stand in their way. Edward II's refusal to attend failed to prevent the parliament from taking place, the first time this had ever happened.

The various titles bestowed on the younger Edward at the end of 1326—which acknowledged his unique position in government while avoiding calling him king—reflected an underlying constitutional crisis, of which contemporaries were keenly aware. The fundamental question was how the crown was transferred between two living kings, a situation which had never arisen before. Valente has described how this "upset the accepted order of things, threatened the sacrosanctity of kingship, and lacked clear legality or established process". Contemporaries were also uncertain as to whether Edward II had abdicated or was being deposed. On 26 October it had been recorded in the Close Rolls that Edward had "left or abandoned his kingdom", and his absence enabled Isabella and Mortimer to rule. They could legitimately argue that King Edward, having provided no regent during his absence (as would be usual), should make his son governor of the kingdom in his father's stead. They also said Edward II held Parliament in contempt by calling it a treasonous assembly and insulted those attending it as traitors". It is unknown whether the King did, in fact, say or believe this, but it certainly suited Isabella and Mortimer for parliament to think so. If Edward did denounce parliament then he probably did not realise how it could be used against him. In any case, Edward's absence saved the couple the embarrassment of having a reigning king present when they deposed him, and Seymour Phillips suggests that if Edward had attended he may have found enough support to disrupt their plans.

Parliament had to consider its next step. Bishop Orleton—emphasising Isabella's fear of the King—asked the assembled lords whom they would prefer to rule, Edward or his son. The response was sluggish, with no rush to either depose or acclaim. Deposition had been raised too suddenly for many members to stomach: the King was still not entirely friendless, and indeed, has been described by Paul Dryburgh as casting an "ominous shadow" over the proceedings. Orleton suspended proceedings until the next day to allow the lords to dwell on the question overnight. Also on the 12th, Sir Richard de Betoyne, the Mayor of London, and the Common Council wrote to the lords in support of both the Earl of Chester being made King and the deposition of Edward II, whom they accused of failing to uphold his coronation oath and the duties of the crown. Mortimer, who was highly regarded by Londoners, may well have instigated this as a means of influencing the lords. The Londoners' petition also proposed that the new king should be governed by his Council until it was clear he understood his coronation oath and regal responsibilities. This petition the lords accepted; another, requesting the King should hold Westminster parliaments annually until he reached his majority, was not.

Whether Edward II resigned his throne or was forced from it under pressure, the crown legally changed hands on 13 January with the support, it was recorded, of "all the baronage of the land". Parliament met in the morning and then suspended itself. A large group of the lords temporal and spiritual made their way to the City of London's Guildhall where they swore an oath "to uphold all that has been ordained or shall be ordained for the common profit". This was intended to present those in parliament who disagreed with deposition with a "fait accompli". At the Guildhall they also swore to uphold the constitutional limitations of the Ordinances of 1311.

The group then returned to Westminster in the afternoon, and the lords formally acknowledged that Edward II was no longer to be King. Several orations were made. Mortimer, speaking on behalf of the lords, announced their decision. Edward II, he proclaimed, would abdicate and "...Sir Edward ... should have the government of the realm and be crowned king". The French chronicler Jean Le Bel described how the lords proceeded to document Edward II's "ill-advised deeds and actions" to create a legal record which was duly presented to parliament. This record declared "such a man was unfit ever to wear the crown or call himself King". This list of misdeeds—probably drawn up by Orleton and Stratford personally—were known as the Articles of Accusation. The bishops gave sermons—Orleton, for example, spoke of how "a foolish king shall ruin his people", and, report Dunham and Wood, he "dwelt weightily upon the folly and unwisdom of the king, and upon his childish doings". This, says Ian Mortimer, was "a tremendous sermon, rousing those present in the way he knew best, through the power of the word of God". Orleton based his sermon on the biblical text "Where there is no governor the people shall fall" from the Book of Proverbs, while the Archbishop of Canterbury took for his text "Vox Populi, Vox Dei".

During the sermons, the articles of deposition were officially presented to the assembly. In contrast to the elaborate and floridly hyperbolic accusations previously launched at the Despensers, this was a relatively simple document. The King was accused of being incapable of fair rule; of indulging false counsellors; preferring his own amusements to good government; neglecting England and losing Scotland; dilapidating the church and imprisoning the clergy; and, all in all, being in fundamental breach of the coronation oath he had made to his subjects. All of which, the rebels claimed, was so well known as to be undeniable. The articles accused Edward's favourites of tyranny although not the King himself, whom they described as "incorrigible, without hope of reform". England's succession of military failures in Scotland and France rankled with the lords: Edward had fought no successful campaigns in either theatre, yet had raised enormous levies to enable him to do so. Such levies says F. M. Powicke, "could only have been justified by military success". Accusations of military failure were not wholly fair in placing the blame for these losses, as they did, so squarely on Edward II's shoulders: Scotland had arguably been almost lost in 1307. Edward's father had, says Seymour Phillips, left him "an impossible task", having started the war without making sufficient gains to allow his son to finish it. And Ireland had been the theatre of one of the King's few military successes—the English victory at the Battle of Faughart in 1318 had crushed Robert the Bruce's ambitions in Ireland (and seen the death of his brother). Only the King's military failures, though, were remembered, and indeed, they were the most damning of all the articles:
Every speaker on 13 January reiterated the articles of accusation, and all concluded by offering the young Edward as king, if the people approved him. The crowd outside, which included a large company of unruly Londoners, says Valente, had been "whipped ... into such fervour" by "dramatic outcries at appropriate points in the orations" from Thomas Wake, who repeatedly rose and demanded of the assembly whether they agreed with each speaker; "Do you agree? Do the people of the country agree?" Wake's exhortations—arms outstretched, says Prestwich, he cried "I say for myself that he shall reign no more")—combined with the intimidating mob, led to tumultuous responses of "Let it be done! Let it be done!" This, says May McKisack, gave the new regime a degree of "support of popular clamour". The Londoners played a key role in ensuring that remaining supporters of Edward II were intimidated and overwhelmed by events.

Edward III was proclaimed king. At the end of the day, said Valente, "the "electio" of the magnates received the "acclamatio" of the "populi", "'Fiat!"." Proceedings drew to a close with a chorus of "Gloria, laus et honor", and perhaps oaths of homage from the lords to the new king. Assent to the new regime was not universal: the Bishops of London, Rochester and Carlisle abstained from the day's affairs in protest, and Rochester was later beaten up by a London mob because of his opposition.

One final action remained to be taken: the ex-King in Kenilworth had to be informed that his subjects had chosen to withdraw their allegiance from him. A delegation was organised to take the news. The delegates were the Bishops of Ely, Hereford and London, and around 30 laymen. Among the latter, the Earl of Surrey represented the lords and Trussell represented the shire knights. The group was intended to be as representative of parliament—and so the kingdom—as possible. It was not composed solely of parliamentarians, but there were enough of them in it to appear parliamentarian. Its size also had the added advantage of spreading collective responsibility far more broadly than would have happened in a small group. They left on or shortly after Thursday 15 January and had arrived in Kenilworth by either 21 or 22 January, when William Trussell asked for the King to be brought to them in the name of parliament.

Edward, dressed in a black gown and under the Earl of Lancaster's escort, was brought to the great hall. Geoffrey le Baker's Chronicle describes how the delegates equivocated at first, "adulterating the word of truth" before coming to the point. Edward was offered the choice of resigning in favour of his son, and being provided for according to his rank, or of being deposed. This, it was emphasised, could lead to the throne being offered to someone, not of royal blood but politically experienced, clearly referring to Mortimer. The King protested—mildly—and wept, fainting at one point. According to Orleton's later report, Edward claimed he had always followed the guidance of his nobles, but regretted any harm he had done. The deposed king took comfort from his son succeeding him. It seems probable that a memorandum of acknowledgement was drawn up between the delegation and Edward, minuting what was said, although this has not survived. Baker says that at the end of the meeting Edward's Steward, Thomas Blunt, dramatically broke his staff of office in half, and dismissed Edward's household.

The delegation left Kenilworth for London on 22 January: their news preceded them. By the time they reached Westminster, around 25 January, Edward III was already officially referred to as king, and his peace had been proclaimed at St Paul's Cathedral on the 24th. Now the new king could be proclaimed in public; Edward III's reign was thus dated from 25 January 1327. Behind the scenes, though, discussions must have begun on the thorny question of what to do with his predecessor, who still had not had any judgement—legal or parliamentary—passed upon him.

Edward III's political education was deliberately accelerated by the tutelage of advisors such as William of Pagula and Walter de Milemete. Still a minor, Edward III was crowned at Westminster Abbey on 1 February 1327: executive power remained with Mortimer and Isabella. Mortimer was made Earl of March in October 1328, but otherwise, received few grants of land or money. Isabella, on the other hand, gained an annual income of 20,000 marks (£13,333) within the month. She achieved this by requesting the return of her dower which her husband had confiscated; it was returned to her substantially augmented. Ian Mortimer has called the grant she received as amounting to "one of the largest personal incomes anyone had ever received in English history". Following Edward's coronation parliament was recalled. According to precedent, a new parliament should have been summoned with the accession of a new monarch, and this failure of process indicates the novelty of the situation. Official records regnally date the entire parliament to the first year of Edward III's reign rather than the last of his father's, even though it spread over both.

When recalled, parliament returned to its usual business, and heard a large number (42) of petitions from the community. These not only included the political—and often lengthy—petitions related directly to the deposition, but a similar number coming from the clergy and the City of London. This was the greatest number of petitions to have been submitted by the Commons in the history of parliament. Their requests ranged from confirmation of the acts against the Despensers and those in favour of Thomas of Lancaster, to the reconfirmation of the Magna Carta. There were ecclesiastical petitions, and those from the shires dealt mainly in annulling debts and amercements of both individuals and towns. There were numerous requests for the King's grace, for example, overturning perceived false judgements in local courts and concerns for law and order in the localities generally. Restoring law and order was a priority of the new regime, as Edward II's reign had foundered on his inability to do so, and his failure then used to depose him. The principle behind Edward's deposition was, supposedly, to redress such wrongs his reign had caused. One petition requested members of the Commons be authorised to take written confirmation of their petition and its concomitant answer to their localities, while another protested against corrupt local royal officials. This eventually resulted in a proclamation in 1330 instructing individuals who had cause of complaint or need of redress from such should attend the approaching parliament.

The Commons too were concerned for the restoration of law and order, and one of their petitions called for the immediate appointment of wide-ranging keepers of the peace who could personally put men on trial. This request was agreed by the King's council. This return to normal parliamentary business demonstrated, it was hoped, both the regime's legitimacy and its ability to repair the injustices of the previous reign. Most of the petitions were accepted—resulting in seventeen statute articles—which indicates how keen Isabella and Mortimer were to placate the Commons. When parliament finally dissolved on 9 March 1327, it had been the second longest, at seventy-one days, of the century to date; further, notes Dodd, because of this it was "the only assembly in the late medieval period to outlive a king and see in his successor".

The dead Earl of Lancaster's titles and estates were restored to his brother Henry, and the 1323 judgement against Mortimer, which exiled him, was overturned. The invaders were also restored to their estates in Ireland. In an attempt at settling the Irish situation, parliament issued ordinances on 23 February pardoning those who had supported Robert Bruce's invasion. The deposed King was referred to only obliquely in official records—for example, as "Edward his father, when he was king," "Edward, the father of the King who now is" or as he had been known as a youth, "Edward of Caernarfon". Isabella and Mortimer were careful to try to prevent the deposition from tarnishing their reputations, reflected in their concern of not just obtaining Edward II's "ex-post facto" agreement to his removal, but then publicising his agreement. The problem they faced was that this effectively involved having to rewrite a piece of history in which many people were actively involved and had taken place only two weeks earlier.

The City of London also benefited. In 1321, Edward II had disenfranchised London, and royal officials, in the words of a contemporary, had "pris[ed] every privilege and penny out of the city", as well as deposing their mayor: Edward had ruled London himself through a system of wardens. Gwyn Williams described this as "an emergency regime of dubious legality". In 1327 Londoners petitioned the recalled parliament for their liberties to be restored, and, since they had been of valuable—probably crucial—importance in enabling the deposition, on 7 March they received not just the rights Edward II had removed from them, but greater privileges than they had ever possessed.

Meanwhile, Edward II was still imprisoned at Kenilworth, and was intended to stay there forever. Attempts to free him led to his transfer to the more secure Berkeley Castle in early April 1327. Plotting continued, and he was frequently moved to other places. Eventually being returned to Berkeley for good, Edward died there on the night of 21 September. Mark Ormrod described this as "suspiciously timely", for Mortimer, as Edward's almost-certain murder permanently removed a rival and a target for restoration.

Parliamentary proceedings were traditionally drawn up contemporaneously and entered onto a parliament roll by clerks. The Roll of 1327 is notable, according to the "History of as Parliament", because "despite the highly charged political situation in January 1327, [it] contains no mention of the process by which Edward II ceased to be king". The roll only begins with the reassembling of parliament under Edward III in February, after the deposition of his father. It is likely, says Phillips, that since those involved were aware of the precarious legal basis for Edward's deposition—and how it would not bear "too close an examination"—there may never have been an enrolment: "Edward II had been airbrushed from the record". Other possible reasons for the lack of an enrolment are that it would never have been entered on a roll because the parliament was clearly illegitimate, or because Edward III later felt it was undesirable to have an official record of a royal deposition in case it suggested a precedent had been set, and removed it himself.

It was not long before the crisis affected Mortimer's relationship with Edward III. Notwithstanding Edward's coronation, Mortimer was the country's "de facto" ruler. The high-handed nature of his rule was demonstrated, according to Ian Mortimer, on the day of Edward III's coronation. Not only did he arrange for his three eldest sons to be knighted, but—feeling a knight's ceremonial robes were inadequate—he had them dressed as earls for the occasion. Mortimer himself occupied his energies in getting rich and alienating people, and the defeat of the English army by the Scots at the Battle of Stanhope Park (and the Treaty of Edinburgh–Northampton which followed it in 1328) worsened his position. Maurice Keen describes Mortimer as being no more successful in the war against Scotland than his predecessor had been. Mortimer did little to rectify this situation and continued to show Edward disrespect. Edward, for his part, had originally (and unsurprisingly) sympathised with his mother against his father, but not necessarily for Mortimer. Michael Prestwich has described the latter as a "classic example of a man whose power went to his head", and compares Mortimer's greed to that of the Despensers and his political sensitivity to that of Piers Gaveston. Edward had married Philippa of Hainault in 1328, and they had a son in June 1330. Edward decided to remove Mortimer from the government: accompanied and assisted by close companions, Edward launched a coup d'état which took Mortimer by surprise at Nottingham Castle on 19 October 1330. He was hanged at Tyburn a month later and Edward III's personal reign began.

The parliament of 1327 is the focus of two main areas of interest for historians: in the long term, the part it played in the development of the English parliament, and in the short term, its place in the deposition of Edward II. On the first point, Gwilym Dodd has described the parliament as a landmark event in the institution's history, and, say Richardson and Sayles, it began a fifty-year period of developing and honing procedure. The assembly also, suggests G. L. Harriss, marks a point in the history of the English monarchy in which its authority was curtailed to a similar degree to the limitation previously imposed on King John by the Magna Carta and Henry III by de Montfort. Maddicott agrees with Richardson and Sayles regarding the significance of 1327 for the development of separate chambers, because it "saw the presentation of the first full set of commons' petitions [and] the first comprehensive statute to derive from such petitions". Maude Clarke described its significance as being in how "feudal defiance" was for the first time subsumed to the "will of the commonality, and the King was rejected not by his vassals but by his subjects".

The second question it raises for scholars is whether Edward II was deposed by parliament, as an institution, or just while parliament sat. While many of the events necessary for the King's removal had taken place in parliament, others of equal significance (for example, the oath-taking at the Guildhall) occurred elsewhere. Parliament was certainly the public setting for the deposition. Victorian constitutional historians saw Edward's deposition as demonstrating fledgeling authority by the House of Commons akin to their own parliamentary system. Twentieth-century historiography remains divided on the issue. Barry Wilkinson, for example, considered it a deposition—but by the magnates, rather than parliament—but G. L. Harriss termed it an abdication, believing "there was no legal process of deposition, and kings like ... Edward II were induced to resign". Edward II's position has been summed up as his being offered "the choice of abdication in favour of his son Edward or forcible deposition in favour of a new king selected by his nobles". Seymour Phillips has argued that it was the "combined determination of the leading magnates, their personal followers and the Londoners" that Edward should be gone. 
Chris Bryant argues it is not clear whether these events were driven by parliament, or merely happened to occur in parliament. although he suggests Isabella and Roger Mortimer thought it necessary to have parliamentary support. Valente has suggested "the deposition was not revolutionary and did not attack kingship itself", it was not "necessarily illegal and outside the bounds of the 'constitution'", even though historians commonly describe it as such. The discussion is confused further, she says, because varying descriptions are given of the assembly by contemporaries. Some described it as being a royal council, others called it a parliament in the King's absence or a parliament with the Queen presiding, or one summoned by her and Prince Edward. Ultimately, she wrote, it was magnates deciding on policy, and being able to do so through the support of the knights and commoners.

Dunham and Wood suggested that Edward's deposition was forced by political rather than legal factors. There is also a choice of who deposed: whether "the magnates alone deposed, that the magnates and people jointly deposed, that Parliament itself deposed, even that it was the 'people' whose voice was decisive". Ian Mortimer has described how "the representatives of the community of the realm would be called upon to act as an authority over and above that of the King". It was no advance of democracy, and was not intended to be—its purpose was to "unite all classes of the realm against the monarch" of the time. John Maddicott has said the proceedings began as a baronial coup but ended up becoming something close to a "national plebiscite", in which the commons were part of a radical reform of the state. This parliament also clarified procedures, such as codifying petitioning, legislating for it, and promulgating statutes, which would become the norm.

The parliament also illustrates how contemporaries viewed the nature of tyranny. The leaders of the revolution, aware that deposition was a barely understood and unpopular concept in the political culture of the day, began almost immediately re-casting events as an abdication instead. Few contemporaries overtly disagreed with Edward's deposition, "but the fact of deposition itself caused immense anxiety", suggested David Matthews. It was an event as yet unheard of in English history. Phillips comments that "using accusations of tyranny to remove a legitimate and anointed king were too contentious and divisive to be of any practical use", which is why Edward had been accused of incompetence and inadequacy and much else, and not of tyranny. The Brut Chronicle, in fact, goes so far as to ascribe Edward's deposition, not to intentions of men and women, but to the fulfilment of a prophecy by Merlin.

Edward's deposition also set a precedent and laid out arguments for subsequent depositions. The 1327 articles of accusation, for example, were drawn on sixty years later during the series of crises between King Richard II and the Lords Appellant. When Richard refused to attend parliament in 1386, Thomas of Woodstock, Duke of Gloucester and William Courtenay, Archbishop of Canterbury visited him at Eltham Palace and reminded him how—per "the statute by which Edward [II] had been adjudged"—a King who did not attend parliament was liable to deposition by his lords.

Indeed, it has been suggested Richard II may have been responsible for the disappearance of the 1327 parliament roll when he recovered personal power two years later. Given-Wilson says that Richard considered Edward's deposition a "stain which he was determined to remove" from the royal family's history by proposing Edward's canonisation. Richard's subsequent deposition by Henry Bolingbroke in 1399 naturally drew direct parallels with that of Edward. Events which had taken place over 70 years earlier were by 1399 considered "ancient custom", which had set legal precedent, if an ill-defined one. A prominent chronicle of Henry's usurpation, composed by Adam of Usk, has been described as bearing "a striking resemblance" to the events of the 1327 parliament. Indeed, said Gaillard Lapsley, "Adam uses words that strongly suggest that he had this precedent in mind."

Edward II's deposition was used as political propaganda as late as the troubled last years of James I in the 1620s. The King was very ill and played a peripheral role in government; his favourite, George Villiers, Duke of Buckingham became proportionately more powerful. Attorney general Henry Yelverton publicly compared Buckingham to Hugh Despenser on account of Villiers' penchant for enriching his friends and relatives through royal patronage. Curtis Perry has suggested that 17th-century "contemporaries applied the story [of Edward's deposition] to the political turmoil of the 1620s in conflicting ways: some used the parallel to point towards the corrupting influence of favourites and to criticize Buckingham; others drew parallels between the verbal intemperance of Yelverton and his ilk and the unruliness of Edward's opponents".

The Parliament of 1327 was the last and only parliament before the Laws in Wales Acts 1535 and 1542 to summon Welsh representatives. They never took their seats, having been deliberately summoned too late to attend, because South Wales supported Edward, and North Wales was equally opposed to Mortimer. The 1327 parliament also provided almost the same list of attendees for the next five years of parliaments.

Christopher Marlowe was the first to dramatise the life and death of Edward II, with his 1592 play Edward II (or "The Troublesome Reign and Lamentable Death of Edward the Second, King of England, with the Tragical Fall of Proud Mortimer"). Marlowe emphasises the importance of parliament in Edward's reign, from his original taking of the coronation oath (Act I, scene 1), to his deposition (in Act V, scene 1).




</doc>
<doc id="56664001" url="https://en.wikipedia.org/wiki?curid=56664001" title="Yeomanry Cavalry">
Yeomanry Cavalry

The Yeomanry Cavalry was the mounted component of the British Volunteer Corps, a military auxiliary established in the late 18th century amid fears of invasion and insurrection during the French Revolutionary Wars. A yeoman was a person of respectable standing, one social rank below a gentleman, and the yeomanry was initially a rural, county-based force. Members were required to provide their own horses and were recruited mainly from landholders and tenant farmers, though the middle class also featured prominently in the rank and file. Officers were largely recruited from among the nobility and landed gentry. A commission generally involved significant personal expense, and although social status was an important qualification, the primary factor was personal wealth. From the beginning, the newly rich, who found in the yeomanry a means of enhancing their social standing, were welcomed into the officer corps for their ability to support the force financially. Urban recruitment increased towards the end of the 19th century, reflected in the early 20th century by increasingly common use of hired mounts.

The yeomanry was first used in support of local authorities to suppress civil unrest, most notably during the food riots of 1795. Its only use in national defence was in 1797, when the Castlemartin Yeomanry helped defeat a small French invasion in the Battle of Fishguard. Although the Volunteer Corps was disbanded following the defeat of Napoleon in 1815, the yeomanry was retained as a politically reliable force which could be deployed in support of the civil authorities. It often served as mounted police until the middle of the 19th century. Most famously, the Manchester and Salford Yeomanry was largely responsible for the Peterloo Massacre, in which some 17 people were killed and up to 650 were injured, while policing a rally for parliamentary reform in Manchester in 1819. The yeomanry was also deployed against striking colliers in the 1820s, during the Swing riots of the early 1830s and the Chartist disturbances of the late 1830s and early 1840s. The exclusive membership set the yeomanry apart from the population it policed, and as better law enforcement options became available the yeomanry was increasingly held back for fear that its presence would provoke confrontation. Its social status made the force a popular target for caricature, particularly after Peterloo, and it was often satirised in the press, in literature and on the stage.

The establishment of civilian police forces and renewed invasion scares in the middle of the 19th century turned the focus of the yeomanry to national defence, but its effectiveness and value in this role was increasingly questioned. It declined in strength, surviving largely due to its members' political influence and willingness to subsidise the force financially. A series of government committees failed to address the force's problems. The last, in 1892, found a place for the yeomanry in the country's mobilisation scheme, but it was not until a succession of failures by the regular army during the Second Boer War that the yeomanry found a new relevance as mounted infantry. It provided the nucleus for the separate Imperial Yeomanry, and after the war, the yeomanry was re-branded "en bloc" as the Imperial Yeomanry. It ceased to exist as a separate institution in 1908, when the yeomanry became the mounted component of the Territorial Force. Yeomanry regiments fought mounted and dismounted in both the First World War and the Second World War. The yeomanry heritage is maintained in the 21st century largely by four yeomanry regiments of the British Army Reserve, in which many 19th century regiments are represented as squadrons.

Europe experienced explosive population growth from the mid-18th century which, in Great Britain, was fed by improved farming methods introduced by the Agricultural Revolution. Around the same time, the Industrial Revolution brought increasing urbanisation, which led to ever greater demands for food. The more intensive cultivation required to meet these demands led to increased costs but left agricultural wages the same, resulting in poverty and starvation in rural communities. Poverty was also a problem in urban centres as increasing use of machinery put skilled labour out of work. Meanwhile, the political system had not kept up with the shifting population. While once prosperous towns that had become de-populated were still able to elect Members of Parliament (MPs) – the so-called rotten and pocket boroughs – major new towns such as Birmingham and Manchester were not represented. Poverty and disenfranchisement led to social discontent, giving rise to fears that the French Revolution would provide a model that might be emulated in Britain.

In 1793, the French revolutionary government declared war on Great Britain, adding fear of foreign invasion to that of domestic insurrection and leading to near panic in London. The regular British Army, which had already deployed six brigades alongside the Austrian army in the Netherlands, was not sufficient to defend the country, and the main military reserve, the militia, was considered neither effective nor trustworthy. It had been demobilised at the conclusion of the American Revolutionary War in 1783, and in the intervening decade it had been subject to cost-cutting measures that had left it deficient. It was embodied in 1792 as a precautionary measure against insurrection, but a body recruited predominantly from among the working class was itself suspect, to the extent that militia units were not trusted to be deployed in their own areas of recruitment until 1795. The government had previously resorted to volunteers to augment its forces in 1779, amid fears of a Franco-Spanish invasion, though this was short-lived and did not long survive the end of the war in the colonies. Considering that there was not enough time to address the militia's deficiencies, the government turned again to volunteers to bolster the nation's defences in 1794.

The appeal for volunteers led to the creation of the Volunteer Corps, of which the Gentlemen and Yeomanry Cavalry, as it was then called, was the mounted component. A yeoman was traditionally a freeholder of respectable standing, one social rank below a gentleman, and the yeomanry was recruited largely from among landholders and tenant farmers. The officers were appointed by royal commission, in the person of the Lord Lieutenant, and generally came from the nobility and landed gentry. Yeomen were expected to provide their own mounts, which represented a high financial barrier to entry and ensured that the yeomanry was an exclusive and prestigious organisation. In addition to farmers, the yeomanry attracted professionals, tradesmen and skilled craftsmen to its ranks, though the strong ties to the farming community meant that yeomanry activities were scheduled with an eye on the agricultural calendar, and harvests in particular informed the training schedule.

The yeomanry was county based and could be called out (embodied) by the Lord Lieutenant or Sheriff. Members were paid while embodied and subject to military law in the event of invasion. Initially, troops were liable for service only in their home or adjacent counties, though some troops voted to be liable for service nationwide while others restricted themselves to service only in their home county. Although some troops quickly combined to form county regiments, such as the Wiltshire Yeomanry Cavalry in 1797, many remained independent for years. By the end of 1794, between 28 and 32 troops of yeomanry, each up to 60 men strong, had been raised. A government attempt to raise more cavalry by compulsion, the Provisional Cavalry Act of 1796, increased interest in volunteer cavalry, and by 1799 there were 206 yeomanry troops. By 1800, the Provisional Cavalry regiments had been either disbanded or absorbed into the yeomanry, where they were frequently ostracised because of their lower social status.

The yeomanry was as much an instrument of law and order as it was a military organisation, and its terms of service stressed defence against both insurrection and invasion. It was only once called upon to repulse a foreign invasion, in 1797, when the French Légion Noire landed at Fishguard in Wales, and the Castlemartin Yeomanry was part of the force that defeated the invaders in the Battle of Fishguard. The yeomanry was more active as a constabulary, and corps were called out during the London Corresponding Society trials in 1794, during the food riots of 1795, and in response to enclosure protests, the destruction wrought by Luddites and disturbances caused by disaffected, demobilised servicemen in the years leading up to the end of the wars with France.

By 1801, the yeomanry was 21,000 strong, with troops in most English, many Welsh and some Scottish counties. They were based in towns, villages and the estates of the nobility, and varied in quantity from one to more than twenty in any given county. Troops were also raised in Ireland, where they reflected the Protestant Ascendancy. The Peace of Amiens in 1802 resulted in a reductions across the military, with cuts to the army and navy and the disembodiment of the militia. Legislation was passed to allow the Volunteer Corps to be retained without pay, but the yeomanry establishment nevertheless declined, only to increase again when war resumed in 1803. There were frequent invasion scares – most notably in 1804, when the beacons were lit in the Scottish lowlands and 3,000 volunteers and yeomanry assembled for what turned out to be a false alarm – and victory at the Battle of Trafalgar in 1805 did not fully eradicate the fears of a French landing.

The threat of invasion occupied much of the British political thinking until the final defeat of Napoleon in 1815, and in the period 1802–1803 alone there were 21 separate pieces of legislation designed to raise forces either voluntarily or compulsorily for the defence of the nation. The Volunteer Consolidation Act of 1804, which effectively governed the yeomanry until 1901, rationalised the confusion of legislation. The net result was to make voluntary service more attractive, a significant motivation being to avoid the compulsion of service in the unpopular militia. Faced with a deluge of volunteers, the War Office attempted to limit numbers. This caused an outcry, and administrative responsibility was transferred to the Home Office in 1803 as a result. By the following year, the number of volunteers and yeomanry together exceeded 342,000 men, significantly more than the government could arm in the immediate term, and in 1805 the yeomanry numbered just under 33,000 men.

A change of government in 1806 resulted in a change of policy, based on the belief that the volunteer force was an expensive solution which escaped central government control and undermined recruitment into the militia and regular army. The Local Militia Acts of 1808 created a new militia with incentives for volunteers to transfer into it. By 1813, the Local Militia had supplanted the need for a volunteer force, which had already declined to just under 69,000 men the previous year, and only a handful of volunteer corps remained. The yeomanry, however, was retained after the Napoleonic Wars as a politically reliable force. It was, nevertheless, reduced in numbers nationwide – figures for 1817 indicate an actual strength of around 18,000 – and in Gloucestershire, for example, of the 13 troops that existed in 1813, only the Gloucester Troop was kept on after 1815, to serve as mounted police.

Policing was the responsibility of the parish constable and his urban counterpart, the watchman, under the auspices of the magistrates. As urban centres grew, increased crime was dealt with by temporary measures such as the Special Constabulary. None of these, however, were sufficient to deal with large-scale riots. Although the regular army, disciplined and trusted, was used, it was too small and too widely dispersed to be an effective response, and the militia, while available as a local force, was not trusted. It fell, therefore, to the yeomanry to deal with civil unrest, and its numbers were soon increased as a result.

Agitation for constitutional reform by the Radical movement following the defeat of Napoleon resulted in frequent use of the yeomanry. Most famously, up to 17 people were killed and 650 wounded in the Peterloo Massacre of 1819, when the Manchester and Salford Yeomanry charged into a 60,000-strong crowd attending a rally in Manchester. On 2 April 1820, the Stirlingshire Yeomanry was called out during the Radical War – a week of strikes and unrest in Scotland – and three days later its Kilsyth Troop assisted the regular army's 10th Hussars in the arrest of 18 Radicals at the 'Battle of Bonnymuir'. In south Wales during the violent collier strikes of 1822, the Monmouth Troop, assisting the Scots Greys, used the flat of its swords to disperse a mob that was damaging coal trains, and the colliers pelted the Chepstow Troop with stones as it escorted coal wagons a few days afterwards. Elsewhere, the Staffordshire Yeomanry resorted to musketry, mortally wounding one person, when it was deployed to protect working colliers from their striking colleagues. In total, the yeomen of 12 different corps were called out to support the civil authorities on 19 separate occasions in 1822, and four years later, 13 different corps attended to 16 incidents.

The demand for assistance was not uniform throughout the country, and even at its peak in 1820, less than 30 per cent of counties had called out their yeomanry. Civil unrest declined in the 1820s, and in 1827 local magistrates called upon the yeomanry only six times, a 90 per cent decrease compared to 1820. Faced with funding a force that it perceived to be increasingly unnecessary, the government reduced the yeomanry establishment on economic grounds. Of the 62 corps or regiments that then existed, those 24 that had not been called out in aid of the civil power in the preceding ten years, primarily from the southern counties of England, were disbanded. The remaining 38 corps were retained, though 16 of them were allowed to continue only at their own expense. It was, however, in the southern counties that the Swing riots erupted in 1830, a largely agrarian protest which resulted in the destruction of machinery in both town and country. As a result, many disbanded corps were resurrected and new ones raised, although it was a slow process and those corps of yeomanry that had survived the cuts were in much demand. The Wiltshire Yeomanry, for example, served in neighbouring counties as well as its own, earning it the prefix "Royal" in recognition of its many services. This regiment was responsible for the one fatality inflicted by the yeomanry during the riots, when its Hindon Troop fought a 500-strong mob of agricultural workers in the 'Battle of Pythouse' at Tisbury, Wiltshire, on 25 November 1830.

There was further civil unrest the year after the Swing riots, prompted by agitation for political reform following the defeat of the Second Reform Bill in the House of Lords. In Wales, the Glamorgan Yeomanry twice suffered humiliation – and in consequence, disbandment soon after – when miners and steelworkers occupied Merthyr Tydfil; one group of yeomen was ambushed and disarmed as they tried to make their way into town, and on a separate occasion another group was routed. Equally ineffective, though this time through no fault of its own, was a troop of the newly re-raised Gloucestershire Yeomanry. It was sent to Bristol when rioting broke out there in the autumn, but was ordered to leave shortly after arriving by the commander of the regular forces deployed in the city. A second troop of Gloucestershire yeomanry was subsequently joined by yeomen from Somerset and Wiltshire to help restore order in the aftermath of the rioting.

Although further urban unrest in the 1830s resulted in the deployment of the yeomanry in Montgomeryshire, Kent and Birmingham, the government legislated another round of cuts on cost grounds in 1838, reducing the 18,300-strong force by up to 4,700, though nine corps were allowed to continue without pay. As in 1827, the timing was unfortunate, and the rise of Chartism between 1837 and 1842 resulted in more demands on the yeomanry, to the extent that the commanders of the northern and Midlands military districts were given the ability to summon it directly rather than apply for permission to the Home Office. The greatest pressure came in 1842 – a year which saw six of the nine unpaid corps returned to the establishment and just under 1,000 new yeomen recruited – when civil unrest in 15 English, Welsh and Scottish counties required the deployment of 84 troops from 18 corps, which between them accumulated a total of 338 days' duty.

Despite being heavily committed, force was applied sparingly, and the yeomanry was deployed wherever possible as a reserve in support of other law enforcement agencies rather than as a primary agent itself. In 1838, a troop of the Yorkshire Yeomanry was held back during a serious disturbance on the North Midland Railway out of fear that their presence would inflame the situation. The following year, Sir Charles Napier, commander of the northern military district, responded to a magistrate request for yeomanry by saying "if the Chartists want a fight, they can be indulged without Yeomen, who are over-zealous for cutting and slashing". There were occasions when force was used, such as the violent confrontations in the Staffordshire Potteries and North Wales in 1839 between protesters and the yeomen of Staffordshire, Shropshire and Montgomeryshire; there were injuries on both sides and at least four deaths among the protesters.

Between 1818 and 1855, the peak years of its employment in support of the civil power, units of yeomanry were on duty somewhere for approximately 26 days per year on average. It remained available as a constabulary throughout the 19th century, if for no other reason than it was often the only option available to the magistrates, even though it was recognised that its presence might escalate tensions. Its use in this role, however, declined, and the last known deployment in support of the civil power was in 1885. The diminishing demand was fuelled by a decrease in large-scale protest and better law-enforcement options. The development of a national rail network from the mid-19th century enabled rapid deployment of regular forces, and the establishment of police forces in all counties by 1856 gave magistrates a better alternative than the yeomanry.

In 1892, the Brownlow Committee, set up to investigate the financial and military position of the yeomanry, recommended that its constitution should be specially adapted for home defence, and in 1907 the yeomanry was formally relieved of any role in aid of the civil power. A select committee report in 1908, "Employment of Military in Cases of Disturbances", encouraged a civil response to civil disorder. It recognised, however, the value of mounted forces, and recommended that police chiefs should maintain the ability to temporarily recruit men with yeomanry experience, casting yeomen thus enlisted as ordinary citizens subject to common law. The evolution of law enforcement can be seen in the government responses to the Tonypandy riots and the Liverpool general transport strike of 1910 and 1911, in which the yeomanry played no part when the regular army was deployed to restore order, supported in the former case by 500 Metropolitan police.

In 1850, Henry FitzHardinge Berkeley, MP for Bristol, derided the yeomanry in Parliament as "maintained at vast expense; in peace a charge, in war a weak defence". By 1891, the force suffered, according to the Earl of Airlie – an experienced cavalry officer who was at the time adjutant of the Hampshire Carabiniers and who would later be killed leading the 12th Lancers in South Africa – from a lack of purpose and training. As its constabulary duties subsided, the yeomanry was left without any real role between the 1860s and 1892. Militarily weak and few in number, its effectiveness and value as a national defence force was increasingly questioned. It was regarded, not least among the members themselves, as light or auxiliary cavalry, and the yeomanry regiments adopted the titles of hussars, dragoons and lancers. Their training, in which they practised complex regular cavalry drills at the halt, emphasised the use of the sword. It was wedded to the idea of a cavalry role, despite increasing efforts by the government to encourage proficiency in the use of firearms.

The yeomanry was left untouched by the Volunteer Act of 1863, which governed the new Volunteer Force, leaving it still subject to legislation passed in 1804, although some changes were made to the way in which it was administered. More substantial changes were considered in a series of committees which attempted to assess the state and role of the yeomanry, and although the first, the Lawrenson Committee of 1861, achieved nothing, some changes to the organisation were made in 1870 by Edward Cardwell, Secretary of State for War. Independent troops and corps with less than four troops were abolished and the established strength set at 36 regiments, and basic training and drill requirements were laid down. There is also evidence that Cardwell hoped to transform the yeomanry from cavalry to mounted rifles, and an attempt to do so was also made in 1882, though both came to nothing. The Stanley Committee of 1875 recommended better training for the yeomanry leadership and the disbandment of regiments that returned an effective strength of less than 200 men for two consecutive years. Although the former was implemented, the latter was ignored.

Training in the latter half of the 19th century focussed more on mounted reconnaissance, flank protection and pickets, activities regarded by traditional cavalrymen as beneath their dignity, but it was rarely realistic, and the yeomanry proved resistant to the introduction of musketry standards. The Brownlow Committee sought to define a more professional role for the yeomanry by incorporating it into the nation's mobilisation scheme. As a result, in 1893, regiments were organised by squadron rather than troop, and understrength regiments were paired into brigades. In another attempt to encourage the use of firearms, allowances were increased for those who achieved a certain level of proficiency in musketry, but those who failed to do so in two consecutive years would be expelled. Nevertheless, the yeomanry's continued existence owed more to its significant representation in Parliament, which gave it a political influence beyond its numbers, than it did to its utility as a national defence force. The changes introduced by the Brownlow Committee were, according to Henry Campbell-Bannerman, leader of the Liberal Party then in opposition, the yeomanry's last chance to justify its existence.

By 1899, the yeomanry was at its lowest point. It was a small force, largely untouched by developments since its founding in 1794, of uncertain value and unclear benefit. It took major failures in the regular forces during the Second Boer War to restore the yeomanry to relevance. In October and November 1899, Lieutenant-Colonel A. G. Lucas, the yeomanry's representative in the War Office and a member of the Loyal Suffolk Hussars, suggested the yeomanry as a source of reinforcement in South Africa. His proposal was initially declined, but the disastrous events of Black Week in December, in which the British Army suffered three defeats in quick succession, prompted a rethink, and on 2 January 1900 the Imperial Yeomanry was created. It was a separate body from the domestic yeomanry, free of the home force's restriction to service only in the UK, and was organised by companies and battalions rather than squadrons and regiments, betraying its role as mounted infantry rather than cavalry.

By the end of the war, some 34,000 volunteers had served in the Imperial Yeomanry, although little more than 12 per cent of that number had been recruited from the domestic yeomanry. The experience in South Africa convinced the authorities of the value of a mounted force and influenced the Militia and Yeomanry Act of 1901. The law transformed the yeomanry, which it renamed "en bloc" to Imperial Yeomanry, from cavalry into mounted infantry, replacing the sword with rifle and bayonet as the yeoman's primary weapon. It introduced khaki uniforms, mandated a standard four-squadron organisation and added a machine-gun section to each regiment. The yeomanry resisted the retirement of the sword and the loss of "cavalry" from its title, a reflection of its own aspirations and the wider debate about the role of cavalry.

A key issue exposed by the Boer War concerned the ability of the auxiliary forces to reinforce the regular army in times of crisis. In 1903, the Director of General Mobilisation and Military Intelligence reported an excess of home defence forces which, because they were not liable for service overseas, could not be used to expand an expeditionary force in foreign campaigns. This occupied much of the debate around military reform in the first decade of the 20th century, and gave the yeomanry the opportunity to retain its role as cavalry by positioning itself as a semi-trained reserve to the numerically weak regular cavalry. This was reflected in a change to the training instructions issued to the Imperial Yeomanry in 1902 and 1905. The former warned the yeomanry not to aspire to a cavalry role and made no distinction between yeomen and mounted infantry, but the latter merely proscribed the traditional cavalry tactic of shock action while otherwise aligning the yeomanry with the cavalry.

The changed focus in training was prompted by plans to allocate six yeomanry regiments as divisional cavalry in the regular army, supported by the establishment within the Imperial Yeomanry of a separate class of yeoman free of the restriction on service overseas. This, however, relied on men volunteering for such service, and offered the regular army no guarantee that enough men would do so. That enough would volunteer was made more doubtful by the requirement that they should abandon their civilian lives for the six months of training considered necessary for them to be effective in such a reserve role. As a result, the plans were dropped from the final legislation that combined the Volunteer Force and the yeomanry, now without the "Imperial" prefix, into a single, unified auxiliary organisation, the Territorial Force, in 1908. The yeomanry ceased to be a discrete institution and was, as one yeoman put it, "slumped in with the volunteers".

In the first half of the 19th century, the number of corps and overall strength fluctuated in line with the incidence of civil disturbance, reflecting the government's reliance on the yeomanry as a police force and its willingness to finance it. Peterloo sullied the yeomanry's reputation in many quarters, but it also prompted a surge in recruitment, reversing the reductions implemented after the end of the Napoleonic Wars. By 1820, the yeomanry establishment had been restored to its war-time peak of some 36,000 men, although its effective strength was actually some 6,000 short of that number. The same cycle was repeated in the late 1820s when, after 10 years of political stability, the government reduced the yeomanry to between 8,350 and 10,700 men, only to increase it again in the 1830s following the outbreak of the Swing riots. Yeomanry strength peaked on this cycle in 1835 at an effective strength of 19,365.

Further government cuts in 1838 were once again reversed after the outbreak of the Chartist disturbances, and effective strength peaked again in 1845 at 15,249 men. Numbers subsequently fell once more, and although they were bolstered by invasion scares in the middle of the 19th century, a general decline set in as the yeomanry role in support of the civil power diminished. By 1900, the yeomanry establishment stood at just over 12,000 with an actual strength some 2,000 short of that figure. A wave of enthusiasm during the Second Boer War doubled the size of the yeomanry, and the Militia and Yeomanry Act of 1901 set an establishment of 35,000, though effective strength was only around 25,000. To achieve these numbers, 18 new regiments were raised, 12 of them resurrected from disbanded 19th century corps.

A commission in the yeomanry's officer corps entailed expenses which, for a troop captain in 1892, were on average £60 per year () in excess of allowances received. These costs imposed a financial qualification on appointments and made such positions the preserve of the elite. While a proportion of the yeomanry's leadership, between eight and fifteen per cent over the course of its existence, came from the nobility, the main demographic from which officers were recruited was the landed gentry. The coincidence of wealth and position in society was reflected in the leadership of the yeomanry. In 1850, for example, 31 per cent of the officer corps were magistrates and a further 14 per cent held even greater authority as a Lord or Deputy Lieutenant, and the 828 yeomanry officers then serving included 5 Lords Lieutenant, 111 Deputy Lieutenants, 255 Justices of the Peace, 65 Members of Parliament and 93 officers with previous military experience. This element changed little over time, and the number of county elites serving in 1914 was practically the same as in 1850.
Although social status was in some cases a prerequisite for commissions, personal wealth was the predominant factor. With its access to the county elite and appetite for wealth, the yeomanry officer corps was an avenue for 'new money' to gain social status and position. This was evident even in the early days – the Staffordshire Yeomanry contained a number of newly rich officers from industry and business before 1820 – and increasing numbers were able to elevate their social position via commissions in the yeomanry throughout the 19th century. Another theme in officer recruitment was family tradition. The Churchill family, for example, was involved in the Queen's Own Oxfordshire Hussars between 1818 and 1914, the last being Winston Churchill, who commanded a squadron even while Home Secretary and later First Lord of the Admiralty. Dukes of Beaufort served with the Royal Gloucestershire Hussars for over 150 years from its formation in 1834, providing the regiment's colonel or honorary colonel for all but 13 of them.

The high barrier to entry meant that the pool of officer candidates was limited, and the yeomanry consistently struggled to find enough officers. Those that were found were sometimes of questionable value. Officers were not always able to attend to their yeomanry duties, either because they lived too far away or, as in the case of Winston Churchill, had more pressing demands on their time. In 1875, an inspecting officer complained about inefficiency in troop leadership, but the introduction of mandatory formal training for yeomanry officers that year did not improve matters. Lord Chesham, Inspector General of the Imperial Yeomanry in South Africa during the Second Boer War, spoke in 1904 of the poor quality of yeomanry officers during that conflict. Promotions were more an indication of an officer's precedence, in both society and regiment, and his ability to spend time and money on the latter, than of his merit for the role.

An element of professionalism was provided by ex-regular army officers, who comprised 23 per cent of the officer corps in 1876 and 14 per cent in 1914. Furthermore, within each corps, training and administration was controlled by a permanent staff led by an adjutant of at least four years regular military experience. Even then, social status was often a factor in the selection of adjutants and, with applications being made directly to the colonel of a regiment, a measure of county influence was required for appointment.

In 1889, an MP described the yeomanry as "a survival from the days when tenants followed their landlords to the field". There is evidence that some of the rank and file were required to serve as a condition of their tenancy, in one case as late as 1893. On the whole, however, landlords did not have the ability, or at least the will, to coerce a tenantry which served, or indeed refused to serve, of its own free will. If the county elite commanded any influence in this matter, it was generally, in the class-driven society of 19th-century Britain, on terms of deference rather than subservience.

Although farmers represented the largest single demographic in the rank and file, statistics indicate that, between 1817 and 1915, just under half of yeomen were recruited from outside of the farming community. Other demographics appearing in the albeit incomplete data were merchants (4.9 per cent), professionals (5.6 per cent), small businessmen (14.9 per cent), artisans (13.5 per cent) and skilled or unskilled labourers (4.9 per cent). In some cases the ratio of farmers within the same corps varied over time, an example being the Ayrshire Yeomanry, which comprised over 81 per cent farmers and their sons in 1831, a number which dropped to just over 60 per cent by 1880. The 1st Devon Yeomanry, on the other hand, shows largely unchanged ratios for the years 1834 (44.7 per cent) and 1915 (40.2 per cent). The ratios also varied between corps; for example, over 76 per cent of the Lanarkshire Yeomanry (Upper Ward) between 1822 and 1826 were farmers, but the Manchester and Salford Yeomanry of 1819 contained none.

The early appearance of the Manchester and Salford Yeomanry demonstrates an urban theme in yeomanry recruitment that became more marked as the 19th century progressed, influenced to some extent by an agricultural downturn in the late-19th century. In contrast to Lanarkshire's Upper Ward regiment, its Glasgow and Lower Ward regiment, raised in 1848 and later to become the Queen's Own Royal Glasgow Yeomanry, was recruited from the city's middle classes. In the 1860s, the Leicestershire Yeomanry and the South Salopian Yeomanry (Shropshire) were both recruiting from towns in their territories, and by 1892 all but one troop of the Middlesex Yeomanry were recruited in London. The urban element was not without its own issues of class. The rank and file of the Edinburgh Troop in the 1830s consisted mainly of gentlemen who were charged £12 () to join, and the commander of the Middlesex Yeomanry's B Troop, which was known as the gentlemen's troop, believed there would be class friction if it was forced by the new squadron system of 1893 to join a troop of lesser status.

The increasing use of hired mounts, particularly after the turn of the century, also indicates a dilution of the rural contingent in the rank and file. The percentage of horses that were hired rose dramatically, from up to 14 per cent in the last quarter of the 19th century to around 50 per cent in the period 1905–1907. Although this was a predictable trend in the case of, for example, the largely urban-recruited Middlesex Yeomanry, the more rurally-based East Kent Yeomanry experienced a progressive decline in the ownership of horses, from 76 per cent in 1880 to 66 per cent in 1884 and a little over a half in 1894.

Peterloo polarised opinions in the press, the Radical outlets framing it in terms of murder and massacre and the establishment outlets tending more to a defence of the yeomanry. Although an extreme episode, the events at St. Peter's Field coloured perceptions of the yeomanry among the politically-involved working class, who equated it with the abuse of civil power. Negative perceptions persisted long after the event, even in the upper echelons of society, and as late as 1850 Peterloo was referenced when the yeomanry's "inclinations" were criticised in Parliament. In the mainstream national press, however, as Peterloo became yesterday's news, so too did the yeomanry, and, outside of public events which it attended in a ceremonial role, it was seldom reported on. More often, the yeomanry was the subject of caricature, in which yeomen were portrayed as old, incompetent and waving blood-stained weapons. Caricature evolved into satire, and magazines such as "Punch" regularly ridiculed the force as the epitome of bumbling high society, with overweight yeomen unable to master their weapons or the sick, undersized horses they rode. Common themes in the portrayal of the yeomanry in books and on stage included amateurs with delusions of grandeur, social climbing, self-importance and a greater concern for leisure and appearance than national defence.

The yeomanry's less confrontational activities resulted in a more positive interaction with the general public. It was often generous in its support for local charities, and its gatherings, whether for training or social events, injected wealth into local economies, to the extent that towns would petition regiments to be selected as venues for such occasions. Sporting events and pageantry, particularly the many occasions on which the yeomanry escorted royalty and visiting dignitaries, also drew appreciative crowds. The presentation of colours to the Wiltshire Yeomanry in 1798, for example, was watched by over 20,000 spectators, yeomanry bands entertained visitors at the opening of the Nottingham Arboretum in 1852, and the Royal Midlothian Yeomanry Cavalry Races in 1863 attracted a considerable attendance.

Although the political allegiance of yeomanry MPs in the House of Commons was fairly evenly split between the two main parties by the early 20th century, this was after a gradual shift in political affiliations since 1843, when the ratio of politically active members of the yeomanry was significantly Tory. "The Satirist" cast the yeomanry as "ultra Tories" in 1838, and the perception of the force as an instrument of the Tory establishment made some local authorities cautious in its use against political reformers during the Chartist disturbances. In terms of the yeomanry leadership at least, the nature of the reform movement in the first half of the 19th century meant that the yeomanry was regularly pitted against a different class, but it was called upon to do so by governments of both political parties. Furthermore, its membership was not without sympathies for the causes it was called upon to police, and there are a number of cases in the early 1830s where the loyalties of some of its corps were doubted.

Yeomen had to provide their own horses, but saddlery and uniforms were paid for, either by the officers or by subscriptions in the counties in which the troops were raised, leading to a variety of colourful and flamboyant choices in attire. Their weapons – swords, pistols and a proportion of carbines per troop – were funded by the government. Other than when called out for duty, when it would be paid as regular cavalry, the yeomanry received no remuneration until 1803, when the first allowances were granted. The confused legislation of the early 19th century meant that different corps, and even different troops within the same corps, operated under different terms and conditions until the Volunteer Consolidation Act of 1804 introduced some uniformity. It restricted pay to a maximum of 24 days per annum, set 12 days of training as the qualification for exemption from conscription into other auxiliary arms, offered bounties for active service and gave yeomen the right to resign on 14 days notice. It did not, however, amend the different areas of liability (military district or nationwide) set by previous legislation. In 1816, the annual training requirement was reduced to eight days, inclusive of two days travelling, and the next year an annual allowance of £1 10s () per yeoman was awarded to help with uniform and equipment costs.

In addition to weapons and allowances, expenses incurred by the government in maintaining the yeomanry included the permanent staff, compensation for losses and injuries to men and horses, and pay at 7s per day for annual training and when called out. Volunteers also benefitted by exemption from hair powder duty until 1869 and horse duty until 1874. Between 1816 and 1821, the cost of maintaining the yeomanry had risen by nearly 46 per cent, and with only seven per cent of the total cost directly attributable to aiding the civil power in 1819, governments struggled to justify the expense. Cuts to the force on economic grounds were legislated twice, in 1827 and 1838, saving £92,000 () and £22,000 () respectively.

Government funding, however, consistently fell short of actual requirements. Subsidisation of the yeomanry by its members, particularly the officers, was common practice throughout its existence, and not only during those periods when corps were maintained at their own expense. Lord Plymouth paid £6,200 () to equip a troop of Worcestershire Yeomanry in 1832, and the Earl of Dudley was reputed to have spent £4,000 (approximately ) per year on the same corps between 1854 and 1871. The second Duke of Buckingham and Chandos was said to have been bankrupted in 1848 in part by the massive contribution he made to his regiment, which received no government funding between 1827 and 1830. In 1882, it was calculated that officers paid an average of £20 each () and the men up to £5 each () towards the cost of their regiments, giving a total subsidy of £61,500 () in a year when the government voted a £69,000 budget () for the yeomanry. Twenty years later, the annual cost of being a yeomanry officer was estimated to be £100 () in excess of the pay and allowances received by the officer. This willingness to support itself with private funding was another major factor in the yeomanry's survival after its usefulness in suppressing civil disorder disappeared.

Changes were made to the yeomanry terms and conditions in the second half of the 19th century. The National Defence Act of 1888 made it liable to serve anywhere in the country, and four years later an annual capitation grant of £1 () was awarded. However, the force remained largely subject to the terms set by the Volunteer Consolidation Act of 1804 until the passage of the Militia and Yeomanry Act in 1901. The new legislation replaced the right to resign on 14 days' notice with a three-year term of service for new recruits; increased the annual training requirement to 18 days, 14 of which were compulsory; introduced a £3 allowance () per man and grants of £20 () and £30 () for squadron and regimental stores; reduced duty pay to 5s 6d per day (), compensated for by extra daily allowances for travel, musketry practice, forage during permanent duty, and squadron drills, which in total amounted to an extra 10s 6d (); and introduced a £5 allowance () for the hire of horses.

The incorporation of the yeomanry into the Territorial Force in 1908 introduced further adjustments. Duty pay was reduced by 1s 2d per day, compensated for by free rations, a messing allowance of 1s per day was introduced and £1 was awarded for reaching a set standard of horsemanship. The new organisation also introduced some significant changes to the terms and conditions, including a four-year term of service and reducing annual camp to fifteen days, eight of which were necessary to gain a certificate of efficiency. The most fundamental change of all, however, was the transfer of administration from the regiments to the newly created County Territorial Associations. These were made responsible for the provision of horses, and relieved the officers of the burden and expense of maintaining the regiments.

Yeomanry regiments served overseas during the First World War, in France, at Gallipoli, in Egypt and during the Sinai and Palestine Campaign. The nature of the conflict in Europe precluded the use of mounted forces; cavalry actions were rare, and several regiments finished the war re-purposed as infantry. The same fate befell a number of yeomanry regiments posted to the Middle East, although the yeomanry 2nd Mounted Division, having fought as infantry at Gallipoli, reverted to the cavalry role on its return to Egypt. Being more conducive to mounted operations, the Sinai and Palestine Campaign saw extensive use of the yeomanry, though it often fought dismounted. Some of the last ever cavalry charges conducted by the British Army were made by yeomanry regiments during the campaign, by the 1/1st Warwickshire Yeomanry and 1/1st Queen's Own Worcestershire Hussars in the charge at Huj on 8 November 1917, followed five days later with a charge by the 1/1st Royal Bucks Hussars in the Battle of Mughar Ridge.

In 1921, of the 56 yeomanry regiments active after the First World War, only 14 were retained in the cavalry role, while 16 were disbanded and the remainder converted to either batteries of the Royal Field Artillery or armoured car companies of the Tank Corps. As with previous attempts to relieve the yeomanry of its cavalry role, a number of regiments resisted the change, concerned that the new roles would result at best in an unacceptable change to the unique character of the force and at worst wholesale resignations. Political lobbying succeeded only in increasing the number of regiments to be retained from the originally proposed ten.
The yeomanry saw active service during the Second World War in armour, artillery, anti-aircraft and anti-tank roles. Units fought in Europe during the Battle of France, the Normandy landings and the subsequent campaign in North-West Europe, in North Africa during the Western Desert Campaign, in Italy and against Japanese forces in Singapore and Burma. Yeomanry regiments were also deployed in their traditional cavalry role to Palestine, though by 1941 only three regiments still retained their horses. The last action by British cavalry on horseback was fought on 10 July against Vichy French forces in Syria by the Queen's Own Yorkshire Dragoons, which also had the distinction of being the last regiment on active service in the British Army to give up its horses. Several post-war reorganisations resulted in more disbandments and the reduction of surviving regiments to cadres, leaving only the Royal Yeomanry, which performed an armoured reconnaissance role. In 1971 the cadres were restored to form three new yeomanry infantry regiments, and in the 21st century these were converted to armour-based roles alongside the Royal Yeomanry in the Royal Armoured Corps.



</doc>
<doc id="56775511" url="https://en.wikipedia.org/wiki?curid=56775511" title="Tutupaca">
Tutupaca

Tutupaca is a volcano in the region of Tacna in Peru. It is part of the Peruvian segment of the Central Volcanic Zone, one of several volcanic belts in the Andes. Tutupaca consists of three overlapping volcanoes formed by lava flows and lava domes made out of andesite and dacite, which grew on top of older volcanic rocks. The highest of these is usually reported to be high and was glaciated in the past. 

Several volcanoes in Peru have been active in recent times, including Tutupaca. Their volcanism is caused by the subduction of the Nazca Plate beneath the South America Plate. One of these volcanoes collapsed in historical time, probably in 1802, generating a large debris avalanche with a volume probably exceeding and a pyroclastic flow. The associated eruption was among the largest in Peru for which there are historical records. The volcano became active about 700,000 years ago, and activity continued into the Holocene, but whether there were historical eruptions was initially unclear; some eruptions were instead attributed to the less eroded Yucamane volcano. The Peruvian government plans to monitor the volcano for future activity. Tutupaca features geothermal manifestations with fumaroles and hot springs.

The people in Candarave considered Tutupaca to be a "bad" mountain, while Yucamane was the "good" one; this may reflect that Tutupaca had recent volcanic eruptions. The Peruvian geographer Mateo Paz Soldán dedicated an ode to Tutupaca.

Tutupaca is north of the town of Candarave in the region of Tacna in Peru. Lake Suches lies north of the volcano, and two rivers flow nearby: the Callazas River, flowing eastward to the north of the volcano, and then southward past Tutupaca's eastern flank, and the Tacalaya River, which flows south along Tutupaca's western flank. The local climate is cold, and the terrain is stony, with little vegetation. During the wet season, the mountain is snow-covered, and meltwater from Tutupaca and other regional mountains is an important source of water for the rivers in the region.

Tutupaca consists of two volcanic complexes: an older complex that is highly eroded, and two northerly peaks which formed more recently. Of these, the eastern peak ("eastern Tutupaca") consists of several presumably Holocene lava domes and is high, while the western one ("western Tutupaca") consists of lava domes, lava flows and Plinian eruption deposits of Pleistocene age, and reaches a height of . The Global Volcanism Program gives heights of for the eastern and for the western summit. The western peak is the highest summit of Tutupaca. 

The basement that Tutupaca rises from lies at elevations ranging from to . The older complex is formed mainly by lava flows, which during the Pleistocene were eroded by glaciers forming up to thick moraines. Cirques and moraines are also found on the western summit, and tephra layers extend west of the volcano. The older complex, which includes lava domes in the form of small hills on its southern part, was the source of an ignimbrite that covers the western and southern parts of the volcano. Postglacial lava flows emanating from a vent located between the two peaks have been identified.

The older complex and western Tutupaca have erupted andesite and dacite, while eastern Tutupaca has only produced dacite. Trachyandesite and trachyte also occur. The volcanic rocks erupted during the Holocene define a potassium-rich calc-alkaline suite. Dacites from eastern Tutupaca contain amphibole, apatite, biotite, clinopyroxene, iron-titanium oxides, orthopyroxene, plagioclase, quartz and sphene. Elemental sulfur deposits have been identified at Tutupaca and a 1996 map of the volcano shows a sulfur mine on its southeastern flank.

A wide amphitheatre in eastern Tutupaca, open to the northeast, was formed by a major collapse of the volcano. Lava domes from the younger Tutupaca as well as highly altered lavas from the older complex are exposed within the collapse scar, which is the origin of a long debris avalanche deposit. The deposit is mostly found within glacial valleys and is interlaid by the Paipatja pyroclastic flow which divides the debris into two units. The pyroclastic flow reaches both Lake Suches north of the volcano and the Callazas River east of it.

The two units of the debris avalanche are distinguished by their appearance. One features long hummock-like hills, as is typical for volcanic debris avalanches, and the other has ridges which vary in length from . The ridges range from only a few meters to more than in height, and from in height. Such ridges have been observed in other collapse deposits such as at Shiveluch volcano in Russia, and have been explained by sorting processes that take place within granular flows. The differences between the two units appear to be because the first unit was formed from the basal part of Tutupaca, while the second unit was formed by the more recent lava domes of the eastern volcano and formed a granular flow.

The collapse apparently started in the hydrothermal system of the volcano and progressed to affect a growing lava dome, with a total volume probably exceeding . The total surface area covered by the collapse is about . This collapse was not the first in the history of Tutupaca: an older collapse occurred on the southeast-east flanks of the volcano. Such large collapses of volcanoes took place in historical time at Mount Bandai in 1888 and at Mount St. Helens in 1980; they can produce large avalanches of debris.

Off the coast of Peru, the Nazca Plate subducts at beneath the South America Plate, causing volcanism in three of the four volcanic belts in the Andes, including the Central Volcanic Zone where Tutupaca is located. Other Peruvian volcanoes include Sara Sara, Solimana, Coropuna, the Andagua volcanic field, AmpatoSabancaya, Chachani, Ubinas, Ticsani, Yucamane and Casiri. During historical times, major eruptions took place in Peru at El Misti 2,000 years ago and at Huaynaputina in 1600.

The basement of the region consists of folded Mesozoic sediments, and Cenozoic volcanic and sedimentary cover which overlies the Mesozoic rocks. There are many tectonic lineaments and faults which were active in the Tertiary; one of these crosses Tutupaca from north to south, and others influence the positions of geothermal features. The Huaylillas ignimbrite complex underlies some of the volcanic centres, which include a first set of eroded volcanoes that were active between 8.4–5 and 4–2 million years ago, principally erupting lava flows. These were followed by a second set of volcanoes which were also mainly active with lava flows, such as Casiri, Tutupaca and Yucamane. A third phase formed dacitic lava domes such as Purupuruni about 100,000 years ago.

In the Western Cordillera, altitudes between are dominated by vegetation such as cacti, herbs, Peruvian feather grass and yareta, but also lichens and mosses. Wetlands, called "bofedales", display a diverse plant life. Above elevation plant life diminishes and in 20032012 by there was perpetual snow.

Tutupaca is about 700,000 years old. The older complex was active at first with lava flows and then with a major explosive eruption. The small lava domes on the older complex have been dated to 260,000 ± 200,000 years ago. Volcanic activity continued into the Holocene, and the volcano is considered to be potentially active. Today, fumaroles occur on the summit of Tutupaca.

There are reports of eruptions in 1780, 1787, 1802, 1862 and 1902, supported by dates obtained through radiocarbon dating showing there were eruptions during this period. Some authors believed that Yucamane volcano was a more likely source for these eruptions, but Samaniego 2015 "et al." showed that Yucumane last erupted 3,000 years ago, implying that the reported eruptions, especially the 1802 and 1787 events, most likely occurred at Tutupaca.

The sector collapse of eastern Tutupaca was accompanied by an eruption that was among the largest in Peruvian history, reaching a volcanic explosivity index of 3 or 4. Contemporaneous chronicles document ashfall as far as to the south in Arica. The collapse has been dated to 1731–1802 with high probability, and is thought to be associated with the 1802 eruption. Shortly before the collapse, a pyroclastic flow was erupted from the volcano probably as a consequence of the collapse of a lava dome. It formed a deposit on the east flank of Tutupaca which reaches thicknesses of . The previous eruption may have destabilized the volcano and triggered the main collapse, which also generated the Paipatja pyroclastic flow. The area was thinly inhabited at the time and thus the impact of the eruption was small.

Based on the history of Tutupaca a future eruption can be envisaged where renewed activity causes another collapse of the volcano. In this case, about 8,000–10,000 people as well as neighbouring geothermal power and mining infrastructure would be in danger. Several small towns, diversion dams, irrigation canals and the two roads IloDesaguadero and TacnaTarataCandarave would also be vulnerable.

The Peruvian Instituto Geológico, Minero y Metalúrgico (INGEMMET) has published a volcano hazard map for Tutupaca, but the volcano itself was not monitored since it is not active. In 2017, Tutupaca was identified as one of the volcanoes to be monitored by the future Peruvian Southern Volcano Observatory. This would entail surveillance of earthquake activity, changes in the composition of fumarole gases and deformation of the volcanoes, and real time video. This project, budgeted to cost 18,500,000 Peruvian sols ( US dollars) and involves the construction of thirty monitoring stations and the main observatory in the Sachaca District, is scheduled to be operative by early 2019.

Tutupaca is also the name of a geothermal field in the neighbourhood of the volcano, which includes the areas of Azufre Chico, Azufre Grande, Callazas River, Pampa Turun Turun and Tacalaya River; they are part of the same geothermal system whose temperature at depth is higher than . The fields feature fumaroles, geysers, mud pots and occurrences of sulfur, both solid and in the form of hydrogen sulfide gas, as well as siliceous sinter and travertine deposits. Hot springs at the foot of the Tutupaca volcano discharge water into the rivers. 

Tutupaca has been mentioned as a potential site for geothermal power generation. In 2013, Canada's Alterra Power and the Philippine Energy Development Corporation developed a joint venture to work on a geothermal prospect at Tutupaca, although work at Tutupaca had not begun by October 2014.



</doc>
<doc id="57284205" url="https://en.wikipedia.org/wiki?curid=57284205" title="Russian occupations of Beirut">
Russian occupations of Beirut

The Russian occupations of Beirut were two separate military expeditions by squadrons of the Imperial Russian Navy's Mediterranean Fleet, with the first one taking place in June 1772 and the second one from October 1773 to early 1774. They formed part of its Levant campaign during the larger Russo-Turkish War of 1768–1774. Russia's main objective in this campaign was to assist local forces led by Egypt's autonomous ruler, Ali Bey al-Kabir, who was in open rebellion against the Ottoman Empire.

Ali, taking advantage of the Empire's preoccupation with Russia, declared Egypt's independence and in 1771 sent an army led by Muhammad Bey Abu al-Dhahab to occupy Ottoman territory in the Levant. Abu al-Dhahab unexpectedly returned to challenge Ali for control of Egypt. Ali requested Russian military assistance against his rival and the Ottomans. When this aid, in the form of a small Russian squadron, arrived in the region, Ali had already fled Egypt and taken refuge in Acre, the power base of his ally, Zahir al-Umar. After helping repel an Ottoman offensive on Sidon, the Russian squadron sailed for Beirut. They bombarded the town in June 1772 and occupied it from June23 to 28.

Ali, now formally allied with the Russian Empire, requested further assistance from his new European partner, to recover Egypt from Abu al-Dhahab. The Russians had recently entered a period of truce with the Ottomans, constraining their involvement in the region. They did, however, promise Ali a large squadron. Impatient, Ali set out for Egypt with a small force that was defeated near Cairo; he was imprisoned and died a few days later. When the Russian squadron arrived in June 1773 and learned of Ali's fate, its commander allied with Zahir and the Druze chieftain Yusuf Shihab. The latter had agreed to pay the Russians a tribute in exchange for their liberation of Beirut from Jazzar Pasha, Shihab's insubordinate vassal whom he had recently appointed as governor of the town. The bombardment of the town began on August2, and Jazzar surrendered after two months, on October10. A few hundred Albanian mercenaries were left as occupiers.

The occupations are of debatable historical importance. Despite their brevity, they marked the first time in over 250 years that Beirut was ruled by a power other than the Ottomans. It also marked the first occasion on which Russian rule was imposed over an Arab city.

The pressure of Austria and Russia on the northern Ottoman frontiers since the beginning of the 18th century had encouraged insubordination among local governors in the largely decentralised Arab provinces of Ottoman Syria. In 1768, while the Russian Empire was suppressing a Polish uprising near the Ottoman border, a Cossack regiment chased some of the rebels across the border and reportedly carried out a massacre in the town of Balta. The Sultan eventually responded by declaring war on Russia. He demanded military assistance from Ali Bey, a Mamluk, who was the most powerful official in Ottoman Egypt at the time. Ali closely observed the course of the war, sending the required 3,000 soldiers to the aid of the Sublime Porte in 1769. Early in the following year, however, he declared Egypt's independence, chiefly in response to the war with Russia. Later he struck an alliance with Zahir al-Umar, a wealthy Arab ruler in northern Palestine. Ali and Zahir shared common ground in their opposition to Islamic fundamentalism, the Sultan's isolationist policies towards Europe and the imposition of Ottoman dignitaries to their courts.
At the same time, Tsarina Catherine the Great, lacking an organised Russian fleet in the Black Sea, drew up plans with Count Alexei Grigoryevich Orlov to detach a large number of ships from the Baltic Fleet and deploy them to the Mediterranean. Russia hoped they would attack the Turkish Straits from the rear, and that its naval presence in the Aegean Sea would provoke a Greek rebellion. This newly formed Mediterranean Fleet, headed by Orlov and commanded by Admiral Grigory Spiridov, sailed from Copenhagen on September23, 1769. By March1, 1770, the first detachment was anchored off the southern Morea, where the Orlov revolt broke out. This was followed by bombardments and troop landings at different locations in the region over the coming months. On July7 a Turkish fleet was crushed at the Battle of Chesma, which crippled the Ottoman Navy and gave the Russians naval command of the Mediterranean for the remainder of the war. Turkish ships that survived retreated to the Dardanelles. Rear-Admiral John Elphinstone proposed a direct assault on Constantinople, but was instead convinced by Orlov to blockade the Straits with his squadron, while the rest of the fleet went on the offensive in the northern Aegean.

Russian chronicles have described Muslims as Hagarians since the 15th century. This is a pejorative term suggesting that they were the descendants of biblical Hagar who was exiled into the deserts of Sinai. The Russian court was aware of the autonomization of Ottoman Tunis, Algeria and Tripolitania in the 18th century. Despite her negative perception of Muslims, Catherine did not hesitate to approach them as potential allies. On July 15, 1769, she instructed Spiridov to avoid attacking ships belonging to the aforementioned states unless provoked, and prompted him to attempt to turn the Barbary states against their Ottoman overlord. Catherine's plans did not materialize as Tunis and Algeria fought on the side of the Ottomans, while the correspondence of Ali Pasha of Tripoli with Orlov bore no significant results.

In late 1770, Ali Bey dispatched an army of 40,000 troops, commanded by his trusted general Muhammad Bey Abu al-Dhahab, to Palestine where it joined forces with Zahir's army in the spring of 1771 and went on to occupy several towns in the Levant. However, soon after the fall of Damascus in early June, Ottoman agents managed to convince Abu al-Dhahab to turn against Ali Bey, promising to appoint him as ruler of Egypt instead of his overlord. Abu al-Dhahab retreated with his army, and became embroiled in a power struggle against his former master over the control of Egypt. Zahir, for his part, became isolated in his sheikhdom and had to face the inevitable Ottoman counteroffensive on his own. Ali Bey was then convinced that he could reverse his recent setbacks through an alliance with Russia. On December2, 1771, he sent an Armenian envoy named Yaqub to meet with Orlov at the Mediterranean Fleet's headquarters on the Aegean island of Paros where he offered the Russians an alliance.

The offer was later accepted by the Tsarina, but her knowledge of the proposed alliance came after Ali Bey was no longer ruling Egypt. He had been forced to leave the country and seek refuge with his ally Zahir when the standoff between him and Abu al-Dhahab finally escalated to armed confrontations. Ignorant of Ali's flight and with orders from Orlov to make contact with him, a detachment commanded by General-Adjutant Rizo, a Greek, sailed for Damietta but quickly left port after learning of his fate. The squadron then searched for him along the Palestinian coast and eventually found him in Acre on June3. Rizo later sent a detachment to the north, which intercepted an Ottoman frigate from Beirut near Tyre. Meanwhile, in Sidon, Zahir's small army of 6,000 was being besieged by an Ottoman force of 30,000 that included Druze contingents. Rizo's ships joined the garrison by bombarding the Ottoman attackers who withdrew shortly afterwards.

The newly assembled coalition, wishing to exploit the Ottoman setback in Sidon, decided to send the Russian squadron to the small port town of Beirut, which was controlled at the time by the Druze. According to the researcher William Persen, the aim of this expedition was to both preoccupy the Druze and punish them for siding with the Porte. Blockading their port would achieve this. Beirut was also the only port in the region which had so far remained under Ottoman rule.

Likely unaware of a recent armistice between Russia and the Porte, Rizo's squadron appeared off the town's coast on June18 after it was reunited with its Tyre and Acre detachments. The force consisted of two frigates, the "Sv. Nikolai" and the "Sv. Pavel", four polaccas, five half-galleys and four xebecs. It was transporting an infantry division that was largely made up of Greek and Albanian mercenaries. Bombardment of the town began on the same day, and Ottoman ships docked in its port were sunk. In his version of the events, Auriant, author of "Catherine II et l'Orient", wrote that Beirut's defenders were given a 24-hour ultimatum by Rizo to fly the Russian flag and pay a tribute. The naval offensive lasted five days and involved an unsuccessful assault by a landing party on June21. After two days of heavier bombardment, they finally landed on June23 and spent hours sacking the town and its bazaar. About 550,000 qirsh worth of loot, in the form of both trade goods and cash, was taken. The Russians left on June28, after receiving additional payment from Yusuf Shihab, a Druze emir who ruled over the surrounding mountains. Shihab also agreed to a four-month alliance with the coalition.

Following the Russians' departure, the Ottomans ceded Beirut to Yusuf Shihab's Mount Lebanon Emirate. Shihab, in turn, charged Ahmed Bey Jazzar, who had previously served under Ali Bey, with the town's defence. While in Acre and with his allies bombarding Beirut, Ali received his envoy Yaqub. He came aboard a Russian frigate carrying gifts and a friendly message from Catherine in which she assured him of her commitment to Russia's alliance with the Egyptian ruler. Ali, however, was dissatisfied with the mere gesture and sought Russian military assistance in the form of infantry, artillery and naval support to reconquer Egypt from Abu al-Dhahab. He sent a new envoy with this message to Orlov. According to the historian Édouard Lockroy, he may have also offered the Russians control of the Christian holy sites in Jerusalem.

Orlov responded by telling Ali that he was bounded by the truce with the Porte. He pledged a small force of Russian officers and artillery, and promised to give Ali all the assistance he needed as soon as circumstances permitted. During the summer, Ali and Zahir's forces laid siege to the city of Jaffa. The siege was joined in September by a Russian transport ship which landed artillery pieces near the city. The ship left a month later, carrying a renewed plea to Orlov. Another Russian naval detachment, commanded by Lieutenant Panaiotti Alexiano, arrived at Jaffa in November, again joining the bombardment and informing Ali of Orlov's plan to commission a large fleet for his cause. Alexiano's squadron consisted of the frigate "Sv. Pavel" and a number of polaccas, and had earlier destroyed two Barbary ships off Damietta and captured some smaller vessels.

Planted rumours of insubordination in Abu al-Dhahab's camp and growing dissent in Egypt had caught Ali's attention. He grew impatient and, in April 1773, he set out for Egypt at the head of a small force, despite being assured a month earlier that Russian assistance would arrive soon. Ali's force was defeated by his rival's army near Cairo. He was taken prisoner and died a few days later, likely by poisoning. Jazzar, meanwhile, had fortified Beirut and chose to act independently of Shihab, declaring that he would recognise only the Sultan's authority over the town. Orlov's promised squadron, commanded by Captain Mikhail Gavrilovich Kozhukhov, reached Acre in June. Hearing of Ali's death, Kozhukhov agreed to a "treaty of friendship" with Zahir.

Kozhukhov appeared with his squadron of at least 222 guns off Beirut's coast on July6, carrying a force of 1,200 Albanian mercenaries and mobile artillery units. Zahir had already negotiated an alliance with Shihab, when the latter was denied assistance by the Pasha of Damascus against Jazzar. During the month-long negotiations that followed, the Druze emir managed to persuade Kozhukhov, through Zahir and his uncle Amir Musa Mansur, to deliver Beirut to him. Shihab was to pay a tribute of 300,000 qirsh and place the town under Russian protection. On his part, Kozhukhov promised that his troops would not pillage the town as the Russian occupiers of 1772 had. Both sides agreed that the Russians would hold Mansur as a hostage, pending full payment by Shihab.

By the time it appeared before Beirut, Kozhukhov's squadron consisted of the following ships:

A bombardment began on August2 and lasted the entire day, destroying the port area and its towers. The noise was so loud it could be heard in Sidon, about 25 miles (40 km) away, according to the city's French consul. Arabic sources said that it could even be heard in Damascus. Much of Beirut was destroyed, but Jazzar refused to surrender. Kozhukhov ordered the landing of troops and artillery units for a ground assault. The walls were breached in several places but Shihab would not commit his forces to storming the town, citing the agreement which gave Kozhukhov the task of delivering the town to the Druze in exchange for payment. Kozhukhov chose to maintain the naval and land blockade, cutting off the town from its food supply. Shihab and Zahir's troops routed an Ottoman relief army commanded by the Pasha of Tripoli which approached from the Beqaa.

Jazzar opened negotiations with the attackers at the end of September. Fearing that submission to either Shihab or Kozhukhov would eventually lead to his execution, he offered to surrender himself to Zahir instead and serve under his command. Jazzar surrendered Beirut on October10, after which he left for Acre with his garrison of 800 Maghrebis. By the end of their offensive, the Russians had lost 34 men killed and 96 wounded. Kozhukhov restricted himself to seizing two half-galleys and some weaponry after docking, in accordance with his arrangement with Shihab. When the latter could not pay the full amount promised to the Russians, Kozhukhov threatened to kill Mansur, his Druze hostage. Shihab managed to make a partial payment, which Kozhukhov accepted, pending payment of the balance. The Russian commander and his squadron departed for the Aegean, leaving behind 300 Albanian mercenaries to guard Mansur, who was under house arrest, and to remain in the town as occupiers.

The Albanians occupied Beirut until late January or early February 1774, though it remains unclear whether they received the balance of the tribute due. According to French consular reports from Sidon, the occupiers kept the Russian flag raised over Beirut, along with a large portrait of Catherine the Great over the town's main gate, to which travellers were forced to pay their respects.

Jazzar and his Maghrebi mercenaries defected from Zahir's camp to the Ottomans shortly after surrendering Beirut. With the departure of the Russians in early 1774 and the signing of the Treaty of Küçük Kaynarca the same year, Zahir had to face the Porte's retribution by himself. The Russo-Turkish peace treaty allowed Russia to establish consulates wherever it wished in the Mediterranean, to open up trade routes with the Middle East, and to guarantee the safety of Christian pilgrims in the Holy Land, but made no mention of Zahir or of Russia's wartime territorial gains in the region. Zahir was killed in August 1775 during a siege of Acre by Ottoman forces, who were initially led by Egypt's Abu al-Dhahab. Shihab was quickly pardoned by the Ottomans, as he was never in direct rebellion against them. But he was pressured into surrendering Beirut in 1776 due to heavy taxation by Jazzar, who had been recently appointed by the Ottoman government as the Pasha of Sidon.

In addition to being the first time an Arab city came under Russian rule, the events also marked the first time that Beirut had not been under Ottoman control since the Ottoman conquest of the region over two and a half centuries earlier. "Place des Canons" ("Cannons Square") became the common name for today's Martyrs' Square in Beirut's Central District in 1773. This name remained in use, though unofficially, until the 1950s. It was a reference to the large artillery pieces that the Russians stationed in the plaza, which was then an empty area known as the Bourj, east of the walled town.

The importance of this short-lived occupation remains a subject of debate among the relatively few historians and scholars who have analysed it. In his 1952 evaluation of the events, William Persen played down the occupation, describing it simply as a "new force of Western penetration in the Middle East." Soviet scholars like P. Perminov, on the other hand, described it as an early manifestation of modern Cold War era assistance by the Soviet Union to national liberation movements in Third World nations.



</doc>
<doc id="57403628" url="https://en.wikipedia.org/wiki?curid=57403628" title="History of the Nashville Sounds">
History of the Nashville Sounds

The Nashville Sounds Minor League Baseball team was established in Nashville, Tennessee, in 1978, after Larry Schmittou and a group of investors purchased the rights to operate an expansion franchise of the Double-A Southern League. The Sounds played their home games at Herschel Greer Stadium from its opening in 1978 until the end of the 2014 season. In 2015, the Sounds left Greer for First Tennessee Park, a new facility located on the site of the historic Sulphur Dell ballpark, home to Nashville's minor league teams from 1885 to 1963.

The Sounds led all of Minor League Baseball in attendance in their inaugural season and continued to draw the Southern League's largest crowds in each of their seven years as members of the league. On the field, the team won six consecutive second-half titles from 1979 to 1984 and won the Southern League championship twice: in 1979 as the Double-A affiliate of the Cincinnati Reds and again in 1982 as the Double-A affiliate of the New York Yankees.

In an effort to position Nashville to contend for a Major League Baseball franchise in the future, Schmittou and team owners purchased the Triple-A Evansville Triplets of the American Association and relocated the team to Nashville before the 1985 season. The Triple-A Sounds carried on the history of the Double-A team that preceded it. The team rarely contended for the American Association championship, making only three appearances in the postseason during their 13 years in the league.

The Sounds became members of the Triple-A Pacific Coast League (PCL) in 1998 following the dissolution of the American Association after the end of the previous season. After a quiet start in the PCL, the team won four division titles, two American Conference titles, and one PCL championship between 2003 and 2007. Their lone PCL title was won in 2005 as the Triple-A affiliate of the Milwaukee Brewers. The only time the Sounds have qualified for the postseason since was in 2016 when they won the division championship but were eliminated in the conference series.

Nashville has hosted Minor League Baseball teams since the late 19th century. The city's professional baseball history dates back to 1884 with the formation of the Nashville Americans, who were charter members of the original Southern League from 1885 to 1886 and played their home games at Athletic Park, later renamed Sulphur Dell. This ballpark was the home of Nashville's minor league teams through 1963. In 1887, Nashville's Southern League team was called the Nashville Blues. The Nashville Tigers competed for the city in the same league from 1893 to 1894. In 1895, the Nashville Seraphs won the city's first professional championship in the Southern League. The Nashville Centennials played in the Central League in 1897 but relocated to Henderson, Indiana, during the season before the league's collapse.

The city's longest-operating baseball team, first known as only the Nashville Baseball Club and later renamed the Nashville Vols (short for Volunteers), was formed in 1901 as a charter member of the Southern Association. They remained in the league through 1961, winning eight pennants, nine playoff championships, and four Dixie Series titles. The league disbanded after the 1961 season, and no team was fielded in 1962, but the Vols played one final season in the South Atlantic League in 1963. Sulphur Dell was demolished in 1969, and the city went without a professional baseball team for 14 years until 1978.

Larry Schmittou, head coach of the Vanderbilt Commodores baseball team from 1968 to 1978, was instrumental in bringing professional baseball back to Nashville. He was inspired to get involved with Minor League Baseball when he observed the large crowds the Chattanooga Lookouts saw after owner Walter Reed acquired the Birmingham Barons and relocated the team to Chattanooga in 1976. Schmittou was told by multiple Major League Baseball teams that they would be willing to put a minor league affiliate in Nashville if he provided a suitable ballpark.

Schmittou learned from a member of the Metro Board of Parks and Recreation that neither the Parks Board or the city of Nashville would be willing to pay for such a park. So, Schmittou, along with help from country musician Conway Twitty, put together a group of investors including other country artists Cal Smith and Jerry Reed, as well as other Nashvillians, to finance a stadium and a minor league team. Twenty shares valued at US$15,000 each were issued; Schmittou purchased 2 shares, or 10 percent of the team, and Twitty purchased 4 shares for a 20 percent stake. The Metro Parks Board agreed to lease to Schmittou the site of Nashville's former softball fields on the grounds of Fort Negley, an American Civil War fortification, approximately south of downtown, for a period of 20 years as long as he built a stadium with a minimum capacity of 6,500 at a cost of at least $400,000 within 10 years. In the second ten years, he would be required to pay the city seven percent of the team's total revenue.

Stoll-Reed Architects advised Schmittou that construction of a suitable stadium would cost between $300,000 and $500,000, but bids for the project ranged from $980,000 to $1.2 million. Schmittou looked to local suppliers to donate construction materials, took out a $30,000 loan from a bank, sold season tickets in advance of having a team, and even mortgaged his own home to help pay for the facility. The actual cost totaled $1.5 million. The ballpark would be named Herschel Greer Stadium in posthumous honor of Herschel Lynn Greer, a prominent Nashville businessman and the first president of the Nashville Vols, whose family donated $25,000 for stadium construction.

Having secured a stadium, Schmittou and general manager Farrell Owens attended the 1976 Winter Meetings in hopes of landing a major league affiliate. After sending letters to all 26 farm team directors, the pair received a letter from Sheldon "Chief" Bender of the Cincinnati Reds. Bender met with the pair and agreed to put a team in Nashville provided a stadium was built. Schmittou was then granted a franchise in the Southern League, a class Double-A league, at an enfranchisement cost of $7,500.

Fans were invited to submit suggestions for the team's name which would be voted on by a group that included local sports writers and county musicians. Among the finalists were "Stars", "Notes", "Hits", "Strings", "Kats", "Pickers", and "Vols". The chosen name, "Sounds", is a play on the term "Nashville sound", a subgenre of American country music that traces its roots to the area in the late-1950s. The team's wordmark and color scheme were lifted from the Memphis Sounds of the American Basketball Association (ABA), who used them from 1974 to 1975. When the ABA merged with the National Basketball Association in 1976, some of the copyrights were allowed to lapse, and Nashville's baseball team adopted the abandoned scheme. The color blue was added to Memphis' red and white palette. Nashville's original logo, which was used from 1978 into 1998, and was initially sketched by Schmittou, reflected the city's association with the country music industry. It depicted a mustachioed baseball player, nicknamed "Slugger", swinging at a baseball with a guitar, a staple of country music, in place of a bat. Further illustrating the city's musical ties was the typeface, with letters that resembled G-clefs, used to display the team name and the cap logo which resembled an eighth note.

With a team in place and a stadium under construction, the Nashville Sounds were set to begin play in 1978 as an expansion team of the Southern League. As the Double-A affiliate of the Cincinnati Reds, the Sounds played their first game on April 15, 1978, against the Memphis Chicks at Memphis' Tim McCarver Stadium, which they lost, 4–2. After falling behind in the first inning, 1–0, Nashville tied the game in the third and went ahead, 2–1, in the top of the sixth on first baseman George Weicker's single which scored center fielder Mickey Duval. In the bottom of the inning, however, Memphis answered with three unearned runs off of Sounds starting pitcher Bill Dawley and reliever Larry Rothschild, sealing the Nashville loss. The Sounds recorded their first win the next evening, defeating Memphis, 3–0. Pitchers Bruce Berenyi and Doug Corbett limited the Chicks to just three hits while catcher Mark Miller drove in a run with a third inning double and later scored on second baseman Randy Davidson's sacrifice fly. The Sounds padded their lead in the fifth inning on outfielder Tony Moretto's RBI double.

Meanwhile, construction on Greer Stadium continued in order to be ready for the home opener. The team had requested to begin the season on the road and had to swap a series with the Chattanooga Lookouts to have enough time to complete the stadium. Much of the sod that had been installed that winter died, and the replacement sod, which arrived late, had to be laid the day before the planned opening game.

The Greer home opener was scheduled to take place the evening of April 25, but was rained out and rescheduled for the next night. On April 26, the Sounds played their first home game, a 12–4 victory against the Savannah Braves in front of a sellout crowd of 8,156 fans. Tractors and grading machines were still preparing the field on game day, the electricity was turned on only 5 minutes before the gates opened, and the game's start was delayed 30 minutes because of traffic problems around the stadium. On the field, Sounds catcher Joe Griffin led the 16-hit Nashville offense with 4 hits of his own and 5 runs batted in (RBI) while starter Bruce Berenyi got the win and closer Doug Corbett earned a save after retiring 11 batters in a row.

The Sounds, under manager Chuck Goggin, finished the first half of their inaugural season with a 28–36 record in fourth place. The Southern League used a split-season schedule wherein the winners of each half from each of two divisions qualified for the postseason championship playoffs. Another fourth-place finish at 36–41 in the second half kept Nashville out of the playoffs. Combining both halves of the season, the Sounds' composite record stood at 64–77 for their first season of play. All-Star pitcher Bruce Berenyi was selected for the league's Most Outstanding Pitcher Award.

The team had more success at the turnstiles than on the field. The Sounds led all of Minor League Baseball in attendance by drawing 380,000 fans to Greer Stadium in their first season. Nashville went on to lead the Southern League in attendance in each of their seven seasons as members of the league. Schmittou's business philosophy revolved around earning profits not from ticket sales, but from the sale of souvenirs and concessions. This philosophy also involved promoting entertainment value, or fun, as much or more than promoting the baseball game. The franchise was recognized for its promotion efforts when it won the Larry MacPhail Award for outstanding minor league promotions in 1978, 1980, and 1981. Schmittou was awarded the "Sporting News" Double-A and Southern League Executive of the Year Awards in 1978 and 1981.

Under manager George Scherger, the Sounds started the 1979 season poorly, before rallying to win 20 of 31 games in late May and June. They entered the last day of the first half in first place, but lost their game to their cross-state rivals, the Chicks, and finished in second place at 35–34, a mere half game from winning the first-half title. The Sounds and Chicks met again on the last day of the second half in a split doubleheader; both games were won by Nashville to give them a 48–27 second-half record and the second-half title. The two teams then faced-off in a best-of-three series to determine the Western Division champion. The Sounds won the series, two games to one, before advancing to the league championship series against the Columbus Astros. Nashville entered game four one win away from capturing their first Southern League championship. In the top of the ninth inning with the game tied 2–2 and the bases loaded, Sounds catcher Dave Van Gorder hit a bases-clearing triple giving his team the lead. Reliever Geoff Combe struck out the last two batters in bottom half of the inning on the way to a 6–2 Sounds win, a three-games-to-one series victory, and the Southern League title. Schmittou wanted to give each player a $1,000 bonus for winning the pennant, but as that would have been against the National Association's rules, he settled for buying them championship rings instead. Combe, with a league-leading 27 saves, won the league's Most Outstanding Pitcher Award. The Sounds compiled an 83–61 composite record in their sophomore season.

Earlier in the season, Nashville played host to the 1979 Southern League All-Star Game. The July 12 contest pitted a team of the league's All-Stars against the major league Atlanta Braves. The All-Stars, coached by Nashville's Scherger, defeated the Braves, 5–2, before a crowd of 11,079 fans. Nashville was further represented by All-Stars Geoff Combe, Paul Householder, Dave Van Gorder, and Duane Walker. Walker, who hit an RBI single, drew a walk, stole two bases, and completed a double play from third base, was selected as the game's Most Valuable Player (MVP).

The Reds originally allowed Nashville to use a designated hitter (DH) in their lineups. However, this allowance was later revoked, as the Reds were a part of the National League, which did not use a DH. Schmittou issued an ultimatum: if Cincinnati would not let the Sounds use a DH in their lineups, they would not renew their contract and would look for a new major league affiliate. The Reds did not budge on their decision to prohibit the DH, so the Sounds looked for a new parent club after 1979. Schmittou was then approached by five or six clubs looking to enter the Southern League as a Sounds affiliate. After two seasons at Double-A for the Reds, Nashville had a 147–138 record.

Schmittou had originally been encouraged by the New York Yankees organization to establish the Sounds as a Triple-A team, but he refused to go back on his previous promise to partner with the Reds at Double-A. After the split with Cincinnati, the Sounds made their first affiliation switch in 1980, becoming the Double-A affiliate of the Yankees. Under Manager of the Year Stump Merrill, the 1980 Sounds finished the first half of the season one-and-a-half games behind the Memphis Chicks with a 46–25 record. In the second half, the team finished in first place, 14 games ahead of the second-place Montgomery Rebels, at 51–21. In the Western Division championship series, Nashville lost to Memphis, three games to one. Nine Southern League records were set during the season, the team's pitching staff led the league in ERA and strikeouts, and Steve Balboni, All-Star outfielder and league MVP, led the league with 101 runs, 34 home runs, 122 RBI, and 288 total bases. Andy McGaffigan was selected as the circuit's top pitcher after he led the league with a 2.38 ERA. The Sounds also set a league attendance record, which still stands as of the completion of the 2018 season, when a total of 575,676 fans visited Greer Stadium. Their 97–46 record is the team's all-time best. The 1980 Sounds were ranked as the sixty-ninth greatest minor league baseball team of all-time by baseball historians in 2001.

On April 16, 1981, the Yankees made a stop in Nashville to play an exhibition game against the Sounds. The 10–1 Yankees victory was played in front of a standing-room-only crowd of 17,318 people. Those on hand for the game included Yankees owner George Steinbrenner, coach Yogi Berra, and players Reggie Jackson, Bucky Dent, Lou Piniella, Bobby Murcer, Goose Gossage, and Tommy John. The Sounds ended the first half at 38–32 in second place. They won the second half of the season with a 43–30 record and went on to win the Western Division championship by defeating the Chicks in three straight games. Ultimately, Nashville suffered defeat in the league championship series, falling to the Orlando Twins, 3–1. Nashville compiled an 81–62 record during the season under Merrill. All-Star right-hander Jamie Werly won the Southern League Most Outstanding Pitcher Award after leading the circuit with 18 complete games and 193 strikeouts. Willie McGee and All-Star Don Mattingly, who both played for the Sounds in 1981, were later promoted to the major leagues. In 1985, Mattingly was named the American League Most Valuable Player and McGee named the National League MVP.

The 1982 Sounds, led by manager Johnny Oates, ended the first half at 32–38, but won the second half, 45–29. After defeating the Knoxville Blue Jays, 3–1, in the Western Division playoffs, the Sounds advanced to the league championship series against the Jacksonville Suns. With a 2–1 series lead, Nashville entered game four with a chance to win their second Southern League championship in front of a home crowd at Greer. The Sounds took an early lead in the second inning, but the Suns tied things up in the ninth sending the game to extra innings. With two outs in the bottom of the thirteenth inning, outfielder Brian Dayett hit a walk-off home run scoring Buck Showalter and giving the Sounds a 5–3 win. Nashville had won the series, 3–1, and won the franchise's second league pennant. The team's season record was 77–67. Dayett, an All-Star, was selected as the Southern League MVP. Stefan Wever, who was also voted onto the All-Star team and paced the league with 191 strikeouts and a 2.78 ERA, was the league's Most Outstanding Pitcher. Wever was the fifth Sounds hurler in five years to win the award. Otis Nixon set the franchise career record for stolen bases (133) during the 1981 and 1982 seasons. The Sounds set the club's and Greer Stadium's all-time single-game attendance record on August 18, 1982, when 22,315 people watched the Sounds defeat the Columbus Astros, 3–0. Portions of the outfield had to be roped off to accommodate the crowd, which was far in excess of Greer's seating capacity.

The Yankees returned for another exhibition game against the Sounds on April 28, 1983. New York had a four-run lead going into the bottom of the ninth inning, but a five-run rally with two outs propelled the Sounds to a 5–4 win in front of 13,641 fans. The tying and winning runs came off the bat of catcher Frank Kneuer who doubled down the left-filed line bringing home Matt Gallegos and Derwin McNealy from second and first. Among the Yankees in attendance for the game were manager Billy Martin, coach Yogi Berra, and players Goose Gossage, Ken Griffey Sr., Dave Winfield, Lou Piniella, Willie Randolph, and former Sound Don Mattingly. After the season's first half, Nashville held a 40–32 record, but that was only good enough for second place. Manager Doug Holmquist, frustrated with the team's disappointing first half, instituted a system of fines for player infractions or poor performance on the field. The program ranged from a $10 fine for a pitcher walking a batter with one on and two outs to a $100 fine for missing curfew. Rebounding, Nashville won the second-half pennant, 48–26, earning the team a shot at the Western Division championship. The Sounds, however, lost the decisive fifth game of the series to the Birmingham Barons, 7–5, ending their season. Nashville finished 30 games over .500, with an 88–58 record. Pitcher Jamie Werly set the franchise career strikeout record (352) when he played for the Sounds in 1980, 1981, and 1983.

Also in the 1983 season, on June 19, the Southern League All-Star Game returned to Nashville. This time, the Sounds were enlisted to serve as the All-Stars' competition. Consequently, no Sounds player could be voted on to All-Star team. In lieu of this, the league chose to recognize all Sounds players as All-Stars. The league's team bested Nashville, 3–2, before an audience of 1,221 people who waited out nearly an hour's rain delay. After rallying back from 3–0 on Scott Bradley's two-run single in the eighth inning, Nashville's Erik Peterson stuck out in the ninth with both the tying and winning runs on base to end the game.

The Sounds were three games shy of winning the first-half pennant in 1984, with a second-place 38–33 record. Winning the first-half title is something that eluded the team during its entire seven-year span in the Southern League. One highlight of the first half took place on May 4, when Jim Deshaies pitched the club's first no-hitter against the Columbus Astros in the second game of a seven-inning doubleheader. The Astros' lone run was scored following three walks and a batter being hit by a pitch, advancing a runner home. Nashville finished the second-half tied for first place with the Birmingham Barons with identical 36–40 records. On September 4, the Sounds defeated the Barons in a one-game playoff, 3–2 in 10 innings, to win the second-half title for the sixth consecutive season. The Sounds met the Knoxville Blue Jays in the Western Division finals, but the Blue Jays emerged the victor, winning three games to one. Skipper Jim Marshall led his Sounds to a 74–73 record for the season. Nashville accumulated a 417–306 record during their five-year affiliation with the Yankees. They had a 564–444 record over their seven years in the Southern League at Double-A.

In 1983, Sounds President Larry Schmittou noticed a 5 percent drop in season ticket sales, a higher ratio of no-shows from season ticket holders, and a slight decline in overall attendance. These issues with spectator turnout were accompanied by a decline in local media coverage, particularly in regard to road games. To boost interest in the team, Schmittou tried to purchase a Triple-A franchise late in the 1983 season, but each of the two teams he considered chose to continue in their markets for 1984. His desire to land a Triple-A team was part of a larger plan to put Nashville in a position to contend for a Major League Baseball franchise in the future. Attendance continued to drop in 1984, as season ticket sales were down 12 percent and overall attendance was down almost 20 percent.

Schmittou arrived at terms in July 1984 to purchase the Triple-A Evansville Triplets of the American Association for $780,000, with plans to move the franchise from Evansville, Indiana, to Nashville for the 1985 season. To prove to the team's Nashville banks, which would back the purchase, that the move was financially viable, Schmittou commissioned a survey to evaluate the potential turnout for a Triple-A team versus a Double-A team. Though the research proved to team owners that the move was a sensible decision, the banks were not impressed. As a result, the team switched banks and went ahead with the purchase and relocation. The Southern League wanted Schmittou to surrender his franchise to the league, but he had plans to relocate the team instead. He wanted to send Nashville's existing Southern League franchise to Evansville to continue as the Triplets at Double-A. However, a combination of the league's disapproval of the move and the City of Evansville being unwilling to upgrade Bosse Field resulted in a move to Huntsville, Alabama, where the team became the Huntsville Stars. The Triple-A Sounds carried on the history of the Double-A team that preceded it. The Triplets' legacy was retired, and the Stars were established as an entirely new franchise.

The Sounds entered the Triple-A playing level in 1985 as a member the American Association affiliated with the Detroit Tigers, continuing the major league affiliation that was in place with the Evansville franchise. They played their first Triple-A game on April 11, a 3–1 win, against the Buffalo Bisons at Greer Stadium. The home team scored all the runs they needed in the first inning. With the bases loaded following a walk, an error, and a batter being hit by pitch, outfielder Bobby Mitchell scored on a passed ball with a head-first slide, and designated hitter Ron Johnson drove in shortstop Pedro Chavez from third on a out. The Triple-A opener was attended by a sparse crowd of only 4,730.

The next day, April 12, Nashville competed in an exhibition game against their parent team. Manager Sparky Anderson's Detroit club included Kirk Gibson, Alan Trammell, Lou Whitaker, Rusty Kuntz, Milt Wilcox, and Larry Herndon. Trailing 2–0 after the first inning, Detroit scored three runs in the top of the fifth on a trio of Sounds errors and a single, but Nashville tied the game with a Chavez RBI double in the bottom half. In the end, the Tigers outlasted the Sounds, scoring six runs in the tenth inning to win, 9–3, before a crowd of 16,182. Seven games into the season, manager Lee Walls came down with an illness, and outfielder Leon Roberts became the acting manager for seven games until Gordon Mackenzie was brought on to lead the club for the rest of the year. On July 17, Bryan Kelly pitched the club's second no-hitter against the Oklahoma City 89ers, as the Sounds won, 6–0. Nashville ended the season in second place in the Eastern Division, two-and-a-half games out of first, with a 71–70 record that excluded them from the playoffs for which only division winners qualified.

The 1986 team was managed by former player Leon Roberts. The Sounds finished third in their division with a 68–74 regular season record, their first losing season since the inaugural 1978 campaign. Also that year, the Sounds were enlisted to serve as the competition in the Southern League All-Star Game, held at Huntsville's Joe Davis Stadium on July 23. Nashville defeated the league's All-Star team, 4–2. The winning run came in the fourth inning when outfielder Bruce Fields singled home catcher Matt Nokes. Starter Brian Kelly earned the win. The Sounds ended their affiliation with Detroit after two seasons of poor attendance and a lackluster 1986 campaign. Over two years with the Tigers, they had a 139–144 record. Their all-time record stood at 703–588 after nine years of play.

The Sounds rejoined the Cincinnati Reds farm system as their Triple-A affiliate in 1987 in a bid to increase attendance. Schmittou indicated that market surveys consistently showed the Reds to be the most popular team in the area. As a result of this affiliation and the prior affiliation with Cincinnati, a number of minor leaguers played in the Reds organization at two different levels with Nashville. Spending the beginning of the 1987 season around the top of the standings, the team hit a slump after losing a few key players midseason. The result was a 64–76 record and a last-place finish under skipper Jack Lind. One player lost due to injuries was third baseman Chris Sabo. He was promoted to Cincinnati in 1988 and was named the National League Rookie of the Year, a first for any former Sounds player.

The 1988 Sounds were in last place and had a losing record until making numerous management changes late in the season. During a two-week period in July and August, the Sounds went through five different managers. The team started the season with Lind, who left due to health problems. His position was filled on an interim basis by pitching coach Wayne Garland until George Scherger, manager of the 1979 Southern League championship Sounds, was brought in. He retired after one game and was replaced by Jim Hoff, who stayed a few weeks before taking up a position with the Reds' front office. Finally, former big league skipper Frank Lucchesi was hired to manage the Sounds for the last 39 games of the season, leading them to a second-place finish, 16 games out of first, with a final record of 73–69.

Greer Stadium was home to a rare baseball occurrence on August 6 and 7, 1988, when Nashville and the Indianapolis Indians exchanged no-hitters on back-to-back nights. First, Indianapolis' Randy Johnson and Pat Pacillo combined for a no-hit loss against the Sounds, a 1–0 Nashville win. That game was won by Nashville when Lenny Harris walked to first base, stole second base and third base, and then came home, scoring on a groundout. The next night, Nashville's Jack Armstrong pitched a no-hit game against the Indians, a 4–0 Sounds victory in which he allowed only one base runner via walk. This was the first time in American Association history that teams played in back-to-back no-hit games, and was the third no-hitter in Sounds franchise history.

Lucchesi continued to manage the Sounds in 1989, leading the team to a third-place finish with a 74–72 record. Pitcher Hugh Kemp set the franchise career record for games started (73) while playing for the team from 1987 to 1989. On April 23, 1990, 14,012 fans attended an exhibition game at Greer between Nashville and Cincinnati. Reds pitchers Danny Jackson and Ron Robinson held the Sounds to just five hits, three by Terry McGriff and two by Keith Lockhart. Luis Quiñones scored the winning run in the first when he came home on a misplayed ball hit on the ground by Paul O'Neill. Lou Piniella's Cincinnati squad shutout Nashville, 3–0.

Despite being blanked by their major league affiliate, the Sounds experienced their most successful season in the American Association in 1990, when they compiled an 86–61 record under manager Pete Mackanin. Ending the regular season in a tie for first place with the Buffalo Bisons, each with an 85–61 record, the Sounds won the Eastern Division championship in a one-game playoff on September 4. The extra-inning affair was ended by Chris Jones' two-run homer in the top of the eighteenth inning. The Sounds advanced to their first American Association championship series, where they lost the best-of-five series to the Omaha Royals, three games to two. In a decisive game five, Omaha got out to an 5–0 lead in the first inning, but Nashville mounted a comeback which included a sixth-inning grand slam by second baseman Keith Lockhart that tied the game. Both teams scored again, but the Royals came out on top, 8–7. Left-hander Chris Hammond, who led the circuit with 15 wins, 149 strikeouts, and a 2.17 ERA, won the league's Most Valuable Pitcher Award for 1990. Nashville set their all-time attendance record that year when a total of 605,122 fans came out to Greer Stadium. Skeeter Barnes, who had previously played with Nashville in 1979, set the franchise career records for games played (514), at bats (1,848), and hits (517) during his second stint from 1988 to 1990.

In 1991, the Sounds started the year in first place, where they stayed for only ten days. By May 1, they had fallen into third place in the Eastern Division, where they remained for the rest of the season. Mackanin's Sounds posted a losing record every month during the campaign and finished the year 16 games behind first-place Buffalo with a 65–78 record. From 1988 to 1991, American Association teams participated in interleague play with teams from the Triple-A International League in a partnership called the Triple-A Alliance. The Sounds had an interleague record of 90–78 over this four-year period. Mackanin was dismissed from his managerial duties on June 28, 1992. He was replaced by Dave Miley, who was managing the Reds' Double-A affiliate, the Chattanooga Lookouts. The 1992 Sounds posted a 67–77 record, winding up in fourth place.

Greer Stadium, once one of the best stadiums in Triple-A baseball in terms of player and fan amenities, began to be outshined by newer ballparks being built in the late 1980s. The Reds let their player development contract with the Sounds expire so they could place their Triple-A team in Indianapolis, which was closer to Cincinnati and planning to build a new stadium. Nashville entered the offseason unsure of their next major league affiliate. Their final record after six years with the Reds at Triple-A was 429–433. Through 15 total years of competition, their all-time record stood at 1,132–1,021.

At the recommendation of the Office of the Commissioner of Baseball and with few options available, the Sounds signed an affiliation agreement with the Chicago White Sox, who wanted to move their Triple-A farm club closer to home than their previous location in Vancouver. The White Sox presented a list of complaints about the relatively poor condition of Greer Stadium. Schmittou was unable to convince Mayor Phil Bredesen or the Metro Council to pay for a new stadium to replace Greer. He considered moving the team to a surrounding county, and explored sites in La Vergne, Cool Springs, and Mount Juliet. He even tried, unsuccessfully, to get the Metro Council to pass a referendum to let taxpayers vote on a temporary tax increase to pay off a proposed $40 million stadium in three years. In the end, Schmittou elected to keep the Sounds at Greer but make significant improvements to the stadium. One of those improvements was the addition of Greer's signature guitar-shaped scoreboard, which was installed in 1993.

In their first year with the White Sox, the Sounds clinched the Eastern Division title with an 81–62 record earning them an opportunity to play for the American Association championship. Winning two elimination games versus the Iowa Cubs, the Sounds forced a game seven in which they held a 2–1 lead from the third inning to the seventh before the Cubs tied the game necessitating extra innings. An eleventh-inning walk-off home run by Iowa's Tuffy Rhodes ended the game and Nashville's championship run in a four-games-to-three series loss. Nashville's Rick Renick, who managed the club from 1993 to 1996, was named the American Association Manager of the Year in his first season.

The Sounds shared Greer Stadium with the Southern League's Nashville Xpress, previously known as the Charlotte Knights, during the 1993 and 1994 seasons. This came about when Charlotte acquired a Triple-A expansion franchise in 1993, leaving the city's Double-A team without a home. Schmittou offered Greer as a temporary home ballpark for the team. To accommodate an additional club at Greer, the Xpress scheduled its home games during the Sounds' road trips. In 1995, the Xpress relocated to Wilmington, North Carolina, and became the Port City Roosters.

An exhibition game against the White Sox was planned for April 3, 1994, but was cancelled due to wet grounds and the possibility of player injury. Nashville was able to host the 1994 Triple-A All-Star Game at Greer on July 13 with 11,601 people in attendance. Rick Renick managed the team of American League affiliated All-Stars which included Sounds Ray Durham, Drew Denson, Scott Ruffcorn, and Steve Schrenk. The team of National League affiliated All-Stars defeated Renick's Americans, 8–5. Durham, who had three hits in three at-bats and scored the game's first run, was selected as the MVP from the American Association. Denson participated in the previous day's Home Run Derby, but was defeated in the final round by Scott Coolbaugh of the Louisville Redbirds, six home runs to two.

The Sounds completed the 1994 season with an 83–61 record, placing them in second. The American Association had moved away from a divisional alignment to one wherein the top four teams qualified for the championship playoffs that season. In the first round, Nashville swept the New Orleans Zephyrs in three straight games to advance to the league finals. In the best-of-five championship series, the Indianapolis Indians defeated the Sounds, 3–1. Ruffcorn led the American Association with 15 wins, and was selected as the Most Valuable Pitcher for 1994.

Nashville compiled a 68–76 record, 20 games out of first place, in 1995. Originally, Michael Jordan, who played with the White Sox's Double-A Birmingham Barons in 1994, was slotted to play the 1995 season as a non-drafted free agent for the Sounds. However, with the ongoing MLB strike, Jordan decided to quit the sport rather than becoming a replacement player and being labeled a strikebreaker. Reliever Joey Vierra pitched in a franchise career-record 238 games from 1990 to 1992 and 1994 to 1995. The team improved their record in 1996, ending up at 77–67. Despite a decent winning percentage, Nashville failed to secure a spot in the playoffs with their third-place finish. All-Star outfielder Jeff Abbott won the Rookie of the Year Award, and Rick Renick earned his second Manager of the Year Award.

The 1996 season marked the last that Schmittou was the team's president and part majority owner. With the city prepared to welcome a National Football League franchise, the Tennessee Titans, Schmittou felt that revenue would be drawn away from his baseball team, so he and businessman Walter Nipper sold their 59 percent stake in the Sounds to Chicago-based businessmen Al Gordon, Mike Murtaugh, and Mike Woleben. In 1997, under the guidance of manager Tom Spencer, Nashville put together a 74–68 campaign, but a third-place finish excluded them from the playoffs. In addition to being selected for both the midseason and postseason All-Star teams, outfielder Magglio Ordóñez won the Triple-A All-Star Game MVP Award and garnered the league's Most Valuable Player and Rookie of the Year Awards. Ordóñez had led the league with 172 hits and tied for first with a .329 batting average and 249 total bases. The five-year White Sox affiliation ended after the 1997 season with the Sounds having a 383–335 record over that period. Their final American Association record stood at 951–912 after 13 years in the league, and their all-time 20-year record was 1,515–1,356.

The American Association, of which the Sounds had been members since 1985, disbanded after the 1997 season. Its teams were absorbed by the two remaining Triple-A leagues—the International League and Pacific Coast League (PCL). Nashville joined the PCL, making it the eastern-most team in the league. The franchise also picked up a new major league affiliation, becoming the top farm club of the Pittsburgh Pirates who sought to escape the chilly climate and lengthy travel associated with their previous affiliate in Calgary. For the first time since the team's foundation in 1978, the Sounds began to adopt a new logo, color scheme, and uniforms over the course of the 1998 and 1999 seasons. The original red, white, and blue colors were replaced by red, black, white, and silver. The new team logo, replacing the original "Slugger", consisted of a black, red, and white eighth note with a baseball at the top set against a circle of the same colors, plus sliver, bearing the team name in white around the sides.

Nashville entered the PCL with a 7–2 loss to the Iowa Cubs at Sec Taylor Stadium on April 7, 1998. In the team's first season as a Pirates affiliate, the Sounds finished last in the American Conference East Division with a 67–76 record. The Sounds were led by manager Trent Jewett, who went on to win 320 games from 1998 to 2000 and 2003 to 2004, placing him first on the all-time wins list for Sounds managers. The team played an exhibition game against Pittsburgh on June 3, 1999, attended by 5,720 fans. Nine home runs were hit in the game which was dominated by offence. The Pirates, whose roster included Jason Kendall, Emil Brown, and Dale Sveum, plated 13 runs in the fifth inning on the way to 16–15 win. The Sounds experienced their longest winning streak in franchise history when they won 15 consecutive games from June 2 to 20, 1999. Overall, the team improved from the previous year, putting together an 80–60 record, but a second-place finish left them out of the PCL playoffs, where only division winners advance to the postseason.

The Sounds ended the 2000 season with a 63–79 record, resulting in a last-place finish in the divisional standings. Richie Hebner, the Sounds' pitching coach, replaced Trent Jewett as manager when he became the Pirates' third base coach on June 6. Former All-Star Sounds third baseman Marty Brown returned to the club to serve as its manager in 2001, becoming the third former Nashville player to serve as the team's skipper. On June 30, Tike Redman became the first Sounds player to hit for the cycle. Redman also holds the franchise record for the most triples (30) during his career with the team (2000–2003, 2009). The 2001 Sounds compiled a 64–77 record, leaving them in third place. Despite finishing the 2002 season with an improved 72–71 record under Brown, it was only good enough for a third-place finish, two-and-a-half games out of first place. Chad Hermansen, who played for the Sounds from 1998 to 2002, holds the franchise career records for runs (303), home runs (92), and runs batted in (286).

Right-hander John Wasdin pitched the first perfect game in Sounds history in his first start of the 2003 season against the Albuquerque Isotopes on April 7. Wasdin threw 100 pitches, striking out 15 batters. The 4–0 Sounds win was only the second nine-inning perfect game in PCL history. That year, Trent Jewett returned to lead the team to an 81–62 record. The Sounds clinched the American Conference Eastern Division title, giving them their first playoff berth in the PCL and first postseason appearance since 1994. Nashville met Albuquerque in the American Conference championship series, defeating the Isotopes, three games to one. The Sounds then lost the best-of-five league championship series in three straight games to the Sacramento River Cats.

On May 21, 2004, catcher J. R. House became the second Sounds player to hit for the cycle. The team completed the 2004 campaign with a 63–79 record, finishing last in the division under Jewett. Jason Bay played four games in Nashville early in the season before being promoted to Pittsburgh to make his major league debut. Following the season, he became the second former Sound to win a major league Rookie of the Year Award. Closer Mark Corey set the franchise career record for saves (46) during the 2003 and 2004 seasons. The Pirates and Sounds ended their affiliation after seven years, during which Nashville had a 490–504 record. Through 27 years of competition, the Sounds' all-time record stood at 2,005–1,860.

The Sounds became the Triple-A affiliate of the Milwaukee Brewers in 2005. One factor in the Brewers' choice to partner with Nashville was the hope that the Sounds would soon get a new stadium to replace the then-27-year-old Greer. Along with a new affiliate, Nashville debuted a new oval-shaped logo with a baseball player silhouetted against a yellow background hitting a ball toward the Nashville skyline with the city's name written above in white within a red border and the team nickname written in red and black script below. The affiliation started well as the club, managed by Frank Kremblas, won the American Conference Northern Division title on the second-to-last day of the season with a 75–69 record. The Sounds went on to win the American Conference championship against the Oklahoma RedHawks, three games to two, before sweeping the Tacoma Rainiers in three straight games in the finals to win the 2005 Pacific Coast League championship. Outfielder Nelson Cruz clinched the title with a three-run home run with two outs in the top of the 13th inning. This was Nashville's first title since their previous league crown in 1982 and their first championship as a Triple-A team.

On May 5–6, 2006, the Sounds participated in a 24-inning game against the New Orleans Zephyrs. The contest, played over the course of two days, lasted a total of eight hours and seven minutes. The game matched the longest game, in terms of innings played, in PCL history. Several team and league records were broken by both clubs. On July 15, Carlos Villanueva, Mike Meyers, and Alec Zumwalt combined to pitch the fifth no-hitter in team history, a 2–0 win over the Memphis Redbirds. The Sounds finished the season with a 76–68 record under Kremblas, tied with the Iowa Cubs for first place. Nashville won the division title and advanced to the postseason by means of a tiebreaker (winning the regular season series versus Iowa, nine games to seven). In the conference championship series, Nashville lost to the Round Rock Express, three games to two.

The 2007 team included Brewers third base prospect Ryan Braun, who made his major league debut on May 25 and was named National League Rookie of the Year following the season, becoming the third former Sound to win this award. On June 25, Manny Parra pitched the club's second perfect game, only the third nine-inning perfect game in PCL history, against Round Rock. Parra threw 107 pitches, striking out 11 batters. Led by PCL Manager of the Year Frank Kremblas, the team won the American Northern Division title for the third straight year and posted a league-best 89–55 record. Ultimately, they were defeated by New Orleans, three games to one, in the conference series. Nashville-native knuckleball pitcher and 13-game winner R.A. Dickey won the PCL Pitcher of the Year Award.

Massive flooding in the Midwest resulted in the Sounds and the Iowa Cubs playing a game with an official attendance of zero on June 14, 2008. Though downtown Des Moines was under a mandatory evacuation, team officials received permission from the city to play the game as long as no fans were allowed into Principal Park. To keep fans away, the lights and scoreboard were not turned on, the game was not broadcast in the local market, and a message on the team's website announced that the game was postponed. PCL Commissioner Branch Rickey III believed that this was the first time such actions were taken out of necessity. Kremblas' Sounds finished with a 59–81 record, the second-lowest in team history.

The Sounds had planned to leave Greer Stadium in the mid-2000s for a new ballpark to be called First Tennessee Field, but the project was abandoned after the city, developers, and team could not come to terms on a plan to finance its construction. On October 30, 2008, following this failure to secure a new ballpark, Al Gordon's Amerisports Companies LLC agreed to sell the Sounds to MFP Baseball, a New York-based group of investors consisting of Masahiro Honzawa, Steve Posner, and Frank Ward. Keeping the team in Nashville was one of the PCL's top criteria for approval of the sale. The transaction received final approval from Major League Baseball and the PCL on February 26, 2009. MFP made significant renovations to Greer while it continued to explore options for building a new downtown ballpark.

Don Money managed the 2009 Sounds to achieve a 75–69 record, an improvement over the previous season. Even though they spent the majority of the year in first place, the club finished two games behind their cross-state rival Memphis. Improving further in 2010, Nashville's 77–67 record under Money was only good enough to place last in the four-team division. Caleb Gindl became the third player in team history to hit for the cycle when he accomplished the feat on July 10, 2011. With Money at the helm, his team placed third with a 71–73 record. Led by Mike Guerrero, the 2012 Sounds finished the season in second place at 67–77, 16 games out of first. Nashville set a franchise-worst win–loss record in 2013 with a 57–87 season that eclipsed the previous record from 2008. Despite the team's performance, Johnny Hellweg won the PCL Pitcher of the Year Award with a league-best .706 (12–5) winning percentage, and Guerrero was selected for the Mike Coolbaugh Award in recognition for his contributions to the game of baseball.

Before the 2014 season, the Sounds, Metro Nashville, and the State of Tennessee finalized a plan to build a new ballpark to replace Greer Stadium at the beginning of the 2015 season. On August 27, 2014, the Sounds hosted the final game at Greer, an 8–5 loss to the Sacramento River Cats. In his only plate appearance, Nashville catcher Lucas May struck out swinging with a full count and the bases loaded to end the game. The attendance was a standing-room-only crowd of 11,067, the first sellout since 2010, and the largest crowd since 2007. The team, led by veteran minor league manager Rick Sweet, finished the season with a 76–67 record, in second place, two-and-a-half games behind Memphis. Jimmy Nelson, the Brewers' top prospect at the start of the season, was elected PCL Pitcher of the Year; he received all but one of the votes after posting a league-leading 1.46 ERA. Tim Dillard set the franchise career records for wins (39) and innings pitched (556⅔) from 2007 to 2014. The Sounds severed ties with the Brewers, with whom they had had the longest affiliation in franchise history, after the 2014 season citing poor on-field performance from recent Brewers Triple-A teams. They had a 723–713 record in their ten years as a Brewers affiliate. Overall, the Sounds' 37-year record stood at 2,728–2,573.

Nashville's next major league affiliate was to be the Oakland Athletics starting in 2015. The Sounds also adopted a new color scheme, set of logos, and uniforms before the season. The team hired sports design firm Brandiose to create their new visual identity. At one point, the firm was asked to explore new team nicknames which included "Platinums", "Picks", "DrumSticks", and "Roosters". Nashville chose to stick with the Sounds moniker, but initially elected to embrace a new color scheme that included Broadway Burnt Orange, Sunburst Tan, Neon Orange, and Cash Black. However, the team returned to the previous red and black color scheme, with the addition of platinum silver as an accent color, before the start of the season after receiving mixed feedback from team fans. The new logos incorporated elements that reflected Nashville's "Music City" nickname, such as guitars, picks, and sound holes, as well as neon signs such as those in the city's Broadway entertainment district.

The start of the 2015 season marked the first time that the Sounds played at the new downtown First Tennessee Park, which is located at the former site of the historic Sulphur Dell ballpark. The Sounds defeated the Colorado Springs Sky Sox, 3–2 in 10 innings, in the inaugural home opener in front of 10,459 people in attendance. Max Muncy secured the win with a walk-off RBI double, scoring Billy Burns from first base, before being mobbed by his Sounds teammates on the field. Under manager Steve Scarsone, Nashville ended their first season as an A's affiliate in third place with a 66–78 record. Barry Zito, who won the American League Cy Young Award in 2002, made his return to professional baseball with the Sounds for his final season in 2015 after sitting out the previous year. Zito was lauded by his Nashville teammates for embracing the Triple-A lifestyle and for his commitment to the team: charting pitches between starts, coaching first base, and even buying dinner for the team on his birthday.

In 2016, Steve Scarsone led the Sounds to a league-best 83–59 record and the American Conference Southern Division title, sending the team to the postseason for the first time since 2007. In a dramatic back-and-forth game five of the American Conference series at First Tennessee Park, the Sounds were eliminated by the Oklahoma City Dodgers, three games to two. Scarsone won the PCL Manager of the Year Award.

Pitchers Chris Smith, Sean Doolittle, Tucker Healy, and Simón Castro combined to pitch the franchise's seventh no-hitter on June 7, 2017, against the Omaha Storm Chasers, a 4–0 Nashville win. The team set First Tennessee Park's single-game attendance record on July 3 when 11,764 people watched the Sounds play to a 6–5 loss against Oklahoma City on the night of their Independence Day celebration. All-Star left fielder Renato Núñez, whose three-run home run propelled the PCL past the International League to win the 2017 Triple-A All-Star Game, was selected as the game's MVP. Joey Wendle set the franchise career record for doubles (102) from 2015 to 2017. The Sounds finished their 40th anniversary season in second place with a 68–71 record under manager Ryan Christenson.

From July 29 to August 14, 2018, Nashville matched their 1999 franchise-record 15-game winning streak during which they outscored opponents, 89–48. First baseman Nick Martini reached base safely in 66 consecutive games to set a new team record. They ended the season in second place with a 72–68 record under manager Fran Riordan. The Sounds declined to renew their expiring contract with the Athletics, choosing instead to seek a new major league affiliate. Through four seasons of competition as the top farm club of the Athletics, the Sounds had a 289–276 record, their best record among all affiliations. Through 41 seasons of play in Nashville, their all-time record stood at 3,017–2,849.

Nashville became the Triple-A affiliate of the Texas Rangers in 2019 in a player development contract that runs through 2022. The team also rebranded with new colors, logos, and uniforms which pull together elements from their original visual identity, the Nashville Vols who preceded them, the musical imagery present through their franchise history, and the flag of the state of Tennessee. The new colors, navy blue, red, and white, are modernized versions of the team's first colors. The primary logo is a pair of concentric red rings with the team name in navy between the two divided horizontally at its center by twin red and blue stripes; a navy "N" resembling the F-hole of a guitar or violin is in the inner ring, which is styled like a baseball.

The Sounds will host the Rangers at First Tennessee Park for an exhibition game on March 24, 2019. Nashville will also begin to participate in "Copa de la Diversión" ("Fun Cup"), an initiative by Minor League Baseball to connect teams with their local Hispanic communities, in which they adopt a culturally-relevant on-field persona for certain games. For "Copa" games, the Sounds will play as the "Vihuelas de Nashville". The vihuela, a high-pitched Mexican guitar popular with Mariachi groups, was chosen so as to reflect the city's musical ties.

Specific
General



</doc>
<doc id="57559085" url="https://en.wikipedia.org/wiki?curid=57559085" title="Ubinas">
Ubinas

Ubinas is a stratovolcano in the Moquegua Region of southern Peru, east of the city of Arequipa. Part of the Central Volcanic Zone of the Andes, it is above sea level. The volcano's summit is cut by a wide and deep caldera, which itself contains a smaller crater. Below the summit, Ubinas has the shape of an upwards-steepening cone with a prominent notch on the southern side. The gently sloping lower part of the volcano is also known as Ubinas I and the steeper upper part as Ubinas II; they represent different stages in the geologic history of Ubinas.

The most active volcano in Peru, Ubinas has a history of small- to moderate-sized explosive eruptions as well as larger eruptions such as in 1667, along with persistent degassing and ash emissions. Activity at the volcano began in the Pleistocene epoch, and led to the growth of the current mountain in two phases. Among the recent eruptions was the 2006–2007 event, which produced eruption columns and led to ash fall in the region, resulting in health issues and evacuations. During the most recent activity, from 2013 to 2017, a lava flow formed inside the crater, and further ash falls led to renewed evacuations of surrounding towns. In light of its activity, Ubinas is monitored by the Peruvian geological service INGEMMET, which has published a volcano hazard map for Ubinas and regular volcano activity reports.

The historian and geographer Mariano Felipe Paz Soldán relates the name Ubinas to two terms in two different languages. In the indigenous language Quechua, "uina" means "to stuff", "to fill", and "uiña" is translated as "to grow", "to increase". In Aymara "hupi" means "weep" or "murmur"; "hupina" is the genitive of "hupi". Local inhabitants believed that Ubinas was infested by demons and the souls of people who had fallen from God. The volcano is also known as Uvillas or Uvinas.

Ubinas lies in the Ubinas District of the General Sánchez Cerro Province, Moquegua Region of Peru, east of Arequipa in the Peruvian Western Cordillera. 

Ubinas, like other Peruvian volcanoes, belongs to the Central Volcanic Zone of the Andes. The Central Volcanic Zone is one of four volcanic belts in the Andes; the others are the Northern Volcanic Zone, the Southern Volcanic Zone and the Austral Volcanic Zone. The Central Volcanic Zone is long, and 69 of its volcanoes have been active in the Holocene epoch.

Peruvian volcanoes include stratovolcanoes, which are typically active for less than 500,000 years, long-lived clusters of lava domes and monogenetic volcanic fields. Historical eruptions have been recorded at seven Peruvian volcanoes: El Misti, Huaynaputina, Sabancaya, Ticsani, Tutupaca, Ubinas and Yucamane. The Ampato, Casiri, Chachani, Coropuna and Sara Sara volcanoes are dormant. 

Reaching an elevation of , Ubinas is a conical, truncated stratovolcano with upper slopes that reach angles of up to 45 degrees, and more gently sloping lower flanks. The more gently sloping lower part of the volcano is also known as Ubinas I and the steeper upper part as Ubinas II. The southern flank is cut by a noticeable notch, which is probably not an eruption vent and may have been formed by mudflows and rockslides. Due to weathering, the upper sector of the volcano has a worn appearance. Glacial valleys such as the Ubinas and Para valleys, and cirques and moraines down to and at the foot of the volcano, indicate that glaciers developed on Ubinas during the last glacial maximum. Other volcanic cones in the region all show heavy erosion by glaciation.

The volcano rises from a circular surface at the margin of a high plateau. Volcanic ash and some lava flows cover the terrain north and east of Ubinas. Four lava domes crop out around the volcano and may be related to it. The Ubinas and Para valleys border the volcano in its southeastern sector; the difference in elevation between the floor of the Ubinas valley and the plateau is about . The total volume of the mountain is estimated to be about . 

The summit of the volcano is an elliptical caldera wide and deep, formed by collapses of the summit and explosive eruptions. The caldera walls are made of lava flows bearing traces of hydrothermal alteration; the caldera floor is covered by lava flows and pyroclastic debris from explosive eruptions. It contains one or two ash cones with a triangle-shaped crater wide and deep; its walls are fractured and hydrothermally altered. Geophysical surveys have indicated the presence of an even larger buried caldera in Ubinas.

A debris avalanche on the southeastern flank reached a distance of from the volcano, and has left a collapse scar that is drained by the Volcanmayo River. This collapse took place early in the history of the volcano and removed a volume of about rocks from the mountain and underlying basement. Further collapses have occurred throughout the history of the volcano and until the Holocene epoch, including one collapse that has left a hummocky deposit on the southern flank. The sloping terrain that Ubinas is built on predisposes the mountain to south-directed landslides; future collapses in that direction are possible, with the heavily fractured southern flank of the caldera particularly at risk.

In the 1970s, an ephemeral crater lake appeared in the crater after wet seasons; another lake formed in 2016 after the crater floor was covered by the ongoing eruptions with impermeable material. Acid springs occur in the crater, and their water is capable of corroding silicon after a few hours' exposure. Lake Piscococha is located on the volcano's western foot, while the Para River and Sacuaya River flow past its eastern and southern slopes respectively. Other rivers on the slopes of Ubinas are the Quebrada Infiernillo on the southeastern, Volcanmayo River on the southern and Quebrada Postcone on the southwestern flank. The Sacuaya River becomes the Ubinas River and after confluence with the Para ends in the Tambo River which eventually flows into the Pacific Ocean; the Ubinas River valley is densely inhabited. 

Ubinas lies in the Salinas y Aguada Blanca National Reserve of Peru, which was founded in 1979. The town of Ubinas and the villages of Querapi, Tonohaya, Ubinas and Viscachani lie southeast, south, southeast and northwest of the volcano, respectively, and other villages in the area include Anascapa, Escacha, Huarina, Huatahua, Sacuaya, San Miguel and Tonohaya. In total about 5,000 people live within from the volcano. Agriculture and animal husbandry are the most important economic activities in these towns, agriculture prevailing at lower elevations. Water reservoirs and mining projects also exist in the wider region. Paved roads run along the northern and southern-southwestern foot of Ubinas, connecting towns close to the volcano to Arequipa.

Off the western coast of South America, the Nazca Plate subducts beneath the South American Plate at a rate of in the Peru-Chile Trench. This subduction process is responsible for the formation of the Andes and the Altiplano-Puna plateau within the last 25 million years, as well as for volcanism and earthquakes. The magma erupted by the volcanoes is formed by the partial melting of the mantle after fluids originating in the downgoing slab have altered the mantle; the magmas often undergo fractional crystallization and absorb crustal material.

Southern Peru has been affected by volcanic activity since the Ordovician and the Permian-Jurassic period, subduction-related volcanism becoming important from the Cretaceous onwards. Beginning 91 million years ago, several volcanic arcs have been active in southern Peru: from the Toquepala arc 91 45 million years ago over the Andahuaylas-Anta 4530 million years ago, the Huaylillas 2410 million years ago, the two Barroso arcs 101 million years ago, to the recent arc in the last million years. The switching between the volcanic arcs was accompanied by northeastward or southwestward shifts of the zone of main volcanic activity. Furthermore, there was little relief in the region before about 45 million years ago when major uplift commenced.

Ubinas, Ticsani and Huaynaputina form a group of volcanoes that extend in north-south direction north of the chain of volcanoes that make up the rest of the Central Volcanic Zone. These volcanoes have erupted rocks with similar geochemical traits and they are located around a graben occupied by the Rio Tambo; the marginal faults of this graben are the sites of the volcanic vents and probably acted as magma conduits. The magmas erupted by all three volcanoes appear to originate in a common magma chamber at depth, with seismic activity localized along the margins of the chamber. Aside from this deep reservoir, Ubinas also has a shallower magma chamber at depth. An underground magmatic connection between Ubinas and Huaynaputina was postulated already by Antonio Vázquez de Espinosa after the 1600 eruption of the latter volcano, which was the largest historical eruption in the Andes and had an enormous impact, including causing a cold summer in the northern hemisphere.

The basement of Ubinas consists of volcanic rocks of different ages. The oldest volcanic rocks of the Matalaque Volcanics date to the Late Cretaceous and crop out east and southeast of Ubinas, far away from the volcano. Most of the volcanics in proximity to Ubinas are the younger, Eocene-to-Oligocene Tacaza Group and the more restricted Miocene-Pliocene Barroso Group, which directly underlies the Ubinas mountain. Even older basement rocks include Paleoproterozoic plutons and the sedimentary Yura Group of Jurassic to Cretaceous age. A depression, whose margin is cut by landslide scars, cuts into the basement southeast of Ubinas and contains the Ubinas valley. Faults cut across the volcano and create unstable areas, especially in its southern sector, and NNW-SSE trending geological lineaments have influenced the stability and the hydrothermal system of Ubinas.

Andesite and dacite are the dominant components of the volcano, though its rocks have compositions ranging from basaltic andesite to rhyolite. The volcanic rocks form a potassium-rich calc-alkaline suite. Assimilation of crustal material and fractional crystallization are involved in the genesis of this magma suite. 

Lava composition has changed over time, dacites being represented mainly during the Ubinas II stage while stage I yielded mostly andesites. There is a trend for more recent volcanic events to produce more diverse rocks than the early eruptions, probably owing to a change in the magma supply regime; after 25,000–14,700 years ago magma supply increased and became more irregular. Otherwise, the magma supply rate at Ubinas amounts to about , with an average rate of .

Ubinas started to develop in the middle and late Pleistocene epoch. The oldest pre-Ubinas volcanics crop out north and south of the volcano and include the volcanoes Parhuane Grande and Parhuane Chico directly to the north. Volcanic activity started after a change in regional tectonics, which may have triggered the formation of magma chambers. The volcano developed in two phases, Ubinas I and Ubinas II: Ubinas I is represented by lava flows at the foot of the volcano and debris and ignimbrite deposits in the south and southeast of Ubinas, and it forms a shield. It was later cut on its southern side by a debris avalanche that probably occurred over 376,000 years ago. The last activity of Ubinas I generated more than four units of pyroclastic flows, with a total volume of about , and possibly an old caldera before 261,000 ± 10,000 years ago.

Ubinas II is steeper and rises above the Ubinas I shield. It consists mainly of thick lava flows but also several lava domes with accompanying block-and-ash flows, all of which formed between 261,000 ± 10,000 and 142,000 ± 30,000 years ago. A lack of more recent volcanic outcrops suggests a period of dormancy lasting until 25,000–14,700 years ago during which glaciation took place on the volcano.

Reactivated volcanic activity started between 25,000 and 14,700 years ago and led to the emplacement of ash flows, pumice layers and tephra from phreatomagmatic and explosive eruptions, with deposits having thicknesses ranging from in many places. The total volume of each eruption deposit ranges from and they crop out as far as from Ubinas. It is likely that the summit caldera formed during this time period, before 9,700 years ago.

Over the last 7,500 years, volcanic activity has been characterized mainly by various kinds of explosive eruptions. These eruptions have expelled less than of material each time and left widespread deposits of ash, volcanic blocks and lapilli. A Plinian eruption occurred 980 ± 60 years BP and expelled of pumice and tephra, which has formed a deposit with five separate layers of pumice, ash and lapilli. 

More eruptions identified by tephrochronology took place 7,480 ± 40, 11,280 ± 70, 11,480 ± 220 and 14,690 ± 200 years ago, yielding scoria and pyroclastic flows. The various explosive eruptions of Ubinas have deposited material as far as away from the volcano. Landslides also took place in this time, including the collapse more than 3,670 ± 60 years ago.

Ubinas is the most active volcano in Peru; small explosive eruptions have occurred since the 16th century at an average rate of one eruption every twenty to thirty-three years. Events are recorded from 1550, 1599, 1600, 1662, 1667, 1677, 1778, 1784, 1826, 1830, 1862, 1865, 1867, 1869, 1906, 1907, 1912, 1923, 1936, 1937, 1951, 1956, 1969, 1996, 2006–2009, 2013–2016 and 2016–2017. Most of these eruptions consisted of emissions of ash and gas, sometimes accompanied by explosions, while more intense events such as in 1667 also produced scoria falls and pyroclastic flows. The 1667 eruption was the largest in historical time, producing about of scoria and reaching a volcanic explosivity index of 3. These eruptions have damaged communities around the volcano, and occasionally caused epidemics and human and cattle fatalities resulting from the ingestion of ash. 

Aside from regular eruptions, there are fumarolic-seismic events such as in 1995–1996 when sulfur dioxide and water vapour, emitted at temperatures of up to , formed clouds that rose over above the crater. Ubinas persistently emanates smoke, and lahars which have damaged fields, irrigation canals and paths have been recorded, such as the 2016 lahars caused by early 2016 precipitation events which mobilized ash that had fallen over the previous years. These lahars destroyed local water supplies and left the Matalaque and Ubinas districts temporarily isolated.

The first episode of the 2006–2007 eruption sequence involved the ejection of large volcanic bombs at high speed and the emission of small quantities of ash. Gas and ash columns were emitted between April and October of 2006 and reached heights of about . Volcanically induced melting of snow that had fallen on the summit during the 2006–2007 summer induced a mudflow in January 2007 that descended into the Ubinas River valley. Volcanic activity - degassing and of Vulcanian eruptions - decreased until late 2009. This eruption was probably triggered by the entry of fresh magma in the magma plumbing system and the subsequent interaction of ascending magmas with the hydrothermal system of Ubinas.

Despite its record of activity, Ubinas was essentially unmonitored before the 2006 event: the inhabitants of the area were largely unaware of volcanic hazards, and there were no emergency plans for future eruptions available. A "Scientific Committee" was formed on March 30, 2006 to remedy these issues. A region of about was hit by the effects of the eruption. The ash fall from the eruption caused health problems and disrupted pastures and agriculture in the region around the volcano, resulting in about of damage and the flight of local residents to Arequipa and Moquegua. The village of Querapi on the southern flank was temporarily evacuated to a location with shelters farther south, and two shelters were designated in low risk areas around the volcano, one at Anascapa and the other at Chacchagen, away from Matalaque. Furthermore, Lake Salinas, an important source of water in the region, was threatened by the eruption.

A new eruption period started on 2 September 2013 with a phreatic explosion, which was followed by more events in the next few days. Strong but variable seismic activity, the observation of thermal anomalies in satellite images and the exhalation of gases characterized this eruption period. Lava effusion began in the summit crater in 2014 and increased after the 2014 Iquique earthquake, culminating in an explosive eruption on April 19, 2014. Volcanic activity decreased afterwards until September 2014. The eruptions were accompanied by earthquakes, rumbling noises from the volcano, ash fall and the forceful ejection of large blocks. In light of the volcanic activity, Peru declared a state of emergency in August 2013 and evacuated the village of Querapi at Ubinas, whose population returned in 2016; The evacuation of the town of Ubinas was also considered.

After these events, in 2015–2017 the volcano often persistently released ash and gas, accompanied by earthquakes as well as occasional explosions and eruption columns. In April 2015, for example, activity at Ubinas led to a declaration of emergency for the districts surrounding the volcano, then in September of the same year an eruption generated a eruption column that produced ash fall in the region, leading to evacuations.

Hazards stemming from volcanic activity at Ubinas are mainly the fallout from explosive eruptions, lahars of different origins, large landslides that can generate debris flows and pyroclastic flows. Small explosive eruptions are the most likely occurrences at Ubinas, while large Plinian eruptions are considerably less likely. The area of the cone itself is the area most likely to be affected by volcanic phenomena, while pyroclastic flows and lahars are a danger for the valleys that drain Ubinas in southeastern direction and landslides are hazardous for an area of the southern flank. The town closest to the volcano is only away from Ubinas. Large Plinian eruptions could have effects on the city of Arequipa.

The Peruvian INGEMMET geological service monitors the seismic activity, any deformation of the mountain, and hot spring and gas composition at Ubinas. It regularly publishes a report on the activity of Ubinas. Hazard maps were created during the 2006 eruptive event to show the relative risk in various locations around the volcano, which is graded in a three-zone scheme with one high-risk, one intermediate-risk and one low-risk zone. A contingency map was created to show and explain the procedures to follow in case of various eruption scenarios. Both maps were widely disseminated after publication to aid in the response to future eruptions.

Fumaroles are active at the bottom of the inner crater, with about five separate fumarole areas identified within the crater before the 2006 eruption. In 1997, a gas cloud from the fumaroles filled the entire caldera during the nights. Fumarolic activity and degassing is limited to the crater; there is no evidence of such gas exhalations elsewhere on the volcano. Ubinas is a major source of volcanic carbon dioxide and sulfur dioxide in the atmosphere of Earth, producing at a rate of about . Reportedly, the sound of the fumaroles can be heard from the village of Ubinas.

The fumaroles along with the spontaneous potential of Ubinas indicate that the volcano hosts an active hydrothermal system below the caldera. Two hot springs are found on the southeastern slopes of Ubinas; these are known as Ubinas Termal and Ubinas Fria and both lie at elevation, the waters flowing out of the springs have temperatures of and respectively. The composition of the waters in these springs indicate that they originate from the mixing of deep saline water, fresh water and volcanic fluids. These hot springs and others in the region are considered to be part of a geothermal province known as "Ubinas" which also includes El Misti, and which deliver large amounts of dissolved minerals including arsenic to the local rivers.

The climate of the area changes with elevation. The summit of Ubinas has a cold climate with temperatures frequently falling below ; at lower elevations temperatures can exceed during daytime, but night frosts are still possible. The region is arid overall, but during the summer wet season rainfall can cause landslides at lower elevation, and the upper parts of the volcano including the caldera can receive a snow cover. Weather data are available for the town of Ubinas at elevation: the average temperature is and the average annual precipitation is about . The present-day snowline exceeds elevation, but during the Pleistocene epoch it descended to about .

Vegetation at elevation consists of grassland, bushes and low trees such as "Buddleja coriacea", "Escallonia myrtilloides", "Polylepis besserii" and "Ribes brachybothrys" forming a shrub vegetation in valleys. Farther up, between lies a vegetation form called "pajonal", which consists of creeping plants, grasses and shrubs made up of high Andean vegetation. Small lakes and areas of waterlogged soil form wetlands called "bofedales", in which aquatic plants and rosette-forming plants grow; both "bofedales" and "pajonal" also feature cushion plants. The upper sector of Ubinas is vegetation-free. Animal species have been described mainly in the context of the National Reserve; they include various birds and camelids such as alpacas, guanacos, llamas and vicuñas.

Sulfur deposits in the crater of Ubinas were considered among the most important sulfur deposits in Peru and were mined in the 19th century. Ubinas has been considered a potential place for geothermal energy production.




</doc>
<doc id="57810862" url="https://en.wikipedia.org/wiki?curid=57810862" title="Wōdejebato">
Wōdejebato

Wōdejebato (formerly known as Sylvania) is a Cretaceous guyot or tablemount in the northern Marshall Islands, Pacific Ocean. Wōdejebato is probably a shield volcano and is connected through a submarine ridge to the smaller Pikinni Atoll southeast of the guyot; unlike Wōdejebato, Pikinni rises above sea level. The seamount rises for to depth and is formed by basaltic rocks. The name Wōdejebato refers to a sea god of Pikinni.

It was probably formed by a hotspot in what is present-day French Polynesia before plate tectonics moved it to its present-day location. The Macdonald, Rarotonga, Rurutu and Society hotspots may have been involved in its formation. The first volcanic phase took place in the Cenomanian and was followed by the formation of a carbonate platform that quickly disappeared below the sea. A second volcanic episode between 85 and 78.4 million years ago (in the Campanian) led to the formation of an island. This island was eventually eroded and rudist reefs generated an atoll or atoll-like structure, covering the former island with carbonates and thus a second carbonate platform.

The second carbonate platform drowned about 68 million years ago (in the Maastrichtian), perhaps because at that time it was moving through the equatorial area which may have been too hot or too nutrient-rich to support the growth of a coral reef. Thermal subsidence lowered the drowned seamount to its present depth. After a hiatus, sedimentation commenced on the seamount and led to the deposition of manganese crusts and pelagic sediments, some of which were later modified by phosphate.

Wōdejebato is also written as Wodejebato. The name of the seamount comes from Wōdejebato, the name of the most feared and respected sea god of Pikinni Atoll. Wōdejebato was formerly called Sylvania, after the , a ship which was involved in its first mapping in 1946. The seamount was discovered in 1944, and was first investigated, using mainly seismic data, during Operation Crossroads (a nuclear bomb test). Later, several times rocks were dredged from the seamount and drill cores were taken; cores 873–877 of the Ocean Drilling Program are from Wōdejebato.

Wōdejebato lies within the Ralik Chain of islands and seamounts in the northern Marshall Islands, which consist of about three northwest-trending groups of islands of volcanic origin. Pikinni Atoll (formerly named Bikini) is located about southeast of the seamount.
The seamount lies at a depth of and is about long with a flat top that narrows southeastward from over to less than . The surface of the flat top slopes inward and is covered by small depressions and knobs with an average relief of about as well as ripple marks. The flat top is surrounded by a ridge, which has a width of and an average height of . On its northern and northeastern side, this ridge is in turn surrounded by another wide slightly raised ridge. The flat top has been interpreted as a lagoon surrounded by reefs which form the inner ridge; the outer ridge appears to be a pile of skeletal sand rather than a reef and may be a spit formed by reworked material. Small mounds, probably of biological origin, are found at the margins of the seamount.

The seamount is high above the sea floor and has an irregular shape, with spurs projecting from its circumference. These spurs have widths of and surface features that are distinct from those on the main flat top. The spurs appear to be rift zones, similar to these formed on Hawaii by dyke injection although some of the ridges at Wōdejebato may have a different origin. Wōdejebato appears to have four such ridges, which is more than is observed at Hawaii. One explanation is that the northwestern ridge is another seamount; another that Wōdejebato consists of more than one volcano although the relatively small size of the seamount would argue against this view. Wōdejebato's slopes descend rather steeply until, at depth, where they become more gentle, they are decorated with forms resembling cones and channels. Part of its southern flank, where there is a downdropped terrace, seems to have collapsed in the past. Another satellite volcanic cone lies north of Wōdejebato at a depth of . Wōdejebato contains a volcanic structure within a superficial sediment cap, and a free-air gravity anomaly has been observed on the seamount.

Wōdejebato is connected to Pikinni by a wide, long and high submarine ridge and both volcanoes share a pedestal; Wōdejebato is the bigger of the two and its flat top has a larger surface than Pikinni's. Magnetic anomalies are also found on both volcanoes, with Wōdejebato featuring the more extensive one. Debris from these two volcanoes has formed an apron on their southwestern foot that is up to thick. The seafloor beneath Wōdejebato was formed during the Jurassic Quiet Zone over 156.9 million years ago. Farther north from Wōdejebato lies Lōjabōn-Bar seamount, and Look Guyot is due east. Wōdejebato appears to be one source of turbidites in the Nauru Basin.

The Pacific Ocean seafloor, especially the Mesozoic seafloor, contains most of the world's guyots (also known as tablemounts). These are submarine mountains which are characterized by steep slopes, a flat top and usually the presence of corals and carbonate platforms. While there are some differences to present-day reef systems, many of these seamounts were formerly atolls. Some atolls still exist, for example at Pikinni. All these structures originally formed as volcanoes in the Mesozoic ocean. Fringing reefs may have developed on the volcanoes, which then became barrier reefs as the volcano subsided and turned into an atoll. The crust underneath these seamounts tends to subside as it cools, and thus the islands and seamounts sink. Continued subsidence balanced by upward growth of the reefs led to the formation of thick carbonate platforms. Sometimes volcanic activity continued even after the formation of the atoll or atoll-like structure, and during episodes where the carbonate platforms rose above sea level, erosional features such as channels and blue holes developed.

The formation of many such seamounts has been explained with the hotspot theory, which describes the formation of chains of volcanoes which get progressively older along the length of the chain, with an active volcano only at one end of the system. Seamounts and islands in the Marshall Islands do not appear to have originated from such simple age-progressive hotspot volcanism as the age progressions in the individual island and seamount chains are often inconsistent with a hotspot origin. One explanation for this contradiction may be that more than one hotspot passed through the Marshall Islands, and it is also possible that hotspot volcanism was affected by extensional deformation of the lithosphere. In the case of Wōdejebato, candidate present-day hotspots are the Macdonald hotspot which passed close to the seamount during the Aptian and Albian ages, between 115 and 94 million years ago in the early Cretaceous, and the Society hotspot and Rarotonga hotspot which approached the seamount in the late Cretaceous 85-80 million years ago, both time periods where volcanism occurred on Wōdejebato. A third hotspot which interacted with Wōdejebato is the Rurutu hotspot. The last two are the hotspots most likely to be long-lived, while many others, such as the Marquesas hotspot, were probably active discontinuously or only for brief time intervals.

Based on plate motion reconstructions, the region of the Marshall Islands was located in the region of present-day French Polynesia during the time of active volcanism. Both regions have numerous island chains, anomalously shallow ocean floors and the presence of volcanoes. About eight hotspots have generated a large number of islands and seamounts in that region, with disparate geochemistries.

The rocks at Wōdejebato include basalt, breccia, carbonates, clay, claystone, limestone, manganese, manganese phosphate, peloid, shale and tuff; with an unusually large amount of pyroclastic rocks present. Organic material such as kerogen, peat and woody material has also been found. Ferromanganese crusts have been found on the seamount. The crusts are composed of asbolane, birnessite and buserite and contain iron and cobalt. Wōdejebato has been evaluated as a possible mining site for its mineral deposits. 

The limestones appear in several forms such as floatstone, grainstone, micrite, packstone, peloid and wackestone. Some grainstones and rudstones appear to be derived from algal and animal fossils. Many carbonate rocks have been altered, for example by cementation and leaching of their components and the dissolution of aragonite; in some samples up to half of all the rock has been altered. These processes are collectively known as diagenesis.

Basalts at Wōdejebato mostly form an alkali basalt suite but also include ankaramite and hawaiite. The rocks contain clinopyroxene, olivine, plagioclase and pyroxene phenocrysts. Alteration has led to the formation of calcite, chabazite, chlorite, hydromica, pyrite, serpentine and smectite, and gaps and cavities in the rock have been filled by sediments. The element geochemistry of lavas from Wōdejebato resembles that of South Central Pacific islands such as Marotiri and Rarotonga and is consistent with magma sources of intraplate volcanism. Isotope ratios show affinities to those of volcanic rocks from the Macdonald, Rurutu, Rarotonga and Society hotspots; differences between isotope ratios of various stages of volcanism may reflect the passage of Wōdejebato over more than one "plumelet".

Wōdejebato formed either before or during the Santonian age (86.3 ± 0.5 – 83.6 ± 0.2 million years ago), with the Albian age (about 113 to 100.5 million years ago) being a likely candidate. Wōdejebato originated in the Southern Hemisphere and was moved by plate tectonics into the Northern Hemisphere, and paleomagnetism indicates that the seamount was located at 10 degrees southern latitude when the most recent lavas erupted. It subsequently underwent several episodes of uplift and subsidence and eventually drowned, forming the present-day seamount. Ruwitūntūn is another seamount in the Marshall Islands with a similar history.

Volcanism at Wōdejebato appears to have occurred during two phases over a timespan of about 20 million years. The first phase took place during the Cenomanian (100.5 – 93.9 million years ago); it was characterized by explosive eruptions and may be the source of 93.9–96.3 million year old volcanic debris found in the surroundings of Wōdejebato. The second phase occurred during the Campanian between 78.4 and 85 million years ago during chron 33R; it appears to be part of a volcanic event that affected a number of other islands and seamounts in the Marshall Islands and at Wōdejebato lasted for at least four million years. The second stage appears to have been a secondary volcanic episode. Volcanic rocks sampled at Wōdejebato all belong to the second stage, probably due to sampling bias as the samples all come from the summit region. Tectonic evidence indicates that Pikinni formed at the same time as Wōdejebato, while the northern parasitic cone may be less than 80 million years old. An earlier proposal by Schlanger "et al." 1987 envisaged Eocene (56 – 33.9 million years ago) eruptions at Wōdejebato but today the older ages are considered to be correct.

The volcanic activity produced breccia and lava flows, probably first generating a shield volcano. Volcanic activity occurred both in shallow water and submarine forming hyaloclastite and highly vesicular rocks during phreatomagmatic eruptions, and above sea level as indicated by the presence of basaltic pebbles. Some early volcanic deposits were buried by later activity. There are conflicting reports about whether hydrothermal activity took place. Vegetation including ferns and fungi grew on the exposed island during the Campanian, leaving abundant wood remnants. Weathering of basaltic rocks produced clay sediments and soils thick have been obtained in drill cores.

After volcanic activity ceased, environmental processes transformed Wōdejebato into a flat-topped platform, equivalent to a present-day atoll, as the crust beneath Wōdejebato seamount subsided. Erosion and subsidence lowered the volcanic pile until seawater flooded it and marine sedimentation commenced. This platform phase lasted only about 10 million years and took place in at least two stages, in line with the generally short duration of such platform phases; they do not generally last longer than 20 million years. The growth of the platform was not continuous and was probably interrupted by one drowning event between the Albian and Campanian ages, similar to other seamounts in the Pacific Ocean which also drowned during this time.

Limestones and carbonates forming a platform accumulated on Wōdejebato, with drill cores showing total thicknesses of . Compositionally, it consists mainly of sandy carbonates that are often leached and cemented by calcitic material. These deposits eventually covered the entire upper area of the volcanic high and formed the inner ridge. Variations in sea level occasionally led to parts of the platform either emerging above sea level or submerging, leading to erosion that generated the outer ridge and to the development of characteristic sequences within the deposits.

Such carbonate platforms look like present-day atolls but unlike the biogenic frameworks of modern atolls they were formed by biogenic sediments; at Wōdejebato sandy shoals appear to have been a principal component. These carbonate deposits would then have been surrounded by a barrier reef and the redeposition, followed by stabilization, of eroded material had a role in the development of the surrounding rim. Reef mounds grew to several tens of metres in height. Foraminiferal fossil data imply that lagoonal environments existed on Wōdejebato. The central part of the guyot surface and its margins feature different platform structures, and the platform has been subdivided into several different assemblages on the basis of foraminifera stages.

Environmental conditions on the platform were characterized by tropical influences. Wōdejebato was probably located in equatorial waters with temperatures likely exceeding , with temperature ranges of during the Maastrichtian. The platform was sometimes affected by storms that reworked the rock material. Soil properties imply that precipitation on Wōdejebato was less than , but erosion by precipitation water and dissolution of parts of the carbonate platform have been inferred from dissolution traces in the rocks. Sea level variations induced the formation of step-like reef tracts on Wōdejebato's carbonate platform.

Much of the reefbuilding was carried out by corals, rudists and stromatoporoids. Unlike present-day coral reefs, reef building in the Cretaceous was carried out mainly by rudists which probably started appearing at Wōdejebato in the Albian; rudist taxa active at Wōdejebato included caprinids and radiolitids, such as "Antillocaprina", "Coralliochama", "Distefanella", "Mitrocaprina" and "Plagioptychus".

Furthermore, benthic foraminifers were active from the Campanian to the Maastrichtian; they include "Asterorbis", "Pseudorbitoides trechmanni", "Omphalocyclus macroporus" and "Sulcoperculina" as well as other discorbids, lituolids, miliolids, opthalmiids, orbitoids, peneroplids, placopsilinids, rotaliids and textulariids.

Other lifeforms that were fossilized in the carbonate reefs were algae including green algae (codiaceans and dasycladaceans) and red algae (corallinaceans, peyseonneliaceans and solenoporaceans); some algae formed rhodoliths. In addition there were bivalves (inoceramids and pycnodonts), bryozoans, corals, gastropods, echinoderms, echinoids, ostracods and sponges.

It is likely that Wōdejebato drowned during the Maastrichtian age around 68 million years ago, probably accompanied by a sea level rise of about . Before the terminal drowning, Wōdejebato's carbonate platform emerged from the sea, leading to the development of karst features; two separate emersion events took place 68 and 71 million years ago.

Sea level rise on its own probably does not explain the drowning. Various paleoenvironmental stressors have been invoked to explain the drowning such as short-term climate fluctuations during the Maastrichtian and the passage of the seamount through the equatorial upwelling zone. The water in this region may have been too hot for the reef to survive: Other guyots in the Pacific Ocean such as Limalok, Lo-En and Takuyo-Daisan also drowned when they were within ten degrees from the equator on the Southern Hemisphere, implying that this region of the Pacific Ocean was in some way harmful to shallow water reefs. The subsidence that occurred after Wōdejebato moved away from the influence of the Rurutu hotspot may have also played a role. Pikinni was probably higher than Wōdejebato at this time and hence escaped drowning.

After the drowning had taken place, thermal subsidence of the crust beneath Wōdejebato occurring at a rate of lowered the platform of Wōdejebato to a depth of about below sea level. Between the Maastrichtian and the Eocene, manganese crusts formed on the exposed limestones and gravels formed by erosion; in turn they were subject to alteration processes such as phosphatization during three different episodes in the Eocene.

Approximately 40 million years passed between the drowning and subsequent deposition events. Pelagic sedimentation took place, which formed an ooze consisting of foraminiferal and nannofossil deposits between the Miocene and Pleistocene, with a Miocene unconformity. In one drill core, this sediment layer is thick. Currents affected mid- to late Pleistocene sedimentation. Among the foraminifera deposited here are "Florisphaera", "Gephyrocapsa", "Globigerina", "Globorotalia", "Helicosphaera", "Pseudoemiliania" and potentially "Sphaeroidinella" species. Foraminifera taken from Wōdejebato usually belong to pelagic species. Ostracods have also been identified; common taxa are cytherurids as well as "Bradleya", "Cytheralison" and "Krithe" species.

Presently, Wōdejebato lies below the thermocline and the temperature of the water washing over the seamount is about . Circumstantial evidence indicates that deep seawater dissolved large amounts of carbonate rocks including aragonite after Wōdejebato was submerged; the seamount is located below the aragonite saturation depth and that causes the aragonite to dissolve. Some of the dissolved aragonite has precipitated again in the form of calcite, and sediments have partially filled cavities within the carbonate rocks.



</doc>
<doc id="57842148" url="https://en.wikipedia.org/wiki?curid=57842148" title="Packers sweep">
Packers sweep

The Packers sweep, also known as the Lombardi sweep, is an American football play popularized by Green Bay Packers coach Vince Lombardi. The Packers sweep is based on the sweep, a football play that involves a back taking a handoff and running parallel to the line of scrimmage before turning upfield behind lead blockers. The play became noteworthy due to its extensive use by the Packers in the 1960s, when the team won five National Football League (NFL) Championships, as well as the first two Super Bowls. Lombardi used the play as the foundation on which the rest of the team's offensive game plan was built. The dominance of the play, as well as the sustained success of Lombardi's teams in the 1960s, solidified the Packers sweep's reputation as one of the most famous football plays in history.

The Packers sweep is a variation on the sweep, which is a basic running play in American football. A sweep play involves a back, typically the halfback or running back, taking a pitch or handoff from the quarterback and running parallel to the line of scrimmage. This allows the offensive linemen (usually the guards) and the fullback to block defenders before the runner turns upfield. The sweep can be run out of multiple formations and go either left or right of the center. It is characterized as power football and usually gives the runner the choice to follow the lead blockers inside or outside, depending on how the defense reacts. Various options and changes to the sweep have been implemented to create further deception. These include running option pass plays out of the same formation, changing which blockers pull from the line of scrimmage, and running the play towards different areas of the field. 

The development of what became known as the Packers sweep, also known as the Lombardi sweep, began with Vince Lombardi. He played football at Fordham University on a football scholarship, and was part of the "Seven Blocks of Granite", a nickname for the team's offensive line. This was the first time Lombardi witnessed the success of the sweep. Jock Sutherland's University of Pittsburgh teams used the sweep extensively against Lombardi's team in an era when the single-wing formation was used almost universally. In 1939, after graduation, Lombardi began his coaching career as an assistant at St. Cecilia High School in Englewood, New Jersey. He was promoted to head coach and over eight seasons led St. Cecilia's to multiple championships. With a 32-game unbeaten streak, the school had one of the top high school football programs in the nation. Lombardi attended coaching clinics during this time, where he continued to develop a better understanding of the sweep, especially the techniques of pulling offensive linemen and having the ball carriers cut back towards openings in the line. He moved on from high school to college football as an assistant under Earl "Red" Blaik at West Point in 1948. For five seasons Lombardi served as an assistant coach and further developed his coaching abilities. Blaik's emphasis on players executing their job and the military discipline of West Point greatly influenced Lombardi's future coaching style.

Lombardi's first NFL coaching job came in 1954, when he accepted an assistant coaching job (now known as an offensive coordinator) for the New York Giants. It was with the Giants that Lombardi first implemented the principles that became the Packers sweep. He started to run the sweep using the T formation and positioned his linemen with greater space between each other. He also had offensive tackles pull from the line and implemented an early variant of zone blocking (blockers are expected to block a "zone" instead of an individual defender); this required the ball carrier to run the football wherever there was space. The phrase "running to daylight" was later coined to describe the freedom the ball carrier had to choose where to run the play. Under his offensive leadership and assisted by his defensive counterpart Tom Landry, Lombardi helped guide the Giants to an NFL Championship in 1956. They appeared again in the 1958 Championship Game, this time losing in overtime to the Baltimore Colts. In 1959, Lombardi accepted a head coaching and general manager position with the struggling Green Bay Packers. The Packers had just completed their worst season in team history with a record of 1–10–1. Even though the Packers had not been successful for a number of years, Lombardi inherited a team in which five players would go on to be Pro Football Hall of Famers. He immediately instituted a rigorous training routine, implemented a strict code of conduct, and demanded the team continually strive for perfection in everything they did.

The first play Lombardi taught his team after he arrived in Green Bay was the sweep. He moved Paul Hornung to the halfback position permanently (in the past he had been poorly utilized in different back positions) and made him the primary ball-carrier for the sweep. The Packers sweep, as it became known, was the team's lead play and the foundation on which the rest of the offensive plan was built. For the team to succeed, Lombardi drilled them constantly on the play, expecting it to be executed perfectly every time (it was common for the team to run the play at the beginning and end of every practice). The play became the epitome of Lombardi's philosophy: a simple, fundamentally sound play that was reliant on the entire team working together to move the ball.

Even though each player had a role to perform, the execution of the center, the pulling guards, and the halfback were essential to the play's success. The center had to cut off the defensive tackle or middle linebacker to prevent the defender from breaking up the play behind the line of scrimmage. This was due to the right guard (when the play was run to the right side of the field), who would vacate this space while pulling to lead the ball carrier. The most difficult block fell on the left guard, who had to pull the whole way across the field to be the lead blocker. The left guard also had to decide, based on how the defense reacted, whether to push the play to the inside or outside of the tight end. The ball carrier, usually the halfback, then decided whether to go inside or outside as well. The fullback, tight end, and left tackle also had essential blocks that dictated the success of the play.

For nine seasons Lombardi ran the Packers sweep with great success, with one estimate claiming the play gained an average of 8.3 yards each time it was run in the first three seasons under Lombardi. Overall though, the play was known as gaining "four-yards-and-a-cloud-of-dust" that would allow the Packers to control the game clock, slowly moving the ball down the field and exhausting the defense.

Even when defenses shifted to try to stop it, Lombardi would either attack other weaknesses or would run variations of the sweep. Tom Landry, as head coach of the Dallas Cowboys, had his defense linemen "flex" (line up in an offset position) to prevent the runner from finding the cutback lanes that were essential to the success of the sweep. In response to Landry's flex defense, Lombardi would run other types of running plays attacking the new positions of the defensive linemen. Lombardi would also counter other defensive adjustments by running the sweep to the left side, having various blockers not pull, switching the ball carrier, or running option pass plays—each of which could be run out the sweep formation.

Other coaches in the league had great respect for the Packers sweep, although most acknowledged the success of the play was based on two criteria: great players and perfect execution. During his tenure, Lombardi had three offensive linemen (Jim Ringo, Forrest Gregg, and Jerry Kramer), two backs (Hornung and Jim Taylor), and one quarterback (Bart Starr) who were later elected to the Pro Football Hall of Fame. Each of those offensive players was instrumental to the success of the Packers sweep and thus the offense. Ringo, Gregg, Kramer, and Taylor each provided key blocks for Hornung to run the sweep. Starr (who as the quarterback orchestrated the play) and Taylor were essential to variations of the sweep that called for different runners or option pass plays.

In addition to the Hall of Famers, Lombardi's teams included other highly decorated players, such as first-team All-Pro Fuzzy Thurston, the left guard who had the most challenging blocking assignment in the sweep. Many of these players identified Lombardi's coaching and drive for perfection as important factors behind their accomplishments and the team's success, acknowledging that perhaps it was Lombardi's coaching of the sweep and other plays that helped the players achieve Hall of Fame status, not just that he happened to have "great players" that made the sweep so effective. 

At its core, the Packers sweep was a simple play that relied on all members of the team precisely executing their responsibilities. This level of teamwork, coordination, and execution epitomized the Packers of the 1960s under Lombardi. In nine seasons at the helm, Lombardi and his sweep led the Packers to five NFL championships, as well as victories in Super Bowl I and II. The team won three straight championships in 1965, 1966, and 1967—only the second team to accomplish this feat (the other being the 1929, 1930, and 1931 Packers).

Five offensive players who played under Lombardi in the 1960s were later elected to the Pro Football Hall of Fame; Lombardi was elected shortly after his death in 1970. Three of these Packers (Hornung, Starr, and Taylor) won NFL MVP awards in the 1960s. Much of the Packers' offensive success was based on the threat of running the sweep. Lombardi exploited the dominance of the play to take advantage of defenses and run the offense to his team's strengths. This sustained success established the Packers sweep as one of the most famous football plays in history.



</doc>
<doc id="57871368" url="https://en.wikipedia.org/wiki?curid=57871368" title="Limalok">
Limalok

Limalok (formerly known as Harrie or Harriet) is a Cretaceous-Paleocene guyot/tablemount in the southeastern Marshall Islands, one of a number of seamounts (a type of underwater volcanic mountain) in the Pacific Ocean. It was probably formed by a volcanic hotspot in present-day French Polynesia. Limalok lies southeast of Mili Atoll and Knox Atoll, which rise above sea level, and is joined to each of them through a volcanic ridge. It is located at a depth of and has a summit platform with an area of .

Limalok is formed by basaltic rocks and was probably a shield volcano at first; the Macdonald, Rarotonga, Rurutu and Society hotspots may have been involved in its formation. After volcanic activity ceased, the volcano was eroded and thereby flattened, and a carbonate platform formed on it during the Paleocene and Eocene. These carbonates were chiefly produced by red algae, forming an atoll or atoll-like structure with reefs.

The platform sank below sea level 48 ± 2 million years ago during the Eocene, perhaps because it moved through the equatorial area, which was too hot or nutrient-rich to support the growth of a coral reef. Thermal subsidence lowered the drowned seamount to its present depth. After a hiatus lasting into the Miocene, sedimentation commenced on the seamount leading to the deposition of manganese crusts and pelagic sediments; phosphate accumulated in some sediments over time.

Limalok was formerly known as Harrie Guyot and is also known as Harriet Guyot; Limalok refers to a traditional chieftess of Mile Atoll. Limalok is one of the seamounts targeted during the Ocean Drilling Program, which was a research program that aimed at elucidating the geological history of the sea by obtaining drill cores from the oceans. The proportion of material recovered during the drilling was low, making it difficult to reconstruct the geologic history of Limalok.

Limalok lies at the southernmost end of the Ratak Chain in the southeastern Marshall Islands in the western Pacific Ocean. Mili Atoll is located from Limalok, with Knox Atoll in between the two.

The relatively small seamount rises from a depth of to a minimum depth of below sea level. The top of Limalok is long and broadens southeastward from less than to more than , forming a summit platform. The carbonate platform of Limalok crops out at the edges of the summit plateau. Wide terraces and numerous fault blocks surround the summit plateau; some of the latter may have formed after the carbonate platform ceased growing.

Mili Atoll and Limalok emerge from a common pedestal and are connected by a ridge at depth. The seafloor is 152158 million years old, but it is possible that Limalok rises from Cretaceous flood basalts rather than the seafloor itself. Volcanic sediments in the Eastern Mariana Basin may come from this seamount.

The Pacific Ocean seafloor, especially the parts that are of Mesozoic age, contains most of the world's guyots (also known as tablemounts). These are submarine mountains which are characterized by steep slopes, a flat top and usually the presence of corals and carbonate platforms. These structures originally formed as volcanoes in the Mesozoic Ocean. Fringing reefs may have developed on the volcanoes, which then were replaced by barrier reefs as the volcanoes subsided and turned into atolls. Continued subsidence balanced by upward growth of the reefs led to the formation of thick carbonate platforms. Volcanic activity can occur even after the formation of the atoll or atoll-like landforms, and during episodes where the platforms were lifted above sea level, erosional features such as channels and blue holes developed. The crust underneath these seamounts tends to subside as it cools and thus the islands and seamounts sink.

The formation of many seamounts including Limalok has been explained with the hotspot theory, in which a "hot spot" rising from the mantle leads to the formation of chains of volcanoes which get progressively older along the length of the chain, with an active volcano only at one end of the system, as the plate moves over the hotspot. Seamounts and islands in the Marshall Islands do not appear to have originated from simple age-progressive hotspot volcanism as the age progressions in the individual island and seamount chains are often inconsistent with this explanation. One solution to this dilemma may be that more than one hotspot passed through the Marshall Islands, and it is also possible that hotspot volcanism was affected by extensional deformation of the lithosphere. For Limalok, geochemical evidence shows affinities to the Rarotonga hotspot which is unlike the geochemical trends in the other volcanoes of the Ratak Chain. Reconstructions of the area's geological history suggest that the first hotspot to pass by Limalok was the Macdonald hotspot 9585 million years ago, followed by the Rurutu hotspot and the Society hotspot 7565 million years ago. The Rarotonga and especially the Rurutu hotspots are considered to be the most likely candidates for the hotspot that formed Limalok. However, some paleogeographical inconsistencies indicate that lithospheric fractures secondary to hotspot activity were also involved.

From plate motion reconstructions, it has been established that the Marshall Islands were located in the era now occupied by present-day French Polynesia during the time of active volcanism. Both regions display numerous island chains, anomalously shallow ocean floors and the presence of volcanoes. About 8 hotspots have formed a large number of islands and seamounts in that region, with disparate geochemistries; the geological province has been called "South Pacific Isotopic and Thermal Anomaly" or DUPAL anomaly.

Limalok has erupted basaltic rocks, which have been classified as alkali basalts, basanite and nephelinite. Minerals contained in the rocks are apatite, augite, biotite, clinopyroxene, olivine, nepheline and plagioclase, and there are ultramafic xenoliths. Shallow crystal fractionation processes appear to have been involved in the genesis of the magmas erupted by Limalok.

Alteration of the original material has formed calcite, chlorite, clay, iddingsite, montmorillonite, zeolite, and a mineral that could be celadonite. Volcanogenic sandstones and traces of hydrothermal alteration also exist on Limalok.

Carbonate, clay, manganese phosphate crust materials and mudstones have been found in boreholes or have been dredged from the seamount. The carbonates take various forms, such as grainstone, packstone, limestone, rudstone and wackestone. Porosity is usually low owing to cementation of the deposits, a process in which grains in rock are solidified and pores filled by the deposition of minerals such as calcium carbonate. The carbonate rocks show widespread evidence of diagenetic alteration, meaning that the carbonates have been chemically or physically modified after they were buried. For example, aragonite, pyrite and organic material were formed by alteration of living beings within the clays and limestones.

Limalok is the youngest guyot in the Marshall Islands. Argon-argon dating has yielded ages of 69.2 and 68.2 ± 0.5 million years ago on volcanic rocks dredged from Limalok. Mili Atoll volcano is probably not much younger than Limalok. During the Cretaceous Limalok was probably located in French Polynesia; paleomagnetism indicates that Limalok formed at 15–10 degrees southern latitude. Early limestones dredged from Limalok were considered to be of Eocene age (5633.9 million years ago) before earlier Paleocene deposits were discovered as well.

Limalok first formed as a shield volcano. The volcanic rocks were emplaced as lava flows with thicknesses reaching . In addition, breccia and pebbles encased within sediments occur. 

Soils formed on the volcano through the weathering of volcanic rocks, reaching a thickness of ; claystones and laterites were also generated through weathering. These deposits formed over a long time on an island that rose at least several metres above sea level – the estimated time it took to generate the soil profiles obtained in drill cores is about 13 million years. Thermal subsidence of the crust and erosion flattened the seamount before carbonate deposition commenced on Limalok, and it is possible that the growth of another volcano south of Limalok 12 million years after Limalok developed may be responsible for a southward tilt of the seamount.

The soils on Limalok were colonized by vegetation that left plant cuticle and woody tissues; angiosperms including palms, ferns and fungi with an overall low diversity developed on the volcano. Organisms burrowed into the soils, leaving cavities. The climate was probably tropical to subtropical, with an annual precipitation of less than .

The erosion of the volcanic island was followed after some time by the beginning of carbonate platform growth. Sedimentation began in the Paleocene with one or two events in which the seamount was submerged; the start of sedimentation has been dated to about 57.5 ± 2.5 million years ago. After a Paleocene phase with open sea or back-reef conditions, lagoonal environments developed on the seamount during the Eocene. It is possible that the platform periodically emerged above sea level, leading to its erosion. It is not clear if the platform took the form of an atoll, or of a shallow platform shielded on one side by islands or shoals, similar to the present-day Bahama Banks. Sea level rise at the Paleocene-Eocene transition may have triggered a transformation from a partially shielded platform to a true ring-shaped atoll.

The carbonate platform reaches an overall thickness of in one drill core. Drill cores in the platform show variations between individual carbonate layers that imply that parts of the platform were submerged and emerged over time while the platform was still active, possibly because of eustatic sea level variations. Furthermore, the platform was affected by storms which redeposited the carbonatic material. The deposition of the platform lasted about 10 million years, spanning the Paleocene-Eocene Thermal Maximum (PETM) Drill core evidence shows that the PETM had little impact on carbonate deposition at Limalok despite a decrease in the δ13C isotope ratio recorded in the carbonates, implying there was little change to ocean pH at that time.

The dominant living beings on Limalok were red algae that occupied many ecological niches and formed rhodoliths. Other lifeforms were bivalves, bryozoans, corals, echinoderms, echinoids, foraminifera, gastropods, molluscs and ostracods. Species and genera composition varied over time, leading to different species being found in different parts of the platform. Red algae were important early colonizers, and algal mats and oncoids were contributed by algae and/or cyanobacteria.

A carbonate platform is said to 'drown' when sedimentation can no longer keep up with relative rises in sea level, and carbonate deposition stops. Limalok drowned during the early-middle Eocene, soon after the start of the Lutetian, 48 ± 2 million years ago. It is the most recent carbonate platform in the region to submerge: the similar platform at neighbouring Mili Atoll is still depositing carbonate.

The drownings of carbonate platforms such as Limalok, MIT, Takuyo-Daisan and Wōdejebato appear to have many causes. One is a sea level drop resulting in the emergence of much of the platform; this reduces the space that carbonate-forming organisms have to grow upward when sea levels again rise. A second factor is that these platforms were not true reefs but rather piles of carbonate sediment formed by organisms; these constructs cannot easily out-grow sea level rises when growing on a constrained area. Two final key factors are the passage of the platforms through nutrient-rich equatorial waters which cause the overgrowth of algae that hampered the growth of reef-forming organisms, and global temperature extremes that may overheat the platforms especially when close to the equator; present-day coral bleaching events are often triggered by overheating and Limalok and the other seamounts were all approaching the equator when they drowned. In the case of Limalok and some other guyots, paleolatitude data support the notion that approaching the equator led to the demise of the platforms.

After the platform ceased growing, subsidence quickly lowered the tablemount below the photic zone, where sunlight can still penetrate. Hardgrounds and iron-manganese crusts formed on the drowned platform which contain Oligocene (33.923.02 million years ago) sediments and planktonic fossils. Some of the rocks underwent phosphatization during three separate episodes in the Eocene and Eocene-Oligocene which may have been triggered by ocean upwelling events at that time.

Until the Miocene, sedimentation on Limalok was probably hindered by strong currents. Renewed sedimentation began at that point after the drowning of Limalok, with sediments consisting mainly of foraminifera and other nanofossils. Some of the sediments were reworked after deposition. At least two layers formed during the Miocene (23.35.333 million years ago) and Pliocene-Pleistocene (5.3330.0117 million years ago), reaching a cumulative thickness of . Chemically, most of the sediments are calcite and they often occur in rudstone or wackestone form. Bivalves, echinoderms, foraminifera and ostracods are fossilized in the sediments, which sometimes contain borings and other traces of biological activity.



</doc>
<doc id="58008339" url="https://en.wikipedia.org/wiki?curid=58008339" title="Siege of Berwick (1333)">
Siege of Berwick (1333)

The Siege of Berwick lasted four months in 1333, and resulted in the Scottish-held town of Berwick-upon-Tweed (commonly known as Berwick) being captured by an English army commanded by King Edward III (). The year before, Edward Balliol had seized the Scottish Crown, surreptitiously supported by Edward III. He was shortly expelled from the kingdom by a popular uprising. Edward III used this as a "casus belli" and invaded Scotland. The immediate target was the strategically important border town of Berwick.

An advance force laid siege to the town in March. Edward III and the main English army joined it in May and pressed the attack. A large Scottish army advanced to relieve the town. After unsuccessfully manoeuvring for position and knowing that Berwick was on the verge of surrender, the Scots felt compelled to attack the English at Halidon Hill on 19 July. They suffered a crushing defeat and Berwick surrendered on terms the next day. Balliol was reinstalled as King of Scotland after ceding a large part of his territory to Edward III and agreeing to do homage for the balance.

The First War of Scottish Independence between England and Scotland began in March 1296, when Edward I of England () stormed and sacked the Scottish border town of Berwick as a prelude to his invasion of Scotland. After 30 years of warfare that followed, the newly-crowned 14-year-old King Edward III was nearly captured in the English disaster at Stanhope Park. This brought his regents, Isabella and Roger Mortimer, to the negotiating table. They agreed to the Treaty of Northampton with Robert Bruce () in 1328 but this treaty was widely resented in England and commonly known as "turpis pax", "the cowards' peace". Some Scots nobles, refusing to swear fealty to Bruce, were disinherited and left Scotland to join forces with Edward Balliol, son of King John I of Scotland (), whom Edward I had deposed in 1296.

Robert Bruce died in 1329; his heir was 5-year-old David II (). In 1331, under the leadership of Edward Balliol and Henry Beaumont, 4th Earl of Buchan, the disinherited Scottish nobles gathered in Yorkshire and plotted an invasion of Scotland. Edward III was aware of the scheme and officially forbade it, in March 1332 writing to his northern officials that anyone planning an invasion of Scotland was to be arrested. The reality was different, Edward III being happy to cause trouble for his northern neighbour. He insisted that Balliol not invade Scotland overland from England but turned a blind eye to his forces sailing for Scotland from Yorkshire ports on 31 July 1332. The Scots were aware of the situation and were waiting for Balliol. David II's regent was an experienced old soldier, Thomas Randolph, 1st Earl of Moray. He had prepared for Balliol and Beaumont, but he died ten days before they sailed.

Five days after landing in Fife, Balliol's force of some 2,000 men met the Scottish army of 12,000–15,000 men. The Scots were crushed at the Battle of Dupplin Moor. Thousands of Scots died, including much of the nobility of the realm. Balliol was crowned King of Scotland at Sconethe traditional place of coronation for Scottish monarchson 24 September 1332. Almost immediately, Balliol granted Edward III Scottish estates to a value of £2,000, which included "the town, castle and county of Berwick". Balliol's support within Scotland was limited and within six months it had collapsed. He was ambushed by supporters of David II at the Battle of Annan a few months after his coronation. Balliol fled to England half-dressed and riding bareback. He appealed to Edward III for assistance.

Berwick, on the North Sea coast of Britain, is on the Anglo-Scottish border, astride the main invasion and trade route in either direction. In the Middle Ages, it was the gateway from Scotland to the English eastern march. According to William Edington, a bishop and chancellor of England, Berwick was "so populous and of such commercial importance that it might rightly be called another Alexandria, whose riches were the sea and the water its walls". It was the most successful trading town in Scotland, and the duty on wool which passed through it was the Scottish Crown's largest single source of income. During centuries of war between the two nations its strategic value and relative wealth led to a succession of raids, sieges and takeovers. Battles were rare, as the Scots preferred guerrilla tactics and border raiding into England. Berwick had been sold to the Scots by Richard I of England () 140 years before, to raise funds for his crusade. The town was captured and sacked by Edward I in 1296, the first significant action of the First War of Scottish Independence. Twenty-two years later Robert Bruce retook it after bribing an English guard, expelling the last English garrison from Scottish soil. King Edward II of England () attempted to recapture Berwick in 1319 but abandoned the siege after a Scottish army bypassed him and defeated a hastily assembled army under the Archbishop of York at the Battle of Myton.
At the beginning of 1333, the atmosphere on the border was tense; Edward III had dropped all pretence of neutrality, recognised Balliol as King of Scotland and was making ready for war. The English parliament met at York and debated the situation for five days without conclusion. Edward III promised to discuss the matter with both Pope John XXII and King Philip VI of France (). Possibly to prevent the Scots from taking the initiative, England began openly preparing for war, while announcing that it was Scotland which was preparing to invade England. In Scotland Archibald Douglas was Guardian of the Realm for the underage David. He was the brother of the "Good" Sir James Douglas, a hero of the First War of Independence. Weapons and supplies were gathered as he made arrangements for the defence of Berwick. Patrick Dunbar, Earl of March, the Keeper of Berwick Castle, had recently spent nearly £200 on its defences. Sir Alexander Seton was appointed Governor of Berwick, responsible for the defence of the town. After it was sacked in 1296, Edward I had replaced the old wooden palisade with stone walls. These were considerably improved by the Scots in 1318. The walls stretched for and were up to thick and high. They were protected by towers, each up to tall. The wall to the south-west was further protected by the River Tweed, which was crossed by a stone bridge and entered the town at a stone gatehouse. Berwick Castle was to the west of the town, separated by a broad moat, making the town and castle independent strongholds. Berwick was well-defended, well-stocked with provisions and materiel, and expected to withstand a long siege.

Balliol, in command of the disinherited Scottish lords and some English magnates, crossed the border on 10 March. Edward III made grants of over £1,000 to the nobles accompanying him on the campaign and a similar amount was paid to Balliol's companions; Balliol received over £700 personally. He marched through Roxburghshire, burning and pillaging as he went and capturing Oxnam. He reached Berwick in late March and cut it off by land. Edward III's navy had already isolated it by sea. Balliol and the nobles accompanying him are said to have sworn not to withdraw until Berwick had fallen. Edward arrived at Berwick with the main English army on 9May, after leaving Queen Philippa at Bamburgh Castle south of Berwick. Balliol had been at Berwick for six weeks and had placed the town under close siege. Trenches had been dug, the water supply cut and all communication with the hinterland prevented. A scorched-earth policy was applied to the surrounding area to deny supplies for the town if an opportunity to break the siege occurred. The pillaging of the countryside also added to the English army's supplies. The army included troops raised in the Welsh Marches and the Midlands, as well as levies from the north which had already mustered on account of the earlier Scottish raids. By the end of the month, this force had been augmented by noble retinues, a muster at Newcastle, and the assembly of the English fleet in the River Tyne. Accompanying the army were craftsmen to build siege engines. Thirty-seven masons prepared nearly 700 stone missiles for the siege, which were transported by sea from Hull on 16 May.Edward III Had arranged for the combined army to be revictualled by sea through the small port of Tweedmouth.
Douglas had assembled a large army north of the border but his inactivity contrasts sharply with Robert Bruce's swift response to the siege of 1319. Douglas seems to have spent the time assembling ever more troops, rather than using those he already had to mount diversionary raids.
Minor raids into Cumberland were launched by Sir Archibald Douglas. These were insufficient to draw the English forces from the siege. But it gave Edward III a pretext for his invasion, of which he took full advantage. The success of Edward III's propaganda is reflected in contemporaneous English chronicles, which portray his invasion as retaliation against Scottish incursions,

With the arrival of Edward III, the assault on Berwick began. It was commanded by the Flemish soldier-merchant John Crabb. Crabb had defended Berwick from the English in 1319, been captured by them in 1332 and now used his knowledge of Berwick's defences on England's behalf. Catapults and trebuchets were used to great effect. The English used some form of firearms during the siege and modern historian Ranald Nicholson states that Berwick was probably "the first town in the British Isles to be bombarded by cannon".

In late June, the defenders set adrift burning brushwood soaked in tar, in an attempt to repel a naval assault. Instead of the English ships, much of the town was set on fire. William Seton, a son of the town's governor, was killed fighting an English seabourne assault. By the end of June the attacks by land and sea had brought the town to a state of ruin and the garrison close to exhaustion. It is believed that a desire for a respite from the plunging fire of the two large counterweight trebuchets used by the English was a significant factor in Seton requesting a short truce from King Edward. This was granted, but only on the condition that he surrender if not relieved by 11 July. Seton's son, Thomas, was to be a hostage to the agreement, along with eleven others.

Douglas was now faced with a situation similar to that which the English had faced before the Battle of Bannockburn. Modern historian Ranald Nicholson considers that "If Berwick were to be saved immediate action on the part of the Scottish guardian was unavoidable". As a matter of national pride Douglas would have to come to the relief of Berwick, just as Edward II had come to the relief of Stirling Castle in 1314. The army that Douglas had spent so much time gathering was now compelled to take to the field. The English army is estimated to have been less than 10,000 strong – outnumbered approximately two-to-one by the Scots. Douglas entered England on 11July, the last day of Seton's truce. He advanced eastwards to Tweedmouth and destroyed it in sight of the English army. Edward III did not move.

Sir William Keith, with Sir Alexander Gray and Sir William Prenderguest, lead a force of around 200 Scottish cavalry. With some difficulty, they forced their way across the ruins of the bridge to the northern bank of the Tweed and made their way into the town. Douglas considered the town relieved. He sent messages to Edward III calling on him to depart, threatening that if he failed to do so, the Scots army would devastate England. The Scots were challenged to do their worst. The defenders argued that Keith's 200 horsemen constituted the relief according to the truce and therefore they did not have to surrender. Edward III stated that this was not the case: they had to be relieved directly from Scotlandliterally from the direction of Scotlandwhereas Keith, Gray and Prenderguest had approached Berwick from the direction of England. Edward III ruled that the truce agreement had been breached – the town having neither surrendered nor been relieved. A gallows was constructed directly outside the town walls and, as the highest-ranking hostage, Thomas Seton was hanged while his parents watched. Edward III issued instructions that each day the town failed to surrender, another two hostages should be hanged.
Keith, having taken command of the town from Seton, concluded a fresh truce on 15 July, promising to surrender if not relieved by sunset on 19July. The truce comprised two indentures, one between Edward III and the town of Berwick and the other between Edward III and March, the Keeper of Berwick Castle. It defined circumstances in which relief would or would not occur. The terms of surrender were not unconditional. The town was to be returned to English soil and law but the inhabitants were to be allowed to leave, with their goods and chattels, under a safe conduct from Edward III. All members of the garrison would also be given free passage. Relief was defined as one of three events: 200 Scottish men-at-arms fighting their way into Berwick; the Scottish army forcing its way across a specific stretch of the River Tweed; or, it defeating the English army in open battle on Scottish soil. On concluding the new treaty, Keith was allowed to immediately leave Berwick, travel to wherever the Guardian of Scotland happened to be, advise him of the terms of the treaty, and return safely to Berwick.

By this time Douglas had marched south to Bamburgh, perhaps hoping for a repeat of the events of 1319, when Edward II had broken off a siege of Berwick after a Scottish army had advanced on York, where his queen was staying, and devastated Yorkshire. Whatever concerns Edward III had for his queen, he knew that Bamburgh could easily withstand a siege. The Scots did not have the time to construct the kind of equipment that would be necessary to take the fortress by assault. The Scots devastated the countryside but Edward III ignored this. He positioned the English army on Halidon Hill, a small rise of some , 2 miles to the north west of Berwick, which gives an excellent view of the town and the vicinity. From this vantage point, he dominated the crossing of the Tweed specified in the indentures and would have been able to attack the flank of any force of men-at-arms attempting to enter Berwick. Receiving Keith's news, Douglas felt that his only option was to engage the English in battle. Crossing the Tweed to the west of the English position, the Scottish army reached the town of Duns, 15 miles from Berwick, on 18July. On the following day it approached Halidon Hill from the north west, ready to give battle on ground chosen by Edward III. Edward III had to face a Scottish army to the front and guard his rear against the risk of a sortie by the garrison of Berwick; by some accounts a large proportion of the English army was left guarding Berwick.
To engage the English, the Scots had to advance downhill, cross a large area of marshy ground and then climb the northern slope of Halidon Hill. The Battle of Dupplin Moor the previous year had shown how vulnerable the Scots were to arrows. The prudent course of action would have been to withdraw and wait for a better opportunity to fight, but this would guarantee the loss of Berwick. The armies encountered each other's scouts around midday on 19July. Douglas ordered an attack. The "Lanercost Chronicle" reports: The Scots suffered many casualties and the lower reaches of the hill were littered with dead and wounded. The survivors continued upwards, through the arrows "as thick as motes in a sun beam", according to an unnamed contemporary quoted by Nicholson, and on to the waiting spears.

The Scottish army broke, the camp followers made off with the horses and the fugitives were pursued by the mounted English knights. Their casualties were numbered in thousands, including Douglas and five earls dead on the field. Scots who surrendered were killed on Edward's orders and some drowned as they fled into the sea. English casualties were reported as fourteen; some chronicles give a lower figure of seven. About a hundred Scots who had been taken prisoner were beheaded the next morning, 20July. This was the date that Berwick's second truce expired, and the town and the castle surrendered on the terms in the indentures.

After the capitulation of Berwick, Edward III appointed Henry de Percy, 2nd Baron Percy as Constable, with Sir Thomas Grey of Heaton, father of the chronicler Thomas Grey, as his deputy. Considering his part done and short of money, he left for the south. On 19June 1334, Balliol did homage to Edward for Scotland, after formally ceding to England the eight counties of south-east Scotland. Balliol ruled a truncated Scottish state from Perth, from where he attempted to put down the remaining resistance. Seton in turn did homage to Balliol. Balliol was deposed again in 1334, restored again in 1335 and finally deposed in 1336, by those loyal to David II. Berwick was to remain the military and political headquarters of the English on the border until 1461, when it was returned to the Scottish by King Henry VI (). Clifford Rogers states that Berwick "remained a bone of contention throughout the Middle Ages", until its final re-capture for the English by Richard, Duke of Gloucester, the future Richard III, in 1482.




</doc>
<doc id="58101181" url="https://en.wikipedia.org/wiki?curid=58101181" title="Siegfried Lederer's escape from Auschwitz">
Siegfried Lederer's escape from Auschwitz

On the night of 5 April 1944, Siegfried Lederer, a Czech Jew, escaped from the Auschwitz concentration camp wearing an SS uniform provided by "SS-Rottenführer" Viktor Pestek. Because of his Catholic faith and infatuation with Renée Neumann, a Jewish prisoner, Pestek opposed the Holocaust. He accompanied Lederer out of the camp, and the two men traveled together to the Protectorate of Bohemia and Moravia to obtain false documents for Neumann and her mother.

Lederer, a former Czechoslovak Army officer and member of the Czech resistance, tried unsuccessfully to warn the Jews at Theresienstadt Ghetto about the mass murders at Auschwitz. He and Pestek returned to Auschwitz in an attempt to rescue Neumann and her mother. Pestek was arrested under disputed circumstances and later executed. Lederer returned to occupied Czechoslovakia, where he rejoined the resistance movement and attempted to smuggle a report on Auschwitz to the International Committee of the Red Cross in Switzerland. After the war he remained in Czechoslovakia. The story of the escape was retold by Lederer and writers including historian Erich Kulka.

Siegfried Lederer or Vítězslav Lederer ( – ) was born to a Jewish family in in the Sudetenland, the German-speaking part of Czechoslovakia. After the Sudetenland was annexed to Germany in 1938, he moved to Plzeň and worked manual jobs, including agricultural work and a stint in a kaolin factory. According to Lederer, he joined the Association of Friends of the Soviet Union, was influenced by Communist leader Marie Škardová, helped those living in hiding, and distributed illegal publications. Lederer also said that he was a member of the resistance group named after Lieutenant Colonel Jaroslav Weidmann. Later, Lederer joined Plzeňák 28, a Czech resistance group in Zbraslav so called because it had twenty-eight members, including Josef Pokorný, commander of the Zbraslav gendarmerie.

In November 1939 and again in November 1940, Lederer was arrested by the Gestapo for alleged resistance activity. On both occasions, he was quickly released because of a lack of evidence. He was arrested a third time and imprisoned with other political prisoners at the Small Fortress of Theresienstadt. On 18 January 1942, he was transferred to the adjacent Jewish ghetto, and was supposed to be deported on the next transport. Leo Holzer, the leader of the ghetto fire brigade—a hotbed of resistance—heard about Lederer's resistance activities and postponed his deportation by recruiting him into the fire brigade. Lederer later claimed that he had maintained contact with the Plzeňák 28 while at Theresienstadt, but survivors of that group testified that they had heard nothing from him until his escape. He was dismissed from the fire brigade and lost his protection from deportation because he was caught smoking. Deported to Auschwitz concentration camp on 18 December 1943, Lederer was forced to wear both yellow and red triangles, marking him as a Jew and a political prisoner. There is no evidence that he was involved in the Auschwitz resistance movement.

Viktor Pestek ( – ) was born in Czernowitz, Bukovina—which was then part of Romania—to a devoutly Catholic ethnic German family. Auschwitz guard Stefan Baretzki grew up in the same town; he and Pestek were acquaintances as children. Pestek, whose father was a blacksmith and a small farmer, learned these trades as a young man. He joined the Waffen-SS, either because of his innate sense of adventure, or because his mother persuaded him to join. During his service, Pestek was involved in anti-partisan warfare near Minsk, Belarus. His unit was ordered to attack a village suspected of containing partisans and to kill the inhabitants. When Soviet partisans opened fire at the Germans, Pestek was wounded in the arm and leg. Separated from his unit, he hid in a barn with another wounded SS man named Werner.

After Werner died of his injuries, Pestek was discovered by partisans who spared his life despite the SS killings in the village. The humanity of his enemy apparently reawakened Pestek's Catholic faith and brought him into conflict with the genocidal German policies. According to Siegfried Lederer, Pestek later said of this incident: "I was a murderer, and a Soviet partisan spared my life anyway". Upon his return to a German-controlled area, he had lost the use of his hand. Found unfit for front-line service, he was posted to Auschwitz concentration camp as a guard. Pestek was a "Rottenführer", a junior non-commissioned rank in the SS.

Jews transported from Theresienstadt to Auschwitz between September 1943 and May 1944 were housed in a separate block at Auschwitz II-Birkenau, known as the Theresienstadt family camp. They were not subject to selection upon arrival, were allowed to retain their civilian clothes, and were not forced to shave their heads. Families were allowed to stay together and write to their friends and relatives in an attempt to mislead the outside world about the Final Solution. The Nazis, however, were planning to kill each group of prisoners six months after their arrival.

Pestek was initially appointed the supervisor of section BIId of Auschwitz II-Birkenau. Although he quickly developed a reputation for trading contraband, he was disgusted by the mass killings at Auschwitz and by the contempt of some German SS members for "Volksdeutsche" (ethnic Germans), who comprised the majority of Auschwitz guards. Some SS men formed relationships with Jewish women in the family camp because, unlike other prisoners, they had been allowed to keep their hair. Pestek fell in love with Renée Neumann, a Czech Jewish prisoner at the family camp, although she did not return his affection. He arranged for Neumann to get a job as a block clerk and offered to help her escape by disguising her as an SS woman. This was unsuccessful, partly because Neumann was unwilling to leave her mother. According to Czech historian Miroslav Kárný, Pestek decided against escaping with Neumann and her mother because of their lack of contacts in the Czech underground who could help him evade prosecution until the end of the war.

On 8 March 1944, exactly six months from their arrival, the Jews from the family camp who had arrived in September were all gassed without a selection to find those able to work. Pestek rescued Neumann and her mother by temporarily moving them to a different block. Lederer was appointed block elder (Blockältester) of Block 14 within the family camp later that month. Alfred Cierer, a Czech Jewish industrialist, and his son Jakov Tsur moved in because they knew Lederer. Realizing he would have to act quickly to save Neumann's life, Pestek began to approach other prisoner functionaries and offer to help them escape. Among them were Rudolf Vrba and Alfred Wetzler, who refused the offer because they believed it was a trick, and advised other prisoners not to trust Pestek. Previously, an SS man named Dobrovolný—an ethnic German from Slovakia—had met a Jewish childhood friend at Auschwitz. Dobrovolný offered to help him escape but then turned him in, resulting in his brutal execution and a bonus for the SS man. This and similar incidents persuaded Vrba, Wetzler and other prisoners that guards could not be trusted under any circumstances. According to Wetzler, Pestek told him, "I hate myself for having to watch women and children be killed. I want to do something to forget the smell of burning human flesh and feel a little cleaner." Pestek also approached the Czech Josef Neumann (not a relative of Renée Neumann), a kapo on the "Leichenkommando", which was responsible for the disposal of corpses; Neumann refused.

According to Jakov Tsur, Pestek escorted Cierer to the Gestapo for interrogation and made an offer to him. When interrogated later, Cierer claimed the offer was only a transfer to another part of the camp, not a complete escape. Cierer, whose three children were with him in the family camp, refused the offer but suggested Lederer. Cierer and Pestek spoke in French to avoid being understood. Cierer later shared his contacts with Lederer in the hope that his escape would be successful, and the two men planned together how to break news of Auschwitz to the outside world—a plan they concealed from Pestek until after the escape. Other sources state that it was Lederer whom Pestek escorted to the Gestapo.

As a member of the family camp and because he was detained for his resistance activities, Lederer believed he had nothing to lose. He told Pestek he was wealthy and that his contacts in the underground would help Pestek and Neumann. Pestek and Lederer planned their escape, and their intended return to rescue Neumann, in considerable detail. Lederer would leave disguised as an SS man. After obtaining false documents in the Protectorate, Lederer and Pestek would return, impersonating SS officers, and present a forged Gestapo warrant for the arrest of Renée Neumann and her mother. The Auschwitz staff would provide a car and driver, who would be killed on the way to the Gestapo station. After disposing of the body, the escapees would take an express train to the Protectorate. The plan was based on Pestek's knowledge of protocol from his experience in the transport office.

Because he was a wounded soldier, Pestek was entitled to a long leave and requested it for 6 April 1944. On 3 April, he stole an SS uniform, pistol, and paybook for Lederer, who hid them in a double wall. Before standing guard at the gate of the family camp on the night of 5 April, Pestek left a bicycle by Lederer's barracks as a signal for him to come out. Pestek gave the correct passwords, telling the other guards Lederer was on special duty, and both men bicycled out of the front gate. They went to the railway station outside Auschwitz and caught a train to Prague, avoiding border control by pretending to be luggage inspectors. Lederer's absence was discovered in the morning of 6 May by an SS man inspecting the family camp who had seen a woman exiting Lederer's block and stepped in to investigate, only to discover Lederer missing. At 11:30, "SS-Sturmbannführer" Friedrich Hartjenstein, the commandant of Auschwitz II-Birkenau, sent a telegram to the German police notifying them that Lederer had escaped, probably disguised as an "SS-Rottenführer". Another telegram four hours later reported that an SS man—presumably Pestek—was under suspicion as a suspect for aiding the escape. Cierer and others suspected of being close to Pestek or Lederer were interrogated.

In Prague, Pestek and Lederer sold jewelry that Lederer had obtained on the Auschwitz black market and bought civilian clothes. They also altered their uniforms to look like Waffen-SS soldiers instead of concentration camp guards. From Prague they went to Plzeň, where the two escapees hid with Josef Černík, a former Czechoslovak Army officer who had previously helped Lederer find work. The police posted a photograph of Lederer but did not offer a reward for his capture. Brigitta Steiner, the daughter of a friend of Lederer's, provided false civilian papers for him. She was a "Mischling" whose partial German ancestry prevented her deportation. She also told them of Faltys, a Jew in hiding in Prague who could arrange the rest of the papers, including SS officer identification for Pestek and Lederer that would give them the authority to arrest Renée Neumann and her mother. Faltys demanded an exorbitant fee but offered a discount if they could smuggle another woman out of Auschwitz.

Several people helped hide Lederer during the summer of 1944. In May 1944, Lederer was hiding in Prague with Bedřich and Božena Dundr, at Vinohrady, Mánesova No. 16. Later, Lederer hid with Mrs. Dundr's brother Adolf Kopřiva in Na Závisti, Zbraslav, a suburb of Prague. The Černík, Dundr, and Kopřiva families collaborated closely, providing basic needs for Lederer, and Černík and his wife were shadowed and interrogated by the "Kriminalpolizei". Josef Plzák, who had known Lederer in the resistance, was arrested in June 1944 under suspicion of helping to hide him. Plzák provided assistance to those hiding Lederer and did not betray him. Steiner, a German bank clerk named Ludwig Wallner whose Jewish sister-in-law had been deported to Auschwitz, and three others were indicted by the Nazi authorities for hiding Pestek and Lederer, and providing false papers for them.

On 20 April, Lederer made the first of four or five visits to the Theresienstadt Ghetto. Unbeknownst to him, Lederer was not the first Auschwitz escapee to bring news of mass executions by gassing. Rabbi Leo Baeck, one of the leaders of the Jewish self-administration, had been informed by an anonymous escapee in August 1943. Lederer went to the nearby village of Travčice, where he met with Václav Veselý, a barber who regularly went into the ghetto to shave the Czech guards; he knew Lederer and had helped the Jews in the past. Veselý told Lederer how to avoid the sentries, taking advantage of a security vulnerability around a hospital located outside the ghetto's perimeter. Lederer crossed the open ground outside the ghetto while the sentry was looking the other way and passed through a fence.
Lederer told Leo Holzer about what he had witnessed at Auschwitz, and according to his later testimony also informed Jirka Petschauer, the captain of the Jewish police inside the ghetto, and Otto Schliesser, a member of the Council of Elders. Holzer notified Baeck and , head of the self-administration. Eppstein, Baeck, and Holzer agreed the truth about Auschwitz must be kept strictly secret, lest a "catastrophe" befall the 35,000 prisoners at Theresienstadt at the time. Although rumors about the fate awaiting them at Auschwitz had already spread around the ghetto, many people refused to believe them. Almost all the Jews who were deported to the family camp in May 1944 were unaware of Lederer's previous visit to Theresienstadt, and the few who had access to Lederer's reports made no effort to avoid deportation. Even the resistance members in the fire brigade opposed armed resistance, trusting the June 1944 Red Cross visit to ensure the survival of Theresienstadt's Jews.

Explaining the reaction to the possibility of imminent death, Israeli historian and survivor Jakov Tsur stated that no one was capable of understanding Auschwitz until he or she had arrived and was undergoing selection. Miroslav Kárný said that he and his friends knew before their deportation on 28 September 1944 that there were gas chambers at Auschwitz, but that "no human being could accept these facts as truth". Lederer made two or three trips into the ghetto in May, smuggling weapons and parts of a radio transmitter that he received from Josef Pokorný.

Pestek and Lederer returned to Auschwitz, between late April and June, planning to rescue Renée Neumann, her mother, and Faltys' relative. What happened afterwards is disputed. It is known the SS arrested Pestek and that Lederer escaped capture. According to Kárný, Pestek had overstayed his leave and was suspected of having helped Lederer escape, and therefore success was impossible under the circumstances. Lederer said Pestek had left some valuables with a Polish girlfriend in Myslowitz and that she reported him when he tried to retrieve them. Kárný disputes that Lederer could have known that she betrayed him because according to Lederer, he had remained at the Auschwitz train station while Pestek continued to Myslowitz. Kárný considers the conflicting accounts make it impossible to know what happened, and he is convinced Lederer's account is not accurate.

Josef Neumann said he had been approached by an unknown SS man—probably Pestek—with an offer of escape. Before they could enact their plans, the alarm was raised and many SS guards arrived. Neumann and Pestek were caught, handcuffed together, and carried away; both were interrogated and tortured at Block 11. SS guard Stefan Baretzki, who knew Pestek well, testified that Pestek had been arrested at Birkenau. Baretzki said he had seen other SS guards beating Pestek. Ryszard Henryk Kordek, a prisoner, said that Baretzki had raised the alarm over Pestek's return and that Baretzki was one of the guards who beat Pestek. SS man Perry Broad said he heard kapos bragging about chasing and catching Pestek in woods around the camp. Kárný hypothesizes that Pestek, realizing he had been recognized, gave up his plans to save Renée Neumann and her mother, and therefore made the offer to Josef Neumann. Pestek was sentenced to death by firing squad in Kattowitz for favoring inmates and desertion. He was executed in Międzybrodzie Bialskie on 8 October 1944 at 7:04. Members of Pestek's unit reported being ordered to witness the shooting. During the second liquidation of the family camp in July 1944, Neumann and her mother were selected for forced labor in the Hamburg area. Both survived the war.

In early June, Lederer attempted to smuggle a report on Auschwitz to the International Committee of the Red Cross (ICRC) in neutral Switzerland. In Plzeň, he met Czech journalist Eduard Kotora, to whom he confided his plans. Kotora accompanied Lederer to the Křimice station, where the latter boarded a train. Using false papers provided by Steiner and a false work permit provided by the Zbraslav resistance, Lederer continued to the Škodovák station, which was used by many Czechs crossing the border to work at the Škoda Works in the Sudetenland. According to Lederer, he was then driven to Constance, alternately dressed as a civilian and an SS officer. He met the widow of Werner, Pestek's SS colleague who was killed in action in Belarus, and gave her some of Werner's personal possessions that had ended up in Pestek's hands. Mrs. Werner introduced Lederer to the captain of a boat on Lake Constance, who agreed to smuggle the report across the border to Switzerland and send it to the ICRC.

There is no evidence the report reached its destination, or even that Lederer sent it as he described. Kárný writes that the most likely interpretation is that the skipper destroyed the report to avoid difficulties with border control. According to Czech historian Erich Kulka, the ICRC probably did not receive the report. Lederer said in 1967 that he had the opportunity to escape to Switzerland but decided not to because his family had already been killed by the Germans and he felt obliged to continue to fight. According to Kárný, Lederer regarded fleeing to Switzerland as cowardice and desertion, even though Kárný notes that his testimony on Auschwitz would have been more credible if he had delivered it in person.

According to Lederer, he joined the Kriváň partisan group and tried to cross the border to fight in the Slovak National Uprising (August–October 1944), and was wounded in the attempt. In November, he made his last visit to Theresienstadt, staying about eight days to compile a detailed report on the Small Fortress, the ghetto, and the Sudeten barracks to which the Germans had transferred the Reich Main Security Office archives in 1943. Lederer's report contained information for which, according to Kárný, "every Allied secret service would have given anything" to obtain. There is no evidence that Lederer tried to send it to the Allies.

Following this, Lederer said he returned to Zbraslav and joined a partisan group named after S. P. Vezděněv and continued his activity with Plzeňák 28. According to Kárný, Lederer's role in the latter group, which during 1944 focused on sabotaging the Roderstein capacitor factory and a local Wehrmacht installation is unclear. Kulka disagrees, stating that the report on Lederer's activities in the Plzeňák 28 group confirms his testimony "to the smallest detail". Lederer remained in Czechoslovakia after the 1948 Communist takeover and married. He died in Prague in 1972, aged 68.

Pestek was one of only two or three Auschwitz guards who risked their lives to help inmates escape. According to Austrian historian and Auschwitz survivor Hermann Langbein, his actions in particular indicate the limits of the absolute totalitarian hierarchy imposed by SS leaders. Langbein evaluates Pestek's actions more favorably than those of the guards who helped inmates escape during the evacuation of the camp in January 1945 in hopes of avoiding punishment for their crimes. One survivor described Pestek as "a decent person who never beat inmates" and Yehuda Bacon said he was "more humane" than other SS guards. Czech prisoners at the family camp reportedly called him "miláček", Czech for "darling". Bacon also said Pestek maintained confidential contact with Fredy Hirsch, a leader in the family camp until his death in the 8 March liquidation. According to psychologist Ruth Linn, Pestek may have helped Lederer in an attempt to distance himself from Nazi crimes because his home in Bukovina had been recently occupied by the advancing Red Army. Pestek is not recognized as "Righteous Among the Nations" by Yad Vashem.

Although described as "one of the most bizarre" escapes of World War II by historian Alan J. Levine, Lederer's flight was overshadowed by that of Rudolf Vrba and Alfred Wetzler two days later, which produced the Vrba–Wetzler report. Although some authors, including Levine, have connected Lederer's report to the fact that the second liquidation of the family camp spared those able to work, Miroslav Kárný emphasizes that the decision was made due to the increasing labor shortage. Kárný, who felt that Lederer's actions needed no embellishment, found that Lederer and the Czech journalist Eduard Kotora, who publicized the former's actions, exaggerated them. These distortions were uncritically repeated by other writers. One influential, although discredited, account of the escape was Erich Kulka's semi-fictional 1966 book "Escape from Auschwitz". Czech-born Israeli historian Yehuda Bauer wrote in the introduction of the book that "The story that Erich Kulka tells is not fiction". Kulka claimed that his work was historically accurate, even while describing it as a "historical novel".



</doc>
<doc id="58589838" url="https://en.wikipedia.org/wiki?curid=58589838" title="Lou Spence">
Lou Spence

Louis Thomas Spence, DFC & Bar (4 April 1917 – 9 September 1950) was a fighter pilot and squadron commander in the Royal Australian Air Force (RAAF). During World War II he flew with No. 3 Squadron, earning the Distinguished Flying Cross (DFC), and commanded No. 452 Squadron, receiving a Mention in Despatches. He led No. 77 Squadron in the opening months of the Korean War, and was awarded a bar to his DFC, as well as the US Legion of Merit and the US Air Medal, for his leadership.

Born in Bundaberg, Queensland, Spence worked in a bank before joining the RAAF in March 1940. In August the following year he was posted to North Africa with No. 3 Squadron, which operated P-40 Tomahawks and Kittyhawks against German and Italian forces; he was credited with shooting down two German aircraft. Spence commanded No. 452 Squadron in 1944, flying Supermarine Spitfires in defence of Australia's North-Western Area against the Japanese. After a brief return to civilian life following World War II, he rejoined the RAAF in October 1946. He took command of No. 77 Squadron, operating P-51 Mustangs as part of the British Commonwealth Occupation Force in Japan, in February 1950. The squadron went into action within a week of the outbreak of the Korean War in June. Spence was killed during a low-level mission over South Korea in September 1950.

Born on 4 April 1917 in Bundaberg, Queensland, Louis Thomas Spence was the fifth child of Robert John Spence, farmer, and Louise Margaretta Marie, "" Koob. His ancestry was Irish on his father's side and German on his mother's. Spence attended Longreach State School from 1924 to 1931 and Thornburgh College in Charters Towers from 1932 to 1934. Successful academically, he also excelled at sports including cricket, rugby league, and tennis. His light-blond hair earned him the nickname "Silver". He was employed as a clerk at the Queensland headquarters of the Bank of New South Wales in Brisbane, and studied at the Bankers' Institute of Australasia.

Spence joined the Royal Australian Air Force (RAAF) on 6 March 1940. After flying training at Point Cook, Victoria, and Archerfield, Queensland, he was commissioned as a pilot officer on 26 August. On 28 October, he was allotted to No. 25 Squadron in Perth, Western Australia. The squadron operated CAC Wirraways. Spence was promoted to flying officer on 26 February 1941. He married Vernon Swain, a nurse, in St George's Cathedral, Perth, on 24 May; the couple had two children. Swain's father had been a pilot with the Royal Flying Corps in World War I. In August, Spence was posted to the Middle East. He underwent operational flying training in Khartoum, Sudan, before joining No. 3 Squadron in September. Based in Egypt, No. 3 Squadron operated P-40 Tomahawk fighters against German and Italian forces.

On 1 January 1942, having converted to P-40 Kittyhawks, No. 3 Squadron attacked sixteen Junkers Ju 87 "Stuka" dive bombers and their escort of six Messerschmitt Bf 109 fighters near Agedabia in Cyrenaica; Spence was credited with shooting down a Ju 87. He landed his single-seat fighter in the desert on 26 January to pick up another No. 3 Squadron pilot, Sergeant Walter Mailey, whose Kittyhawk had been forced down. On 14 February, No. 3 Squadron and No. 112 Squadron RAF intercepted over thirty Italian and German aircraft attempting to raid Tobruk. The Allied pilots claimed twenty enemy aircraft destroyed, one of which, a Bf 109, was credited to Spence. He claimed a probable Bf 109 on 15 March, and was promoted to flight lieutenant on 1 April. In the first of his five sorties in the Bir Hacheim area on 16 June, Spence, along with Nicky Barr, bombed and strafed a column of German tanks and support vehicles, igniting fires that sent the smell of burning flesh into the cockpits of the low-flying aircraft—a "ghastly horror", according to Spence, that made him physically ill. His many ground-attack missions and two aerial victories earned him the Distinguished Flying Cross. Barr recalled Spence as being the "hottest dive bomber in the Desert" and "one of the rocks of the squadron".
Returning to Australia in September 1942, Spence was posted as an instructor to No. 2 Operational Training Unit in Mildura, Victoria. The unit operated several types of aircraft including Kittyhawks, CAC Boomerangs and Supermarine Spitfires. Spence was promoted to acting squadron leader on 1 February 1944 and assumed command of No. 452 Squadron two days later. Based near Darwin, Northern Territory, No. 452 Squadron was one of three Spitfire squadrons comprising No. 1 (Fighter) Wing, whose role was to defend North-Western Area from Japanese air attack. On 8 March, No. 452 Squadron was urgently dispatched to the vicinity of Perth, Western Australia, in response to concerns that a Japanese naval force would raid the area, but it proved abortive; no attack ensued, and the squadrons were directed to return to Darwin on 20 March. The journey to Perth had taken No. 452 Squadron through bad weather, and Spence was mentioned in despatches for his efforts shepherding his Spitfires to their destination. From 9 to 21 May, Spence held command of No. 1 Wing in the absence of the officer commanding, Group Captain Peter Jeffrey. The next month, No. 452 Squadron transferred from No. 1 Wing to the recently formed No. 80 (Fighter) Wing, commanded by Group Captain Clive Caldwell. Spence was promoted to temporary squadron leader on 1 July. No. 452 Squadron completed conversion from Mk V Spitfires to Mk VIIIs the same month.

Spence was posted out of No. 452 Squadron at the end of November 1944. Early the following year he joined No. 8 Operational Training Unit, which was based at Parkes, New South Wales, and operated Wirraways, Boomerangs and Spitfires, among other types. He was discharged from the Air Force on 19 November 1945.

Rather than resume his banking career after the war, Spence joined the Department of Information in Canberra, initially in administration and later in aviation journalism. He rejoined the RAAF in 1946, receiving a commission as a flying officer (temporary squadron leader) effective from 17 October. His first appointment was as senior administrative officer at RAAF Station Canberra. On 13 September 1947, he flew to Surabaya, Java, as one of Australia's military observers with the United Nations commission monitoring the ceasefire between Dutch forces and Indonesian nationalists. That November, he was assigned to the recently formed RAAF College, Point Cook, where he was appointed adjutant and subsequently led the school's Cadet Squadron. In the latter role he inaugurated the college's adventure training, including canoe trips on the Murray River in boats constructed by the students themselves.

Promoted to wing commander, Spence was posted to Japan to take command of No. 77 Squadron on 28 February 1950. Based at Iwakuni, the squadron operated P-51 Mustangs as part of the British Commonwealth Occupation Force (BCOF). Originally one of three RAAF fighter units under BCOF, No. 77 Squadron had since late-1948 been Australia's sole air component in Japan, becoming the largest flying squadron in the Air Force, with around 300 officers and men, forty Mustangs, and several transport aircraft. Occupation duties had been uneventful, the main operational task being surveillance patrols, but the RAAF maintained an intensive training regime and undertook combined exercises with other Allied forces. On 23 June, No. 77 Squadron made what was expected to be its last flight before rotating back to Australia.

Spence and his family were about to go on holiday before returning to Australia when, on 25 June 1950, No. 77 Squadron was placed on standby for action in the Korean War, which had just broken out. The unit was specifically requested by General Douglas MacArthur, commander of United Nations (UN) forces. No. 77 Squadron flew its initial escort and patrol sorties from Iwakuni on 2 July, becoming the first non-American UN unit to go into action. That day, Spence took eight Mustangs on an escort mission for United States Air Force (USAF) B-26 Invaders attacking a bridge south of Seoul. Families still living at Iwakuni, pending their repatriation from what had become an operational theatre, could watch the Mustangs depart for missions over Korea.
A friendly fire incident occurred on 3 July 1950, when No. 77 Squadron attacked a train full of US and South Korean troops on the main highway between Suwon and Pyongtaek, inflicting many casualties, twenty-nine of them fatal. Prior to the mission, Spence had raised concerns that the North Koreans could not have penetrated so far south, but was assured by USAF controllers that the target was correct. The incident was widely reported in American newspapers but a public statement by Lieutenant General George E. Stratemeyer, commander of the US Far East Air Force, cleared the RAAF of blame. Spence was recommended for the US Air Medal for "meritorious service" in operations from 25 June to 15 July. His increasingly heavy taskload included diplomatic duties and public relations, as well as squadron administration and many combat sorties.

No. 77 Squadron did not encounter enemy aircraft in the opening phase of the war, but often faced intense ground fire. During July and August 1950, equipped with bombs, rockets and napalm, the Mustangs supported UN troops retreating before the North Korean advance. According to the official history of the Air Force in 1946–1971, the squadron's part in the Battle of Pusan Perimeter earned recognition "not only for the RAAF but also Australia at the highest political levels in the United States". On 15 August, Prime Minister Robert Menzies presented the Gloucester Cup to No. 77 Squadron as the RAAF's most proficient unit of the past year. General Stratemeyer arrived at Iwakuni a week later to surprise Spence with the award of the US Legion of Merit for "outstanding leadership in the preparation of his unit for combat".

On 9 September, in cloudy conditions, Spence led three other Mustangs in a low-level napalm attack on An'gang-ni. He attempted to pull out of a very steep dive but crashed in the middle of the town and was killed. Whether he was hit by ground fire or had misjudged his attack is uncertain. Spence's wife and children were still living at Iwakuni, and his death hastened the departure for Australia of all families on the base. He was succeeded by Squadron Leader Dick Cresswell, who had twice commanded No. 77 Squadron during World War II. Cresswell commented that "thanks to Lou Spence", the squadron "was led perfectly in all the jobs it did".

According to his brother, quoted in the Brisbane "Courier-Mail", Spence wrote in his last letter home:
Flight Lieutenant (later Air Vice-Marshal) Fred Barnes described Spence's death as having "a tremendous impact ... He was very popular and respected as a professional. It was accepted that he was on the way to high rank." The official history of the post-war Air Force contended that Spence "appeared destined for the highest levels of the RAAF". Stratemeyer rated him "one of the most capable field commanders I have been associated with", and "one of the noblest and finest officers of any service".

Spence was recommended for the Distinguished Service Order for his "outstanding fearless leadership and distinction" in Korea, but the award was changed to a bar to his DFC. The decoration was promulgated in "The London Gazette" on 17 April 1951 and presented to his wife on 22 February 1952. His awards of the US Legion of Merit and Air Medal were gazetted on 22 June 1951; the latter was presented privately to Vernon Spence.

Advancing UN troops located Spence's body near his crashed Mustang in October 1950. He is buried at the United Nations Memorial Cemetery in Busan, South Korea. His name appears on Panel 2 of the Commemorative Area of the Australian War Memorial, Canberra.




</doc>
<doc id="58684856" url="https://en.wikipedia.org/wiki?curid=58684856" title="Allison Guyot">
Allison Guyot

Allison Guyot (formerly known as Navoceano Guyot) is a tablemount (guyot) in the underwater Mid-Pacific Mountains of the Pacific Ocean. It is a trapezoidal flat mountain rising 1,500 metres above the seafloor, with a summit platform 35 by 70 kilometres wide. The Mid-Pacific Mountains lie west of Hawaii and northeast of the Marshall Islands, but at the time of their formation were located in the Southern Hemisphere.

The tablemount was probably formed by a hotspot in the present-day Southern Pacific before plate tectonics moved it to its current location. Several hotspots, including the Easter, Marquesas and Society hotspots, may have been involved in the formation of the Mid-Pacific Mountains. Volcanic activity is dated to have occurred circa 111-85 million years ago and formed a volcanic island. Subsequently carbonate deposition commenced as Allison Guyot subsided and eventually buried the island, forming an atoll-like structure and a carbonate platform. Among other animals, crocodilians lived on Allison Guyot.

The platform emerged above sea level during the Albian and Turonian ages. It drowned about 99 ± 2 million years ago for unknown reasons; possibly a phase of renewed emergence damaged the reefs, or it was located in unfavourable waters. Later, pelagic sedimentation commenced on the seamount and led to the deposition of sediments including limestone, ooze and sand, which bear traces of climatic events and ocean currents.

Allison Guyot is named after E.C. Allison, an oceanographer and paleontologist at the San Diego State College; formerly it was named "Navoceano Guyot". The name "Hamilton Guyot" has also been applied to Allison Guyot but is not correct; Hamilton Guyot is a separate formation in the Mid-Pacific Mountains. The seamount is the source of the Ocean Drilling Program drill core 865A, which was bored into the summit platform of Allison Guyot in 1992 but did not reach the volcanic structure of the underwater mountain. Two other cores 865C and 865B were obtained during the same operation; Allison Guyot is Site 865 Ocean Drilling Program. These drill cores were part of a larger project to investigate and clarify the history of the flat-topped submarine mountains in the Pacific Ocean.

Allison Guyot is located in the equatorial Pacific Ocean, part of the western Mid-Pacific Mountains. The Mid-Pacific Mountains contain seamounts which were covered by limestones during the Barremian and Albian (circa 129.4 – circa 125 million years ago and circa 113-100.5 million years ago, respectively). Hawaii lies due east and the Marshall Islands southwest; Resolution Guyot is 716 kilometres northwest.

The guyot (also known as tablemount) has an outline resembling a trapezoid and an approximate radius of 12 kilometres. It consists of two connected volcanic ridges facing north-northwest to east-northeast. Its western parts may be a distinct volcano. The surface platform has dimensions of 35 by 70 kilometres, with an upwards-doming form 0.3–0.5 kilometres high, and is covered by large sediment mounds; the rim surrounding the platform lies at a depth of about 1,650 metres and there is evidence of former reefs. The structure appears to consist of lagoonal sediments surrounded by a reef. Volcanic cones dot the eastern side of the summit plateau. The seamount bears traces of slumping, which on the southeastern side of Allison Guyot has removed part of the platform perimeter.

The seamount rises 1.5 kilometres above the seafloor. Underneath Allison Guyot, the seafloor is about 130–119 million years old, and a 128-million year-old magnetic lineation is located nearby. The Molokai Fracture Zone forms a ridge which passes close to Allison Guyot and intersects with another ridge at the seamount. Tectonically the seamount is part of the Pacific Plate.

The Pacific Ocean seafloor contains many guyots of the Mesozoic age that developed in shallow seas. These are submarine mountains which are characterised by a flat top and usually the presence of carbonate platforms that rose above the sea surface during the middle Cretaceous period. Many of these seamounts were formerly atolls, although there are some differences to present-day reef systems. All these structures originally formed as volcanoes in the Mesozoic ocean. The crust underneath these volcanoes tends to subside as it cools, and thus the islands and seamounts sink. Fringing reefs may have developed on the volcanoes, which then became barrier reefs as the volcanoes subside and turn into atolls; these rims surround lagoons or tidal flats. Continued subsidence offset by growth of the reefs led to the formation of thick carbonate platforms. Sometimes volcanic activity continued after the formation of the atoll or atoll-like structure, and during episodes where the platforms rose above sea level erosional features such as channels and blue holes developed. Eventually, these platforms drowned for reasons that are often unclear.

The formation of many such seamounts has been explained with the hotspot theory, which describes the formation of chains of volcanoes that get progressively older along the length of the chain, with an active volcano only at one end of the system. This volcano lies on a spot of the lithosphere heated from below; as the plate moves the volcano is moved away from the heat source and volcanic activity ceases, producing a chain of volcanoes that become older away from the currently active one. 

The "South Pacific Superswell" is a region in the Southern Pacific at the present-day Austral Islands, Cook Islands and Society Islands, where intense volcanic activity was underway during the Cretaceous, and is where the Cretaceous seamounts of the Mid-Pacific Mountains originated. The Easter hotspot, Marquesas hotspot and the Society hotspot may have been involved in the formation of the Mid-Pacific Mountains. After the Mountains had formed, plate tectonics shifted them northwards to their present-day position. Allison Guyot appears to have formed in the same region.

One drill core on Allison Guyot has found a 136-metre-thick layer of pelagic sediments, under which are 735-metre-thick limestones that formed in lagoons and might continue down for almost 600 metres. The limestone consists mostly of calcite with little dolomite and occurs in the form of bafflestone, grainstone, packstone, peloid, rudstone and wackestone; ooliths have also been found. The carbonates are of biogenic origin, and fossils of dasyclads, echinoderms, gastropods, green algae, molluscs, ostracods, oysters, red algae, rudists and sponges occur within the limestones; some of the fossils have partially dissolved and are thus poorly preserved. Remnants of crocodilians have been found within Aptian-Albian mudstones, together with fossils of fish and unidentified vertebrates. The limestone is partly altered by karstification and phosphatisation, and manganese has accumulated in the upper layers. 

Basalts occur in the form of cobbles and sills within the limestones. These basalts define an alkali basalt suite and contain clinopyroxene, feldspar, ilmenite, plagioclase, pyroxene, spinel and titanomagnetite. They probably also contained olivine but the basaltic rocks sampled are so heavily altered that no olivine is left. The basalts are typical of intraplate volcanism and their geochemistry shows evidence that fractional crystallisation and mixing between different magmas were involved in their genesis. The component minerals have often been completely altered to calcite, clays, gypsum, hematite, quartz and other, unidentified minerals, either when exposed above sea level or through hydrothermal fluids when the sills formed. The formation of the sills led to the hardening and hydrothermal alteration of the surrounding sediments.

Clays are found both within the limestones and in layers between the carbonates. They consist of berthierine, chlorite, feldspar, hydromica, illite, kaolinite, mica, quartz, serpentine, smectite and possible zeolite. The clays were in part derived from lateritic soils that developed on the volcanic island before they were completely buried in carbonates, and in part formed in settings with limited water exchange during lagoonal stages. Dolomite, gypsum and pyrite co-occur with some clays, and claystones have been found in some places. Mudstones with evidence of animal burrows and containing amber, glauconite, organic material including plant debris and pyrite have also been encountered; pyrite indicates that anoxic environments existed on Allison Guyot.

Black shale and coal form layers in one drill core. The lower limestones contain substantial amounts of organic material that originated from terrestrial settings, and remnants of animal burrows and plant roots have been found in many layers of the platform. The clays and mudstones are rich in organic material. Most of this organic material appears to come from plants but some material has been attributed to algae. Cells and tracheids can be found in the plant remnants.

Radiometric dating has been performed on some of the volcanic rocks. Potassium-argon dating on the sills has yielded ages of 102 ± 6 million years ago and 87 ± 3 million years ago, while argon-argon dating also on the sills produced ages of 111.1 ± 2.6 million years, 111.2 ± 1.2 million years ago and 104.8 ± 0.8 million years ago. Other ages from the sills are about 110.7 ± 1.2 million years ago and 104.9 ± 2.0 million years ago. Rocks dredged from the slopes of Allison Guyot have yielded ages of 101.2 ± 0.8 million years, 102.7 ± 2.7 million years and 85.6 ± 1.3 million years ago. Overall, the volcano is considered to be at least 111 million years old and volcanic activity probably spanned 30-25 million years and several stages.

Both the sills and the dredged rocks were probably erupted after the main shield stage and they may constitute a late stage of secondary volcanism; two or three separate stages might have taken place, including one which formed a secondary cone on the eastern side of Allison Guyot. This may indicate that the seamount passed over more than one hotspot. The volcano of Allison Guyot was apparently already partially eroded when the secondary volcanism took place. Paleomagnetic data taken from limestones show that Allison Guyot developed in the Southern Hemisphere, at a latitude of about 11.2° ± 2.0° south.

Allison Guyot began as a volcanic island with a relief of perhaps 1.3 kilometres. Located in equatorial waters suited to the deposition of carbonate platforms, limestone platform grew on the guyot as it rapidly subsided during the Albian. Eventually, the seamount became an atoll. Volcanic rocks cropped out for some time before they were buried into the carbonates, and weathering products of the volcanic rocks accumulated in the limestones. The islands were covered by vegetation, and vegetation cover decreased over time as the volcanic edifice sank. The climate was probably humid and runoff was intense.

The platform contains lagoon and swamp environments, with water depths not exceeding 10 metres, and at some stage contained sand shoals and islands formed by storms as well. The interior was not protected from the sea and the sector of the platform that was investigated by drill cores apparently became increasingly accessible to it over time. The inner platform had a quiet muddy water setting; generally Allison Guyot at that time resembled the present-day Bikini and Eniwetok atolls in terms of morphology when Allison Guyot was emergent. 

The carbonate deposits indicate sea level changes following orbital cycles consistent with Milankovitch forcing; parts of the platform occasionally rose above sea level. At some point, karst environments existed on Allison Guyot and are probably the reason for the irregular surface of the summit platform and the presence of sinkholes; there are clear indications of about 200 metres of emergence.

Bivalves including rudists, corals, echinoderms, foraminifera, green algae, hydrozoans, red algae and sponges have been found in the platform's deposits. Rudists were at that time important reef builders and together with sponges colonised the margin of the platform. Among the rudist species discovered on Allison Guyot is "Requienia cf. migliorinii". Teeth of crocodilians have been found on the seamount. Its 110 million years old remnants are the oldest known crocodilians in the region of the Pacific Ocean. They indicate that such species lived within the lagoon of Allison Guyot and may give clues about the history of Pacific animals and their dispersal.

A carbonate platform is said to 'drown' when sedimentation can no longer keep up with relative rises in sea level. Carbonate sedimentation on Allison Guyot ended during late Albian times, about 99 ± 2 million years ago, at the same time as at Resolution Guyot. By Turonian (93.9 – 89.8 ± 0.3 million years ago) times, pelagic sedimentation was prevailing on Allison Guyot. On both Allison and Resolution Guyots, the drowning was preceded by an episode where the platform rose above the sea; possibly it was this emergence and the following submergence which terminated carbonate deposition and prevented it from beginning again. Such emergence and drowning has been recorded at carbonate platforms of that age around the world and may be the consequence of tectonic events across the Pacific Ocean, culminating in the uplift of a part thereof. At that time, a last phase of volcanic activity on Allison Guyot generated several cones on its eastern part. The evidence for this theory is not conclusive, and another theory holds that the drowning of Allison Guyot occurred when it moved through equatorial waters, where upwelling increased the amount of nutrients available, hampering the growth of platforms. The waters might also have been too hot to support the survival of reef builders, as happens in present-day coral bleaching events.

About 160 metres of pelagic sediment in the form of sand, ooze and pelagic limestone accumulated on Allison Guyot; pelagic limestone is of Turonian to Campanian (83.6 ± 0.2 – 72.1 ± 0.2 million years ago) age while the oozes and sands were deposited starting in the early Paleocene (66–56 million years ago). In drill cores, the ooze has a sandy, watery habitus owing to the prevalence of fossil foraminifera in the sediment. The pelagic sediments have been bioturbated in some places and modified by sea currents, which have formed the large mound of pelagic sediment. In drill cores, the ooze overlies Cretaceous shallow-water limestones, which were modified by phosphatisation and manganese accumulation. As plate tectonics moved Allison Guyot northward, its surrounding water masses changed, as did the properties of the pelagic cap. Slumping of the platform occurred during the Cenozoic (the last 66 million years).

The pelagic ooze bears evidence of the Paleocene-Eocene thermal maximum, including temporary dissolution of carbonates, changes in the isotope ratios of carbon in sediments on Allison Guyot and changes in foraminifera and ostracod fossils found in the ooze. The latter underwent a major extinction during the Paleocene-Eocene Thermal Maximum on the seamount and took a long time to recover.

Sea currents have altered the pelagic deposits by removing smaller particles. In particular deposits from warmer periods have been altered in this way on Allison Guyot, perhaps because warmer climates increased hurricane activity and thus the energy available in sea currents or deep-sea circulations shifted. Furthermore, pauses in the sedimentation or episodes of slowdown have been identified.



</doc>
<doc id="58686049" url="https://en.wikipedia.org/wiki?curid=58686049" title="Gascon campaign of 1345">
Gascon campaign of 1345

Between August and November 1345 Henry, Earl of Derby, conducted the energetic Gascon campaign of 1345 in Gascony, an English-controlled territory in south west France. The campaign was part of the Hundred Years' War, and Derby, commanding an Anglo-Gascon force, oversaw the first successful land campaign of the war. He twice defeated large French armies in battle, taking many noble and knightly prisoners. They were ransomed by their captors, greatly enriching Derby and his soldiers in the process. Following this campaign, morale and prestige swung England's way in the border region between English-occupied Gascony and French-ruled territory, providing an influx of taxes and recruits for the English armies. As a result, France's ability to raise tax money and troops from the region was much reduced.

Ralph, Earl of Stafford, sailed for Gascony in February 1345 with an advance force and, following conventional practice, laid siege to two French strongholds. Derby arrived in August and immediately concentrated available Anglo-Gascon forces and headed directly for the largest French force, which was gathering at Bergerac, east of Bordeaux. Bergerac had good river supply links to Bordeaux and would provide a suitable forward base from which to carry the war to the French. He decisively defeated the French there, before moving to besiege the provincial capital of Périgueux. By this time the French had diverted their main effort to the south west, under the overall command of John, Duke of Normandy, the son and heir of the French king. Unable to take Périgueux, and threatened by John's much larger force, Derby left garrisons blockading it and withdrew. One garrison, at Auberoche, was besieged by the French. Derby advanced with a small force, launched a surprise attack against the much larger French army and won another decisive victory.

The French army started to disintegrate: men were unpaid, even unfed; there was a lack of fodder for the horses; desertion was rife; and troops were selling their equipment. John lost heart on hearing of the defeat at Auberoche. The French abandoned all of their ongoing sieges of other Anglo-Gascon garrisons and retreated to Angouleme, where John disbanded his army, possibly because the French had run out of money. Derby moved back to the Garonne valley, captured the strong and well garrisoned town of La Réole, all of the French outposts downstream of it, and other strong French positions in the area. In November Derby paid off his army and overwintered in La Réole. Various small Anglo-Gascon groups maintained the pressure on the French, capturing several significant fortified places between December 1345 and March 1346.

Since the Norman conquest of England in 1066, English monarchs had held titles and lands within France, the possession of which made them vassals of the kings of France. The status of the English king's French fiefs was a major source of conflict between the two monarchies throughout the Middle Ages. French monarchs systematically sought to check the growth of English power, stripping away lands as the opportunity arose. Over the centuries, English holdings in France had varied in size, but by 1337 only Gascony in south western France and Ponthieu in northern France were left. The independent-minded Gascons had their own customs and claimed to have a separate language; they preferred their relationship with a distant English king who left them alone, to one with a French king who would interfere in their affairs. Following a series of disagreements between Philip VI of France () and Edward III of England (), on 24 May 1337 Philip's Great Council agreed that the Duchy of Aquitaine, effectively Gascony, should be taken back into Philip's hands on the grounds that Edward was in breach of his obligations as a vassal. This marked the start of the Hundred Years' War, which was to last one hundred and sixteen years.

During the first half of the 14th century well over 1,000 ships a year departed Gascony for England. Among their cargos were more than of wine. The duty levied by the English Crown on wine from Bordeaux was more than all other customs duties combined and by far the largest source of state income. Bordeaux, the capital of Gascony, grew rich on this trade; it had a population of over 50,000, greater than London's, and Bordeaux was possibly richer. However, by this time English Gascony had become so truncated by French encroachments that it relied on imports of food, largely from England. Any interruptions to regular shipping were liable to starve Gascony and financially cripple England; the French were well aware of this.

Although Gascony was the cause of the war, Edward was able to spare few resources for it. When an English army had campaigned on the continent earlier in the war it had operated in northern France, causing the Gascons to largely rely on their own resources; they had been hard pressed as a consequence. In 1339 the French besieged Bordeaux, even breaking into the city with a large force before they were repulsed. Typically the Gascons could field 3,000–6,000 men, the large majority of whom were infantry, although up to two-thirds of them would be tied down in garrisons.

There was no formal border between English and French territory. Many landholders owned a patchwork of widely separated estates, perhaps owing fealty to a different overlord for each. Each small estate was likely to have a tower house, larger estates having castles. Fortifications were also constructed at transport choke points, to collect tolls and to restrict military passage, and fortified towns grew up alongside all bridges and most fords over the many rivers in the region. Military forces could support themselves by foraging so long as they moved on at relatively frequent intervals. If they wished to remain in one place for any length of time, as was necessary to besiege a castle, then access to water transport was essential for supplies of food and fodder and desirable for such items as siege equipment. Warfare was usually a struggle for possession of castles and other fortified points, and for the mutable loyalty of the local nobility; the region had been in a state of flux for centuries and many local lords served whichever country was stronger, regardless of national ties.

By 1345, after eight years of war, English-controlled territory mostly consisted of a coastal strip from Bordeaux to Bayonne, and isolated strongholds further inland. The French had strong fortifications throughout what had once been English controlled Gascony. Several directly threatened Bordeaux: Libourne, to the east allowed French armies to assemble a day's march from Bordeaux; the strongly fortified town of Blaye was situated on the north bank of the Gironde downstream of Bordeaux and in a position to interdict its vital seaborne communications; the fortress of Langon, south of Bordeaux, blocked upstream communication along the Garonne, and facilitated the supply of any French force advancing on Bordeaux.

Edward determined early in 1345 to attack France on three fronts. The Earl of Northampton would lead a small force to Brittany, a slightly larger force would proceed to Gascony under the command of Henry, Earl of Derby and the main force would accompany Edward to either northern France or Flanders. The previous Seneschal of Gascony, Nicholas de la Beche, was replaced by the more senior Ralph, Earl of Stafford, who sailed for Gascony in February with an advance force. Derby was appointed the King's Lieutenant in Gascony on 13 March 1345 and received a contract to raise a force of 2,000 men in England, and further troops in Gascony itself. The highly detailed contract of indenture had a term of six months from the opening of the campaign in Gascony, with an option for Edward to extend it for a further six months on the same terms. Derby was given a high degree of autonomy, for example his strategic instructions were: "si guerre soit, et a faire le bien q'il poet" (... if there is war, do the best you can...).

French intelligence had uncovered the English plan for offensives in the three theatres, but they did not have the money to raise an army in each. They anticipated, correctly, that the English planned to make their main effort in northern France. Thus they directed what resources they had there, planning to assemble their main army at Arras on 22 July. South western France was encouraged to rely on its own resources, but as the Truce of Malestroit, signed in early 1343, was still in effect, the local lords were reluctant to spend money, and little was done.

Derby's force embarked at Southampton at the end of May. Bad weather forced his fleet of 151 ships to shelter in Falmouth for several weeks en route, finally departing on 23 July. The Gascons, primed by Stafford to expect Derby's arrival in late May and sensing the French weakness, took the field without him. The Gascons captured the large, weakly garrisoned castles of Montravel and Monbreton on the Dordogne in early June; both were taken by surprise and their seizure broke the tenuous Truce of Malestroit. Stafford carried out a short march north to besiege Blaye with his advance party and perhaps 1,000 men-at-arms and 3,000 infantry of the Gascon lords. Having established the siege he left the Gascons to prosecute it and proceeded to Langon, south of Bordeaux, and set up a second siege. The Anglo-Gascon forces at both sieges could be readily supplied by ship. The French issued an urgent call to arms.

Meanwhile, small independent parties of Gascons raided across the region. Local French groups joined them, and several minor nobles threw in their lot with the Anglo-Gascons. They had some successes, but their main effect was to tie down most of the weak French garrisons in the region and to cause them to call for reinforcements. The few French troops in the region not garrisoning their fortifications immobilised themselves with sieges: of Casseneuil in the Agenais; Monchamp near Condom; and Montcuq, a strong but strategically insignificant castle south of Bergerac. Large areas were effectively undefended.

Edward's main army sailed on 29 June. They anchored off Sluys in Flanders until 22 July, while Edward attended to diplomatic affairs. When they sailed, probably intending to land in Normandy, they were scattered by a storm and found their way to English ports over the following week. After more than five weeks on board ship the men and horses had to be disembarked. There was a further week's delay while the King and his council debated what to do, by which time it proved impossible to take any action with the main English army before winter. Aware of this, Philip despatched reinforcements to Brittany and Gascony. Peter, Duke of Bourbon was appointed commander-in-chief of the south west front on 8 August.

On 9 August 1345 Derby arrived in Bordeaux with 500 men-at-arms, 1,500 English and Welsh archers, 500 of them mounted on ponies to increase their mobility, and ancillary and support troops, such as a team of 24 miners. A high proportion of the archers and some of the men-at-arms were convicted felons promised pardons if they served for the duration of the campaign, but the majority, including many of the felons, were veterans of other campaigns. After two weeks of further recruiting and organising Derby marched his army to Langon, rendezvoused with Stafford and took command of the combined force. While Stafford had to this point pursued a cautious strategy, Derby's intention was quite different. Rather than continue a war of sieges he was determined to strike directly at the French before they could concentrate their forces. The French forces in the region were under Bertrand de l'Isle-Jourdain, since the Duke of Bourbon had not yet arrived. Hearing of Derby's arrival, he decided to fall back to the communications centre and strategically important town of Bergerac, east of Bordeaux, where there was an important bridge over the Dordogne River. This was a convenient place to concentrate French forces and assimilate reinforcements.

After a council of war Derby decided to strike at the French at Bergerac. The capture of the town, which had good river supply links to Bordeaux, would provide the Anglo-Gascon army with a base from which to carry the war to the French. It would also force the lifting of the siege of the nearby castle of Montcuq and sever communications between French forces north and south of the Dordogne. The English believed that the town could be easily taken if the French field army could be beaten or distracted. After eight years of defensive warfare by the Anglo-Gascons, there was no expectation among the French that they might make any offensive moves.
Derby moved rapidly and took the French army at Bergerac by surprise on 26 August, decisively beating them in a running battle. The exact details of the battle are confused and there are contradictions between the original sources, which is reflected in the modern accounts. Clifford Rogers provides a summary of the contemporary accounts, their discrepancies, and the treatment of these by modern historians. In any event, French casualties were heavy, many being killed or captured. Prisoners included Henri de Montigny, Seneschal of Périgord, ten other senior noblemen and many lesser nobles. Derby's share of the ransoms and the loot was estimated at £34,000 (£ in 2019 terms), approximately four times the annual income from his lands. The survivors of the French field army rallied around de l'Isle and retreated north to Périgueux. Within days of the battle, Bergerac fell to an Anglo-Gascon assault and was subsequently sacked. Strategically, the Anglo-Gascon army had secured an important base for further operations. Politically, local lords who had been undecided in their allegiance had been shown that the English were again a force to be reckoned with.

Derby consolidated and reorganised for two weeks, left a large garrison in the town and moved north to the Anglo-Gascon stronghold of Mussidan in the Isle valley with 6,000–8,000 men. He then pushed west to Périgueux, the provincial capital of Périgord, taking several strongpoints on the way. Périgueux's defences were antiquated and derelict, but the size of the French force defending it prohibited an assault. Derby blockaded Périgueux and captured strongholds blocking the main routes into the city. John, Duke of Normandy, the son and heir of Philip VI, replaced the Duke of Bourbon, gathered an army reportedly numbering over 20,000 and manoeuvred in the area. In early October a very large detachment relieved the city and drove off Derby's force, which withdrew towards Bordeaux. Further reinforced, the French started besieging the English-held strongpoints. A French force of 7,000, commanded by Louis of Poitiers, besieged the castle of Auberoche, east of Périgueux. Auberoche perches on a rocky promontory completely commanding the River Auvézère. The French encampment was divided in two, the majority of the soldiers camped close to the river between the castle and village while a smaller force was situated to prevent any relief attempts from the north. The chronicler Froissart tells an improbable tale that a soldier attempting to reach the English lines with a letter requesting help was captured and returned to the castle via a trebuchet. A messenger did get through to Derby, who was already returning to the area with a scratch force of 1,200 English and Gascon soldiers: 400 men-at-arms and 800 mounted archers.

After a night march Derby attacked the French camp on 21 October while they were at dinner, taking them by surprise and causing heavy initial casualties. The French rallied and there was a protracted hand-to-hand struggle, which ended when the commander of the small English garrison in the castle sortied and fell upon the rear of the French. They broke and fled. Derby's mounted men-at-arms pursued them relentlessly. French casualties are uncertain, but were heavy. They are described by modern historians as "appalling", "extremely high", "staggering", and "heavy". Many French nobles were taken prisoner; lower ranking men were, as was customary, put to the sword. The French commander, Louis of Poitiers, died of his wounds. Surviving prisoners included the second in command, Bertrand de l'Isle-Jourdain, two counts, seven viscounts, three barons, the seneschals of Clermont and Toulouse, a nephew of the Pope and so many knights that they were not counted. The ransoms alone made a fortune for many of the soldiers in Derby's army, as well as Derby himself, who was said to have made at least £50,000 (£ in 2019 terms) from the day's captives. Over the following year Philip paid large amounts from the royal treasury as contributions towards the captives' ransoms.

The Duke of Normandy lost heart on hearing of the defeat. There are accounts that he resigned his command and returned to Paris, only to be reinstated and sent back by his father, the King. The French abandoned all of their ongoing sieges of other Anglo-Gascon strongpoints. There were reports of the French army disintegrating: men unpaid, even unfed; lack of fodder for the horses; desertion; troops selling their equipment. Despite heavily outnumbering the Anglo-Gascon force the Duke of Normandy retreated to Angouleme and disbanded his army, possibly because the French had run out of money. Derby was left almost completely unopposed for five months.

Derby moved south after his victory, falling back on his communications as winter weather was setting in. He started clearing French fortifications from the border of English territory: the small castle at Pellegrue surrendered; that at Monségur was stormed. He then moved on the large, strongly fortified town of La Réole. This occupied a key position on the north bank of the Garonne river, only from Bordeaux. The town had been English until captured by the French twenty-one years earlier. It had enjoyed considerable autonomy and lucrative trading privileges, which it had lost under the French. After negotiations with Derby, on 8 November the citizens distracted the large French garrison and opened a gate for the English. The garrison fled to the citadel, which was considered exceptionally strong; the English proceeded to mine it. The garrison agreed a provisional surrender; if they were not relieved within five weeks they would leave. They were allowed to communicate this to the Duke of Normandy, but as he had just disbanded his army, and it was anyway mid-winter, there was little he could do. In early January 1346 the garrison left and the English replaced them. The town regained its previous privileges. Derby spent the rest of the winter there.

While this was happening the main Gascon forces disbanded. Many of the English soldiers took ship for home. After the main forces had gone home for the winter, small groups of Anglo-Gascons remained active. They cleared the valley of the Garonne downstream of La Réole of French presence, and raided the poorly fortified towns and weakly garrisoned French castles and smaller fortifications within their reach. Langon, which had resisted Stafford in the summer, was taken. Frequently Derby negotiated trade concessions or privileges with towns, or reinstated previous ones, to encourage them to open their gates to the English forces. The towns could more readily participate in the lucrative export trade through Bordeaux if under English rule, and Derby’s performance lessened their fear of French retribution. Derby had considerable success with this approach, which also allowed the towns to avoid the possible dangers of a siege or sack. The citizens of several French garrisoned towns persuaded the soldiers to withdraw, so that they could pre-emptively surrender to Derby. In at least one case they seized the French soldiers in their beds and expelled them. A letter from a committee of French garrison commanders sent to Philip VI in November claimed that towns were defecting to the English on a "daily" basis, and to Anglo-Gascon forces of trivial size. The French defenders were thoroughly demoralised.

The Duke of Bourbon, the newly-appointed French Seneschal of Gascony, unexpectedly found himself under assault. Stafford marched on the vitally important town of Aiguillon, which commanded the junction of the Rivers Garonne and Lot, "the key to Gascony", in late November. The inhabitants attacked the garrison and opened the gates to the English. By March 1346 almost the entire province of Agenais was in English hands. Bourbon held only Agen, the capital, and four castles; all were blockaded by the English. Morale was not good. As Bourbon started to assemble a new army at Agen, fights broke out with the townsfolk. Several Italian mercenaries were lynched. Since the start of the campaign the English had captured over 100 towns and castles.

The campaign had been a disaster for the French, the worse for being unexpected; during the previous eight years of the war, the Anglo-Gascons had made no large-scale offensive moves. They had lost towns and castles; suffered heavy casualties; and had many nobles taken prisoner, who would not be available to fight until they had paid their heavy ransoms, much of which would go to fund the English war effort. Towns throughout south west France embarked on urgent and expensive programmes to repair, improve or in some cases build from scratch their fortifications. They also paid attention to keeping them adequately garrisoned. It became next to impossible to raise tax money from the region, or to persuade men to serve away from home. This extended to areas far from where Derby had campaigned.

After two and a half years of uneasy peace, the war had restarted with a series of French humiliations. In the border region morale and, more importantly, prestige had decidedly swung England's way following this campaign, providing an influx of taxes and recruits for the English armies. Large parts of the Gascon nobility had been wavering with regard to committing themselves, and Derby’s victories tipped the balance for many. Local lords of note declared for the English, bringing their retinues with them. Derby's success was essentially defensive: he had secured Gascony. He had also set the scene for a possible future Anglo-Gascon offensive; this was to come late the following year, with his series of mounted raids. For the time being, while French financial resources were hampered, their ability to raise and project large forces were not seriously impaired.

Modern historians have praised the generalship demonstrated by Derby in this campaign: "superb and innovative tactician"; "ris[ing] to the level of genius"; "brilliant in the extreme"; "stunning"; "brilliant". A chronicler writing fifty years after the event described him as "one of the best warriors in the world". The four-month campaign has been described as "the first successful land campaign of ... the Hundred Years' War", which had commenced more than eight years earlier. Derby went on to lead another successful campaign in 1346.

In October 1345 Northampton finally commenced his campaign in northern Brittany, but it fizzled out in a series of failures to capture French-held Breton towns. The French decided to make their main effort in 1346 against Gascony. A large French army, "enormously superior" to any force Derby could field, assembled early in the campaigning season under the Duke of Normandy and marched up the Garonne valley. Their plan was to retake La Réole; to ensure their lines of supply they first had to retake Aiguillon. Stafford, in charge of Aiguillon's Anglo-Gascon garrison of 900 men, withstood an eight-month siege. Derby concentrated the main Anglo-Gascon force at La Réole, as a threat, and ensured that the French were never able to fully blockade the town. They found that their own supply lines were seriously harassed.

Edward III was meanwhile assembling a large army in England. The French were aware of this, but anticipated that it would sail to Gascony and attempt to relieve Aiguillon. Instead it landed in Normandy in July, achieving strategic surprise and starting the Crécy campaign. Philip VI ordered his son to abandon the siege and march north; after several delays John did, arriving in Picardy two weeks after Philip's army had been decisively beaten at the Battle of Crécy with very heavy losses. The areas facing Derby were left effectively defenceless, and he sent local, Gascon, forces to besiege the few major strongholds in the region still held by the French. Taking a force of approximately 2,000 Derby set out from La Réole on a grand "chevauchée", a great mounted raid. During the following two months this was devastatingly successful. Not only Gascony, but much of the Duchy of Aquitaine was left securely in English hands. It was to be held until formally ceded by the French in 1360 in the Treaty of Brétigny.



</doc>
<doc id="58719686" url="https://en.wikipedia.org/wiki?curid=58719686" title="Georgetown Car Barn">
Georgetown Car Barn

The Georgetown Car Barn is a historic building in the Georgetown neighborhood of Washington, D.C., in the United States. Designed by the architect Waddy Butler Wood, it was built between 1895 and 1897 by the Capital Traction Company as a union terminal for several Washington and Virginia streetcar lines. The adjacent "Exorcist" steps, later named after their appearance in William Friedkin's 1973 horror film "The Exorcist", were built during the initial construction to connect M Street with Prospect Street.

Intended for dual use as a passenger station and as a storage house for the streetcars, the Car Barn began Washington's only cable car system. Almost immediately after the building opened, the system was electrified and the Car Barn was converted to accommodate electric streetcars. Throughout its history as a terminal and storage facility, the Car Barn was never utilized to the extent anticipated by its construction.

The building has undergone several renovations, the most extensive in 1911, when the original Romanesque Revival façade was significantly modified and the interior was almost completely gutted. Not long after its opening, the building fell into disrepair. Changing ownership over time, it maintained its original function of housing streetcars until 1950, when it was redeveloped as office space. Among its occupants was the International Police Academy, an arm of the Central Intelligence Agency, which operated out of the Car Barn in the 1960s and 1970s. Today, it is used as an academic building by Georgetown University.

The Car Barn's original foundation supported a warehouse constructed in 1761 to store tobacco for auction unloaded from ships docked at the present-day Key Bridge. The warehouse was converted to keep horses and their trolleys around 1861. On August 23, 1894, Congress authorized the extension of an existing trolley line terminating at the intersection of Bridge and High Streets (now Wisconsin Avenue and M Street respectively) to the intersection of M and 36th Streets. With the authorization, Congress required that a union station be erected at the site. Thereafter, the site was used to store horse-drawn trolley cars.

Construction on the building then known as Union Station began in early 1895 under the architectural direction of Waddy Butler Wood. The superintendent and chief engineer of the Capital Traction Company, D.S. Carll, was in charge of the construction. Before construction of the Car Barn, the two streets were joined by a steep hillside that carried 36th Street. Large amounts of earth were excavated— in total—resulting in the sharp cliff that exists today. Adjacent to the Car Barn are a set of stairs commonly known as the "Exorcist" steps and a large retaining wall, which were built at the time the Car Barn was constructed, to connect M and Prospect Streets. The steps are so named as they provided the location for the scene in the 1973 horror film "The Exorcist" where the priest is thrown down the stairs to his death.

The building's construction was opposed by the next-door resident of the Prospect House, who furnished affidavits by prominent architects stating that blasting from the construction was damaging her house; this led to court-ordered supervision of the blasting in 1894. After the Car Barn's construction, the large edifice obstructed the view of the Potomac River and Virginia from homes on Prospect Street, including the well-known cottage of E. D. E. N. Southworth. For this reason, some considered it a "desecration" of the local scenery.
The three-story, building was opened on May 27, 1897, containing offices for the several tenant trolley companies and waiting rooms that were decorated with red oak wainscot panelling, ornate iron stair railings, and stuccoed ceilings. The exterior was designed in the Romanesque Revival style. Its tower, which reached a height of , contained an elevator that shuttled passengers between the terminals. Many of the building's decorations reflect its original function. The pediment facing M Street reads "Capital Traction Company" and contains three decorative flywheels.

The M Street-facing first floor served the Washington and Georgetown Railroad. The second and third floors were connected with steel trestles to allow for trolleys coming across the Potomac River from Rosslyn, which served Washington, Arlington, Falls Church, and were projected to serve the Great Falls and Old Dominion Railroad. The roof, which was level with Prospect Street, was used by the Metropolitan Railroad and had a covered walkway for passengers to get from the elevator to Prospect street.

The station operated as Washington's only cable car trolley terminal for less than a year. Almost immediately after opening, the Car Barn was converted to operate the new electric streetcars. The Virginia lines never made use of the terminal and the Metropolitan Railroad did not use the station to the extent intended. It intended to place storage tracks on the roof of the building, but never did.

Although regarded as well-designed before 1900, the Car Barn thereafter began a period of deterioration and neglect lasting for 50 years. The first stage of the transition from a trolley station to an office building was carried out between 1906 and 1908, when portions of the second floor were converted into office space. The electrification of streetcars necessitated a large-scale re-design of the Barn, which began in 1910. To accommodate the larger cars, the entrances to the building were extended and a new elevator was installed to lift streetcars to the roof. This transition required a near complete reconstruction of the building. The steel support beams were replaced and the entire façade was changed to extend toward M Street and increase its height to allow more office space. These modifications were complete in 1911.

Further conversions of track space to office space occurred between 1921 and 1922. Extensive remodeling occurred again in 1933 with the designation of the Car Barn as the headquarters of the new Capital Transit Company, as a result of the merger between the Capital Traction Company and the Washington Railway and Electric Company, which increased the number of office workers at the building. These changes involved the removal of the roof in the center of the building, the creation of a lightwell on the third floor, the conversion of the third floor into office space, and the removal of the covered passageway on the roof.

The last streetcar operations at the Car Barn ended with the closure of the Roslyn–Benning Line on April 30, 1949. The building continued to store streetcars until May 1950. Toward the end of 1952, the first floor was converted into office space.

When the Capital Transit Company merged with its competitors, the building came under the ownership of its new corporate successor, the DC Transit System, in 1956. By then, the building had fallen into such a state of disrepair that the company deliberated over whether to demolish it entirely. Seeking to preserve the historic structure, it elected to redevelop it. The building underwent considerable interior renovations between 1957 and 1960, which were intended to turn the structure entirely into an office building. This involved lowering the ceilings, which were previously designed to accommodate the height of the streetcars. The building was included in the Historic American Buildings Survey in 1967.

By 1963, the Car Barn was home to the International Police Academy, operated by the Central Intelligence Agency (though officially part of the Agency for International Development) that trained Latin American police forces; members of these forces met at the Car Barn until the program was shut down in 1975.

In 1992, the owner of the DC Transit System, O. Roy Chalk, was subject to foreclosure, and the building came under the ownership of the Lutheran Brotherhood. The Car Barn was purchased by Douglas Development Corporation in 1997—which continues to own the building—and it was renovated the following year. The primary tenant is Georgetown University, which first began leasing space in the 1950s. After two years of renovation by the university that ended in 2017, the first floor was converted from a garage to house the Graduate School of Arts and Sciences and the Georgetown University Press. The building today has four floors and has a floor area of .




</doc>
<doc id="58834262" url="https://en.wikipedia.org/wiki?curid=58834262" title="Siege of Aiguillon">
Siege of Aiguillon

The Siege of Aiguillon, an episode in the Hundred Years' War, commenced on 1 April 1346 when a French army commanded by John, Duke of Normandy, laid siege to the Gascon town of Aiguillon. The town was defended by an Anglo-Gascon army under Ralph, Earl of Stafford.

In 1345 Henry, Earl of Lancaster was sent to Gascony in south west France with 2,000 men and large financial resources. In 1346 the French focused their effort on the south west and, early in the campaigning season, an army of 15,000–20,000 men marched down the valley of the Garonne. Aiguillon commands both the Rivers Garonne and Lot, and it was not possible to sustain an offensive further into Gascony unless the town was taken. Duke John, the son and heir of Philip VI, laid siege to the town. The garrison, some 900 men, sortied repeatedly to interrupt the French operations, while Lancaster concentrated the main Anglo-Gascon force at La Réole, some away, as a threat. Duke John was never able to fully blockade the town, and found that his own supply lines were seriously harassed. On one occasion Lancaster used his main force to escort a large supply train into the town.

In July the main English army landed in northern France and moved towards Paris. Philip VI repeatedly ordered his son, Duke John, to break off the siege and bring his army north. Duke John, considering it a matter of honour, refused. By August, the French supply system had broken down, there was a dysentery epidemic in their camp, desertion was rife and Philip VI's orders were becoming imperious. On 20 August the French abandoned the siege and their camp and marched away. Six days later the main French army was decisively beaten in the Battle of Crécy with very heavy losses. Two weeks after this defeat, Duke John's army joined the French survivors.

Since the Norman Conquest of 1066, English monarchs had held titles and lands within France, the possession of which made them vassals of the kings of France. Over the centuries, English holdings in France had varied in size, but by 1337 only Gascony in south western France and Ponthieu in northern France were left. The independent minded Gascons had their own customs and claimed to have a separate language. They preferred their relationship with a distant English king who left them alone to one with a French king who would interfere in their affairs. Following a series of disagreements between Philip VI of France and Edward III of England, on 24 May 1337 Philip's Great Council in Paris agreed that the Duchy of Aquitaine, effectively Gascony, should be taken back into Philip's hands on the grounds that Edward was in breach of his obligations as a vassal. This marked the start of the Hundred Years' War, which was to last one hundred and sixteen years.

During the first half of the 14th century well over 1,000 ships departed Gascony for England each year. Among their cargo were more than of locally produced wine. The duty levied by the English Crown on wine from Bordeaux exceeded all other customs duties combined and was by far the largest source of state income. Bordeaux, the capital of Gascony, had a population of over 50,000, greater than London's, and Bordeaux was possibly richer. However, by this time English Gascony had become so truncated by French encroachments that it relied on imports of food, largely from England. Any interruptions to regular shipping were liable to starve Gascony and financially cripple England; the French were well aware of this.

Although Gascony was the cause of the war, Edward was able to spare few resources for it and whenever an English army campaigned on the continent it had operated in northern France. In most campaigning seasons the Gascons had had to rely on their own resources and had been hard-pressed by the French. In 1339, the French besieged Bordeaux, the capital of Gascony, even breaking into the city with a large force before they were repulsed. Typically the Gascons could field 3,000–6,000 men, the large majority infantry, although up to two thirds of them would be tied down in garrisons.
The border between English and French territory in Gascony was extremely unclear, to the extent that the idea of a "border" is anachronistic. Most significant landholders owned a patchwork of widely separated estates, perhaps owing fealty to a different overlord for each. Each small estate was likely to have a fortified tower or keep, and larger estates had castles. Fortifications were also constructed at transport choke points, to collect tolls and to restrict military passage, and fortified towns grew up alongside all bridges and most fords over the many rivers in the region. Military forces could support themselves by foraging so long as they moved on at relatively frequent intervals. If they wished to remain in one place for any length of time, as was necessary to besiege a castle, then access to water transport was essential for supplies of food and fodder and desirable for such items as siege equipment. Warfare was usually a struggle for possession of castles and other fortified points, and for the mutable loyalty of the local nobility; the region had been in a state of flux for centuries and many local lords served whichever country was stronger, regardless of national ties.

By 1345, after eight years of war, English controlled territory mostly consisted of a coastal strip from Bordeaux to Bayonne, with isolated strongholds further inland. During 1345, Henry, Duke of Lancaster, had led a whirlwind campaign at the head of an Anglo-Gascon army. He had smashed two large French armies at the battles of Bergerac and Auberoche, captured French towns and fortifications in much of Périgord and most of Agenais and given the English possessions in Gascony strategic depth. During the winter following this successful campaign Lancaster's second in command, Ralph, Earl of Stafford, had marched on the vitally important town of Aiguillon, which commanded the junction of the Rivers Garonne and Lot. The town's inhabitants had attacked the garrison and opened the gates to the English.

John, Duke of Normandy, the son and heir of Philip VI, was placed in charge of all French forces in south west France, as he had been the previous autumn. He assembled at Orléans the largest army the French had yet fielded in the south west. It was supported by every military officer of the royal household. As always, money was short. In spite of borrowing over 330,000 florins (£ in 2019 terms) from the Pope, Duke John had to issue orders to local officials to: "Amass all of the money you can for the support of our wars. Take it from each and every person you can..." It was a clear indication of the desperate state of the French finances. A second army was formed at Toulouse, based on contingents from the Languedoc; it included a siege train and five cannon. Duke John planned to march on and besiege the large, strongly fortified town of La Réole on the north bank of the Garonne river, only from Bordeaux, which Lancaster had captured the previous year. Aiguillon commanded both the Lot and the Garonne and its possession was essential to supply any army around La Réole.

Lancaster understood that no French offensive could have a permanent effect so long as Aiguillon, described by modern historian Kenneth Fowler as "the key to the Gascon plain", was held, so he garrisoned it very strongly: 300 men-at-arms and 600 archers commanded by Stafford. The town was well stocked with supplies and materiel, although the physical defences were in a poor state. The main wall, long, was modern but incomplete – gaps were filled with improvised defences. A bridge over the River Lot was fortified and had a barbican gate, but it was centuries old and poorly maintained. There were two small forts within the town, both overlooking the Garonne. The northern wall of the town was protected by the Lot and the western by the Garonne, while the southern and eastern walls were more easily approached. Lancaster based himself in La Réole, on the Garonne downstream, throughout the siege.

The French armies assembled and marched unusually early in the campaigning season. By March they were both in the province of Quercy. The size of the French forces at this point is not recorded, but it has been estimated that later in the campaign they numbered between 15,000 and 20,000; modern historians have described the French army as a "huge force" and as "enormously superior" to any force the Anglo-Gascons could field. The army marched down the valley of the Garonne from Agen, reaching Aiguillon on 1 April. On 2 April the "arrière-ban", the formal call to arms for all able-bodied males, was announced for the south of France.

Isolating the town presented a problem for the French. The junction of the two rivers created three different areas, each of which would need to be interdicted. But separating their army into three divisions was inviting defeat in detail. They needed to be able to rapidly combine their forces if one part was threatened. A bridge over the Lot, from Aiguillon, was easily taken, but it was necessary to construct a new bridge over the Garonne. Duke John employed over 300 carpenters in its construction, escorted by 1,400 crossbowmen and an unknown but significant number of men-at-arms. The garrison sortied repeatedly against this work, sometimes several times a day. They twice broke it up, but it was completed by the end of May. The three parts of the French army each dug impressive earthworks, to protect themselves both from sorties by the garrison and from Lancaster's main army.

As was normal, within a matter of days the large French army had swept the surrounding area clear of supplies and was entirely dependent on the rivers for its logistics. The Anglo-Gascon army based in La Réole harassed the French foragers, intercepted their supplies and kept them in a constant state of alarm. Dysentery soon broke out in the French camps. In mid-June the French attempted to pass two large supply barges down the Lot to their contingent west of the Garonne. They needed to pass under the fortified bridge held by the garrison. The garrison sortied from the bridge's barbican, through the French lines, and captured the barges; bringing them into the town. Fierce fighting broke out as the sortie party attempted to retreat to the barbican, which after several hours was lost to the French. The garrison closed the gates and secured the town at the cost of trapping most of this party outside; the survivors were taken prisoner.

The French were unable to effectively isolate the town. Throughout the siege the Anglo-Gascons were able to run the blockade at will with small quantities of supplies and reinforcements. In July a larger force fought its way through with a greater quantity of supplies. From the start of the siege the French had concentrated their efforts against the southern side of the defences. At least twelve large siege engines, probably trebuchets, carried out a round-the-clock area bombardment. The results were considered unsatisfactory. In July an attack was attempted from the north, across the Lot, using three siege towers mounted on large barges. As they were being manoeuvred across the river one was hit by a missile from an English trebuchet, capsizing it with heavy loss. The attack was abandoned.

The siege became an end in itself for Duke John. Having laid siege to Aiguillon it was a matter of knightly honour not to retreat before it fell. At one point he solemnly vowed not to abandon the siege until he had occupied the town. By July the French were drawing supplies from over away, a distance barely sustainable with 14th-century overland transportation. In early 1346 the English captured the castle of Bajamont, from Agen, the capital of Agenais, on the Garonne. This was one of several strongholds from which the English carried out raids on the French lines of communication. In late July a French force of 2,000 men marched against it. The small English garrison under Galhart de Durfort attacked the French, defeated them, and captured their commander, Robert de Houdetot, the Seneschal of the Agenais. The French army began to starve; horses died for lack of fodder; the dysentery epidemic worsened; cases of desertion, increasingly to the English, mounted.

In 1345 Edward III had sent expeditionary forces to Gascony and Brittany and had assembled his main army for action in northern France or Flanders. It had sailed but never landed, after the fleet was scattered in a storm. Knowledge of Edward III's intent had kept French focus on the north until late in the campaigning season. In 1346 Edward III again gathered a large army, and the French once again became aware of this. The French assumed that Edward would sail for Gascony, where Lancaster was heavily outnumbered. To guard against any possibility of an English landing in northern France, Philip VI relied on his powerful navy.

This reliance was misplaced given the naval technology of the time and on 12 July an English army of 7,000–10,000 landed near Saint-Vaast-la-Hougue in north west Normandy. This force pillaged its way across the richest parts of France, capturing and sacking every town in its path. The English fleet paralleled its march, devastating everything up to five miles inland and destroying most of the French navy in its ports. Philip VI recalled his main army, under Duke John, from Gascony. After a furious argument with his advisers, and according to some accounts his father's messenger, Duke John refused to move until his honour was satisfied. On 29 July Philip VI called an "arrière-ban" for northern France at Rouen. On 7 August the English reached the Seine. Philip VI sent orders to Duke John insisting that he abandon the siege of Aiguillon and march his army north. Edward III marched south east and on 12 August his army was from Paris.

On 14 August, Duke John attempted to arrange a local truce. Lancaster, well aware of the situation in the north and in the French camps around Aiguillon, refused. On 20 August, after over five months, the French abandoned the siege and marched away in considerable haste and disorder. The French camps were left under the guard of local levies, who promptly deserted. The entire equipment of the French army was captured: supplies, materiel, siege engines and many horses. In at least the early stages of their retreat, discipline amongst the French was poor; there are accounts of men being jostled off the bridge over the Garonne and drowned. Stafford's garrison and other local Anglo-Gascon forces pursued closely. Part of Duke John's personal baggage was captured. French castles and minor fortifications along the Lot upstream from Aiguillon were mopped up, as were French positions between the Lot and the Dordogne.

Duke John and his army made contact with Philip VI on, or shortly after, 7 September, two weeks after the French army of the north, 20,000–25,000 strong, had been decisively beaten at the Battle of Crécy with very heavy losses. After Crécy the French stripped their garrisons in the south west to build-up a new army to face the main English threat in the north east. The areas facing Lancaster were effectively defenceless.

He launched three separate offensives between September and November. Local Gascon forces besieged the few major strongholds in the Bazadais region still held by the French; they were all taken, including the town of Bazas. Further Gascon forces raided to the east, deep into Quercy, penetrating over ; the modern historian Jonathan Sumption describes this as "dislocating the royal administration in central and southern France for three months". Meanwhile Lancaster himself took a small force, 1,000 men-at-arms and an unknown number of archers (possibly 1,000), to the north on a grand "chevauchée", a great mounted raid, during which he captured the rich provincial capital of Poitiers, and many towns and castles throughout Saintonge and Aunis. With these offensives, Lancaster moved the focus of the fighting from the heart of Gascony to 50 miles or more beyond its borders.


</doc>
