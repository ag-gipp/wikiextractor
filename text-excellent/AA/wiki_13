<doc id="49471" url="https://en.wikipedia.org/wiki?curid=49471" title="Osiris myth">
Osiris myth

The Osiris myth is the most elaborate and influential story in ancient Egyptian mythology. It concerns the murder of the god Osiris, a primeval king of Egypt, and its consequences. Osiris's murderer, his brother Set, usurps his throne. Meanwhile, Osiris's wife Isis restores her husband's body, allowing him to posthumously conceive their son, Horus. The remainder of the story focuses on Horus, the product of the union of Isis and Osiris, who is at first a vulnerable child protected by his mother and then becomes Set's rival for the throne. Their often violent conflict ends with Horus's triumph, which restores Maat (cosmic and social order) to Egypt after Set's unrighteous reign and completes the process of Osiris's resurrection.

The myth, with its complex symbolism, is integral to ancient Egyptian conceptions of kingship and succession, conflict between order and disorder, and especially death and the afterlife. It also expresses the essential character of each of the four deities at its center, and many elements of their worship in ancient Egyptian religion were derived from the myth.

The Osiris myth reached its basic form in or before the 24th century BCE. Many of its elements originated in religious ideas, but the struggle between Horus and Set may have been partly inspired by a regional conflict in Egypt's Early Dynastic or Prehistoric Egypt. Scholars have tried to discern the exact nature of the events that gave rise to the story, but they have reached no definitive conclusions.

Parts of the myth appear in a wide variety of Egyptian texts, from funerary texts and magical spells to short stories. The story is, therefore, more detailed and more cohesive than any other ancient Egyptian myth. Yet no Egyptian source gives a full account of the myth, and the sources vary widely in their versions of events. Greek and Roman writings, particularly "On Isis and Osiris" by Plutarch, provide more information but may not always accurately reflect Egyptian beliefs. Through these writings, the Osiris myth persisted after knowledge of most ancient Egyptian beliefs was lost, and it is still well known today.

The myth of Osiris was deeply influential in ancient Egyptian religion and was popular among ordinary people. One reason for this popularity is the myth's primary religious meaning, which implies that any dead person can reach a pleasant afterlife. Another reason is that the characters and their emotions are more reminiscent of the lives of real people than those in most Egyptian myths, making the story more appealing to the general populace. In particular, the myth conveys a "strong sense of family loyalty and devotion", as the Egyptologist J. Gwyn Griffiths put it, in the relationships between Osiris, Isis, and Horus.

With this widespread appeal, the myth appears in more ancient texts than any other myth and in an exceptionally broad range of Egyptian literary styles. These sources also provide an unusual amount of detail. Ancient Egyptian myths are fragmentary and vague; the religious metaphors contained within the myths were more important than coherent narration. Each text that contains a myth, or a fragment of one, may adapt the myth to suit its particular purposes, so different texts can contain contradictory versions of events. Because the Osiris myth was used in such a variety of ways, versions often conflict with each other. Nevertheless, the fragmentary versions, taken together, give it a greater resemblance to a cohesive story than most Egyptian myths.
The earliest mentions of the Osiris myth are in the "Pyramid Texts", the first Egyptian funerary texts, which appeared on the walls of burial chambers in pyramids at the end of the Fifth Dynasty, during the 24th century BCE. These texts, made up of disparate spells or "utterances", contain ideas that are presumed to date from still earlier times. The texts are concerned with the afterlife of the king buried in the pyramid, so they frequently refer to the Osiris myth, which is deeply involved with kingship and the afterlife. Major elements of the story, such as the death and restoration of Osiris and the strife between Horus and Set, appear in the utterances of the "Pyramid Texts". Funerary texts written in later times, such as the "Coffin Texts" from the Middle Kingdom (c. 2055–1650 BCE) and the "Book of the Dead" from the New Kingdom (c. 1550–1070 BCE), also contain elements of the myth.

Other types of religious texts give evidence for the myth, such as two Middle Kingdom texts: the Dramatic Ramesseum Papyrus and the Ikhernofret Stela. The papyrus describes the coronation of Senusret I, whereas the stela alludes to events in the annual festival of Khoiak. Rituals in both these festivals reenacted elements of the Osiris myth. The most complete ancient Egyptian account of the myth is the Great Hymn to Osiris, an inscription from the Eighteenth Dynasty (c. 1550–1292 BCE) that gives the general outline of the entire story but includes little detail. Another important source is the Memphite Theology, a religious narrative that includes an account of Osiris's death as well as the resolution of the dispute between Horus and Set. This narrative associates the kingship that Osiris and Horus represent with Ptah, the creator deity of Memphis. The text was long thought to date back to the Old Kingdom (c. 2686–2181 BCE) and was treated as a source for information about the early stages in the development of the myth. Since the 1970s, however, Egyptologists have concluded that the text dates to the New Kingdom at the earliest.

Rituals in honor of Osiris are another major source of information. Some of these texts are found on the walls of temples that date from the New Kingdom, the Ptolemaic era (323–30 BCE), or the Roman era (30 BCE to the fourth century AD). Some of these late ritual texts, in which Isis and Nephthys lament their brother's death, were adapted into funerary texts. In these texts, the goddesses' pleas were meant to rouse Osiris—and thus the deceased person—to live again.

Magical healing spells, which were used by Egyptians of all classes, are the source for an important portion of the myth, in which Horus is poisoned or otherwise sickened, and Isis heals him. The spells identify a sick person with Horus so that he or she can benefit from the goddess's efforts. The spells are known from papyrus copies, which serve as instructions for healing rituals, and from a specialized type of inscribed stone stela called a "cippus". People seeking healing poured water over these cippi, an act that was believed to imbue the water with the healing power contained in the text, and then drank the water in hope of curing their ailments. The theme of an endangered child protected by magic also appears on inscribed ritual wands from the Middle Kingdom, which were made centuries before the more detailed healing spells that specifically connect this theme with the Osiris myth.

Episodes from the myth were also recorded in writings that may have been intended as entertainment. Prominent among these texts is "The Contendings of Horus and Set", a humorous retelling of several episodes of the struggle between the two deities, which dates to the Twentieth Dynasty (c. 1190–1070 BCE). It vividly characterizes the deities involved; as the Egyptologist Donald B. Redford says, "Horus appears as a physically weak but clever Puck-like figure, Seth [Set] as a strong-man buffoon of limited intelligence, Re-Horakhty <nowiki>[</nowiki>Ra<nowiki>]</nowiki> as a prejudiced, sulky judge, and Osiris as an articulate curmudgeon with an acid tongue." Despite its atypical nature, "Contendings" includes many of the oldest episodes in the divine conflict, and many events appear in the same order as in much later accounts, suggesting that a traditional sequence of events was forming at the time that the story was written.

Ancient Greek and Roman writers, who described Egyptian religion late in its history, recorded much of the Osiris myth. Herodotus, in the 5th century BCE, mentioned parts of the myth in his description of Egypt in "The Histories", and four centuries later, Diodorus Siculus provided a summary of the myth in his "Bibliotheca historica". In the early 2nd century AD, Plutarch wrote the most complete ancient account of the myth in "On Isis and Osiris", an analysis of Egyptian religious beliefs. Plutarch's account of the myth is the version that modern popular writings most frequently retell. The writings of these classical authors may give a distorted view of Egyptian beliefs. For instance, "On Isis and Osiris" includes many interpretations of Egyptian belief that are influenced by various Greek philosophies, and its account of the myth contains portions with no known parallel in Egyptian tradition. Griffiths concluded that several elements of this account were taken from Greek mythology, and that the work as a whole was not based directly on Egyptian sources. His colleague John Baines, on the other hand, says that temples may have kept written accounts of myths, which later were lost, and that Plutarch could have drawn on such sources to write his narrative.

At the start of the story, Osiris rules Egypt, having inherited the kingship from his ancestors in a lineage stretching back to the creator of the world, Ra or Atum. His queen is Isis, who, along with Osiris and his murderer, Set, is one of the children of the earth god Geb and the sky goddess Nut. Little information about the reign of Osiris appears in Egyptian sources; the focus is on his death and the events that follow. Osiris is connected with life-giving power, righteous kingship, and the rule of "maat", the ideal natural order whose maintenance was a fundamental goal in ancient Egyptian culture. Set is closely associated with violence and chaos. Therefore, the slaying of Osiris symbolizes the struggle between order and disorder, and the disruption of life by death.

Some versions of the myth provide Set's motive for killing Osiris. According to a spell in the "Pyramid Texts", Set is taking revenge for a kick Osiris gave him, whereas in a Late Period text, Set's grievance is that Osiris had sex with Nephthys, who is Set's consort and the fourth child of Geb and Nut. The murder itself is frequently alluded to, but never clearly described. The Egyptians believed that written words had the power to affect reality, so they avoided writing directly about profoundly negative events such as Osiris's death. Sometimes they denied his death altogether, even though the bulk of the traditions about him make it clear that he has been murdered. In some cases the texts suggest that Set takes the form of a wild animal, such as a crocodile or bull, to slay Osiris; in others they imply that Osiris's corpse is thrown in the water or that he is drowned. This latter tradition is the origin of the Egyptian belief that people who had drowned in the Nile were sacred. Even the identity of the victim is changeable in texts, as it is sometimes the god Haroeris, an elder form of Horus, who is murdered by Set and then avenged by another form of Horus, who is Haroeris's son by Isis.

By the end of the New Kingdom, a tradition had developed that Set had cut Osiris's body into pieces and scattered them across Egypt. Cult centers of Osiris all over the country claimed that the corpse, or particular pieces of it, were found near them. The dismembered parts could be said to number as many as forty-two, each piece being equated with one of the forty-two nomes, or provinces, in Egypt. Thus the god of kingship becomes the embodiment of his kingdom.
Osiris's death is followed either by an interregnum or by a period in which Set assumes the kingship. Meanwhile, Isis searches for her husband's body with the aid of Nephthys. When searching for or mourning Osiris, the two goddesses are often likened to falcons or kites, possibly because kites travel far in search of carrion, because the Egyptians associated their plaintive calls with cries of grief, or because of the goddesses' connection with Horus, who is often represented as a falcon. In the New Kingdom, when Osiris's death and renewal came to be associated with the annual flooding of the Nile that fertilized Egypt, the waters of the Nile were equated with Isis's tears of mourning and/or with Osiris's bodily fluids. Osiris thus represented the life-giving divine power that was present in the river's water and in the plants that grew after the flood.

The goddesses find and restore Osiris's body, often with the help of other deities, including Thoth, a deity credited with great magical and healing powers, and Anubis, the god of embalming and funerary rites. Osiris becomes the first mummy, and the gods' efforts to restore his body are the mythological basis for Egyptian embalming practices, which sought to prevent and reverse the decay that follows death. This part of the story is often extended with episodes in which Set or his followers try to damage the corpse, and Isis and her allies must protect it. Once Osiris is made whole, Isis conceives his son and rightful heir, Horus. One ambiguous spell in the Coffin Texts may indicate that Isis is impregnated by a flash of lightning, while in other sources, Isis, still in bird form, fans breath and life into Osiris's body with her wings and copulates with him. Osiris's revival is apparently not permanent, and after this point in the story he is only mentioned as the ruler of the Duat, the distant and mysterious realm of the dead. Although he lives on only in the Duat, he and the kingship he stands for will, in a sense, be reborn in his son.

The cohesive account by Plutarch, which deals mainly with this portion of the myth, differs in many respects from the known Egyptian sources. Set—whom Plutarch, using Greek names for many of the Egyptian deities, refers to as "Typhon"—conspires against Osiris with seventy-two unspecified accomplices, as well as a queen from ancient Aethiopia (Nubia). Set has an elaborate chest made to fit Osiris's exact measurements and then, at a banquet, declares that he will give the chest as a gift to whoever fits inside it. The guests, in turn, lie inside the coffin, but none fit inside except Osiris. When he lies down in the chest, Set and his accomplices slam the cover shut, seal it, and throw it into the Nile. With Osiris's corpse inside, the chest floats out into the sea, arriving at the city of Byblos, where a tree grows around it. The king of Byblos has the tree cut down and made into a pillar for his palace, still with the chest inside. Isis must remove the chest from within the tree in order to retrieve her husband's body. Having taken the chest, she leaves the tree in Byblos, where it becomes an object of worship for the locals. This episode, which is not known from Egyptian sources, gives an etiological explanation for a cult of Isis and Osiris that existed in Byblos in Plutarch's time and possibly as early as the New Kingdom.

Plutarch also states that Set steals and dismembers the corpse only after Isis has retrieved it. Isis then finds and buries each piece of her husband's body, with the exception of the penis, which she has to reconstruct with magic, because the original was eaten by fish in the river. According to Plutarch, this is the reason the Egyptians had a taboo against eating fish. In Egyptian accounts, however, the penis of Osiris is found intact, and the only close parallel with this part of Plutarch's story is in "The Tale of Two Brothers", a folk tale from the New Kingdom with similarities to the Osiris myth.

A final difference in Plutarch's account is Horus's birth. The form of Horus that avenges his father has been conceived and born before Osiris's death. It is a premature and weak second child, Harpocrates, who is born from Osiris's posthumous union with Isis. Here, two of the separate forms of Horus that exist in Egyptian tradition have been given distinct positions within Plutarch's version of the myth.

In Egyptian accounts, the pregnant Isis hides from Set, to whom the unborn child is a threat, in a thicket of papyrus in the Nile Delta. This place is called "Akh-bity", meaning "papyrus thicket of the king of Lower Egypt" in Egyptian. Greek writers call this place "Khemmis" and indicate that it is near the city of Buto, but in the myth, the physical location is less important than its nature as an iconic place of seclusion and safety. The thicket's special status is indicated by its frequent depiction in Egyptian art; for most events in Egyptian mythology, the backdrop is minimally described or illustrated. In this thicket, Isis gives birth to Horus and raises him, and hence it is also called the "nest of Horus". The image of Isis nursing her child is a very common motif in Egyptian art.

There are texts in which Isis travels in the wider world. She moves among ordinary humans who are unaware of her identity, and she even appeals to these people for help. This is another unusual circumstance, for in Egyptian myth, gods and humans are normally separate. As in the first phase of the myth, she often has the aid of other deities, who protect her son in her absence. According to one magical spell, seven minor scorpion deities travel with and guard Isis as she seeks help for Horus. They even take revenge on a wealthy woman who has refused to help Isis by stinging the woman's son, making it necessary for Isis to heal the blameless child. This story conveys a moral message that the poor can be more virtuous than the wealthy and illustrates Isis's fair and compassionate nature.

In this stage of the myth, Horus is a vulnerable child beset by dangers. The magical texts that use Horus's childhood as the basis for their healing spells give him different ailments, from scorpion stings to simple stomachaches, adapting the tradition to fit the malady that each spell was intended to treat. Most commonly, the child god has been bitten by a snake, reflecting the Egyptians' fear of snakebite and the resulting poison. Some texts indicate that these hostile creatures are agents of Set. Isis may use her own magical powers to save her child, or she may plead with or threaten deities such as Ra or Geb, so they will cure him. As she is the archetypal mourner in the first portion of the story, so during Horus's childhood she is the ideal devoted mother. Through the magical healing texts, her efforts to heal her son are extended to cure any patient.

The next phase of the myth begins when the adult Horus challenges Set for the throne of Egypt. The contest between them is often violent but is also described as a legal judgment before the Ennead, an assembled group of Egyptian deities, to decide who should inherit the kingship. The judge in this trial may be Geb, who, as the father of Osiris and Set, held the throne before they did, or it may be the creator gods Ra or Atum, the originators of kingship. Other deities also take important roles: Thoth frequently acts as a conciliator in the dispute or as an assistant to the divine judge, and in "Contendings", Isis uses her cunning and magical power to aid her son.

The rivalry of Horus and Set is portrayed in two contrasting ways. Both perspectives appear as early as the "Pyramid Texts", the earliest source of the myth. In some spells from these texts, Horus is the son of Osiris and nephew of Set, and the murder of Osiris is the major impetus for the conflict. The other tradition depicts Horus and Set as brothers. This incongruity persists in many of the subsequent sources, where the two gods may be called brothers or uncle and nephew at different points in the same text.
The divine struggle involves many episodes. "Contendings" describes the two gods appealing to various other deities to arbitrate the dispute and competing in different types of contests, such as racing in boats or fighting each other in the form of hippopotami, to determine a victor. In this account, Horus repeatedly defeats Set and is supported by most of the other deities. Yet the dispute drags on for eighty years, largely because the judge, the creator god, favors Set. In late ritual texts, the conflict is characterized as a great battle involving the two deities' assembled followers. The strife in the divine realm extends beyond the two combatants. At one point Isis attempts to harpoon Set as he is locked in combat with her son, but she strikes Horus instead, who then cuts off her head in a fit of rage. Thoth replaces Isis's head with that of a cow; the story gives a mythical origin for the cow-horn headdress that Isis commonly wears.

In a key episode in the conflict, Set sexually abuses Horus. Set's violation is partly meant to degrade his rival, but it also involves homosexual desire, in keeping with one of Set's major characteristics, his forceful and indiscriminate sexuality. In the earliest account of this episode, in a fragmentary Middle Kingdom papyrus, the sexual encounter begins when Set asks to have sex with Horus, who agrees on the condition that Set will give Horus some of his strength. The encounter puts Horus in danger, because in Egyptian tradition semen is a potent and dangerous substance, akin to poison. According to some texts, Set's semen enters Horus's body and makes him ill, but in "Contendings", Horus thwarts Set by catching Set's semen in his hands. Isis retaliates by putting Horus's semen on lettuce-leaves that Set eats. Set's defeat becomes apparent when this semen appears on his forehead as a golden disk. He has been impregnated with his rival's seed and as a result "gives birth" to the disk. In "Contendings", Thoth takes the disk and places it on his own head; in earlier accounts, it is Thoth who is produced by this anomalous birth.

Another important episode concerns mutilations that the combatants inflict upon each other: Horus injures or steals Set's testicles and Set damages or tears out one, or occasionally both, of Horus's eyes. Sometimes the eye is torn into pieces. Set's mutilation signifies a loss of virility and strength. The removal of Horus's eye is even more important, for this stolen Eye of Horus represents a wide variety of concepts in Egyptian religion. One of Horus's major roles is as a sky deity, and for this reason his right eye was said to be the sun and his left eye the moon. The theft or destruction of the Eye of Horus is therefore equated with the darkening of the moon in the course of its cycle of phases, or during eclipses. Horus may take back his lost Eye, or other deities, including Isis, Thoth, and Hathor, may retrieve or heal it for him. The Egyptologist Herman te Velde argues that the tradition about the lost testicles is a late variation on Set's loss of semen to Horus, and that the moon-like disk that emerges from Set's head after his impregnation is the Eye of Horus. If so, the episodes of mutilation and sexual abuse would form a single story, in which Set assaults Horus and loses semen to him, Horus retaliates and impregnates Set, and Set comes into possession of Horus's Eye when it appears on Set's head. Because Thoth is a moon deity in addition to his other functions, it would make sense, according to te Velde, for Thoth to emerge in the form of the Eye and step in to mediate between the feuding deities.

In any case, the restoration of the Eye of Horus to wholeness represents the return of the moon to full brightness, the return of the kingship to Horus, and many other aspects of "maat". Sometimes the restoration of Horus's eye is accompanied by the restoration of Set's testicles, so that both gods are made whole near the conclusion of their feud.

As with so many other parts of the myth, the resolution is complex and varied. Often, Horus and Set divide the realm between them. This division can be equated with any of several fundamental dualities that the Egyptians saw in their world. Horus may receive the fertile lands around the Nile, the core of Egyptian civilization, in which case Set takes the barren desert or the foreign lands that are associated with it; Horus may rule the earth while Set dwells in the sky; and each god may take one of the two traditional halves of the country, Upper and Lower Egypt, in which case either god may be connected with either region. Yet in the Memphite Theology, Geb, as judge, first apportions the realm between the claimants and then reverses himself, awarding sole control to Horus. In this peaceable union, Horus and Set are reconciled, and the dualities that they represent have been resolved into a united whole. Through this resolution, order is restored after the tumultuous conflict.

A different view of the myth's end focuses on Horus's sole triumph. In this version, Set is not reconciled with his rival but utterly defeated, and sometimes he is exiled from Egypt or even destroyed. His defeat and humiliation is more pronounced in sources from later periods of Egyptian history, when he was increasingly equated with disorder and evil, and the Egyptians no longer saw him as an integral part of natural order.

With great celebration among the gods, Horus takes the throne, and Egypt at last has a rightful king. The divine decision that Set is in the wrong corrects the injustice created by Osiris's murder and completes the process of his restoration after death. Sometimes Set is made to carry Osiris's body to its tomb as part of his punishment. The new king performs funerary rites for his father and gives food offerings to sustain him—often including the Eye of Horus, which in this instance represents life and plenty. According to some sources, only through these acts can Osiris be fully enlivened in the afterlife and take his place as king of the dead, paralleling his son's role as king of the living. Thereafter, Osiris is deeply involved with natural cycles of death and renewal, such as the annual growth of crops, that parallel his own resurrection.

As the Osiris myth first appears in the "Pyramid Texts", most of its essential features must have taken shape sometime before the texts were written down. The distinct segments of the story—Osiris's death and restoration, Horus's childhood, and Horus's conflict with Set—may originally have been independent mythic episodes. If so, they must have begun to coalesce into a single story by the time of the "Pyramid Texts", which loosely connect those segments. In any case, the myth was inspired by a variety of influences. Much of the story is based in religious ideas and the general nature of Egyptian society: the divine nature of kingship, the succession from one king to another, the struggle to maintain "maat", and the effort to overcome death. For instance, the lamentations of Isis and Nephthys for their dead brother may represent an early tradition of ritualized mourning.

There are, however, important points of disagreement. The origins of Osiris are much debated, and the basis for the myth of his death is also somewhat uncertain. One influential hypothesis was given by the anthropologist James Frazer, who in 1906 said that Osiris, like other "dying and rising gods" across the ancient Near East, began as a personification of vegetation. His death and restoration, therefore, were based on the yearly death and re-growth of plants. Many Egyptologists adopted this explanation. But in the late 20th century, J. Gwyn Griffiths, who extensively studied Osiris and his mythology, argued that Osiris originated as a divine ruler of the dead, and his connection with vegetation was a secondary development. Meanwhile, scholars of comparative religion have criticized the overarching concept of "dying and rising gods", or at least Frazer's assumption that all these gods closely fit the same pattern. More recently, the Egyptologist Rosalie David maintains that Osiris originally "personified the annual rebirth of the trees and plants after the [Nile] inundation."
Another continuing debate concerns the opposition of Horus and Set, which Egyptologists have often tried to connect with political events early in Egypt's history or prehistory. The cases in which the combatants divide the kingdom, and the frequent association of the paired Horus and Set with the union of Upper and Lower Egypt, suggest that the two deities represent some kind of division within the country. Egyptian tradition and archaeological evidence indicate that Egypt was united at the beginning of its history when an Upper Egyptian kingdom, in the south, conquered Lower Egypt in the north. The Upper Egyptian rulers called themselves "followers of Horus", and Horus became the patron god of the unified nation and its kings. Yet Horus and Set cannot be easily equated with the two halves of the country. Both deities had several cult centers in each region, and Horus is often associated with Lower Egypt and Set with Upper Egypt. One of the better-known explanations for these discrepancies was proposed by Kurt Sethe in 1930. He argued that Osiris was originally the human ruler of a unified Egypt in prehistoric times, before a rebellion of Upper Egyptian Set-worshippers. The Lower Egyptian followers of Horus then forcibly reunified the land, inspiring the myth of Horus's triumph, before Upper Egypt, now led by Horus worshippers, became prominent again at the start of the Early Dynastic Period.

In the late 20th century, Griffiths focused on the inconsistent portrayal of Horus and Set as brothers and as uncle and nephew. He argued that, in the early stages of Egyptian mythology, the struggle between Horus and Set as siblings and equals was originally separate from the murder of Osiris. The two stories were joined into the single Osiris myth sometime before the writing of the "Pyramid Texts". With this merging, the genealogy of the deities involved and the characterization of the Horus–Set conflict were altered so that Horus is the son and heir avenging Osiris's death. Traces of the independent traditions remained in the conflicting characterizations of the combatants' relationship and in texts unrelated to the Osiris myth, which make Horus the son of the goddess Nut or the goddess Hathor rather than of Isis and Osiris. Griffiths therefore rejected the possibility that Osiris's murder was rooted in historical events. This hypothesis has been accepted by more recent scholars such as Jan Assmann and George Hart.

Griffiths sought a historical origin for the Horus–Set rivalry, and he posited two distinct predynastic unifications of Egypt by Horus worshippers, similar to Sethe's theory, to account for it. Yet the issue remains unresolved, partly because other political associations for Horus and Set complicate the picture further. Before even Upper Egypt had a single ruler, two of its major cities were Nekhen, in the far south, and Naqada, many miles to the north. The rulers of Nekhen, where Horus was the patron deity, are generally believed to have unified Upper Egypt, including Naqada, under their sway. Set was associated with Naqada, so it is possible that the divine conflict dimly reflects an enmity between the cities in the distant past. Much later, at the end of the Second Dynasty (c. 2890–2686 BCE), King Peribsen used the Set animal in writing his "serekh"-name, in place of the traditional falcon hieroglyph representing Horus. His successor Khasekhemwy used both Horus and Set in the writing of his "serekh". This evidence has prompted conjecture that the Second Dynasty saw a clash between the followers of the Horus-king and the worshippers of Set led by Peribsen. Khasekhemwy's use of the two animal symbols would then represent the reconciliation of the two factions, as does the resolution of the myth.

Noting the uncertainty surrounding these events, Herman te Velde argues that the historical roots of the conflict are too obscure to be very useful in understanding the myth and are not as significant as its religious meaning. He says that "the origin of the myth of Horus and Seth is lost in the mists of the religious traditions of prehistory."

The effect of the Osiris myth on Egyptian culture was greater and more widespread than that of any other myth. In literature, the myth was not only the basis for a retelling such as "Contendings"; it also provided the basis for more distantly related stories. "The Tale of Two Brothers", a folk tale with human protagonists, includes elements similar to the myth of Osiris. One character's penis is eaten by a fish, and he later dies and is resurrected. Another story, "The Tale of Truth and Falsehood", adapts the conflict of Horus and Set into an allegory, in which the characters are direct personifications of truth and lies rather than deities associated with those concepts.

From at least the time of the "Pyramid Texts", kings hoped that after their deaths they could emulate Osiris's restoration to life and his rule over the realm of the dead. By the early Middle Kingdom (c. 2055–1650 BCE), non-royal Egyptians believed that they, too, could overcome death as Osiris had, by worshipping him and receiving the funerary rites that were partly based on his myth. Osiris thus became Egypt's most important afterlife deity. The myth also influenced the notion, which grew prominent in the New Kingdom, that only virtuous people could reach the afterlife. As the assembled deities judged Osiris and Horus to be in the right, undoing the injustice of Osiris's death, so a deceased soul had to be judged righteous in order for his or her death to be undone. As ruler of the land of the dead and as a god connected with "maat", Osiris became the judge in this posthumous trial, offering life after death to those who followed his example. New Kingdom funerary texts such as the "Amduat" and the "Book of Gates" liken Ra himself to a deceased soul. In them, he travels through the Duat and unites with Osiris to be reborn at dawn. Thus, Osiris was not only believed to enable rebirth for the dead; he renewed the sun, the source of life and "maat", and thus renewed the world itself.

As the importance of Osiris grew, so did his popularity. By late in the Middle Kingdom, the centuries-old tomb of the First Dynasty ruler Djer, near Osiris's main center of worship in the city of Abydos, was seen as Osiris's tomb. Accordingly, it became a major focus of Osiris worship. For the next 1,500 years, an annual festival procession traveled from Osiris's main temple to the tomb site. Kings and commoners from across Egypt built chapels, which served as cenotaphs, near the processional route. In doing so they sought to strengthen their connection with Osiris in the afterlife.

Another major funerary festival, a national event spread over several days in the month of Khoiak in the Egyptian calendar, became linked with Osiris during the Middle Kingdom. During Khoiak the "djed" pillar, an emblem of Osiris, was ritually raised into an upright position, symbolizing Osiris's restoration. By Ptolemaic times (305–30 BCE), Khoiak also included the planting of seeds in an "Osiris bed", a mummy-shaped bed of soil, connecting the resurrection of Osiris with the seasonal growth of plants.

The myth's religious importance extended beyond the funerary sphere. Mortuary offerings, in which family members or hired priests presented food to the deceased, were logically linked with the mythological offering of the Eye of Horus to Osiris. By analogy, this episode of the myth was eventually equated with other interactions between a human and a being in the divine realm. In temple offering rituals, the officiating priest took on the role of Horus, the gifts to the deity became the Eye of Horus, and whichever deity received these gifts was momentarily equated with Osiris.

The myth influenced popular religion as well. One example is the magical healing spells based on Horus's childhood. Another is the use of the Eye of Horus as a protective emblem in personal apotropaic amulets. Its mythological restoration made it appropriate for this purpose, as a general symbol of well-being.

The ideology surrounding the living king was also affected by the Osiris myth. The Egyptians envisioned the events of the Osiris myth as taking place sometime in Egypt's dim prehistory, and Osiris, Horus, and their divine predecessors were included in Egyptian lists of past kings such as the Turin Royal Canon. Horus, as a primeval king and as the personification of kingship, was regarded as the predecessor and exemplar for all Egyptian rulers. His assumption of his father's throne and pious actions to sustain his spirit in the afterlife were the model for all pharaonic successions to emulate. Each new king was believed to renew "maat" after the death of the preceding king, just as Horus had done. In royal coronations, rituals alluded to Osiris's burial, and hymns celebrated the new king's accession as the equivalent of Horus's own.

The Osiris myth contributed to the frequent characterization of Set as a disruptive, harmful god. Although other elements of Egyptian tradition credit Set with positive traits, in the Osiris myth the sinister aspects of his character predominate. He and Horus were often juxtaposed in art to represent opposite principles, such as good and evil, intellect and instinct, and the different regions of the world that they rule in the myth. Egyptian wisdom texts contrast the character of the ideal person with the opposite type—the calm and sensible "Silent One" and the impulsive, disruptive "Hothead"—and one description of these two characters calls them the Horus-type and the Set-type. Yet the two gods were often treated as part of a harmonious whole. In some local cults they were worshipped together; in art they were often shown tying together the emblems of Upper and Lower Egypt to symbolize the unity of the nation; and in funerary texts they appear as a single deity with the heads of Horus and Set, apparently representing the mysterious, all-encompassing nature of the Duat.

Overall Set was viewed with ambivalence, until during the first millennium BCE he came to be seen as a totally malevolent deity. This transformation was prompted more by his association with foreign lands than by the Osiris myth. Nevertheless, in these late times, the widespread temple rituals involving the ceremonial annihilation of Set were often connected with the myth.

Both Isis and Nephthys were seen as protectors of the dead in the afterlife because of their protection and restoration of Osiris's body. The motif of Isis and Nephthys protecting Osiris or the mummy of the deceased person was very common in funerary art. Khoiak celebrations made reference to, and may have ritually reenacted, Isis's and Nephthys's mourning, restoration, and revival of their murdered brother. As Horus's mother, Isis was also the mother of every king according to royal ideology, and kings were said to have nursed at her breast as a symbol of their divine legitimacy. Her appeal to the general populace was based in her protective character, as exemplified by the magical healing spells. In the Late Period, she was credited with ever greater magical power, and her maternal devotion was believed to extend to everyone. By Roman times she had become the most important goddess in Egypt. The image of the goddess holding her child was used prominently in her worship—for example, in panel paintings that were used in household shrines dedicated to her. Isis's iconography in these paintings closely resembles and may have influenced the earliest Christian icons of Mary holding Jesus.

In the late centuries BCE, the worship of Isis spread from Egypt across the Mediterranean world, and she became one of the most popular deities in the region. Although this new, multicultural form of Isis absorbed characteristics from many other deities, her original mythological nature as a wife and mother was key to her appeal. Horus and Osiris, being central figures in her story, spread along with her. The Greek and Roman cult of Isis developed a series of initiation rites dedicated to Isis and Osiris, based on earlier Greco-Roman mystery rites but colored by Egyptian afterlife beliefs. The initiate went through an experience that simulated descent into the underworld. Elements of this ritual resemble Osiris's merging with the sun in Egyptian funerary texts. Isis's Greek and Roman devotees, like the Egyptians, believed that she protected the dead in the afterlife as she had done for Osiris, and they said that undergoing the initiation guaranteed to them a blessed afterlife. It was to a Greek priestess of Isis that Plutarch wrote his account of the myth of Osiris.

Through the work of classical writers such as Plutarch, knowledge of the Osiris myth was preserved even after the middle of the first millennium AD, when Egyptian religion ceased to exist and knowledge of the writing systems that were originally used to record the myth were lost. The myth remained a major part of Western impressions of ancient Egypt. In modern times, when understanding of Egyptian beliefs is informed by the original Egyptian sources, the story continues to influence and inspire new ideas, from works of fiction to scholarly speculation and new religious movements.




</doc>
<doc id="49557" url="https://en.wikipedia.org/wiki?curid=49557" title="Castle">
Castle

A castle (from ) is a type of fortified structure built during the Middle Ages by predominantly the nobility or royalty and by military orders. Scholars debate the scope of the word "castle", but usually consider it to be the private fortified residence of a lord or noble. This is distinct from a palace, which is not fortified; from a fortress, which was not always a residence for royalty or nobility; and from a fortified settlement, which was a public defence – though there are many similarities among these types of construction. Usage of the term has varied over time and has been applied to structures as diverse as hill forts and country houses. Over the approximately 900 years that castles were built, they took on a great many forms with many different features, although some, such as curtain walls and arrowslits, were commonplace.

European-style castles originated in the 9th and 10th centuries, after the fall of the Carolingian Empire resulted in its territory being divided among individual lords and princes. These nobles built castles to control the area immediately surrounding them and the castles were both offensive and defensive structures; they provided a base from which raids could be launched as well as offered protection from enemies. Although their military origins are often emphasised in castle studies, the structures also served as centres of administration and symbols of power. Urban castles were used to control the local populace and important travel routes, and rural castles were often situated near features that were integral to life in the community, such as mills, fertile land, or a water source.

Many castles were originally built from earth and timber, but had their defences replaced later by stone. Early castles often exploited natural defences, lacking features such as towers and arrowslits and relying on a central keep. In the late 12th and early 13th centuries, a scientific approach to castle defence emerged. This led to the proliferation of towers, with an emphasis on flanking fire. Many new castles were polygonal or relied on concentric defence – several stages of defence within each other that could all function at the same time to maximise the castle's firepower. These changes in defence have been attributed to a mixture of castle technology from the Crusades, such as concentric fortification, and inspiration from earlier defences, such as Roman forts. Not all the elements of castle architecture were military in nature, so that devices such as moats evolved from their original purpose of defence into symbols of power. Some grand castles had long winding approaches intended to impress and dominate their landscape.

Although gunpowder was introduced to Europe in the 14th century, it did not significantly affect castle building until the 15th century, when artillery became powerful enough to break through stone walls. While castles continued to be built well into the 16th century, new techniques to deal with improved cannon fire made them uncomfortable and undesirable places to live. As a result, true castles went into decline and were replaced by artillery forts with no role in civil administration, and country houses that were indefensible. From the 18th century onwards, there was a renewed interest in castles with the construction of mock castles, part of a romantic revival of Gothic architecture, but they had no military purpose.

The word "castle" is derived from the Latin word "castellum", which is a diminutive of the word "castrum", meaning "fortified place". The Old English "castel", Old French "castel" or "chastel", French "château", Spanish "castillo", Italian "castello", and a number of words in other languages also derive from "castellum". The word "castle" was introduced into English shortly before the Norman Conquest to denote this type of building, which was then new to England.

In its simplest terms, the definition of a castle accepted amongst academics is "a private fortified residence". This contrasts with earlier fortifications, such as Anglo-Saxon burhs and walled cities such as Constantinople and Antioch in the Middle East; castles were not communal defences but were built and owned by the local feudal lords, either for themselves or for their monarch. Feudalism was the link between a lord and his vassal where, in return for military service and the expectation of loyalty, the lord would grant the vassal land. In the late 20th century, there was a trend to refine the definition of a castle by including the criterion of feudal ownership, thus tying castles to the medieval period; however, this does not necessarily reflect the terminology used in the medieval period. During the First Crusade (1096–1099), the Frankish armies encountered walled settlements and forts that they indiscriminately referred to as castles, but which would not be considered as such under the modern definition.
Castles served a range of purposes, the most important of which were military, administrative, and domestic. As well as defensive structures, castles were also offensive tools which could be used as a base of operations in enemy territory. Castles were established by Norman invaders of England for both defensive purposes and to pacify the country's inhabitants. As William the Conqueror advanced through England, he fortified key positions to secure the land he had taken. Between 1066 and 1087, he established 36 castles such as Warwick Castle, which he used to guard against rebellion in the English Midlands.

Towards the end of the Middle Ages, castles tended to lose their military significance due to the advent of powerful cannons and permanent artillery fortifications; as a result, castles became more important as residences and statements of power. A castle could act as a stronghold and prison but was also a place where a knight or lord could entertain his peers. Over time the aesthetics of the design became more important, as the castle's appearance and size began to reflect the prestige and power of its occupant. Comfortable homes were often fashioned within their fortified walls. Although castles still provided protection from low levels of violence in later periods, eventually they were succeeded by country houses as high status residences.

"Castle" is sometimes used as a catch-all term for all kinds of fortifications and, as a result, has been misapplied in the technical sense. An example of this is Maiden Castle which, despite the name, is an Iron Age hill fort which had a very different origin and purpose.

Although "castle" has not become a generic term for a manor house (like château in French and Schloss in German), many manor houses contain "castle" in their name while having few if any of the architectural characteristics, usually as their owners liked to maintain a link to the past and felt the term "castle" was a masculine expression of their power. In scholarship the castle, as defined above, is generally accepted as a coherent concept, originating in Europe and later spreading to parts of the Middle East, where they were introduced by European Crusaders. This coherent group shared a common origin, dealt with a particular mode of warfare, and exchanged influences.

In different areas of the world, analogous structures shared features of fortification and other defining characteristics associated with the concept of a castle, though they originated in different periods and circumstances and experienced differing evolutions and influences. For example, "shiro" in Japan, described as castles by historian Stephen Turnbull, underwent "a completely different developmental history, were built in a completely different way and were designed to withstand attacks of a completely different nature". While European castles built from the late 12th and early 13th century onwards were generally stone, "shiro" were predominantly timber buildings into the 16th century.

By the 16th century, when Japanese and European cultures met, fortification in Europe had moved beyond castles and relied on innovations such as the Italian "trace italienne" and star forts. Forts in India present a similar case; when they were encountered by the British in the 17th century, castles in Europe had generally fallen out of use militarily. Like "shiro", the Indian forts, "durga" or "durg" in Sanskrit, shared features with castles in Europe such as acting as a domicile for a lord as well as being fortifications. They too developed differently from the structures known as castles that had their origins in Europe.

A motte was an earthen mound with a flat top. It was often artificial, although sometimes it incorporated a pre-existing feature of the landscape. The excavation of earth to make the mound left a ditch around the motte, called a moat (which could be either wet or dry). "Motte" and "moat" derive from the same Old French word, indicating that the features were originally associated and depended on each other for their construction. Although the motte is commonly associated with the bailey to form a motte-and-bailey castle, this was not always the case and there are instances where a motte existed on its own.

"Motte" refers to the mound alone, but it was often surmounted by a fortified structure, such as a keep, and the flat top would be surrounded by a palisade. It was common for the motte to be reached over a flying bridge (a bridge over the ditch from the counterscarp of the ditch to the edge of the top of the mound), as shown in the Bayeux Tapestry's depiction of Château de Dinan. Sometimes a motte covered an older castle or hall, whose rooms became underground storage areas and prisons beneath a new keep.

A bailey, also called a ward, was a fortified enclosure. It was a common feature of castles, and most had at least one. The keep on top of the motte was the domicile of the lord in charge of the castle and a bastion of last defence, while the bailey was the home of the rest of the lord's household and gave them protection. The barracks for the garrison, stables, workshops, and storage facilities were often found in the bailey. Water was supplied by a well or cistern. Over time the focus of high status accommodation shifted from the keep to the bailey; this resulted in the creation of another bailey that separated the high status buildings – such as the lord's chambers and the chapel – from the everyday structures such as the workshops and barracks.

From the late 12th century there was a trend for knights to move out of the small houses they had previously occupied within the bailey to live in fortified houses in the countryside. Although often associated with the motte-and-bailey type of castle, baileys could also be found as independent defensive structures. These simple fortifications were called ringworks. The enceinte was the castle's main defensive enclosure, and the terms "bailey" and "enceinte" are linked. A castle could have several baileys but only one enceinte. Castles with no keep, which relied on their outer defences for protection, are sometimes called enceinte castles; these were the earliest form of castles, before the keep was introduced in the 10th century.

A keep was a great tower and usually the most strongly defended point of a castle before the introduction of concentric defence. "Keep" was not a term used in the medieval period – the term was applied from the 16th century onwards – instead "donjon" was used to refer to great towers, or "turris" in Latin. In motte-and-bailey castles, the keep was on top of the motte. "Dungeon" is a corrupted form of "donjon" and means a dark, unwelcoming prison. Although often the strongest part of a castle and a last place of refuge if the outer defences fell, the keep was not left empty in case of attack but was used as a residence by the lord who owned the castle, or his guests or representatives.

At first this was usual only in England, when after the Norman Conquest of 1066 the "conquerors lived for a long time in a constant state of alert"; elsewhere the lord's wife presided over a separate residence ("domus", "aula" or "mansio" in Latin) close to the keep, and the donjon was a barracks and headquarters. Gradually, the two functions merged into the same building, and the highest residential storeys had large windows; as a result for many structures, it is difficult to find an appropriate term. The massive internal spaces seen in many surviving donjons can be misleading; they would have been divided into several rooms by light partitions, as in a modern office building. Even in some large castles the great hall was separated only by a partition from the lord's "chamber", his bedroom and to some extent his office.

Curtain walls were defensive walls enclosing a bailey. They had to be high enough to make scaling the walls with ladders difficult and thick enough to withstand bombardment from siege engines which, from the 15th century onwards, included gunpowder artillery. A typical wall could be thick and tall, although sizes varied greatly between castles. To protect them from undermining, curtain walls were sometimes given a stone skirt around their bases. Walkways along the tops of the curtain walls allowed defenders to rain missiles on enemies below, and battlements gave them further protection. Curtain walls were studded with towers to allow enfilading fire along the wall. Arrowslits in the walls did not become common in Europe until the 13th century, for fear that they might compromise the wall's strength.

The entrance was often the weakest part in a circuit of defences. To overcome this, the gatehouse was developed, allowing those inside the castle to control the flow of traffic. In earth and timber castles, the gateway was usually the first feature to be rebuilt in stone. The front of the gateway was a blind spot and to overcome this, projecting towers were added on each side of the gate in a style similar to that developed by the Romans. The gatehouse contained a series of defences to make a direct assault more difficult than battering down a simple gate. Typically, there were one or more portcullises – a wooden grille reinforced with metal to block a passage – and arrowslits to allow defenders to harry the enemy. The passage through the gatehouse was lengthened to increase the amount of time an assailant had to spend under fire in a confined space and unable to retaliate.

It is a popular myth that so-called murder holes – openings in the ceiling of the gateway passage – were used to pour boiling oil or molten lead on attackers; the price of oil and lead and the distance of the gatehouse from fires meant that this was impractical. This method was, however, a common practice in the MENA region and the Mediterranean castles and fortifications where such resources were abundant. They were most likely used to drop objects on attackers, or to allow water to be poured on fires to extinguish them. Provision was made in the upper storey of the gatehouse for accommodation so the gate was never left undefended, although this arrangement later evolved to become more comfortable at the expense of defence.

During the 13th and 14th centuries the barbican was developed. This consisted of a rampart, ditch, and possibly a tower, in front of the gatehouse which could be used to further protect the entrance. The purpose of a barbican was not just to provide another line of defence but also to dictate the only approach to the gate.

A moat was a defensive ditch with steep sides, and could be either dry or filled with water. Its purpose was twofold; to stop devices such as siege towers from reaching the curtain wall and to prevent the walls from being undermined. Water moats were found in low-lying areas and were usually crossed by a drawbridge, although these were often replaced by stone bridges. Fortified islands could be added to the moat, adding another layer of defence. Water defences, such as moats or natural lakes, had the benefit of dictating the enemy's approach to the castle. The site of the 13th-century Caerphilly Castle in Wales covers over and the water defences, created by flooding the valley to the south of the castle, are some of the largest in Western Europe.

Battlements were most often found surmounting curtain walls and the tops of gatehouses, and comprised several elements: crenellations, hoardings, machicolations, and loopholes. Crenellation is the collective name for alternating crenels and merlons: gaps and solid blocks on top of a wall. Hoardings were wooden constructs that projected beyond the wall, allowing defenders to shoot at, or drop objects on, attackers at the base of the wall without having to lean perilously over the crenellations, thereby exposing themselves to retaliatory fire. Machicolations were stone projections on top of a wall with openings that allowed objects to be dropped on an enemy at the base of the wall in a similar fashion to hoardings.

Arrowslits, also commonly called loopholes, were narrow vertical openings in defensive walls which allowed arrows or crossbow bolts to be fired on attackers. The narrow slits were intended to protect the defender by providing a very small target, but the size of the opening could also impede the defender if it was too small. A smaller horizontal opening could be added to give an archer a better view for aiming. Sometimes a sally port was included; this could allow the garrison to leave the castle and engage besieging forces. It was usual for the latrines to empty down the external walls of a castle and into the surrounding ditch.

Historian Charles Coulson states that the accumulation of wealth and resources, such as food, led to the need for defensive structures. The earliest fortifications originated in the Fertile Crescent, the Indus Valley, Egypt, and China where settlements were protected by large walls. Northern Europe was slower than the East to develop defensive structures and it was not until the Bronze Age that hill forts were developed, which then proliferated across Europe in the Iron Age. These structures differed from their eastern counterparts in that they used earthworks rather than stone as a building material. Many earthworks survive today, along with evidence of palisades to accompany the ditches. In Europe, oppida emerged in the 2nd century BC; these were densely inhabited fortified settlements, such as the oppidum of Manching, and developed from hill forts. The Romans encountered fortified settlements such as hill forts and oppida when expanding their territory into northern Europe. Although primitive, they were often effective, and were only overcome by the extensive use of siege engines and other siege warfare techniques, such as at the Battle of Alesia. The Romans' own fortifications ("castra") varied from simple temporary earthworks thrown up by armies on the move, to elaborate permanent stone constructions, notably the milecastles of Hadrian's Wall. Roman forts were generally rectangular with rounded corners – a "playing-card shape".

In the medieval period, castles were influenced by earlier forms of elite architecture, contributing to regional variations. Importantly, while castles had military aspects, they contained a recognisable household structure within their walls, reflecting the multi-functional use of these buildings.

The subject of the emergence of castles in Europe is a complex matter which has led to considerable debate. Discussions have typically attributed the rise of the castle to a reaction to attacks by Magyars, Muslims, and Vikings and a need for private defence. The breakdown of the Carolingian Empire led to the privatisation of government, and local lords assumed responsibility for the economy and justice. However, while castles proliferated in the 9th and 10th centuries the link between periods of insecurity and building fortifications is not always straightforward. Some high concentrations of castles occur in secure places, while some border regions had relatively few castles.

It is likely that the castle evolved from the practice of fortifying a lordly home. The greatest threat to a lord's home or hall was fire as it was usually a wooden structure. To protect against this, and keep other threats at bay, there were several courses of action available: create encircling earthworks to keep an enemy at a distance; build the hall in stone; or raise it up on an artificial mound, known as a motte, to present an obstacle to attackers. While the concept of ditches, ramparts, and stone walls as defensive measures is ancient, raising a motte is a medieval innovation.

A bank and ditch enclosure was a simple form of defence, and when found without an associated motte is called a ringwork; when the site was in use for a prolonged period, it was sometimes replaced by a more complex structure or enhanced by the addition of a stone curtain wall. Building the hall in stone did not necessarily make it immune to fire as it still had windows and a wooden door. This led to the elevation of windows to the first floor – to make it harder to throw objects in – and to change the entrance from ground floor to first floor. These features are seen in many surviving castle keeps, which were the more sophisticated version of halls. Castles were not just defensive sites but also enhanced a lord's control over his lands. They allowed the garrison to control the surrounding area, and formed a centre of administration, providing the lord with a place to hold court.

Building a castle sometimes required the permission of the king or other high authority. In 864 the King of West Francia, Charles the Bald, prohibited the construction of "castella" without his permission and ordered them all to be destroyed. This is perhaps the earliest reference to castles, though military historian R. Allen Brown points out that the word "castella" may have applied to any fortification at the time.

In some countries the monarch had little control over lords, or required the construction of new castles to aid in securing the land so was unconcerned about granting permission – as was the case in England in the aftermath of the Norman Conquest and the Holy Land during the Crusades. Switzerland is an extreme case of there being no state control over who built castles, and as a result there were 4,000 in the country. There are very few castles dated with certainty from the mid-9th century. Converted into a donjon around 950, Château de Doué-la-Fontaine in France is the oldest standing castle in Europe.

From 1000 onwards, references to castles in texts such as charters increased greatly. Historians have interpreted this as evidence of a sudden increase in the number of castles in Europe around this time; this has been supported by archaeological investigation which has dated the construction of castle sites through the examination of ceramics. The increase in Italy began in the 950s, with numbers of castles increasing by a factor of three to five every 50 years, whereas in other parts of Europe such as France and Spain the growth was slower. In 950 Provence was home to 12 castles, by 1000 this figure had risen to 30, and by 1030 it was over 100. Although the increase was slower in Spain, the 1020s saw a particular growth in the number of castles in the region, particularly in contested border areas between Christian and Muslim.

Despite the common period in which castles rose to prominence in Europe, their form and design varied from region to region. In the early 11th century, the motte and keep – an artificial mound surmounted by a palisade and tower – was the most common form of castle in Europe, everywhere except Scandinavia. While Britain, France, and Italy shared a tradition of timber construction that was continued in castle architecture, Spain more commonly used stone or mud-brick as the main building material.

The Muslim invasion of the Iberian Peninsula in the 8th century introduced a style of building developed in North Africa reliant on "tapial", pebbles in cement, where timber was in short supply. Although stone construction would later become common elsewhere, from the 11th century onwards it was the primary building material for Christian castles in Spain, while at the same time timber was still the dominant building material in north-west Europe.
Historians have interpreted the widespread presence of castles across Europe in the 11th and 12th centuries as evidence that warfare was common, and usually between local lords. Castles were introduced into England shortly before the Norman Conquest in 1066. Before the 12th century castles were as uncommon in Denmark as they had been in England before the Norman Conquest. The introduction of castles to Denmark was a reaction to attacks from Wendish pirates, and they were usually intended as coastal defences. The motte and bailey remained the dominant form of castle in England, Wales, and Ireland well into the 12th century. At the same time, castle architecture in mainland Europe became more sophisticated.

The donjon was at the centre of this change in castle architecture in the 12th century. Central towers proliferated, and typically had a square plan, with walls thick. Their decoration emulated Romanesque architecture, and sometimes incorporated double windows similar to those found in church bell towers. Donjons, which were the residence of the lord of the castle, evolved to become more spacious. The design emphasis of donjons changed to reflect a shift from functional to decorative requirements, imposing a symbol of lordly power upon the landscape. This sometimes led to compromising defence for the sake of display.

Until the 12th century, stone-built and earth and timber castles were contemporary, but by the late 12th century the number of castles being built went into decline. This has been partly attributed to the higher cost of stone-built fortifications, and the obsolescence of timber and earthwork sites, which meant it was preferable to build in more durable stone. Although superseded by their stone successors, timber and earthwork castles were by no means useless. This is evidenced by the continual maintenance of timber castles over long periods, sometimes several centuries; Owain Glyndŵr's 11th-century timber castle at Sycharth was still in use by the start of the 15th century, its structure having been maintained for four centuries.

At the same time there was a change in castle architecture. Until the late 12th century castles generally had few towers; a gateway with few defensive features such as arrowslits or a portcullis; a great keep or donjon, usually square and without arrowslits; and the shape would have been dictated by the lay of the land (the result was often irregular or curvilinear structures). The design of castles was not uniform, but these were features that could be found in a typical castle in the mid-12th century. By the end of the 12th century or the early 13th century, a newly constructed castle could be expected to be polygonal in shape, with towers at the corners to provide enfilading fire for the walls. The towers would have protruded from the walls and featured arrowslits on each level to allow archers to target anyone nearing or at the curtain wall.
These later castles did not always have a keep, but this may have been because the more complex design of the castle as a whole drove up costs and the keep was sacrificed to save money. The larger towers provided space for habitation to make up for the loss of the donjon. Where keeps did exist, they were no longer square but polygonal or cylindrical. Gateways were more strongly defended, with the entrance to the castle usually between two half-round towers which were connected by a passage above the gateway – although there was great variety in the styles of gateway and entrances – and one or more portcullis.

A peculiar feature of Muslim castles in the Iberian Peninsula was the use of detached towers, called Albarrana towers, around the perimeter as can be seen at the Alcazaba of Badajoz. Probably developed in the 12th century, the towers provided flanking fire. They were connected to the castle by removable wooden bridges, so if the towers were captured the rest of the castle was not accessible.
When seeking to explain this change in the complexity and style of castles, antiquarians found their answer in the Crusades. It seemed that the Crusaders had learned much about fortification from their conflicts with the Saracens and exposure to Byzantine architecture. There were legends such as that of Lalys – an architect from Palestine who reputedly went to Wales after the Crusades and greatly enhanced the castles in the south of the country – and it was assumed that great architects such as James of Saint George originated in the East. In the mid-20th century this view was cast into doubt. Legends were discredited, and in the case of James of Saint George it was proven that he came from Saint-Georges-d'Espéranche, in France. If the innovations in fortification had derived from the East, it would have been expected for their influence to be seen from 1100 onwards, immediately after the Christians were victorious in the First Crusade (1096–1099), rather than nearly 100 years later. Remains of Roman structures in Western Europe were still standing in many places, some of which had flanking round-towers and entrances between two flanking towers.

The castle builders of Western Europe were aware of and influenced by Roman design; late Roman coastal forts on the English "Saxon Shore" were reused and in Spain the wall around the city of Ávila imitated Roman architecture when it was built in 1091. Historian Smail in "Crusading warfare" argued that the case for the influence of Eastern fortification on the West has been overstated, and that Crusaders of the 12th century in fact learned very little about scientific design from Byzantine and Saracen defences. A well-sited castle that made use of natural defences and had strong ditches and walls had no need for a scientific design. An example of this approach is Kerak. Although there were no scientific elements to its design, it was almost impregnable, and in 1187 Saladin chose to lay siege to the castle and starve out its garrison rather than risk an assault.

After the First Crusade, Crusaders who did not return to their homes in Europe helped found the Crusader states of the Principality of Antioch, the County of Edessa, the Kingdom of Jerusalem, and the County of Tripoli. The castles they founded to secure their acquisitions were designed mostly by Syrian master-masons. Their design was very similar to that of a Roman fort or Byzantine "tetrapyrgia" which were square in plan and had square towers at each corner that did not project much beyond the curtain wall. The keep of these Crusader castles would have had a square plan and generally be undecorated.

While castles were used to hold a site and control movement of armies, in the Holy Land some key strategic positions were left unfortified. Castle architecture in the East became more complex around the late 12th and early 13th centuries after the stalemate of the Third Crusade (1189–1192). Both Christians and Muslims created fortifications, and the character of each was different. Saphadin, the 13th-century ruler of the Saracens, created structures with large rectangular towers that influenced Muslim architecture and were copied again and again, however they had little influence on Crusader castles.

In the early 13th century, Crusader castles were mostly built by Military Orders including the Knights Hospitaller, Knights Templar, and Teutonic Knights. The orders were responsible for the foundation of sites such as Krak des Chevaliers, Margat, and Belvoir. Design varied not just between orders, but between individual castles, though it was common for those founded in this period to have concentric defences.

The concept, which originated in castles such as Krak des Chevaliers, was to remove the reliance on a central strongpoint and to emphasise the defence of the curtain walls. There would be multiple rings of defensive walls, one inside the other, with the inner ring rising above the outer so that its field of fire was not completely obscured. If assailants made it past the first line of defence they would be caught in the killing ground between the inner and outer walls and have to assault the second wall.

Concentric castles were widely copied across Europe, for instance when Edward I of England – who had himself been on Crusade – built castles in Wales in the late 13th century, four of the eight he founded had a concentric design. Not all the features of the Crusader castles from the 13th century were emulated in Europe. For instance, it was common in Crusader castles to have the main gate in the side of a tower and for there to be two turns in the passageway, lengthening the time it took for someone to reach the outer enclosure. It is rare for this bent entrance to be found in Europe.
One of the effects of the Livonian Crusade in the Baltic was the introduction of stone and brick fortifications. Although there were hundreds of wooden castles in Prussia and Livonia, the use of bricks and mortar was unknown in the region before the Crusaders. Until the 13th century and start of the 14th centuries, their design was heterogeneous, however this period saw the emergence of a standard plan in the region: a square plan, with four wings around a central courtyard. It was common for castles in the East to have arrowslits in the curtain wall at multiple levels; contemporary builders in Europe were wary of this as they believed it weakened the wall. Arrowslits did not compromise the wall's strength, but it was not until Edward I's programme of castle building that they were widely adopted in Europe.

The Crusades also led to the introduction of machicolations into Western architecture. Until the 13th century, the tops of towers had been surrounded by wooden galleries, allowing defenders to drop objects on assailants below. Although machicolations performed the same purpose as the wooden galleries, they were probably an Eastern invention rather than an evolution of the wooden form. Machicolations were used in the East long before the arrival of the Crusaders, and perhaps as early as the first half of the 8th century in Syria.

The greatest period of castle building in Spain was in the 11th to 13th centuries, and they were most commonly found in the disputed borders between Christian and Muslim lands. Conflict and interaction between the two groups led to an exchange of architectural ideas, and Spanish Christians adopted the use of detached towers. The Spanish Reconquista, driving the Muslims out of the Iberian Peninsula, was complete in 1492.
Although France has been described as "the heartland of medieval architecture", the English were at the forefront of castle architecture in the 12th century. French historian François Gebelin wrote: "The great revival in military architecture was led, as one would naturally expect, by the powerful kings and princes of the time; by the sons of William the Conqueror and their descendants, the Plantagenets, when they became dukes of Normandy. These were the men who built all the most typical twelfth-century fortified castles remaining to-day". Despite this, by the beginning of the 15th century, the rate of castle construction in England and Wales went into decline. The new castles were generally of a lighter build than earlier structures and presented few innovations, although strong sites were still created such as that of Raglan in Wales. At the same time, French castle architecture came to the fore and led the way in the field of medieval fortifications. Across Europe – particularly the Baltic, Germany, and Scotland – castles were built well into the 16th century.

Artillery powered by gunpowder was introduced to Europe in the 1320s and spread quickly. Handguns, which were initially unpredictable and inaccurate weapons, were not recorded until the 1380s. Castles were adapted to allow small artillery pieces – averaging between  – to fire from towers. These guns were too heavy for a man to carry and fire, but if he supported the butt end and rested the muzzle on the edge of the gun port he could fire the weapon. The gun ports developed in this period show a unique feature, that of a horizontal timber across the opening. A hook on the end of the gun could be latched over the timber so the gunner did not have to take the full recoil of the weapon. This adaptation is found across Europe, and although the timber rarely survives, there is an intact example at Castle Doornenburg in the Netherlands. Gunports were keyhole shaped, with a circular hole at the bottom for the weapon and a narrow slit on top to allow the gunner to aim.

This form is very common in castles adapted for guns, found in Egypt, Italy, Scotland, and Spain, and elsewhere in between. Other types of port, though less common, were horizontal slits – allowing only lateral movement – and large square openings, which allowed greater movement. The use of guns for defence gave rise to artillery castles, such as that of Château de Ham in France. Defences against guns were not developed until a later stage. Ham is an example of the trend for new castles to dispense with earlier features such as machicolations, tall towers, and crenellations.

Bigger guns were developed, and in the 15th century became an alternative to siege engines such as the trebuchet. The benefits of large guns over trebuchets – the most effective siege engine of the Middle Ages before the advent of gunpowder – were those of a greater range and power. In an effort to make them more effective, guns were made ever bigger, although this hampered their ability to reach remote castles. By the 1450s guns were the preferred siege weapon, and their effectiveness was demonstrated by Mehmed II at the Fall of Constantinople.

The response towards more effective cannons was to build thicker walls and to prefer round towers, as the curving sides were more likely to deflect a shot than a flat surface. While this sufficed for new castles, pre-existing structures had to find a way to cope with being battered by cannon. An earthen bank could be piled behind a castle's curtain wall to absorb some of the shock of impact.

Often, castles constructed before the age of gunpowder were incapable of using guns as their wall-walks were too narrow. A solution to this was to pull down the top of a tower and to fill the lower part with the rubble to provide a surface for the guns to fire from. Lowering the defences in this way had the effect of making them easier to scale with ladders. A more popular alternative defence, which avoided damaging the castle, was to establish bulwarks beyond the castle's defences. These could be built from earth or stone and were used to mount weapons.

Around 1500, the innovation of the angled bastion was developed in Italy. With developments such as these, Italy pioneered permanent artillery fortifications, which took over from the defensive role of castles. From this evolved star forts, also known as "trace italienne". The elite responsible for castle construction had to choose between the new type that could withstand cannon fire and the earlier, more elaborate style. The first was ugly and uncomfortable and the latter was less secure, although it did offer greater aesthetic appeal and value as a status symbol. The second choice proved to be more popular as it became apparent that there was little point in trying to make the site genuinely defensible in the face of cannon. For a variety of reasons, not least of which is that many castles have no recorded history, there is no firm number of castles built in the medieval period. However, it has been estimated that between 75,000 and 100,000 were built in western Europe; of these around 1,700 were in England and Wales and around 14,000 in German-speaking areas.

Some true castles were built in the Americas by the Spanish and French colonies. The first stage of Spanish fort construction has been termed the "castle period", which lasted from 1492 until the end of the 16th century. Starting with Fortaleza Ozama, "these castles were essentially European medieval castles transposed to America". Among other defensive structures (including forts and citadels), castles were also built in New France towards the end of the 17th century. In Montreal the artillery was not as developed as on the battle-fields of Europe, some of the region's outlying forts were built like the fortified manor houses of France. Fort Longueuil, built from 1695–1698 by a baronial family, has been described as "the most medieval-looking fort built in Canada". The manor house and stables were within a fortified bailey, with a tall round turret in each corner. The "most substantial castle-like fort" near Montréal was Fort Senneville, built in 1692 with square towers connected by thick stone walls, as well as a fortified windmill. Stone forts such as these served as defensive residences, as well as imposing structures to prevent Iroquois incursions.

Although castle construction faded towards the end of the 16th century, castles did not necessarily all fall out of use. Some retained a role in local administration and became law courts, while others are still handed down in aristocratic families as hereditary seats. A particularly famous example of this is Windsor Castle in England which was founded in the 11th century and is home to the monarch of the United Kingdom. In other cases they still had a role in defence. Tower houses, which are closely related to castles and include pele towers, were defended towers that were permanent residences built in the 14th to 17th centuries. Especially common in Ireland and Scotland, they could be up to five storeys high and succeeded common enclosure castles and were built by a greater social range of people. While unlikely to provide as much protection as a more complex castle, they offered security against raiders and other small threats.

According to archaeologists Oliver Creighton and Robert Higham, "the great country houses of the seventeenth to twentieth centuries were, in a social sense, the castles of their day". Though there was a trend for the elite to move from castles into country houses in the 17th century, castles were not completely useless. In later conflicts, such as the English Civil War (1641–1651), many castles were refortified, although subsequently slighted to prevent them from being used again. Some country residences, which were not meant to be fortified, were given a castle appearance to scare away potential invaders such as adding turrets and using small windows. An example of this is the 16th century Bubaqra Castle in Bubaqra, Malta, which was modified in the 18th century.

Revival or mock castles became popular as a manifestation of a Romantic interest in the Middle Ages and chivalry, and as part of the broader Gothic Revival in architecture. Examples of these castles include Chapultepec in Mexico, Neuschwanstein in Germany, and Edwin Lutyens' Castle Drogo (1911–1930) – the last flicker of this movement in the British Isles. While churches and cathedrals in a Gothic style could faithfully imitate medieval examples, new country houses built in a "castle style" differed internally from their medieval predecessors. This was because to be faithful to medieval design would have left the houses cold and dark by contemporary standards.

Artificial ruins, built to resemble remnants of historic edifices, were also a hallmark of the period. They were usually built as centre pieces in aristocratic planned landscapes. Follies were similar, although they differed from artificial ruins in that they were not part of a planned landscape, but rather seemed to have no reason for being built. Both drew on elements of castle architecture such as castellation and towers, but served no military purpose and were solely for display.

A toy castle is also used as a common children attraction in playing fields and fun parks, such as the castle of the Playmobil FunPark in Ħal Far, Malta.

Once the site of a castle had been selected – whether a strategic position or one intended to dominate the landscape as a mark of power – the building material had to be selected. An earth and timber castle was cheaper and easier to erect than one built from stone. The costs involved in construction are not well-recorded, and most surviving records relate to royal castles. A castle with earthen ramparts, a motte, timber defences and buildings could have been constructed by an unskilled workforce. The source of man-power was probably from the local lordship, and the tenants would already have the necessary skills of felling trees, digging, and working timber necessary for an earth and timber castle. Possibly coerced into working for their lord, the construction of an earth and timber castle would not have been a drain on a client's funds. In terms of time, it has been estimated that an average sized motte – high and wide at the summit – would have taken 50 people about 40 working days. An exceptionally expensive motte and bailey was that of Clones in Ireland, built in 1211 for £20. The high cost, relative to other castles of its type, was because labourers had to be imported.

The cost of building a castle varied according to factors such as their complexity and transport costs for material. It is certain that stone castles cost a great deal more than those built from earth and timber. Even a very small tower, such as Peveril Castle, would have cost around £200. In the middle were castles such as Orford, which was built in the late 12th century for £1,400, and at the upper end were those such as Dover, which cost about £7,000 between 1181 and 1191. Spending on the scale of the vast castles such as Château Gaillard (an estimated £15,000 to £20,000 between 1196 and 1198) was easily supported by The Crown, but for lords of smaller areas, castle building was a very serious and costly undertaking. It was usual for a stone castle to take the best part of a decade to finish. The cost of a large castle built over this time (anywhere from £1,000 to £10,000) would take the income from several manors, severely impacting a lord's finances. Costs in the late 13th century were of a similar order, with castles such as Beaumaris and Rhuddlan costing £14,500 and £9,000 respectively. Edward I's campaign of castle-building in Wales cost £80,000 between 1277 and 1304, and £95,000 between 1277 and 1329. Renowned designer Master James of Saint George, responsible for the construction of Beaumaris, explained the cost:

Not only were stone castles expensive to build in the first place, but their maintenance was a constant drain. They contained a lot of timber, which was often unseasoned and as a result needed careful upkeep. For example, it is documented that in the late 12th century repairs at castles such as Exeter and Gloucester cost between £20 and £50 annually.

Medieval machines and inventions, such as the treadwheel crane, became indispensable during construction, and techniques of building wooden scaffolding were improved upon from Antiquity. When building in stone a prominent concern of medieval builders was to have quarries close at hand. There are examples of some castles where stone was quarried on site, such as Chinon, Château de Coucy and Château Gaillard. When it was built in 992 in France the stone tower at Château de Langeais was high, wide, and long with walls averaging . The walls contain of stone and have a total surface (both inside and out) of . The tower is estimated to have taken 83,000 average working days to complete, most of which was unskilled labour.

Many countries had both timber and stone castles, however Denmark had few quarries and as a result most of its castles are earth and timber affairs, or later on built from brick. Brick-built structures were not necessarily weaker than their stone-built counterparts. Brick castles are less common in England than stone or earth and timber constructions, and often it was chosen for its aesthetic appeal or because it was fashionable, encouraged by the brick architecture of the Low Countries. For example, when Tattershall Castle was built between 1430 and 1450, there was plenty of stone available nearby, but the owner, Lord Cromwell, chose to use brick. About 700,000 bricks were used to build the castle, which has been described as "the finest piece of medieval brick-work in England". Most Spanish castles were built from stone, whereas castles in Eastern Europe were usually of timber construction.

Due to the lord's presence in a castle, it was a centre of administration from where he controlled his lands. He relied on the support of those below him, as without the support of his more powerful tenants a lord could expect his power to be undermined. Successful lords regularly held court with those immediately below them on the social scale, but absentees could expect to find their influence weakened. Larger lordships could be vast, and it would be impractical for a lord to visit all his properties regularly so deputies were appointed. This especially applied to royalty, who sometimes owned land in different countries.

To allow the lord to concentrate on his duties regarding administration, he had a household of servants to take care of chores such as providing food. The household was run by a chamberlain, while a treasurer took care of the estate's written records. Royal households took essentially the same form as baronial households, although on a much larger scale and the positions were more prestigious. An important role of the household servants was the preparation of food; the castle kitchens would have been a busy place when the castle was occupied, called on to provide large meals. Without the presence of a lord's household, usually because he was staying elsewhere, a castle would have been a quiet place with few residents, focused on maintaining the castle.

As social centres castles were important places for display. Builders took the opportunity to draw on symbolism, through the use of motifs, to evoke a sense of chivalry that was aspired to in the Middle Ages amongst the elite. Later structures of the Romantic Revival would draw on elements of castle architecture such as battlements for the same purpose. Castles have been compared with cathedrals as objects of architectural pride, and some castles incorporated gardens as ornamental features. The right to crenellate, when granted by a monarch – though it was not always necessary – was important not just as it allowed a lord to defend his property but because crenellations and other accoutrements associated with castles were prestigious through their use by the elite. Licences to crenellate were also proof of a relationship with or favour from the monarch, who was the one responsible for granting permission.

Courtly love was the eroticisation of love between the nobility. Emphasis was placed on restraint between lovers. Though sometimes expressed through chivalric events such as tournaments, where knights would fight wearing a token from their lady, it could also be private and conducted in secret. The legend of Tristan and Iseult is one example of stories of courtly love told in the Middle Ages. It was an ideal of love between two people not married to each other, although the man might be married to someone else. It was not uncommon or ignoble for a lord to be adulterous – Henry I of England had over 20 bastards for instance – but for a lady to be promiscuous was seen as dishonourable.

The purpose of marriage between the medieval elites was to secure land. Girls were married in their teens, but boys did not marry until they came of age. There is a popular conception that women played a peripheral role in the medieval castle household, and that it was dominated by the lord himself. This derives from the image of the castle as a martial institution, but most castles in England, France, Ireland, and Scotland were never involved in conflicts or sieges, so the domestic life is a neglected facet. The lady was given a dower of her husband's estates – usually about a third – which was hers for life, and her husband would inherit on her death. It was her duty to administer them directly, as the lord administered his own land. Despite generally being excluded from military service, a woman could be in charge of a castle, either on behalf of her husband or if she was widowed. Because of their influence within the medieval household, women influenced construction and design, sometimes through direct patronage; historian Charles Coulson emphasises the role of women in applying "a refined aristocratic taste" to castles due to their long term residence.

The positioning of castles was influenced by the available terrain. Whereas hill castles such as Marksburg were common in Germany, where 66 per cent of all known medieval were highland area while 34 per cent were on low-lying land, they formed a minority of sites in England. Because of the range of functions they had to fulfil, castles were built in a variety of locations. Multiple factors were considered when choosing a site, balancing between the need for a defendable position with other considerations such as proximity to resources. For instance many castles are located near Roman roads, which remained important transport routes in the Middle Ages, or could lead to the alteration or creation of new road systems in the area. Where available it was common to exploit pre-existing defences such as building with a Roman fort or the ramparts of an Iron Age hillfort. A prominent site that overlooked the surrounding area and offered some natural defences may also have been chosen because its visibility made it a symbol of power. Urban castles were particularly important in controlling centres of population and production, especially with an invading force, for instance in the aftermath of the Norman Conquest of England in the 11th century the majority of royal castles were built in or near towns.
As castles were not simply military buildings but centres of administration and symbols of power, they had a significant impact on the surrounding landscape. Placed by a frequently-used road or river, the toll castle ensured that a lord would get his due toll money from merchants. Rural castles were often associated with mills and field systems due to their role in managing the lord's estate, which gave them greater influence over resources. Others were adjacent to or in royal forests or deer parks and were important in their upkeep. Fish ponds were a luxury of the lordly elite, and many were found next to castles. Not only were they practical in that they ensured a water supply and fresh fish, but they were a status symbol as they were expensive to build and maintain.

Although sometimes the construction of a castle led to the destruction of a village, such as at Eaton Socon in England, it was more common for the villages nearby to have grown as a result of the presence of a castle. Sometimes planned towns or villages were created around a castle. The benefits of castle building on settlements was not confined to Europe. When the 13th-century Safad Castle was founded in Galilee in the Holy Land, the 260 villages benefitted from the inhabitants' newfound ability to move freely. When built, a castle could result in the restructuring of the local landscape, with roads moved for the convenience of the lord. Settlements could also grow naturally around a castle, rather than being planned, due to the benefits of proximity to an economic centre in a rural landscape and the safety given by the defences. Not all such settlements survived, as once the castle lost its importance – perhaps succeeded by a manor house as the centre of administration – the benefits of living next to a castle vanished and the settlement depopulated.

During and shortly after the Norman Conquest of England, castles were inserted into important pre-existing towns to control and subdue the populace. They were usually located near any existing town defences, such as Roman walls, although this sometimes resulted in the demolition of structures occupying the desired site. In Lincoln, 166 houses were destroyed to clear space for the castle, and in York agricultural land was flooded to create a moat for the castle. As the military importance of urban castles waned from their early origins, they became more important as centres of administration, and their financial and judicial roles. When the Normans invaded Ireland, Scotland, and Wales in the 11th and 12th centuries, settlement in those countries was predominantly non-urban, and the foundation of towns was often linked with the creation of a castle.

The location of castles in relation to high status features, such as fish ponds, was a statement of power and control of resources. Also often found near a castle, sometimes within its defences, was the parish church. This signified a close relationship between feudal lords and the Church, one of the most important institutions of medieval society. Even elements of castle architecture that have usually been interpreted as military could be used for display. The water features of Kenilworth Castle in England – comprising a moat and several satellite ponds – forced anyone approaching a water castle entrance to take a very indirect route, walking around the defences before the final approach towards the gateway. Another example is that of the 14th-century Bodiam Castle, also in England; although it appears to be a state of the art, advanced castle it is in a site of little strategic importance, and the moat was shallow and more likely intended to make the site appear impressive than as a defence against mining. The approach was long and took the viewer around the castle, ensuring they got a good look before entering. Moreover, the gunports were impractical and unlikely to have been effective.

As a static structure, castles could often be avoided. Their immediate area of influence was about and their weapons had a short range even early in the age of artillery. However, leaving an enemy behind would allow them to interfere with communications and make raids. Garrisons were expensive and as a result often small unless the castle was important. Cost also meant that in peacetime garrisons were smaller, and small castles were manned by perhaps a couple of watchmen and gate-guards. Even in war, garrisons were not necessarily large as too many people in a defending force would strain supplies and impair the castle's ability to withstand a long siege. In 1403, a force of 37 archers successfully defended Caernarfon Castle against two assaults by Owain Glyndŵr's allies during a long siege, demonstrating that a small force could be effective.

Early on, manning a castle was a feudal duty of vassals to their magnates, and magnates to their kings, however this was later replaced with paid forces. A garrison was usually commanded by a constable whose peacetime role would have been looking after the castle in the owner's absence. Under him would have been knights who by benefit of their military training would have acted as a type of officer class. Below them were archers and bowmen, whose role was to prevent the enemy reaching the walls as can be seen by the positioning of arrowslits.

If it was necessary to seize control of a castle an army could either launch an assault or lay siege. It was more efficient to starve the garrison out than to assault it, particularly for the most heavily defended sites. Without relief from an external source, the defenders would eventually submit. Sieges could last weeks, months, and in rare cases years if the supplies of food and water were plentiful. A long siege could slow down the army, allowing help to come or for the enemy to prepare a larger force for later. Such an approach was not confined to castles, but was also applied to the fortified towns of the day. On occasion, siege castles would be built to defend the besiegers from a sudden sally and would have been abandoned after the siege ended one way or another.
If forced to assault a castle, there were many options available to the attackers. For wooden structures, such as early motte-and-baileys, fire was a real threat and attempts would be made to set them alight as can be seen in the Bayeux Tapestry. Projectile weapons had been used since antiquity and the mangonel and petraria – from Eastern and Roman origins respectively – were the main two that were used into the Middle Ages. The trebuchet, which probably evolved from the petraria in the 13th century, was the most effective siege weapon before the development of cannons. These weapons were vulnerable to fire from the castle as they had a short range and were large machines. Conversely, weapons such as trebuchets could be fired from within the castle due to the high trajectory of its projectile, and would be protected from direct fire by the curtain walls.

Ballistas or springalds were siege engines that worked on the same principles as crossbows. With their origins in Ancient Greece, tension was used to project a bolt or javelin. Missiles fired from these engines had a lower trajectory than trebuchets or mangonels and were more accurate. They were more commonly used against the garrison rather than the buildings of a castle. Eventually cannons developed to the point where they were more powerful and had a greater range than the trebuchet, and became the main weapon in siege warfare.

Walls could be undermined by a sap. A mine leading to the wall would be dug and once the target had been reached, the wooden supports preventing the tunnel from collapsing would be burned. It would cave in and bring down the structure above. Building a castle on a rock outcrop or surrounding it with a wide, deep moat helped prevent this. A counter-mine could be dug towards the besiegers' tunnel; assuming the two converged, this would result in underground hand-to-hand combat. Mining was so effective that during the siege of Margat in 1285 when the garrison were informed a sap was being dug they surrendered. Battering rams were also used, usually in the form of a tree trunk given an iron cap. They were used to force open the castle gates, although they were sometimes used against walls with less effect.

As an alternative to the time-consuming task of creating a breach, an escalade could be attempted to capture the walls with fighting along the walkways behind the battlements. In this instance, attackers would be vulnerable to arrowfire. A safer option for those assaulting a castle was to use a siege tower, sometimes called a belfry. Once ditches around a castle were partially filled in, these wooden, movable towers could be pushed against the curtain wall. As well as offering some protection for those inside, a siege tower could overlook the interior of a castle, giving bowmen an advantageous position from which to unleash missiles.




</doc>
<doc id="50214" url="https://en.wikipedia.org/wiki?curid=50214" title="Alice in Chains">
Alice in Chains

Alice in Chains is an American rock band from Seattle, Washington, formed in 1987 by guitarist and vocalist Jerry Cantrell and drummer Sean Kinney, who later recruited bassist Mike Starr and lead vocalist Layne Staley. Starr was replaced by Mike Inez in 1993. William DuVall joined the band in 2006 as co-lead vocalist and rhythm guitarist, replacing Staley, who died in 2002. The band took its name from Staley's previous group, the glam metal band Alice N' Chains.

Although widely associated with grunge music, the band's sound incorporates heavy metal elements. Since its formation, Alice in Chains has released six studio albums, three EPs, three live albums, four compilations, two DVDs, 32 music videos and 31 singles. The band is known for its distinctive vocal style, which often included the harmonized vocals between Staley and Cantrell (and later Cantrell and William DuVall). Cantrell started to sing lead vocals on the 1992 acoustic EP "Sap", and his role continued to grow in the following albums, making Alice in Chains a two-vocal band.

Alice in Chains rose to international fame as part of the grunge movement of the early 1990s, along with other Seattle bands such as Nirvana, Pearl Jam, and Soundgarden. The band was one of the most successful music acts of the 1990s, selling over 30 million records worldwide, and over 14 million records in the US alone, with two No. 1 albums and six Top 10 albums on the "Billboard 200" chart. The band has had 18 Top 10 songs on Billboard's Mainstream Rock Tracks chart, 5 No. 1 hits, and 11 Grammy Award nominations. Their debut album, "Facelift", featuring the hit single "Man in the Box", was released in 1990 and has been certified double-platinum by the RIAA, selling over two million copies. In 1992, the band's second album, "Dirt", was released to critical acclaim and was certified quadruple platinum. Their second acoustic EP, "Jar of Flies", debuted at No. 1 on the "Billboard" 200 chart in 1994, becoming the first ever EP and first Alice in Chains release to top the charts, and it has been certified triple platinum. The band's third album, "Alice in Chains" debuted at No. 1 on the "Billboard" 200 in 1995 and is certified double platinum.

Although never officially disbanding, Alice in Chains was plagued by extended inactivity from 1996 onwards due to Staley's substance abuse, which resulted in his death in 2002. The band reunited in 2005 for a live benefit show, performing with a number of guest vocalists. They toured in 2006, with William DuVall taking over as lead vocalist full-time. The new line-up released the band's fourth studio album, "Black Gives Way to Blue", in 2009, which debuted at No. 5 on the Billboard 200, received gold certification by the RIAA and two Grammy nominations. The lead single, "Check My Brain", became the first Alice in Chains song to chart on the Billboard Hot 100, peaking at No. 92. Their fifth studio album, "The Devil Put Dinosaurs Here", was released in 2013 and debuted at No. 2 on the Billboard 200. The band toured extensively and released several videos in support of these albums. Alice in Chains' sixth studio album, "Rainier Fog", was released on August 24, 2018, debuted at No. 12 on the Billboard 200 and received a Grammy nomination for Best Rock Album.

Before the formation of Alice in Chains, then-drummer Layne Staley landed his first gig as a vocalist when he auditioned to sing for a local glam metal band known as Sleze after receiving some encouragement from his stepbrother Ken Elmer. Other members of this group at that time were guitarists Johnny Bacolas and Zoli Semanate, drummer James Bergstrom, and bassist Byron Hansen. This band went through several lineup changes culminating with Nick Pollock as their sole guitarist and Bacolas switching to bass before discussions arose about changing their name to "Alice in Chains". This was prompted by a conversation that Bacolas had with Russ Klatt, the lead singer of Slaughter Haus 5, about backstage passes. One of the passes said "Welcome to Wonderland", and they started talking about that being a reference to Alice in Wonderland, until Klatt said, "What about Alice in Chains? Put her in bondage and stuff like that." Bacolas thought the name "Alice in Chains" was cool and brought it up to his Sleze bandmates and everyone liked it, so they decided to change the name of the band. Due to concerns over the reference to female bondage, the group ultimately chose to spell it differently as Alice N' Chains to allay any parental concerns, though Staley's mother Nancy McCallum has said she was still not happy with this name at first. According to Bacolas, the decision to use the apostrophe-N combination in their name had nothing to do with the Los Angeles band Guns N' Roses. The name change happened in 1986, a year before Guns N' Roses became a household name with their first album "Appetite for Destruction", released in July 1987.

Staley met guitarist Jerry Cantrell at a party in Seattle around August 1987. A few months before that, Cantrell had watched a concert of Staley's then-band, "Alice N' Chains", in his hometown at the Tacoma Little Theatre, and was impressed by his voice. Cantrell was homeless after being kicked out of his family's house, so Staley invited Cantrell to live with him at the rehearsal studio Music Bank, and the two struggling musicians became roommates.

Alice N' Chains soon disbanded, and Staley joined a funk band. Cantrell's band, Diamond Lie, broke up and he wanted to form a new band, so Staley gave him the phone number of Melinda Starr, the girlfriend of drummer Sean Kinney, so that Cantrell could talk to him. Cantrell called the number and set up a meeting with Kinney. Kinney and his girlfriend went to the Music Bank and listened to Cantrell's demos, who mentioned that they needed a bass player to jam with them, and he had someone in mind: Mike Starr, with whom Cantrell had played in a band in Burien called Gypsy Rose. Kinney then mentioned that his girlfriend was actually Mike Starr's sister, and that he had been playing in bands together with Starr since they were kids. Kinney called Starr and a few days later he started jamming with him and Cantrell at the Music Bank, but they didn't have a singer.

Staley's funk band also required a guitarist at the time, and Staley asked Cantrell to join as a sideman. Cantrell agreed on condition that Staley join his band. Because Cantrell, Starr and Kinney wanted Staley to be their lead singer, they started auditioning terrible lead singers in front of Staley to send a hint. The last straw for Staley was when they auditioned a male stripper – he decided to join the band after that. Eventually the funk project broke up, and in 1987 Staley joined Cantrell's band on a full-time basis. Two weeks after the band's formation, they were playing a gig at the University of Washington, trying to fill out a 40-minute set with a couple of original material along with Hanoi Rocks and David Bowie covers.

The band played a couple of gigs in clubs around the Pacific Northwest, calling themselves different monikers, including Diamond Lie, the name of Cantrell's previous band, and "Fuck", before eventually adopting the name that Staley's previous band had initially flirted with, "Alice in Chains". Staley contacted his former bandmates and asked for permission to use the name. Nick Pollock wasn't particularly thrilled about it at the time and thought he should come up with a different name, but ultimately both he and James Bergstrom gave Staley their blessing to use the name.
Local promoter Randy Hauser became aware of the band at a concert and offered to pay for demo recordings. However, one day before the band was due to record at the Music Bank studio in Washington, police shut down the studio during the biggest cannabis raid in the history of the state. The final demo, completed in 1988, was named "The Treehouse Tapes" and found its way to the music managers Kelly Curtis and Susan Silver, who also managed the Seattle-based band Soundgarden. Curtis and Silver passed the demo on to Columbia Records' A&R representative Nick Terzo, who set up an appointment with label president Don Ienner. Based on "The Treehouse Tapes", Terzo signed Alice in Chains to Columbia in 1989. The band also recorded another untitled demo over a three-month period in 1989. This recording can be found on the bootleg release "Sweet Alice".

Alice in Chains soon became a top priority of the label, which released the band's first official recording in July 1990, a promotional EP called "We Die Young". The EP's lead single, "We Die Young", became a hit on metal radio. After its success, the label rushed Alice in Chains' debut album into production with producer Dave Jerden. Cantrell stated the album was intended to have a "moody aura" that was a "direct result of the brooding atmosphere and feel of Seattle".

The resulting album, "Facelift", was released on August 21, 1990, peaking at number 42 in the summer of 1991 on the "Billboard" 200 chart. "Facelift" was not an instant success, selling under 40,000 copies in the first six months of release, until MTV added "Man in the Box" to regular daytime rotation. The single hit number 18 on the Mainstream rock charts, with the album's follow up single, "Sea of Sorrow", reaching number 27, and in six weeks "Facelift" sold 400,000 copies in the US. The album was a critical success, with Steve Huey of AllMusic citing "Facelift" as "one of the most important records in establishing an audience for grunge and alternative rock among hard rock and heavy metal listeners". Sammy Hagar claimed he invited the band to tour with Van Halen after he saw the music video for "Man In The Box" on MTV.

"Facelift" was certified gold by the Recording Industry Association of America (RIAA) for selling a half-million copies on September 11, 1991, becoming the first album from Seattle's Grunge movement to be certified gold. The band continued to hone its audience, opening for such artists as Iggy Pop, Van Halen, Poison, and Extreme. "Facelift" has since been certified triple-platinum by the RIAA, for shipments of three million copies in the United States.

The concert at the Moore Theatre in Seattle on December 22, 1990, was recorded and released on VHS on July 30, 1991 as "Live Facelift". It features five live songs and three music videos. The home video has been certified gold by the RIAA for sales exceeding 50,000 copies.

In early 1991, Alice in Chains landed the opening slot for the Clash of the Titans tour with Anthrax, Megadeth, and Slayer, exposing the band to a wide metal audience but receiving mainly poor reception. Alice in Chains was nominated for a Best Hard Rock Performance Grammy Award in 1992 for "Man in the Box" but lost to Van Halen for their 1991 album "For Unlawful Carnal Knowledge".

Following the tour, Alice in Chains entered the studio to record demos for its next album, but ended up recording five acoustic songs instead. While in the studio, drummer Sean Kinney had a dream about "making an EP called "Sap"". The band decided "not to mess with fate", and on February 4, 1992, Alice in Chains released their second EP, "Sap". The EP was released while Nirvana's "Nevermind" was at the top of the "Billboard" 200 charts, resulting in a rising popularity of Seattle-based bands, and of the term "grunge music". "Sap" was certified gold within two weeks. The EP features Cantrell on lead vocals on the opening track, "Brother", and guest vocals by Ann Wilson from the band Heart, who joined Staley and Cantrell for the choruses of "Brother" and "Am I Inside". The EP also features Mark Arm of Mudhoney and Chris Cornell of Soundgarden, who shared vocals with Staley and Cantrell on the song "Right Turn", credited to "Alice Mudgarden" in the liner notes.

In 1992, Alice in Chains appeared in the Cameron Crowe film "Singles", performing as a "bar band". The band also contributed the song "Would?" to the film's , whose video received an award for Best Video from a Film at the 1993 MTV Video Music Awards.
In March 1992, the band returned to the studio. With new songs written primarily on the road, the material has an overall darker feel than "Facelift", with six of the album's thirteen songs dealing with the subject of addiction. "We did a lot of soul searching on this album. There's a lot of intense feelings." Cantrell said, "We deal with our daily demons through music. All of the poison that builds up during the day we cleanse when we play".
On September 29, 1992, Alice in Chains released its second album, "Dirt". The album peaked at number six on the "Billboard" 200 and since its release has been certified quadruple platinum by the RIAA, making "Dirt" the band's highest selling album to date. The album was a critical success, with Steve Huey of Allmusic praising the album as a "major artistic statement, and the closest they ever came to recording a flat-out masterpiece". Chris Gill of "Guitar World" called "Dirt" "huge and foreboding, yet eerie and intimate", and "sublimely dark and brutally honest".

"Dirt" spawned five top 30 singles, "Would?", "Rooster", "Them Bones", "Angry Chair", and "Down in a Hole", and remained on the charts for nearly two years. Alice in Chains was added as openers to Ozzy Osbourne's "No More Tours" tour. Days before the tour began, Layne Staley broke his foot in an ATV accident, forcing him to use crutches on stage.

Starr left the band shortly after the Hollywood Rock concert in Rio de Janeiro on January 22, 1993, stating that he wanted to spend more time with his family. Staley told "Rolling Stone" in 1994 about Starr leaving the band, "It was just a difference in priorities. We wanted to continue intense touring and press. Mike was ready to go home." Years later, Starr claimed that he was fired due to his drug addiction.

Starr was replaced by former Ozzy Osbourne bassist Mike Inez. Inez had met Alice in Chains during Ozzy Osbourne's "No More Tours" tour and became friends with them. When the band was in Brazil, they called Inez to join them and he accepted. Inez wanted to do the shows in Brazil and even got his immunization shots, but the band called him back telling that Starr wanted to do the last two shows in Brazil, so they would meet Inez in London instead. Inez ended up getting sick with his vaccination shots for a couple of days. Inez played his first concert with Alice in Chains on January 27, 1993 at the Camden Underworld in London.

In April 1993, the band recorded two songs with Inez, "What the Hell Have I" and "A Little Bitter", for the "Last Action Hero" soundtrack. During the summer of 1993, Alice in Chains toured with the alternative music festival Lollapalooza, their last major tour with Staley.

Following Alice in Chains' extensive 1993 world tour, Staley said the band "just wanted to go into the studio for a few days with our acoustic guitars and see what happened". "We never really planned on the music we made at that time to be released. But the record label heard it and they really liked it. For us, it was just the experience of four guys getting together in the studio and making some music."

Columbia Records released Alice in Chains' second acoustic-based EP, "Jar of Flies", on January 25, 1994. Written and recorded in one week, "Jar of Flies" debuted at number one on the "Billboard" 200, becoming the first EP—and first Alice in Chains release—to top the charts.

Paul Evans of "Rolling Stone" called the EP "darkly gorgeous", and Steve Huey said, "'Jar of Flies' is a low-key stunner, achingly gorgeous and harrowingly sorrowful all at once". "Jar of Flies" features Alice in Chains' first number-one single on the Mainstream Rock charts, "No Excuses". The second single, "I Stay Away", reached number ten on the Mainstream rock charts, while the final single "Don't Follow", reached number 25. "Jar of Flies" has been certified triple platinum by the RIAA, with over 2 million copies sold in the United States during its first year. "Jar of Flies" received two Grammy nominations, Best Hard Rock Performance for "I Stay Away", and Best Recording Package.

After the release of "Jar of Flies", Staley entered rehab for heroin addiction. The band was scheduled to tour during the summer of 1994 with Metallica, Suicidal Tendencies, Danzig, and Fight, as well as a slot during Woodstock '94, but while in rehearsal for the tour, Staley began using heroin again. Staley's condition prompted the other band members to cancel all scheduled dates one day before the start of the tour, putting the band on hiatus. Alice in Chains was replaced by Candlebox on the tour. Susan Silver's management office sent out a statement saying that the decision to withdraw from the Metallica tour and Woodstock was "due to health problems within the band".

The band broke up for six months. Kinney told "Rolling Stone" in 1996, "Nobody was being honest with each other back then. If we had kept going, there was a good chance we would have self-destructed on the road, and we definitely didn't want that to happen in public."

While Alice in Chains was inactive during 1995, Staley joined the "grunge supergroup" Mad Season, which also featured Pearl Jam guitarist Mike McCready, bassist John Baker Saunders from The Walkabouts, and Screaming Trees drummer Barrett Martin. Mad Season released one album, "Above", for which Staley provided lead vocals and the album artwork. The album spawned a number-two single, "River of Deceit", as well as a home video release of "Live at the Moore".

In April 1995, Alice in Chains entered Bad Animals Studio in Seattle with producer Toby Wright, who had previously worked with Corrosion of Conformity and Slayer. While in the studio, an inferior version of the song "Grind" was leaked to radio, and received major airplay. On October 6, 1995, the band released the studio version of the song to radio via satellite uplink to stem excessive spread of taped copies of the song.

On November 7, 1995, Columbia Records released the eponymous album, "Alice in Chains", which debuted at number one on the "Billboard" 200 and has since been certified triple platinum. Of the album's four singles, "Grind", "Again", "Over Now", and "Heaven Beside You", three feature Cantrell on lead vocals. Jon Wiederhorn of "Rolling Stone" called the album "liberating and enlightening, the songs achieve a startling, staggering and palpable impact."

On December 12, 1995, the band released the home video "The Nona Tapes", a mockumentary featuring interviews with the band members conducted by journalist Nona Weisbaum (played by Jerry Cantrell), and the music video for "Grind".

The song "Got Me Wrong" unexpectedly charted three years after its release on the "Sap" EP. The song was re-released as a single on the soundtrack for the independent film "Clerks" in 1994, reaching number seven on the Mainstream Rock Tracks chart. The band opted not to tour in support of "Alice in Chains", adding to the rumors of drug abuse.

Alice in Chains resurfaced on April 10, 1996, to perform their first concert in two and a half years for "MTV Unplugged", a program featuring all-acoustic set lists. The performance featured some of the band's highest charting singles, including "Rooster", "Down in a Hole", "Heaven Beside You", "No Excuses" and "Would?", and introduced a new song, "Killer Is Me", with Cantrell on lead vocals. The show marked Alice in Chains' only appearance as a five-piece band, adding second guitarist Scott Olson. A live album of the performance was released in July 1996, debuting at number three on the "Billboard" 200, and was accompanied by a home video release, both of which received platinum certification by the RIAA. Following the Unplugged concert, Alice in Chains performed "Again" on the TV show "Saturday Night Special" on April 20, 1996. The band also made an appearance on "The Late Show with David Letterman" on May 10, 1996, performing the songs "Again" and "We Die Young".

Alice in Chains performed four shows supporting the reunited original Kiss lineup on their 1996/97 Alive/Worldwide Tour, including the final live appearance of Layne Staley on July 3, 1996, in Kansas City, Missouri. Shortly after the show, Staley was found unresponsive after he overdosed on heroin and was taken to the hospital. Although he recovered, the band was forced to go on hiatus.

Although Alice in Chains never officially disbanded, Staley became a recluse, rarely leaving his Seattle condominium following the death of his ex-fiancée Demri Parrott due to a drug overdose on October 29, 1996. "Drugs worked for me for years", Staley told "Rolling Stone" in February 1996, "and now they're turning against me ... now I'm walking through hell and this sucks. I didn't want my fans to think that heroin was cool. But then I've had fans come up to me and give me the thumbs up, telling me they're high. That's exactly what I didn't want to happen.".

Unable to continue with new Alice in Chains material, Cantrell released his first solo album, "Boggy Depot", in 1998, also featuring Sean Kinney and Mike Inez. Cantrell and Kinney were also featured on Metallica's 1998 album "Garage Inc.", both were guest musicians in the track "Tuesday's Gone", a cover of Lynyrd Skynyrd.

In October 1998, Staley reunited with Alice in Chains to record two new songs, "Get Born Again" and "Died". Originally intended for Cantrell's second solo album, the songs were reworked by Alice in Chains and were released in the fall of 1999 on the box set, "Music Bank". The set contains 48 songs, including rarities, demos, and previously released album tracks and singles. The band also released a 15-track compilation titled "", serving as a sampler for "Music Bank", as well as the band's first compilation album; a live album, simply titled "Live", released on December 5, 2000; and a second compilation, titled "Greatest Hits" in 2001.

In November 1998, Layne Staley recorded a cover of Pink Floyd's "Another Brick in the Wall" with the supergroup Class of '99, formed by guitarist Tom Morello of Rage Against the Machine, bassist Martyn LeNoble, drummer Stephen Perkins, both from Jane's Addiction and Porno for Pyros, and keyboardist Matt Serletic. The song was featured on the soundtrack to the 1998 horror/sci-fi film, "The Faculty".

After they toured as part of Cantrell's solo band in 1998, Sean Kinney and Queensrÿche guitarist Chris DeGarmo formed a new band called Spys4Darwin. Mike Inez and Sponge lead vocalist Vin Dombroski joined the supergroup soon after. The band released their first and only album in 2001, a 6-track EP entitled "Microfish". In June 2001, Mike Inez joined Zakk Wylde's Black Label Society for the remaining dates of Ozzfest, following the departure of bassist Steve Gibb due to medical reasons. Inez joined the band again for their West Coast and Japanese tour in 2003.

By 2002, Cantrell had finished work on his second solo album, "Degradation Trip". Written in 1998, the album's lyrical content focused heavily on what Cantrell regarded as the demise of Alice in Chains, which still remained evident as the album approached its June 2002 release. However, in March that year, Cantrell commented, "We're all still around, so it's possible [Alice in Chains] could all do something someday, and I fully hope someday we will."

Reflecting on the band's hiatus in a 2011 interview, Kinney said that Staley wasn't the only one battling addiction. "He was the focal point, like singers are. So they'd single him out. But the truth was, it was pretty much everybody. I definitely had my hand firmly on the wheel going off the cliff. And the reason we pulled back – you know when you stop when you have two #1 records, it's not really the greatest career move – but we did that because we love each other and we didn't want to die in public. And I know for a fact in my heart that if we were to continue that I wouldn't be on the phone right now talking to you. I wouldn't have made it. I just wouldn't have."

After a decade of battling drug addiction, Layne Staley was found dead in his condominium in Seattle on April 19, 2002. The autopsy and toxicology report on Staley's body revealed that he died from a mixture of heroin and cocaine, known as "speedball". The autopsy concluded that Staley died on April 5, two weeks before his body was found. Cantrell dedicated his 2002 solo album, "Degradation Trip", released two months after Staley's death, to his memory. Mike Starr later claimed on "Celebrity Rehab" that he was the last person to see Staley alive, and admitted to feeling guilty about not calling 911 after Staley had warned him against it. "I wish I hadn't been high on benzodiazepine [that night], I wouldn't have just walked out the door", Starr said.

Following Staley's death, Mike Inez joined Heart and toured and recorded with the band from 2002 through 2006. Jerry Cantrell collaborated with several artists such as Heart, Ozzy Osbourne, and Damageplan. In 2004, Cantrell formed the band Cardboard Vampyres along with The Cult guitarist Billy Duffy, Motley Crue and Ratt vocalist John Corabi, The Cult bassist Chris Wyse and drummer Josh Howser.

On October 22, 2004, Sony BMG terminated their contract with Alice in Chains, 15 years after the band signed with the label, in 1989.

In 2005, Sean Kinney came up with the idea of doing a benefit concert for the victims of the tsunami disaster that struck South Asia in 2004. Kinney made calls to his former bandmates, as well as friends in the music community, such as former Alice in Chains manager Susan Silver. Kinney was surprised by the enthusiastic response to his idea. On February 18, 2005, Jerry Cantrell, Mike Inez, and Sean Kinney reunited to perform for the first time in 10 years at "K-Rock Tsunami Continued Care Relief Concert" in Seattle. The band featured Damageplan vocalist Pat Lachman, as well as other special guests including Maynard James Keenan of Tool and Ann Wilson of Heart. A few months after that experience, the band called Susan Silver and Cantrell's manager Bill Siddons and said they wanted to tour as Alice in Chains again.

Alice in Chains was approached by the producers of the CBS reality show "Rock Star" about being featured on its second season, but the band turned the offer down. In the show, aspiring singers competed to become the lead vocalist of a featured group.

On March 10, 2006, the surviving members performed at VH1's Decades Rock Live concert, honoring fellow Seattle musicians Ann and Nancy Wilson of Heart. They played "Would?" with vocalist Phil Anselmo of Pantera and Down and bass player Duff McKagan of Guns N' Roses and Velvet Revolver, and at the end of the performance Cantrell dedicated the show to Layne Staley and the late Pantera and Damageplan guitarist Dimebag Darrell. They also played "Rooster" with Comes with the Fall vocalist William DuVall and Ann Wilson. The band followed the concert with a short United States club tour, several festival dates in Europe, and a brief tour in Japan. Duff McKagan again joined the band for the reunion tour, playing rhythm guitar on selected songs. During the tour, the band played a 5-minute video tribute to Staley during the changeover from the electric to acoustic set.

To coincide with the band's reunion, Sony Music released the long-delayed third Alice in Chains compilation, "The Essential Alice in Chains", a double album that includes 28 songs.

Jerry Cantrell met William DuVall in Los Angeles in 2000 through a mutual acquaintance who introduced Cantrell to Comes with the Fall's first album. Cantrell started hanging out with the band and occasionally joined them onstage. Between 2001 and 2002, Comes with the Fall was both the opening act on Cantrell's tour for his second solo album, "Degradation Trip", and also his backing band, with DuVall singing Staley's parts at the concerts. DuVall joined Alice in Chains as lead singer during the band's reunion concerts in 2006, and made his first public performance with the band at VH1's "Decades Rock Live" concert. According to Cantrell, it only took one audition for DuVall to get the gig. For his first rehearsal with the band, DuVall sang "Love, Hate, Love". After they finished, Sean Kinney looked at his bandmates and said, "I think the search is pretty much over". According to Mike Inez, DuVall didn't try to emulate Staley, and that's what drew them to him.

Cantrell revealed that before he suggested DuVall for the band, Sean Kinney and Mike Inez invited Sponge and Spys4Darwin lead vocalist Vin Dombroski to jam with the band in their rehearsal space. Dombroski jammed with them to a couple of songs but they did not feel he was right for the band. According to Cantrell, Stone Temple Pilots and Velvet Revolver lead singer Scott Weiland was also interested in joining the band.

Cantrell explained the reunion saying, "We want to celebrate what we did and the memory of our friend. We have played with some [singers] who can actually bring it and add their own thing to it without being a Layne clone. We're not interested in stepping on [Staley's] rich legacy. It's a tough thing to go through. Do you take the Led Zeppelin approach and never play again, because the guy was that important? That's the approach we've taken for a lot of years. Or, do you give it a shot, try something? We're willing to take a chance on it. It's completely a reunion because the three of us who're left are back together. But it's not about separating and forgetting — it's about remembering and moving on." Before the tour, Kinney mentioned in an interview that he would be interested in writing new material, but not as Alice in Chains.

During the VH1 Rock Honors concert honoring Heart on May 12, 2007, Alice in Chains performed Heart's "Barracuda" fronted by country singer Gretchen Wilson. Heart's guitarist Nancy Wilson also joined them onstage.

Alice in Chains joined Velvet Revolver for a run of U.S. and Canadian gigs from August through October 2007. During that tour, the band also performed four special acoustic-only shows, named as "The Acoustic Hour". The acoustic performance at The Rave/Eagles Club in Milwaukee, Wisconsin on August 31, 2007 was recorded for an upcoming live album.
On November 2, 2007, Alice in Chains performed a four-song set at Benaroya Hall in Seattle for Matt Messina and the Symphony Guild's 10th anniversary benefit concert for the Seattle Children's Hospital & Regional Medical Center. In addition to the band's original material, they also played a cover of Led Zeppelin's "Kashmir" while backed by over 200 musicians, including the Northwest Symphony Orchestra and the Northwest Girlchoir.

Sean Kinney said about the band's reunion: "I never called Jerry; he never called me, and said, 'Hey, let's get the band back together,' you know? We had been taking every step extremely cautious and slow, and just doing whatever feels right: If it's genuine and we're doing it for genuine reasons and we're all okay with it then we take a little step. None of us is broke. Nobody needs to be a rock dork, and you know, stroke their ego. I mean, we don't really operate like that. So as long as it felt good and from the right place and it's about making music and carrying on…".

About the pressure being put on DuVall for replacing Staley as lead vocalist, Cantrell said, "To put all that weight on Will's shoulders is unfair. We're just figuring out how we work as a team. Although the band has changed, we've lost Layne, we've added Will, and there was no master plan. Playing again in 2005 felt right, so we did the next thing and toured. We did it step by step. It's more than just making music, and it always has been. We've been friends a long time. We've been more of a family than most, and it had to be okay from here", Cantrell said pointing to his heart.

Former The Doors manager Bill Siddons and his management company, Core Entertainment, co-managed Alice in Chains with original manager Susan Silver from 2005 to 2007.

The band started writing and demoing songs for a new album with DuVall in April 2007. But the band did not show further signs of progress until October 2008, when they announced that they had begun recording with producer Nick Raskulinecz in the studio.

Blabbermouth.net reported in September 2008 that Alice in Chains would enter the studio that October to begin recording a new album for a summer 2009 release.

On September 14, 2008, Alice in Chains performed at halftime during the Seattle Seahawks vs San Francisco 49ers game at the CenturyLink Field (then-named Qwest Field) in Seattle. The 12-minute performance for a crowd of 67,000 people featured a cover of Led Zeppelin's "Kashmir" accompanied by the Northwest Symphony Orchestra.

In October 2008, Alice in Chains began recording its fourth studio album at the Foo Fighters' Studio 606 in Los Angeles with producer Nick Raskulinecz. The band did not have a record label at the time and the album was funded by Jerry Cantrell and Sean Kinney. At the Revolver Golden God Awards, Cantrell said that the group had finished recording on March 18, 2009 and were mixing the album for a September release. The recording process was completed on Cantrell's 43rd birthday and also the same day that William DuVall's son was born. In April 2009, it was reported that the new Alice in Chains album would be released by Virgin/EMI, making it the band's first label change in its 20-plus year career. Susan Silver, who started managing Alice in Chains in 1988, now co-manages the band with David Benveniste and his Velvet Hammer firm.

On June 11, 2009, Blabbermouth.net reported that the new album would be titled "Black Gives Way to Blue" and was officially set to be released on September 29, 2009. The title first appeared on Amazon.com without any prior announcement from the band. In addition, it was announced that Elton John plays piano on the title track, a tribute to Layne Staley written and sung by Cantrell. The album features new vocalist and rhythm guitarist William DuVall sharing vocal duties with lead guitarist/vocalist Jerry Cantrell, who sings lead vocals on most of the songs. DuVall sings lead vocals on the song "Last of My Kind".

On June 30, 2009, the song "A Looking in View" was made available for purchase via iTunes and Amazon, and for a limited time it was available as a free download through the official Alice in Chains website in early July. Although it was not the album's first radio single, Rock stations across the U.S. started playing the song. The music video for "A Looking in View" debuted via the band's official website on July 7, 2009. The song was nominated for a Grammy Award for Best Hard Rock Performance.

"Check My Brain" was released to radio stations as the first official single from the album on August 14, 2009, and was made available for purchase on August 17, 2009. The music video for "Check My Brain" premiered on September 14, 2009. The song was also nominated for a Grammy Award for Best Hard Rock Performance.

To promote the album, the band released an EPK featuring all four of the members being interviewed while the Kiss makeup is being applied on them. An app for iPhone was released on October 27, 2009, featuring songs, music videos, news, photos and networking.

Sean Kinney said about the new album and the fans' mixed reactions about the band moving on after Staley's death: "Look, it's a big move to fucking stand up and move on. Some people, the music connected with them so strongly, their opinions, how they feel about it ... It's amazing that they have such a connection but they seem to act like it happened to them. This happened to us and Layne's family, not them. This is actually our lives. If we're okay with it, why can't you be? This happened to us, this didn't happen to you. But this album isn't about that, it's a bigger universal point. We're all going to fucking die, we're all going to lose somebody, and it fucking hurts. How do you move on? This record is us moving on, and hurting. That, to me, is a victory. I already feel like I've won." "Sometimes people ask us, 'Wouldn't Layne have been pissed off that we did this?' And I tell them it would have been the opposite: he would have been pissed off that it took us so long to do this. We're not doing this for money; there is no money in the music business anymore. Jerry and I funded the whole album, and we spent lots of our own money, because we believe in this. And one of the reasons I'm doing this is so more light is turned on to something where the light was turned off." And Cantrell added: "We've toured around the world, we've lost some friends, we buried a dear friend, and somebody that you just can't fucking replace, and then we've chosen by circumstance to get together again. That turned into 'maybe we can fucking do this.' And that turned into this".
In September 2008, it was announced that Alice in Chains would headline Australia's Soundwave Festival in 2009, alongside Nine Inch Nails and Lamb of God. In February 2009, it was also announced that Alice in Chains would play at the third annual Rock on the Range festival. On August 1, 2009, Alice in Chains performed, along with Mastodon, Avenged Sevenfold, and Glyder, at Marlay Park, Dublin as direct support to Metallica. The band made an appearance on "Later... with Jools Holland" on November 10, 2009, performing "Lesson Learned", "Black Gives Way to Blue", and "Check My Brain" as the final performance of the episode.

To coincide with the band's European tour, Alice in Chains released its next single, "Your Decision", on November 16, 2009 in the UK and on December 1 in the US. The last single from the album was "Lesson Learned" and was released to rock radio in mid-June 2010.

"Black Gives Way to Blue" debuted at No. 5 on the "Billboard" 200. On May 18, 2010, the album was certified gold by the RIAA for selling over 500,000 copies in the U.S. The singles ""Check My Brain"" and ""Your Decision"" reached No. 1 on Billboard's Mainstream Rock Tracks, while ""Lesson Learned"" reached No. 4. ""Check My Brain"" was also the band's first #1 song on the Alternative Songs chart, and on the Hot Rock Songs chart, it also reached No. 92 on Billboard's Hot 100, becoming the band's first single to appear on the chart.

Along with Mastodon and Deftones, Alice in Chains toured the United States and Canada in late 2010 on the Blackdiamondskye tour, an amalgam of the three bands' latest album titles ("Black Gives Way to Blue", "Diamond Eyes", and "Crack the Skye").

On March 8, 2011, former Alice in Chains bassist Mike Starr was found dead at his home in Salt Lake City. Police told Reuters they were called to Starr's home at 1:42 pm and found his body; Starr was 44. Reports later surfaced that Starr's roommate had seen him mixing methadone and anxiety medication hours before he was found dead. Later reports indicated Starr's death may have been linked to two different types of antidepressants prescribed to him by his doctor. A public memorial was held for Starr at the Seattle Center's International Fountain on March 20, 2011. A private memorial was also held, which Jerry Cantrell and Sean Kinney attended according to Mike Inez.

On March 21, 2011, Alice in Chains announced that they were working on a fifth studio album, and both Cantrell and Inez later made statements that they had begun the recording process. The album was expected to be finished by summer of 2012 and released by the end of 2012 or beginning of 2013. While Alice in Chains were writing for the album in 2011, Cantrell underwent surgery on his right shoulder, which delayed recording the new material. In an interview published in May 2012, Cantrell explained, "The thing that set me back is I had some bone spurs [and] cartilage issues in my shoulders. I had the same issue in the other shoulder about six years ago so I've had them both done now. It's a repetitive motion injury from playing." Cantrell could not play guitar for eight months while he was recovering from surgery. While recuperating at home in a sling, Cantrell heard a riff in his head and sang it into his phone. The riff later became the song "Stone".

Alice in Chains played their first concert in nearly 10 months and their first concert after Cantrell's shoulder surgery on August 13, 2011 at the Winstar Casino in Thackerville, Oklahoma. The band played more three concerts in 2011, at Canada's Rock on the Range festival, Chile's Maquinaria festival and Brazil's SWU festival.

Alice in Chains' only concert in 2012 was a five-song acoustic set on May 31 at the eighth annual MusiCares MAP Fund Benefit Concert honoring Jerry Cantrell.

On December 12, 2012, Cantrell confirmed that the new album had been completed. The first single, "Hollow", debuted online on December 18, available for digital download in January 2013, along with an official music video. On February 13, 2013, Alice in Chains posted on Facebook that their new album title would be an anagram of the letters H V L E N T P S U S D A H I E E O E D T I U R R. The next day they announced that the album would be called "The Devil Put Dinosaurs Here", which was released on May 28, 2013, debuting at No. 2 on the "Billboard" 200.

To promote the album, Alice in Chains teamed up with Funny or Die for an 11-minute mockumentary titled "", in which Film Studies professor Alan Poole McLard (played by W. Earl Brown) attempts to make a documentary on Alice in Chains without any help from the actual band, interviewing other musicians instead. Among them are country singer Donnie 'Skeeter' Dollarhide Jr. (played by Jerry Cantrell), Reggae singer Nesta Cleveland (played by William DuVall), Black Metal musician Unta Gleeben Glabben Globben Globin (played by Mike Inez) and the hipster Stanley Eisen (played by Sean Kinney). The video was released on April 3, 2013 and also features cameos by Ann and Nancy Wilson from Heart, Mike McCready from Pearl Jam, Kim Thayil from Soundgarden, Duff McKagan from Guns N' Roses, Brent Hinds and Bill Kelliher from Mastodon, and Lars Ulrich and Robert Trujillo from Metallica.

In June 2013, the band released a pinball game app for iOS as part of "Pinball Rocks" HD compilation, featuring the single "Hollow", the band's logo and the album artwork, as well as references to the band's previous albums such as "Jar of Flies" and the self-titled record.

The band released videos for the songs "Hollow", "Stone," "Voices," the title track and "Phantom Limb". ""Hollow"" and ""Stone"" reached No. 1 on Billboard's Mainstream Rock Tracks, while ""Voices"" reached No. 3, and each one of the three songs stayed on the chart for 20 weeks. "The Devil Put Dinosaurs Here" was nominated for a Grammy Award for Best Engineered Album, Non-Classical in 2014.

Alice in Chains toured extensively in the U.S., Canada, and Europe in 2013 and 2014. In May 2013, the band co-headlined the annual MMRBQ festival with Soundgarden in Camden, New Jersey. Asked in September 2013 if Alice in Chains would make another album, Cantrell replied, "It'll be a while. It's [been] four years since we put the last one out, but at least it's not the gap that was between the last one, so that's about right - about three to four years."

On January 18, 2015, Alice in Chains performed in the halftime show of the NFC Championship game between the Seattle Seahawks and the Green Bay Packers at the CenturyLink Field in Seattle. Cantrell is a lifelong Seahawks fan and often attends their games. In August 2015, Bassist Mike Inez said that the band had been "throwing around riffs for a new record" and "taking it nice and slow". The band toured in the summer of 2015 and the summer of 2016, including select shows opening for Guns N' Roses as part of the Not in This Lifetime... Tour. The band finished their 2016 tour with a concert at the Grand Sierra Resort and Casino in Reno, Nevada on October 8, 2016.

In November 2016, Alice in Chains released a cover of the Rush song "Tears", which was included in the 40th anniversary release of the album "2112".

The home video "Live Facelift" was released on vinyl for the first time on November 25, 2016, as part of Record Store Day's Black Friday event. The album features six songs and only 5000 copies were issued.

To celebrate the tenth anniversary of Record Store Day, on April 22, 2017 Legacy Recordings released ""Get Born Again"/"What The Hell Have I"", a special 45 RPM double 7" single featuring four tracks remastered and available on vinyl for the first time, "What The Hell Have I", "A Little Bitter", "Get Born Again" and "Died".

In January 2017, Mike Inez stated in an interview that the band had begun work on a new album. In June 2017, it was reported that the band would return to Studio X (formerly Bad Animals Studios) in Seattle to record a new album later that month, for a tentative early 2018 release. The sessions were helmed by Nick Raskulinecz, who produced the band's last two albums. Studio X was the studio where Alice in Chains recorded its 1995 self-titled album. According to Inez, the band was not signed to a label, having completed its previous two-record contract with the Universal Music Group. "This [upcoming album], we're not sure where it's gonna land ... I mean, we financed ['Black Gives Way To Blue'] on our own too, so we're not too worried about that stuff. We've just gotta get it out to ... a significant label [with worldwide distribution]."

The band started recording their sixth studio album on June 12, 2017. On January 11, 2018, producer Nick Raskulinecz announced via Instagram that the album was nearly finished and that there was only one more day left of recording. During an interview with "Guitar World" published on April 11, 2018, Jerry Cantrell said that the album was recorded at four studios. After recording at Studio X in Seattle, the band went to Nashville to record vocals and lead guitars at Nick Raskulinecz's home studio. But Cantrell had to take an unexpected break from work for a couple of weeks after getting sick on a trip to Cabo for Sammy Hagar's birthday. Cantrell had the band's engineer, Paul Figueroa, come in to his house and record a lot of his vocals and solos there. The band finished recording the album at the Henson Recording Studios in Los Angeles. Cantrell also said he expects the album to be released "probably sometime this summer".

At the press room of the Rock and Roll Hall of Fame Induction Ceremony on April 14, 2018, Cantrell revealed that Alice in Chains had just signed with BMG, and that they had finished mixing their new album.

Alice in Chains did not perform live in 2017. The band performed their first concert since October 2016 at the House of Blues in Boston on April 28, 2018. In May 2018, Alice in Chains headlined the festivals Carolina Rebellion, Lunatic Luau, Pointfest, Northern Invasion, the WMMR BBQ festival in Philadelphia, and the Rock on the Range festival in Columbus, Ohio on May 18, 2018, in which they paid tribute to Chris Cornell on the first anniversary of his death covering two Soundgarden songs to close their set, "Hunted Down" and "Boot Camp", respectively. At the end of the show, the lights on stage spelled out "CC" for Chris Cornell and "SG" for Soundgarden as feedback rang out. The band started their European tour in June 2018, and headlined the Tons Of Rock Festival in Norway alongside Ozzy Osbourne and Helloween. Alice in Chains are also scheduled to headline KISW's Pain in the Grass festival in August 2018.

The band released a new single, "The One You Know", via Spotify, Amazon and iTunes on May 3, 2018. A music video directed by Adam Mason was released on YouTube the same day. "The One You Know" peaked at No. 9 on Billboard's Mainstream Rock chart.

During an interview with Eddie Trunk on "Trunk Nation" on May 7, 2018, Jerry Cantrell said that the new album would be released at the end of August 2018. the band also revealed that they talked to director Adam Mason, who is making a dark sci-film, about doing two separate pieces of art and maybe molding them together, and that the music video for "The One You Know" is the first chapter of molding Mason's film and the band's music videos together.

The second single, "So Far Under", was released on Alice in Chains' YouTube channel and on streaming platforms on June 27, 2018. It was also announced that the album would be titled "Rainier Fog", with the release date scheduled for August 24, 2018. The album's artwork and the track listing were also revealed on the same day. Jerry Cantrell told "Rolling Stone" that the title "Rainier Fog" was inspired by the Mount Rainier in Seattle, and the title track is a tribute to the Seattle music scene. "This song is a little homage to all of that: where we come from, who we are, all of the triumphs, all of the tragedies, lives lived".

The album's third single, "Never Fade", was released on August 10, 2018 through digital and streaming services. The song is a tribute dedicated to frontman William DuVall's grandmother, Chris Cornell, and Alice in Chains' original singer Layne Staley. "Never Fade" peaked at No. 10 on Billboard's Mainstream Rock chart. A music video directed by Adam Mason was released on November 1, 2018 and continued the storyline from the music video of "The One You Know".

In June 2018, William DuVall said in an interview with Swedish website "Rocksverige" that the music video for "The One You Know" is the first chapter of what the band is hoping will be visuals for all ten songs from the album "Rainier Fog", and in addition to that, will be a companion piece to the film that director Adam Mason was shooting.

On August 20, 2018, the baseball team Seattle Mariners hosted a special "Alice in Chains Night" at the Safeco Field in Seattle to promote "Rainier Fog", with the team offering the fans a package that included a Safeco Field terrace club ticket, access to a pre-game listening party of the album, an Alice in Chains T-shirt and a Rainier Fog CD. Jerry Cantrell also threw out the ceremonial first pitch and delivered a strike before the Seattle Mariners vs. Houston Astros game.

To mark the launch of the album, on August 21, 2018, Alice in Chains performed an acoustic set at the top of Seattle's Space Needle and debuted the song "Fly". Alice in Chains were the first band to perform on the Space Needle's new "Loupe" glass floor, the world's first and only revolving glass floor 500 feet high. The concert was exclusive for an audience of SiriusXM subscribers. SiriusXM broadcast the concert on their channel Lithium on August 31, 2018.

On August 22, 2018, Alice in Chains sent fans on a Scavenger hunt to access a secret gig that the band would be performing in Seattle on August 24. Ten signed CD copies of "Rainier Fog" were hidden around the city as a ticket into the show, and the band asked the fans to keep an eye on their Instagram story for details on the 10 hidden locations. Once all 10 albums were found, the band revealed that the secret gig would be at the rock club The Crocodile, with limited tickets available with the purchase of their album at a pop-up event at the same venue the next day. Preview clips of each of the album tracks were posted on the band's Instagram.

The band also commemorated the release of the album with a pop-up museum installation at The Crocodile in Seattle on August 23 and 24. The museum featured rare Alice in Chains photos, limited-edition merchandise and memorabilia that showcased the band's 30+ year career.

"Rainier Fog" debuted at No. 12 on the "Billboard" 200 chart, selling 31,000 copies (29,000 in traditional album sales), in its first week of release. The album also debuted at No. 1 on Billboard's Top Rock Albums, Alternative Albums and Hard Rock Albums charts, and at No. 3 on the Vinyl Albums chart. "Rainier Fog" became Alice in Chains' first top 10 in the UK, peaking at No. 9, and topping UK's Rock & Metal Albums chart. The album has been nominated for a Grammy Award for Best Rock Album.

On December 13, 2018, the teaser of the film "Black Antenna" featuring the song "Rainier Fog" was released on Alice in Chains' official YouTube channel, with drummer Sean Kinney stating; "We've always toyed with the idea of creating videos for every song on one of our albums. Not only did we do that for "Rainier Fog", it got totally out of hand and we made a whole goddamn movie. Everything that will be seen in the videos will be footage from "Black Antenna" to preface the complete film's release.".

Although Alice in Chains has been labeled grunge by the mainstream media, Jerry Cantrell identifies the band as primarily heavy metal. He told "Guitar World" in 1996, "We're a lot of different things  ... I don't quite know what the mixture is, but there's definitely metal, blues, rock and roll, maybe a touch of punk. The metal part will never leave, and I never want it to". The "Edmonton Journal" has stated, "Living and playing in Seattle might have got them the grunge tag, but they've always pretty much been a classic metal band to the core."

Over the course of their career, the band's sound has also been described as alternative metal, sludge metal, doom metal, drone rock, hard rock, and alternative rock. Regarding the band's constant categorization by the media, Cantrell stated "When we first came out we were metal. Then we started being called alternative metal. Then grunge came out and then we were hard rock. And now, since we've started doing this again I've seen us listed as: hard rock, alternative, alternative metal and just straight metal. I walked into an HMV the other day to check out the placement and see what's on and they've got us relegated back into the metal section. Right back where we started!". Drummer Sean Kinney rejects the grunge label, stating in a 2013 interview "I mean, before we first came out there was no grunge, they hadn't invented that word. Before they invented the word grunge we were alternative rock and alternative metal and metal and rock, and we didn't give a shit whatever, we were a rock and roll band!". According to Mike Inez, they were always the metal stepchildren of the Seattle scene.

Jerry Cantrell's guitar style combines "pummeling riffs and expansive guitar textures" to create "slow, brooding minor-key grinds". He is also recognized for his natural ability to blend acoustic and electric guitars. While down-tuned, distorted guitars mixed with Staley's distinctive "snarl-to-a-scream" vocals appealed to heavy metal fans, the band also had "a sense of melody that was undeniable", which introduced Alice in Chains to a much wider audience outside of the heavy metal underground.

According to Stephen Thomas Erlewine of AllMusic, Alice in Chains' sound has a "Black Sabbath-style riffing and an unconventional vocal style". The band has been described by Erlewine as "hard enough for metal fans, yet their dark subject matter and punky attack placed them among the front ranks of the Seattle-based grunge bands". Three of the band's releases feature acoustic music, and while the band initially kept these releases separate, Alice in Chains' self-titled album combined the styles to form "a bleak, nihilistic sound that balanced grinding hard rock with subtly textured acoustic numbers".

Alice in Chains is also noted for the unique vocal harmonies of Staley (or DuVall) and Cantrell, which included overlapping passages, dual lead vocals, and trademark harmonies typically separated by a major third. Cantrell said it was Staley who gave him the self-assurance to sing his own songs. Alyssa Burrows said the band's distinctive sound "came from Staley's vocal style and his lyrics dealing with personal struggles and addiction". Staley's songs were often considered "dark", with themes such as drug abuse, depression, and suicide, while Cantrell's lyrics often dealt with personal relationships.

Alice in Chains has sold over 14 million records in the United States, and over 30 million records worldwide, released two number-one albums, had 23 top 40 singles, and has received eleven Grammy nominations. The band was ranked number 34 on VH1's "100 Greatest Artists of Hard Rock". Alice in Chains was named 15th greatest live band by "Hit Parader", with vocalist Layne Staley placing as 27th greatest heavy metal vocalist of all time. The band's second album, "Dirt", was named 5th best album in the last two decades by "Close-Up" magazine in 2008.

In October 2008, "Guitar World" ranked Jerry Cantrell's solo in "Man In The Box" at No. 77 on their list of "100 Greatest Guitar Solos". In August 2009, Alice in Chains won the "Kerrang!" Icon Award.

In November 2011, "Jar of Flies" was ranked number four on "Guitar World" magazine's top ten list of guitar albums of 1994. It was also featured in "Guitar World" magazine's "Superunknown: 50 Iconic Albums That Defined 1994" list, and in May 2014, the EP was placed at number five on Loudwire's "10 Best Hard Rock Albums of 1994" list.

Pantera and Damageplan guitarist Dimebag Darrell had expressed his admiration for Jerry Cantrell's guitar work in an interview for "Guitar International" in 1995, saying that "the layering and the honest feel that Jerry Cantrell gets on [Alice in Chains' "Dirt"] record is worth a lot more than someone who plays five million notes".

Street musician Wesley Willis wrote a song about the band entitled "Alice in Chains", featured on his 1996 album "Feel The Power".

Billy Corgan revealed that the song "Bleeding The Orchid" from The Smashing Pumpkins' 2007 album "Zeitgeist" has a bit of an homage to Alice in Chains in the harmonies and was indirectly inspired by the death of Layne Staley.

Elton John stated that he is a fan of Alice in Chains and a big admirer of Jerry Cantrell. According to Jon Wiederhorn of MTV, Godsmack has "sonically followed Alice in Chains' lead while adding their own distinctive edge". Godsmack singer and founder Sully Erna has also cited Layne Staley as his primary influence. Godsmack was named after the Alice in Chains song "God Smack" from the album "Dirt". Staind has covered Alice in Chains' song "Nutshell" live, which appears on the compilation "", and also wrote a song entitled "Layne", dedicated to Staley, on the album "14 Shades of Grey". Three Days Grace also performs a cover of "Rooster", which can be seen on the DVD "Live at the Palace". Other bands that have been influenced by Alice in Chains include Korn, Creed, Nickelback, Taproot, Stone Sour, Puddle of Mudd, Queens of the Stone Age, Rains, Theory of a Deadman, A Pale Horse Named Death, Smile Empty Soul, Avenged Sevenfold, Seether, Incubus, Hoobastank, Mudvayne, 10 Years, Breaking Benjamin, Days of the New, and Tantric. Metallica said they have always wanted to tour with the band, citing Alice in Chains as a major inspiration for their 2008 release, "Death Magnetic".

Alice in Chains has also had a significant influence on modern heavy metal. Their songs were covered by various metal bands such as In Flames, Opeth, Dream Theater, Secrets of the Moon, Suicide Silence, 36 Crazyfists, Cane Hill, Ektomorf, Dritt Skitt, Grave and Thou, who described their 2018 EP "Rhea Sylvia" as "a melodic grunge, Alice in Chains homage." In 2009, Anders Fridén of Swedish melodic death metal band In Flames cited Layne Staley as an inspiration for his vocals on the band's later albums. In addition to fellow musicians, the band has also received praise from critics, with Steve Huey of AllMusic calling them "one of the best metal bands of the '90s" upon reviewing the 1999 compilation "Nothing Safe".

In 2009, the Vitamin String Quartet released the album "The String Quartet Tribute to Alice in Chains", featuring instrumental versions on viola, violin and cello of 12 of the band's biggest hits.

In 2017, "Metal Injection" ranked Alice in Chains at number 1 on their list of "10 Heaviest Grunge Bands".

In June 2017, Ozzy Osbourne ranked "Facelift" at number 9 on his list of "10 Favorite Metal Albums".

The claymation dolls of the band members used in the music video for "I Stay Away" are on display at the Rock and Roll Hall of Fame museum in Cleveland, Ohio.

Current members
Former members



</doc>
<doc id="50236" url="https://en.wikipedia.org/wiki?curid=50236" title="Battle of Towton">
Battle of Towton

The Battle of Towton was fought on 29 March 1461 during the English Wars of the Roses, near the village of Towton in Yorkshire. A culminating battle in the dynastic struggles between the houses of Lancaster and York for control of the English throne, the engagement ended in an overwhelming victory for the Yorkists. It brought about a change of monarchs in England, with the victor, the Yorkist Edward IV having displaced the Lancastrian Henry VI (on the throne since 1422) as king, and thus driving the head of the Lancastrians and his key supporters out of the country.

It is described as "probably the largest and bloodiest battle ever fought on English soil", and according to historical sources, probably the longest. According to chroniclers, more than 50,000 soldiers from the houses of York and Lancaster fought for hours amidst a snowstorm on that day, which was Palm Sunday. A newsletter circulated a week after the battle reported that 28,000 died on the battlefield.

Contemporary accounts described Henry VI as peaceful and pious, not suited for the violent dynastic civil wars, such as the Wars of the Roses. He had periods of insanity while his inherent benevolence eventually required his wife, Margaret of Anjou, to assume control of his kingdom, which contributed to his own downfall. His ineffectual rule had encouraged the nobles' schemes to establish control over him, and the situation deteriorated into a civil war between the supporters of Margaret and those of Richard, Duke of York. After the Yorkists captured Henry in 1460, the English parliament passed an Act of Accord to let York and his line succeed Henry as king. Henry's consort, Margaret of Anjou, refused to accept the dispossession of her son's right to the throne and, along with fellow Lancastrian malcontents, raised an army. Richard of York was killed at the Battle of Wakefield and his titles, including the claim to the throne, passed to his eldest son Edward. Nobles who were previously hesitant to support Richard's claim to the throne considered the Lancastrians to have reneged on the Act – a legal agreement – and Edward found enough backing to denounce Henry and declare himself king. The Battle of Towton was to affirm the victor's right to rule over England through force of arms.

On reaching the battlefield, the Yorkists found themselves heavily outnumbered. Part of their force under the Duke of Norfolk had yet to arrive. The Yorkist leader Lord Fauconberg turned the tables by ordering his archers to take advantage of the strong wind to outrange their enemies. The one-sided missile exchange, with Lancastrian arrows falling short of the Yorkist ranks, provoked the Lancastrians into abandoning their defensive positions. The ensuing hand-to-hand combat lasted hours, exhausting the combatants. The arrival of Norfolk's men reinvigorated the Yorkists and, encouraged by Edward, they routed their foes. Many Lancastrians were killed while fleeing; some trampled each other and others drowned in the rivers, which are said to have made them run red with blood for several days. Several who were taken as prisoners were executed.

The power of the House of Lancaster was severely reduced after this battle. Henry fled the country, and many of his most powerful followers were dead or in exile after the engagement, letting Edward rule England uninterrupted for nine years, before a brief restoration of Henry to the throne. Later generations remembered the battle as depicted in William Shakespeare's dramatic adaptation of Henry's life—"Henry VI, Part 3", Act 2, Scene 5. In 1929, the Towton Cross was erected on the battlefield to commemorate the event. Various archaeological remains and mass graves related to the battle were found in the area centuries after the engagement.

In 1461, England was in the sixth year of the Wars of the Roses, a series of civil wars between the houses of York and Lancaster over the English throne. The Lancastrians backed the reigning King of England, Henry VI, an indecisive man who had bouts of madness. The leader of the Yorkists was initially Richard, Duke of York, who resented the dominance of a small number of aristocrats favoured by the king, principally his close relatives the Beaufort family. Fuelled by rivalries between influential supporters of both factions, York's attempts to displace Henry's favourites from power led to war. After capturing Henry at the Battle of Northampton in 1460, the duke, who was of royal blood, issued his claim to the throne. Even York's closest supporters among the nobility were reluctant to usurp the dynasty; the nobles passed by a majority vote the Act of Accord, which ruled that the duke and his heirs would succeed the throne upon Henry's death.

The Queen of England, Margaret of Anjou, refused to accept an arrangement that deprived her son—Edward of Westminster—of his birthright. She had fled to Scotland after the Yorkist victory at Northampton; there she began raising an army, promising her followers the freedom to plunder on the march south through England. Her Lancastrian supporters also mustered in the north of England, preparing for her arrival. York marched with his army to meet this threat but he was lured into a trap at the Battle of Wakefield and killed. The duke and his second son Edmund, Earl of Rutland were decapitated by the Lancastrians and their heads were impaled on spikes atop the Micklegate Bar, a gatehouse of the city of York. The leadership of the House of York passed onto the duke's heir, Edward.
The victors of Wakefield were joined by Margaret's army and they marched south, plundering settlements in their wake. They liberated Henry after defeating the Yorkist army of Richard Neville, Earl of Warwick, in the Second Battle of St Albans and continued pillaging on their way to London. The city of London refused to open its gates to Henry and Margaret for fear of being looted. The Lancastrian army was short of supplies and had no adequate means to replenish them. When Margaret learned that Richard of York's eldest son Edward, Earl of March and his army had won the Battle of Mortimer's Cross in Herefordshire and were marching towards London, she withdrew the Lancastrians to York. Warwick and the remnants of his army marched from St Albans to join Edward's men and the Yorkists were welcomed into London. Having lost custody of Henry, the Yorkists needed a justification to continue the rebellion against the king and his Lancastrian followers. On 4 March, Warwick proclaimed the young Yorkist leader as King Edward IV. The proclamation gained greater acceptance than Richard of York's earlier claim, as several nobles opposed to letting Edward's father ascend the throne viewed the Lancastrian actions as a betrayal of the legally established Accord.

The country now had two kings—a situation that could not be allowed to persist, especially if Edward was to be formally crowned. Edward offered amnesty to any Lancastrian supporter who renounced Henry. The move was intended to win over the commoners; his offer did not extend to wealthy Lancastrians (mostly the nobles). The young king summoned and ordered his followers to march towards York to take back his family's city and to formally depose Henry through force of arms. The Yorkist army moved along three routes. Warwick's uncle, Lord Fauconberg, led a group to clear the way to York for the main body, which was led by Edward. The Duke of Norfolk was sent east to raise forces and rejoin Edward before the battle. Warwick's group moved to the west of the main body, through the Midlands, gathering men as they went. On 28 March, the leading elements of the Yorkist army came upon the remains of the crossing in Ferrybridge that spanned the River Aire. They were rebuilding the bridge when they were attacked and routed by a band of about 500 Lancastrians, led by Lord Clifford.

Learning of the encounter, Edward led the main Yorkist army to the bridge and was forced into a gruelling battle; although the Yorkists were superior in numbers, the narrow bridge was a bottleneck, forcing them to confront Clifford's men on equal terms. Edward sent Fauconberg and his horsemen to ford the river at Castleford, which should have been guarded by Henry Earl of Northumberland but he arrived late, by which time the Yorkists had crossed the ford and were heading to attack the Lancastrians at Ferrybridge from the flank. The Lancastrians retreated but were chased to Dinting Dale where they were all killed; Clifford was slain by an arrow to his throat. Having cleared the vicinity of enemy forces, the Yorkists repaired the bridge and pressed onwards to camp overnight at Sherburn-in-Elmet. The Lancastrian army marched to Tadcaster, about north of Towton and made camp. As dawn broke, the two rival armies struck camp under dark skies and strong winds. Although it was Palm Sunday, a day of holy significance to Christians, the forces prepared for battle and a few documents named the engagement the Battle of "Palme Sonday Felde" but the name did not gain wide acceptance. Popular opinion favoured naming the battle after the village of Towton because of its proximity and it being the most prominent in the area.

Contemporary sources declare that the two armies were huge, stating that more than 100,000 men fought in the battle. An account in William Gregory's "Chronicle of London" (15th century) by a soldier who had served in the Wars, claimed that the Yorkists had 200,000 soldiers, while the Lancastrian army had even more. Later historians believe that these figures were exaggerated and that a combined figure of around 50,000 is more likely. Nevertheless, the armies gathered at Towton were among the largest at the time. An analysis of skeletons found in a mass grave in 1996 showed that the soldiers came from all walks of life; they were on average 30 years old and several were veterans of previous engagements. Many knights and noblemen, approximately three-quarters of the English peers fought in the battle. Eight of them were sworn to the Yorkist cause whereas the Lancastrians had at least 19.

The battle would decide which of the two kings would rule over England, but while Edward fought beside his men, Henry remained in York with Margaret. The Lancastrians regarded their king as a mere puppet of his wife and were wary of his mental instability. In comparison, Edward was inspirational to his followers. Eighteen years old, he was 6ft. 3½ inches tall and an imposing sight in armour. Skilled in combat, Edward led his men from the front, motivating them to do their best and uplifting their spirits. Edward's preference for bold offensive tactics determined the Yorkist plan of action for this engagement. Historian Nigel H. Jones nonetheless points out the superior number of Lancastrian peers over their Yorkist counterparts as "a testimony to the semi-religious power of an anointed medieval king, however incapable, to command the unquestioning fidelity of his subjects".
The Yorkists had other prominent leaders, Warwick having a flamboyant appeal to his followers. Edward Hall, a 16th-century chronicler, attributed to Warwick an inspirational scene before the Battle of Towton; Hall wrote that Warwick, wounded at Ferrybridge, slew his horse and cried, "Let him fly that will, for surely I will tarry with him that will tarry with me", daring any Yorkist to quit the fight ahead. The description is likely apocryphal; military historian Christopher Gravett said that the tale demonstrates Warwick's loyalty to Edward and his fellow men if it is true. Warwick placed great value on his uncle, Lord Fauconberg, whom Hall called a "man of great policy, and of much experience in martial feats". Small in stature and a veteran of the Anglo-French wars, Fauconberg was highly esteemed by his peers in matters of military affairs. He was quick to adapt to new situations; among his previous achievements were the administration of the French town of Calais, leading several piracy expeditions of import, and the command of the vanguard at the Battle of Northampton. Of those appointed to raise men for the battle, Norfolk likely never made it to the engagement due to his advanced age, and his knights—Walter Blount and Robert Horne—would have taken command of his contingent. In any event, Norfolk was an "unpredictable ally"; he had joined the Yorkists to establish a power base for himself in eastern England, and wavered in his support for their cause on various occasions.

Without their king on the battlefield, the Lancastrians relied on Henry Beaufort, Duke of Somerset, to command their army. He was fairly experienced in matters of war and is credited with clever manoeuvres that led the Lancastrians to victory at Wakefield and St Albans. According to several historians, however, Sir Andrew Trollope, and not Somerset, was the Lancastrians' primary strategist. Trollope formerly served under Warwick in Calais before defecting to the Lancastrians in the early stages of the Wars of the Roses. His change of allegiance was a major blow to the Yorkists, for he was familiar with their men and had played a key role in their victories in France. Other notable Lancastrian leaders included Henry Holland, Duke of Exeter, and the northern magnates Henry Percy, Earl of Northumberland, Lord de Ros and Ralph Dacre, who also accompanied the army. Another leading northern Lancastrian was Lord Clifford, who had died earlier in the retreat from Ferrybridge.

Very few historical sources give detailed accounts of the battle and they do not describe the exact deployments of the armies. The paucity of such primary sources led early historians to adopt Hall's chronicle as their main resource for the engagement, despite its authorship 70 years after the event and questions over the origin of his information. The Burgundian chronicler Jean de Waurin (c. 1398 – c. 1474) was a more contemporary source, but his chronicle was made available to the public only from 1891, and several mistakes in it discouraged historians at that time from using it. Later reconstructions of the battle were based on Hall's version, supplemented by minor details from other sources.

The battle took place on a plateau between the villages of Saxton (to the south) and Towton (to the north). The region was agricultural land, with plenty of wide open areas and small roads on which to manoeuvre the armies. Two roads ran through the area: the Old London Road, which connected Towton to the English capital, and a direct road between Saxton and Towton. The steeply banked Cock Beck flowed in an S-shaped course around the plateau from the north to west. The plateau was bisected by the Towton Dale, which ran from the west and extended into the North Acres in the east. Woodlands were scattered along the beck; Renshaw Woods lined the river on the north-western side of the plateau, and south of Towton Dale, Castle Hill Wood grew on the west side of the plateau at a bend in the beck. The area to the north-east of this forest would be known as Bloody Meadow after the battle.

According to Gravett and fellow military enthusiast Trevor James Halsall, Somerset's decision to engage the Yorkist army on this plateau was sound. Defending the ground just before Towton would block any enemy advance towards the city of York, whether they moved along the London–Towton road or an old Roman road to the west. The Lancastrians deployed on the north side of the dale, using the valley as a "protective ditch"; the disadvantage of this position was that they could not see beyond the southern ridge of the dale. The Lancastrian flanks were protected by marshes; their right was further secured by the steep banks of the Cock Beck. The width of their deployment area did not allow for a longer front line, depriving the Lancastrians of the opportunity to use their numerical superiority. Waurin's account gave rise to the suggestion that Somerset ordered a force of mounted spearmen to conceal itself in Castle Hill Wood, ready to charge into the Yorkist left flank at an opportune time in battle.

The Yorkists appeared as the Lancastrians finished deployment. Line after line of soldiers crested the southern ridge of the dale and formed up in ranks opposite their enemies as snow began to fall. Edward's army was outnumbered and Norfolk's troops had yet to arrive to join them.

The Lancastrian army was organised in three divisions. The Duke of Somerset, as the overall commander of the whole force, headed the main division in the center alongside the Duke of Exeter. The right-wing was commanded by the Earl of Northumberland, whereas the left division was led by the Earl of Devon and Lord Dacre.

As Somerset was content to stand and let his foes come to him, the opening move of the battle was made by the Yorkists. Noticing the direction and strength of the wind, Fauconberg ordered all Yorkist archers to step forward and unleash a volley of their arrows from what would be the standard maximum range of their longbows. With the wind behind them, the Yorkist missiles travelled farther than usual, plunging deep into the masses of soldiers on the hill slope. 

The response from the Lancastrian archers was ineffective as the heavy wind blew snow in their faces. They found it difficult to judge the range and pick out their targets and their arrows fell short of the Yorkist ranks; Fauconberg had ordered his men to retreat after loosing one volley, thus avoiding any casualties. Unable to observe their results, the Lancastrians loosed their arrows until most had been used, leaving a thick, prickly carpet in the ground in front of the Yorkists.

After the Lancastrians had ceased loosing their arrows, Fauconberg ordered his archers to step forward again to shoot. When they had exhausted their ammunition, the Yorkists plucked arrows off the ground in front of them—arrows loosed by their foes—and continued loosing. Coming under attack without any effective response of its own, the Lancastrian army moved from its position to engage the Yorkists in close combat. Seeing the advancing mass of men, the Yorkist archers shot a few more volleys before retreating behind their ranks of men-at-arms, leaving thousands of arrows in the ground to hinder the Lancastrian attack.

As the Yorkists reformed their ranks to receive the Lancastrian charge, their left flank came under attack by the horsemen from Castle Hill Wood mentioned by Waurin. The Yorkist left wing fell into disarray and several men started to flee. Edward had to take command of the left wing to save the situation. By engaging in the fight and encouraging his followers, his example inspired many to stand their ground. The armies clashed and archers shot into the mass of men at short range. The Lancastrians continuously threw fresher men into the fray and gradually the numerically inferior Yorkist army was forced to give ground and retreat up the southern ridge. Gravett thought that the Lancastrian left had less momentum than the rest of its formation, skewing the line of battle such that its western end tilted towards Saxton.

The fighting continued for three hours, according to research by English Heritage, a government body in charge of conservation of historic sites. It was indecisive until the arrival of Norfolk's men. Marching up the Old London Road, Norfolk's contingent was hidden from view until they crested the ridge and attacked the Lancastrian left flank. The Lancastrians continued to give fight but the advantage had shifted to the Yorkists. By the end of the day, the Lancastrian line had broken up, as small groups of men began fleeing for their lives. Polydore Vergil, chronicler for Henry VII of England, claimed that combat lasted for a total of 10 hours.

The tired Lancastrians flung off their helmets and armour to run faster. Without such protection, they were much more vulnerable to the attacks of the Yorkists. Norfolk's troops were much fresher and faster. Fleeing across what would later become known as Bloody Meadow, many Lancastrians were cut down from behind or were slain after they had surrendered. Before the battle, both sides had issued the order to give no quarter and the Yorkists were in no mood to spare anyone after the long, gruelling fight. A number of Lancastrians, such as Trollope, also had substantial bounties on their heads. Gregory's chronicle stated 42 knights were killed after they were taken prisoner.

Archaeological findings in the late 20th century shed light on the final moments of the battle. In 1996 workmen at a construction site in the town of Towton uncovered a mass grave, which archaeologists believed to contain the remains of men who were slain during or after the battle in 1461. The bodies showed severe injuries to their upper torsos; arms and skulls were cracked or shattered. One exhumed specimen, known as Towton 25, had the front of his skull bisected: a weapon had slashed across his face, cutting a deep wound that split the bone. The skull was also pierced by another deep wound, a horizontal cut from a blade across the back.

The Lancastrians lost more troops in their rout than from the battlefield. Men struggling across the river were dragged down by currents and drowned. Those floundering were stepped on and pushed under water by their comrades behind them as they rushed to get away from the Yorkists. As the Lancastrians struggled across the river, Yorkist archers rode to high vantage points and shot arrows at them. The dead began to pile up and the chronicles state that the Lancastrians eventually fled across these "bridges" of bodies. The chase continued northwards across the River Wharfe, which was larger than Cock Beck. A bridge over the river collapsed under the flood of men and many drowned trying to cross. Those who hid in Tadcaster and York were hunted down and killed.

A newsletter dated 4 April 1461 reported a widely circulated figure of 28,000 casualties in the battle, which Charles Ross and other historians believe was exaggerated. The number was taken from the heralds' estimate of the dead and appeared in letters from Edward and the Bishop of Salisbury, Richard Beauchamp. Other contemporary sources gave higher numbers, ranging from 30,000 to 38,000; Hall quoted an exact figure of 36,776. An exception was the "Annales rerum anglicarum", which stated the Lancastrians had 9,000 casualties—an estimate Ross found to be more believable. The Lancastrian nobility had heavy losses. Trollope and Northumberland fell in battle, and Lord Dacre was said to have been killed by an archer who was perched in a "bur tree" (a local term for an elder). Conversely, the Yorkists lost only one notable member of the gentry—Horne—at Towton.

On receiving news of their army's defeat, Henry fled into exile in Scotland with his wife and son. They were later joined by Somerset, Roos, Exeter, and the few Lancastrian nobles who escaped from the battlefield. The Battle of Towton severely reduced the power of the House of Lancaster in England; the linchpins of their power at court (Northumberland, Clifford, Roos, and Dacre) had either died or fled the country, ending the house's domination over the north of England. Edward further exploited the situation, naming 14 Lancastrian peers as traitors. Approximately 96 Lancastrians of the rank of knight and below were also attainted—24 of them members of parliament. 

The new king preferred winning over his enemies to his cause; the nobles he attainted either died in the battle or had refused to submit to him. The estates of a few of these nobles were confiscated by the crown but the rest were untouched, remaining in the care of their families. Edward also pardoned many of those he attainted after they submitted to his rule.

Although Henry was at large in Scotland with his son, the battle put an end (for the time being) to disputes over the country's state of leadership since the Act of Accord. The English people were assured that there was now one true king—Edward. He turned his attention to consolidating his rule over the country, winning over the people and putting down the rebellions raised by the few remaining Lancastrian diehards. He knighted several of his supporters and elevated several of his gentry supporters to the peerage; Fauconberg was made the Earl of Kent. Warwick benefited from Edward's rule after the battle. He received parts of Northumberland's and Clifford's holdings, and was made "the king's lieutenant in the North and admiral of England." Sir David ap Mathew (Sir David Mathew) of Llandaff, Wales (1400–1484), a loyal Yorkist and Seneschal, was named Standard Bearer of England, and is credited with saving the life of Edward York, Edward IV of England in the battle; as a result, he was granted the right to use 'Towton' on the Mathew family arms.

By 1464, the Yorkists had "wiped out all effective Lancastrian resistance in the north of England." Edward's reign was not interrupted until 1470; by then, his relationship with Warwick had deteriorated to such an extent that the earl defected to the Lancastrians and forced Edward to flee England, restoring Henry to the throne. The interruption of Yorkist rule was brief, as Edward regained his throne after defeating Warwick and his Lancastrian cohorts at the Battle of Barnet in 1471.

In the sixteenth century William Shakespeare wrote a number of dramatisations of historic figures. The use of history as a backdrop, against which the familiar characters act out Shakespeare's drama, lends a sense of realism to his plays. Shakespeare wrote a three-part play about Henry VI, relying heavily on Hall's chronicle as a source. His vision of the Battle of Towton ("Henry VI", Part 3, Act 2, Scene 5), touted as the "bloodiest" engagement in the Wars of the Roses, became a set piece about the "terror of civil war, a "national" terror that is essentially "familial"". Historian Bertram Wolffe said it was thanks to Shakespeare's dramatisation of the battle that the weak and ineffectual Henry was at least remembered by English society, albeit for his pining to have been born a shepherd rather than a king.

Shakespeare's version of the battle presents a notable scene that comes immediately after Henry's soliloquy. Henry witnesses the laments of two soldiers in the battle. One slays his opponent in hope of plunder, only to find the victim is his son; the other kills his enemy, who turns out to be his father. Both killers have acted out of greed and fell into a state of deep grieving after discovering their misdeeds. Shakespearian scholar Arthur Percival Rossiter names the scene as the most notable of the playwright's written "rituals". The delivery of the event follows the pattern of an opera: after a long speech, the actors alternate among one another to deliver single-line asides to the audience. In this scene of grief — in a reversal of the approach adopted in his later historical plays — Shakespeare uses anonymous fictional characters to illustrate the ills of civil war while a historical king reflects on their fates. Emeritus Professor of English Literature Michael Hattaway comments that Shakespeare intended to show Henry's sadness over the war, to elicit the same emotion among the audience and to expose Henry's ineptness as king.

The Battle of Towton was re-examined by Geoffrey Hill in his poem "Funeral Music" (1968). Hill presents the historical event through the voices of its combatants, looking at the turmoil of the era through their eyes. The common soldiers grouse about their physical discomforts and the sacrifices that they had made for the ideas glorified by their leaders. They share their superiors' determination to seek the destruction of their opponents, even at the cost of their lives. Hill depicts the participants' belief that the event was pre-destined and of utmost importance as a farce; the world went about its business regardless of the Battle of Towton.

In 1483 Richard III, younger brother of Edward IV, started to build a chapel to commemorate the battle. Richard died at the Battle of Bosworth in 1485 and the building was never completed. It eventually fell into disrepair and collapsed. The ruins of the structure were evident five centuries later. In 1929, a stone cross supposedly from the chapel was used to create the Towton Cross (also known as Lord Dacre's Cross) to commemorate those who died in the battle. Several mounds on the battlefield were thought to contain casualties of the battle, although historians believe these to be tumuli of much earlier origin. More burial sites related to the battle are found on Chapel Hill and around Saxton. Lord Dacre was buried at the Church of All Saints in Saxton and his tomb was reported in the late 19th century to be well maintained, although several of its panels had been weathered away. The bur tree from which Dacre's killer shot his arrow was cut down by the late 19th century, leaving its stump on the battlefield. Centuries after the battle, relics that have been found in the area include rings, arrowheads and coins.

The people of Elizabethan-era England remembered the battle as dramatised by Shakespeare, and the image of the engagement as the charnel house where many sons of England were cut down endured for centuries. However, at the start of the 21st century, the "largest and bloodiest battle ever fought on English soil" was no longer prominent in the public consciousness. British journalists lamented that people were ignorant of the Battle of Towton and of its significance. According to English Heritage, the battle was of the "greatest importance"; it was one of the largest, if not "the" largest, fought in England and it resulted in the replacement of one royal dynasty by another. Hill expressed a different opinion. Although impressed with the casualty figures touted by the chroniclers, he believed the battle brought no monumental changes to the lives of the English people.

The Battle of Towton was associated with a tradition previously upheld in the village of Tysoe, Warwickshire. For several centuries a local farmer had scoured a hill figure, the Red Horse of Tysoe, each year, as part of the terms of his land tenancy. While the origins of the tradition have never been conclusively identified, it was locally claimed this was done to commemorate the Earl of Warwick's inspirational deed of slaying his horse to show his resolve to stand and fight with the common soldiers. The tradition died in 1798 when the Inclosure Acts implemented by the English government redesignated the common land, on which the equine figure was located, as private property. The scouring was revived during the early 20th century but has since stopped.





</doc>
<doc id="50397" url="https://en.wikipedia.org/wiki?curid=50397" title="Cerebellum">
Cerebellum

The cerebellum (Latin for "little brain") is a major feature of the hindbrain of all vertebrates. Although usually smaller than the cerebrum, in some animals such as the mormyrid fishes it may be as large as or even larger. In humans, the cerebellum plays an important role in motor control. It may also be involved in some cognitive functions such as attention and language as well as in regulating fear and pleasure responses, but its movement-related functions are the most solidly established. The human cerebellum does not initiate movement, but contributes to coordination, precision, and accurate timing: it receives input from sensory systems of the spinal cord and from other parts of the brain, and integrates these inputs to fine-tune motor activity. Cerebellar damage produces disorders in fine movement, equilibrium, posture, and motor learning in humans.

Anatomically, the human cerebellum has the appearance of a separate structure attached to the bottom of the brain, tucked underneath the cerebral hemispheres. Its cortical surface is covered with finely spaced parallel grooves, in striking contrast to the broad irregular convolutions of the cerebral cortex. These parallel grooves conceal the fact that the cerebellar cortex is actually a continuous thin layer of tissue tightly folded in the style of an accordion. Within this thin layer are several types of neurons with a highly regular arrangement, the most important being Purkinje cells and granule cells. This complex neural organization gives rise to a massive signal-processing capability, but almost all of the output from the cerebellar cortex passes through a set of small deep nuclei lying in the white matter interior of the cerebellum.

In addition to its direct role in motor control, the cerebellum is necessary for several types of motor learning, most notably learning to adjust to changes in sensorimotor relationships. Several theoretical models have been developed to explain sensorimotor calibration in terms of synaptic plasticity within the cerebellum. These models derive from those formulated by David Marr and James Albus, based on the observation that each cerebellar Purkinje cell receives two dramatically different types of input: one comprises thousands of weak inputs from the parallel fibers of the granule cells; the other is an extremely strong input from a single climbing fiber. The basic concept of the Marr–Albus theory is that the climbing fiber serves as a "teaching signal", which induces a long-lasting change in the strength of parallel fiber inputs. Observations of long-term depression in parallel fiber inputs have provided support for theories of this type, but their validity remains controversial.

At the level of gross anatomy, the cerebellum consists of a tightly folded layer of cortex, with white matter underneath and a fluid-filled ventricle at the base. Four deep cerebellar nuclei are embedded in the white matter. Each part of the cortex consists of the same small set of neuronal elements, laid out in a highly stereotyped geometry. At an intermediate level, the cerebellum and its auxiliary structures can be separated into several hundred or thousand independently functioning modules called "microzones" or "microcompartments".

The cerebellum is located in the posterior cranial fossa. The fourth ventricle, pons and medulla are in front of the cerebellum. It is separated from the overlying cerebrum by a layer of leathery dura mater, the tentorium cerebelli; all of its connections with other parts of the brain travel through the pons. Anatomists classify the cerebellum as part of the metencephalon, which also includes the pons; the metencephalon is the upper part of the rhombencephalon or "hindbrain". Like the cerebral cortex, the cerebellum is divided into two hemispheres; it also contains a narrow midline zone (the vermis). A set of large folds is, by convention, used to divide the overall structure into 10 smaller "lobules". Because of its large number of tiny granule cells, the cerebellum contains more neurons than the total from the rest of the brain, but takes up only 10% of the total brain volume. The number of neurons in the cerebellum is related to the number of neurons in the neocortex. There are about 3.6 times as many neurons in the cerebellum as in the neocortex, a ratio that is conserved across many different mammalian species.

The unusual surface appearance of the cerebellum conceals the fact that most of its volume is made up of a very tightly folded layer of gray matter: the cerebellar cortex. Each ridge or gyrus in this layer is called a folium. It is estimated that, if the human cerebellar cortex were completely unfolded, it would give rise to a layer of neural tissue about 1 meter long and averaging 5 centimeters wide—a total surface area of about 500 square cm, packed within a volume of dimensions 6 cm × 5 cm × 10 cm. Underneath the gray matter of the cortex lies white matter, made up largely of myelinated nerve fibers running to and from the cortex. Embedded within the white matter—which is sometimes called the "arbor vitae" (tree of life) because of its branched, tree-like appearance in cross-section—are four deep cerebellar nuclei, composed of gray matter.

Connecting the cerebellum to different parts of the nervous system are three paired cerebellar peduncles. These are the superior cerebellar peduncle, the middle cerebellar peduncle and the inferior cerebellar peduncle, named by their position relative to the vermis. The superior cerebellar peduncle is mainly an output to the cerebral cortex, carrying efferent fibers via thalamic nuclei to upper motor neurons in the cerebral cortex. The fibers arise from the deep cerebellar nuclei. The middle cerebellar peduncle is connected to the pons and receives all of its input from the pons mainly from the pontine nuclei. The input to the pons is from the cerebral cortex and is relayed from the pontine nuclei via transverse pontine fibers to the cerebellum. The middle peduncle is the largest of the three and its afferent fibers are grouped into three separate fascicles taking their inputs to different parts of the cerebellum. The inferior cerebellar peduncle receives input from afferent fibers from the vestibular nuclei, spinal cord and the tegmentum. Output from the inferior peduncle is via efferent fibers to the vestibular nuclei and the reticular formation. The whole of the cerebellum receives modulatory input from the inferior olivary nucleus via the inferior cerebellar peduncle.

Based on the surface appearance, three lobes can be distinguished within the cerebellum: the anterior lobe (above the primary fissure), the posterior lobe (below the primary fissure), and the flocculonodular lobe (below the posterior fissure). These lobes divide the cerebellum from rostral to caudal (in humans, top to bottom). In terms of function, however, there is a more important distinction along the medial-to-lateral dimension. Leaving out the flocculonodular lobe, which has distinct connections and functions, the cerebellum can be parsed functionally into a medial sector called the spinocerebellum and a larger lateral sector called the cerebrocerebellum. A narrow strip of protruding tissue along the midline is called the cerebellar vermis. ("Vermis" is Latin for "worm".)

The smallest region, the flocculonodular lobe, is often called the vestibulocerebellum. It is the oldest part in evolutionary terms (archicerebellum) and participates mainly in balance and spatial orientation; its primary connections are with the vestibular nuclei, although it also receives visual and other sensory input. Damage to this region causes disturbances of balance and gait.

The medial zone of the anterior and posterior lobes constitutes the spinocerebellum, also known as paleocerebellum. This sector of the cerebellum functions mainly to fine-tune body and limb movements. It receives proprioceptive input from the dorsal columns of the spinal cord (including the spinocerebellar tract) and from the cranial trigeminal nerve, as well as from visual and auditory systems. It sends fibers to deep cerebellar nuclei that, in turn, project to both the cerebral cortex and the brain stem, thus providing modulation of descending motor systems.

The lateral zone, which in humans is by far the largest part, constitutes the cerebrocerebellum, also known as neocerebellum. It receives input exclusively from the cerebral cortex (especially the parietal lobe) via the pontine nuclei (forming cortico-ponto-cerebellar pathways), and sends output mainly to the ventrolateral thalamus (in turn connected to motor areas of the premotor cortex and primary motor area of the cerebral cortex) and to the red nucleus. There is disagreement about the best way to describe the functions of the lateral cerebellum: It is thought to be involved in planning movement that is about to occur, in evaluating sensory information for action, and in a number of purely cognitive functions, such as determining the verb which best fits with a certain noun (as in "sit" for "chair").

Two types of neuron play dominant roles in the cerebellar circuit: Purkinje cells and granule cells. Three types of axons also play dominant roles: mossy fibers and climbing fibers (which enter the cerebellum from outside), and parallel fibers (which are the axons of granule cells). There are two main pathways through the cerebellar circuit, originating from mossy fibers and climbing fibers, both eventually terminating in the deep cerebellar nuclei.

Mossy fibers project directly to the deep nuclei, but also give rise to the following pathway: mossy fibers → granule cells → parallel fibers → Purkinje cells → deep nuclei. Climbing fibers project to Purkinje cells and also send collaterals directly to the deep nuclei. The mossy fiber and climbing fiber inputs each carry fiber-specific information; the cerebellum also receives dopaminergic, serotonergic, noradrenergic, and cholinergic inputs that presumably perform global modulation.

The cerebellar cortex is divided into three layers. At the bottom lies the thick granular layer, densely packed with granule cells, along with interneurons, mainly Golgi cells but also including Lugaro cells and unipolar brush cells. In the middle lies the Purkinje layer, a narrow zone that contains the cell bodies of Purkinje cells and Bergmann glial cells. At the top lies the molecular layer, which contains the flattened dendritic trees of Purkinje cells, along with the huge array of parallel fibers penetrating the Purkinje cell dendritic trees at right angles. This outermost layer of the cerebellar cortex also contains two types of inhibitory interneuron: stellate cells and basket cells. Both stellate and basket cells form GABAergic synapses onto Purkinje cell dendrites.

Purkinje cells are among the most distinctive neurons in the brain, and one of the earliest types to be recognized—they were first described by the Czech anatomist Jan Evangelista Purkyně in 1837. They are distinguished by the shape of their dendritic tree: The dendrites branch very profusely, but are severely flattened in a plane perpendicular to the cerebellar folds. Thus, the dendrites of a Purkinje cell form a dense planar net, through which parallel fibers pass at right angles. The dendrites are covered with dendritic spines, each of which receives synaptic input from a parallel fiber. Purkinje cells receive more synaptic inputs than any other type of cell in the brain—estimates of the number of spines on a single human Purkinje cell run as high as 200,000. The large, spherical cell bodies of Purkinje cells are packed into a narrow layer (one cell thick) of the cerebellar cortex, called the "Purkinje layer". After emitting collaterals that affect nearby parts of the cortex, their axons travel into the deep cerebellar nuclei, where they make on the order of 1,000 contacts each with several types of nuclear cells, all within a small domain. Purkinje cells use GABA as their neurotransmitter, and therefore exert inhibitory effects on their targets.

Purkinje cells form the heart of the cerebellar circuit, and their large size and distinctive activity patterns have made it relatively easy to study their response patterns in behaving animals using extracellular recording techniques. Purkinje cells normally emit action potentials at a high rate even in the absence of the synaptic input. In awake, behaving animals, mean rates averaging around 40 Hz are typical. The spike trains show a mixture of what are called simple and complex spikes. A simple spike is a single action potential followed by a refractory period of about 10 ms; a complex spike is a stereotyped sequence of action potentials with very short inter-spike intervals and declining amplitudes. Physiological studies have shown that complex spikes (which occur at baseline rates around 1 Hz and never at rates much higher than 10 Hz) are reliably associated with climbing fiber activation, while simple spikes are produced by a combination of baseline activity and parallel fiber input. Complex spikes are often followed by a pause of several hundred milliseconds during which simple spike activity is suppressed.

A specific, recognizable feature of Purkinje neurons is the expression of calbindin. Calbindin staining of rat brain after unilateral chronic sciatic nerve injury suggests that Purkinje neurons may be newly generated in the adult brain, initiating the organization of new cerebellar lobules.

Cerebellar granule cells, in contrast to Purkinje cells, are among the smallest neurons in the brain. They are also easily the most numerous neurons in the brain: In humans, estimates of their total number average around 50 billion, which means that about 3/4 of the brain's neurons are cerebellar granule cells. Their cell bodies are packed into a thick layer at the bottom of the cerebellar cortex. A granule cell emits only four to five dendrites, each of which ends in an enlargement called a "dendritic claw". These enlargements are sites of excitatory input from mossy fibers and inhibitory input from Golgi cells.

The thin, unmyelinated axons of granule cells rise vertically to the upper (molecular) layer of the cortex, where they split in two, with each branch traveling horizontally to form a parallel fiber; the splitting of the vertical branch into two horizontal branches gives rise to a distinctive "T" shape. A human parallel fiber runs for an average of 3 mm in each direction from the split, for a total length of about 6 mm (about 1/10 of the total width of the cortical layer). As they run along, the parallel fibers pass through the dendritic trees of Purkinje cells, contacting one of every 3–5 that they pass, making a total of 80–100 synaptic connections with Purkinje cell dendritic spines. Granule cells use glutamate as their neurotransmitter, and therefore exert excitatory effects on their targets.

Granule cells receive all of their input from mossy fibers, but outnumber them by 200 to 1 (in humans). Thus, the information in the granule cell population activity state is the same as the information in the mossy fibers, but recoded in a much more expansive way. Because granule cells are so small and so densely packed, it is difficult to record their spike activity in behaving animals, so there is little data to use as a basis for theorizing. The most popular concept of their function was proposed in 1969 by David Marr, who suggested that they could encode combinations of mossy fiber inputs. The idea is that with each granule cell receiving input from only 4–5 mossy fibers, a granule cell would not respond if only a single one of its inputs were active, but would respond if more than one were active. This combinatorial coding scheme would potentially allow the cerebellum to make much finer distinctions between input patterns than the mossy fibers alone would permit.

Mossy fibers enter the granular layer from their points of origin, many arising from the pontine nuclei, others from the spinal cord, vestibular nuclei etc. In the human cerebellum, the total number of mossy fibers has been estimated at about 200 million. These fibers form excitatory synapses with the granule cells and the cells of the deep cerebellar nuclei. Within the granular layer, a mossy fiber generates a series of enlargements called "rosettes". The contacts between mossy fibers and granule cell dendrites take place within structures called glomeruli. Each glomerulus has a mossy fiber rosette at its center, and up to 20 granule cell dendritic claws contacting it. Terminals from Golgi cells infiltrate the structure and make inhibitory synapses onto the granule cell dendrites. The entire assemblage is surrounded by a sheath of glial cells. Each mossy fiber sends collateral branches to several cerebellar folia, generating a total of 20–30 rosettes; thus a single mossy fiber makes contact with an estimated 400–600 granule cells.

Purkinje cells also receive input from the inferior olivary nucleus on the contralateral side of the brainstem via climbing fibers. Although the inferior olive lies in the medulla oblongata and receives input from the spinal cord, brainstem and cerebral cortex, its output goes entirely to the cerebellum. A climbing fiber gives off collaterals to the deep cerebellar nuclei before entering the cerebellar cortex, where it splits into about 10 terminal branches, each of which gives input to a single Purkinje cell. In striking contrast to the 100,000-plus inputs from parallel fibers, each Purkinje cell receives input from exactly one climbing fiber; but this single fiber "climbs" the dendrites of the Purkinje cell, winding around them and making a total of up to 300 synapses as it goes. The net input is so strong that a single action potential from a climbing fiber is capable of producing an extended complex spike in the Purkinje cell: a burst of several spikes in a row, with diminishing amplitude, followed by a pause during which activity is suppressed. The climbing fiber synapses cover the cell body and proximal dendrites; this zone is devoid of parallel fiber inputs.

Climbing fibers fire at low rates, but a single climbing fiber action potential induces a burst of several action potentials in a target Purkinje cell (a complex spike). The contrast between parallel fiber and climbing fiber inputs to Purkinje cells (over 100,000 of one type versus exactly one of the other type) is perhaps the most provocative feature of cerebellar anatomy, and has motivated much of the theorizing. In fact, the function of climbing fibers is the most controversial topic concerning the cerebellum. There are two schools of thought, one following Marr and Albus in holding that climbing fiber input serves primarily as a teaching signal, the other holding that its function is to shape cerebellar output directly. Both views have been defended in great length in numerous publications. In the words of one review, "In trying to synthesize the various hypotheses on the function of the climbing fibers, one has the sense of looking at a drawing by Escher. Each point of view seems to account for a certain collection of findings, but when one attempts to put the different views together, a coherent picture of what the climbing fibers are doing does not appear. For the majority of researchers, the climbing fibers signal errors in motor performance, either in the usual manner of discharge frequency modulation or as a single announcement of an 'unexpected event'. For other investigators, the message lies in the degree of ensemble synchrony and rhythmicity among a population of climbing fibers."

The deep nuclei of the cerebellum are clusters of gray matter lying within the white matter at the core of the cerebellum. They are, with the minor exception of the nearby vestibular nuclei, the sole sources of output from the cerebellum. These nuclei receive collateral projections from mossy fibers and climbing fibers as well as inhibitory input from the Purkinje cells of the cerebellar cortex. The four nuclei (dentate, globose, emboliform, and fastigial) each communicate with different parts of the brain and cerebellar cortex. (The globose and the emboliform nuclei are also referred to as combined in the interposed nucleus). The fastigial and interposed nuclei belong to the spinocerebellum. The dentate nucleus, which in mammals is much larger than the others, is formed as a thin, convoluted layer of gray matter, and communicates exclusively with the lateral parts of the cerebellar cortex. The flocculonodular lobe is the only part of the cerebellar cortex that does not project to the deep nuclei—its output goes to the vestibular nuclei instead.

The majority of neurons in the deep nuclei have large cell bodies and spherical dendritic trees with a radius of about 400 μm, and use glutamate as their neurotransmitter. These cells project to a variety of targets outside the cerebellum. Intermixed with them are a lesser number of small cells, which use GABA as a neurotransmitter and project exclusively to the inferior olivary nucleus, the source of climbing fibers. Thus, the nucleo-olivary projection provides an inhibitory feedback to match the excitatory projection of climbing fibers to the nuclei. There is evidence that each small cluster of nuclear cells projects to the same cluster of olivary cells that send climbing fibers to it; there is strong and matching topography in both directions.

When a Purkinje cell axon enters one of the deep nuclei, it branches to make contact with both large and small nuclear cells, but the total number of cells contacted is only about 35 (in cats). Conversely, a single deep nuclear cell receives input from approximately 860 Purkinje cells (again in cats).

From the viewpoint of gross anatomy, the cerebellar cortex appears to be a homogeneous sheet of tissue, and, from the viewpoint of microanatomy, all parts of this sheet appear to have the same internal structure. There are, however, a number of respects in which the structure of the cerebellum is compartmentalized. There are large compartments that are generally known as "zones"; these can be divided into smaller compartments known as "microzones".

The first indications of compartmental structure came from studies of the receptive fields of cells in various parts of the cerebellar cortex. Each body part maps to specific points in the cerebellum, but there are numerous repetitions of the basic map, forming an arrangement that has been called "fractured somatotopy". A clearer indication of compartmentalization is obtained by immunostaining the cerebellum for certain types of protein. The best-known of these markers are called "zebrins", because staining for them gives rise to a complex pattern reminiscent of the stripes on a zebra. The stripes generated by zebrins and other compartmentalization markers are oriented perpendicular to the cerebellar folds—that is, they are narrow in the mediolateral direction, but much more extended in the longitudinal direction. Different markers generate different sets of stripes, the widths and lengths vary as a function of location, but they all have the same general shape.

Oscarsson in the late 1970s proposed that these cortical zones can be partitioned into smaller units called microzones. A microzone is defined as a group of Purkinje cells all having the same somatotopic receptive field. Microzones were found to contain on the order of 1000 Purkinje cells each, arranged in a long, narrow strip, oriented perpendicular to the cortical folds. Thus, as the adjoining diagram illustrates, Purkinje cell dendrites are flattened in the same direction as the microzones extend, while parallel fibers cross them at right angles.

It is not only receptive fields that define the microzone structure: The climbing fiber input from the inferior olivary nucleus is equally important. The branches of a climbing fiber (usually numbering about 10) usually activate Purkinje cells belonging to the same microzone. Moreover, olivary neurons that send climbing fibers to the same microzone tend to be coupled by gap junctions, which synchronize their activity, causing Purkinje cells within a microzone to show correlated complex spike activity on a millisecond time scale. Also, the Purkinje cells belonging to a microzone all send their axons to the same small cluster of output cells within the deep cerebellar nuclei. Finally, the axons of basket cells are much longer in the longitudinal direction than in the mediolateral direction, causing them to be confined largely to a single microzone. The consequence of all this structure is that cellular interactions within a microzone are much stronger than interactions between different microzones.

In 2005, Richard Apps and Martin Garwicz summarized evidence that microzones themselves form part of a larger entity they call a multizonal microcomplex. Such a microcomplex includes several spatially separated cortical microzones, all of which project to the same group of deep cerebellar neurons, plus a group of coupled olivary neurons that project to all of the included microzones as well as to the deep nuclear area.

The strongest clues to the function of the cerebellum have come from examining the consequences of damage to it. Animals and humans with cerebellar dysfunction show, above all, problems with motor control, on the same side of the body as the damaged part of the cerebellum. They continue to be able to generate motor activity but lose precision, producing erratic, uncoordinated, or incorrectly timed movements. A standard test of cerebellar function is to reach with the tip of the finger for a target at arm's length: A healthy person will move the fingertip in a rapid straight trajectory, whereas a person with cerebellar damage will reach slowly and erratically, with many mid-course corrections. Deficits in non-motor functions are more difficult to detect. Thus, the general conclusion reached decades ago is that the basic function of the cerebellum is to calibrate the detailed form of a movement, not to initiate movements or to decide which movements to execute.

Prior to the 1990s the function of the cerebellum was almost universally believed to be purely motor-related, but newer findings have brought that view into question. Functional imaging studies have shown cerebellar activation in relation to language, attention, and mental imagery; correlation studies have shown interactions between the cerebellum and non-motor areas of the cerebral cortex; and a variety of non-motor symptoms have been recognized in people with damage that appears to be confined to the cerebellum. In particular, the cerebellar cognitive affective syndrome or Schmahmann's syndrome has been described in adults and children. Estimates based on functional mapping of the cerebellum using functional MRI suggest that more than half of the cerebellar cortex is interconnected with association zones of the cerebral cortex.

Kenji Doya has argued that the cerebellum's function is best understood not in terms of the behaviors it affects, but the neural computations it performs; the cerebellum consists of a large number of more or less independent modules, all with the same geometrically regular internal structure, and therefore all, it is presumed, performing the same computation. If the input and output connections of a module are with motor areas (as many are), then the module will be involved in motor behavior; but, if the connections are with areas involved in non-motor cognition, the module will show other types of behavioral correlates. Thus the cerebellum has been implicated in the regulation of many differing functional traits such as affection, emotion and behavior. The cerebellum, Doya proposes, is best understood as predictive action selection based on "internal models" of the environment or a device for supervised learning, in contrast to the basal ganglia, which perform reinforcement learning, and the cerebral cortex, which performs unsupervised learning.

The comparative simplicity and regularity of the cerebellar anatomy led to an early hope that it might imply a similar simplicity of computational function, as expressed in one of the first books on cerebellar electrophysiology, "The Cerebellum as a Neuronal Machine" by John C. Eccles, Masao Ito, and János Szentágothai. Although a full understanding of cerebellar function has remained elusive, at least four principles have been identified as important: (1) feedforward processing, (2) divergence and convergence, (3) modularity, and (4) plasticity.


There is considerable evidence that the cerebellum plays an essential role in some types of motor learning. The tasks where the cerebellum most clearly comes into play are those in which it is necessary to make fine adjustments to the way an action is performed. There has, however, been much dispute about whether learning takes place within the cerebellum itself, or whether it merely serves to provide signals that promote learning in other brain structures. Most theories that assign learning to the circuitry of the cerebellum are derived from the ideas of David Marr and James Albus, who postulated that climbing fibers provide a teaching signal that induces synaptic modification in parallel fiber–Purkinje cell synapses. Marr assumed that climbing fiber input would cause synchronously activated parallel fiber inputs to be strengthened. Most subsequent cerebellar-learning models, however, have followed Albus in assuming that climbing fiber activity would be an error signal, and would cause synchronously activated parallel fiber inputs to be weakened. Some of these later models, such as the "Adaptive Filter" model of Fujita made attempts to understand cerebellar function in terms of optimal control theory.

The idea that climbing fiber activity functions as an error signal has been examined in many experimental studies, with some supporting it but others casting doubt. In a pioneering study by Gilbert and Thach from 1977, Purkinje cells from monkeys learning a reaching task showed increased complex spike activity—which is known to reliably indicate activity of the cell's climbing fiber input—during periods when performance was poor. Several studies of motor learning in cats observed complex spike activity when there was a mismatch between an intended movement and the movement that was actually executed. Studies of the vestibulo–ocular reflex (which stabilizes the visual image on the retina when the head turns) found that climbing fiber activity indicated "retinal slip", although not in a very straightforward way.

One of the most extensively studied cerebellar learning tasks is the eyeblink conditioning paradigm, in which a neutral conditioned stimulus (CS) such as a tone or a light is repeatedly paired with an unconditioned stimulus (US), such as an air puff, that elicits a blink response. After such repeated presentations of the CS and US, the CS will eventually elicit a blink before the US, a conditioned response or CR. Experiments showed that lesions localized either to a specific part of the interposed nucleus (one of the deep cerebellar nuclei) or to a few specific points in the cerebellar cortex would abolish learning of a conditionally timed blink response. If cerebellar outputs are pharmacologically inactivated while leaving the inputs and intracellular circuits intact, learning takes place even while the animal fails to show any response, whereas, if intracerebellar circuits are disrupted, no learning takes place—these facts taken together make a strong case that the learning, indeed, occurs inside the cerebellum.

The large base of knowledge about the anatomical structure and behavioral functions of the cerebellum have made it a fertile ground for theorizing—there are perhaps more theories of the function of the cerebellum than of any other part of the brain. The most basic distinction among them is between "learning theories" and "performance theories"—that is, theories that make use of synaptic plasticity within the cerebellum to account for its role in learning, versus theories that account for aspects of ongoing behavior on the basis of cerebellar signal processing. Several theories of both types have been formulated as mathematical models and simulated using computers.

Perhaps the earliest "performance" theory was the "delay line" hypothesis of Valentino Braitenberg. The original theory put forth by Braitenberg and Roger Atwood in 1958 proposed that slow propagation of signals along parallel fibers imposes predictable delays that allow the cerebellum to detect time relationships within a certain window. Experimental data did not support the original form of the theory, but Braitenberg continued to argue for modified versions. The hypothesis that the cerebellum functions essentially as a timing system has also been advocated by Richard Ivry. Another influential "performance" theory is the Tensor network theory of Pellionisz and Llinás, which provided an advanced mathematical formulation of the idea that the fundamental computation performed by the cerebellum is to transform sensory into motor coordinates.

Theories in the "learning" category almost all derive from publications by Marr and Albus. Marr's 1969 paper proposed that the cerebellum is a device for learning to associate elemental movements encoded by climbing fibers with mossy fiber inputs that encode the sensory context. Albus proposed in 1971 that a cerebellar Purkinje cell functions as a perceptron, a neurally inspired abstract learning device. The most basic difference between the Marr and Albus theories is that Marr assumed that climbing fiber activity would cause parallel fiber synapses to be strengthened, whereas Albus proposed that they would be weakened. Albus also formulated his version as a software algorithm he called a CMAC (Cerebellar Model Articulation Controller), which has been tested in a number of applications.

The cerebellum is provided with blood from three paired major arteries: the superior cerebellar artery (SCA), the anterior inferior cerebellar artery (AICA), and the posterior inferior cerebellar artery (PICA). The SCA supplies the upper region of the cerebellum. It divides at the upper surface and branches into the pia mater where the branches anastomose with those of the anterior and posterior inferior cerebellar arteries. The AICA supplies the front part of the undersurface of the cerebellum. The PICA arrives at the undersurface, where it divides into a medial branch and a lateral branch. The medial branch continues backward to the cerebellar notch between the two hemispheres of the cerebellum; while the lateral branch supplies the under surface of the cerebellum, as far as its lateral border, where it anastomoses with the AICA and the SCA.

Damage to the cerebellum often causes motor-related symptoms, the details of which depend on the part of the cerebellum involved and how it is damaged. Damage to the flocculonodular lobe may show up as a loss of equilibrium and in particular an altered, irregular walking gait, with a wide stance caused by difficulty in balancing. Damage to the lateral zone typically causes problems in skilled voluntary and planned movements which can cause errors in the force, direction, speed and amplitude of movements. Other manifestations include hypotonia (decreased muscle tone), dysarthria (problems with speech articulation), dysmetria (problems judging distances or ranges of movement), dysdiadochokinesia (inability to perform rapid alternating movements such as walking), impaired check reflex or rebound phenomenon, and intention tremor (involuntary movement caused by alternating contractions of opposing muscle groups). Damage to the midline portion may disrupt whole-body movements, whereas damage localized more laterally is more likely to disrupt fine movements of the hands or limbs. Damage to the upper part of the cerebellum tends to cause gait impairments and other problems with leg coordination; damage to the lower part is more likely to cause uncoordinated or poorly aimed movements of the arms and hands, as well as difficulties in speed. This complex of motor symptoms is called "ataxia".

To identify cerebellar problems, neurological examination includes assessment of gait (a broad-based gait being indicative of ataxia), finger-pointing tests and assessment of posture. If cerebellar dysfunction is indicated, a magnetic resonance imaging scan can be used to obtain a detailed picture of any structural alterations that may exist.

The list of medical problems that can produce cerebellar damage is long, including stroke, hemorrhage, swelling of the brain (cerebral edema), tumors, alcoholism, physical trauma such as gunshot wounds or explosives, and chronic degenerative conditions such as olivopontocerebellar atrophy. Some forms of migraine headache may also produce temporary dysfunction of the cerebellum, of variable severity. Infection can result in cerebellar damage in such conditions as the prion diseases and Miller Fisher syndrome, a variant of Guillain–Barré syndrome.

The human cerebellum changes with age. These changes may differ from those of other parts of the brain.
The cerebellum is the youngest brain region (and body part) in centenarians according to an epigenetic biomarker of tissue age known as epigenetic clock: it is about 15 years younger than expected in a centenarian. Further, gene expression patterns in the human cerebellum show less age-related alteration than that in the cerebral cortex.
Some studies have reported reductions in numbers of cells or volume of tissue, but the amount of data relating to this question is not very large.

Congenital malformation, hereditary disorders, and acquired conditions can affect cerebellar structure and, consequently, cerebellar function. Unless the causative condition is reversible, the only possible treatment is to help people live with their problems. Visualization of the fetal cerebellum by ultrasound scan at 18 to 20 weeks of pregnancy can be used to screen for fetal neural tube defects with a sensitivity rate of up to 99%.

In normal development, endogenous sonic hedgehog signaling stimulates rapid proliferation of cerebellar granule neuron progenitors (CGNPs) in the external granule layer (EGL). Cerebellar development occurs during late embryogenesis and the early postnatal period, with CGNP proliferation in the EGL peaking during early development (postnatal day 7 in the mouse). As CGNPs terminally differentiate into cerebellum granule cells (also called cerebellar granule neurons, CGNs), they migrate to the internal granule layer (IGL), forming the mature cerebellum (by post-natal day 20 in the mouse). Mutations that abnormally activate Sonic hedgehog signaling predispose to cancer of the cerebellum (medulloblastoma) in humans with Gorlin Syndrome and in genetically engineered mouse models.

Congenital malformation or underdevelopment (hypoplasia) of the cerebellar vermis is a characteristic of both Dandy–Walker syndrome and Joubert syndrome. In very rare cases, the entire cerebellum may be absent. The inherited neurological disorders Machado–Joseph disease, ataxia telangiectasia, and Friedreich's ataxia cause progressive neurodegeneration linked to cerebellar loss. Congenital brain malformations outside the cerebellum can, in turn, cause herniation of cerebellar tissue, as seen in some forms of Arnold–Chiari malformation.

Other conditions that are closely linked to cerebellar degeneration include the idiopathic progressive neurological disorders multiple system atrophy and Ramsay Hunt syndrome type I, and the autoimmune disorder paraneoplastic cerebellar degeneration, in which tumors elsewhere in the body elicit an autoimmune response that causes neuronal loss in the cerebellum. Cerebellar atrophy can result from an acute deficiency of vitamin B1 (thiamine) as seen in beriberi and in Wernicke–Korsakoff syndrome, or from vitamin E deficiency.

Cerebellar atrophy has been observed in many other neurological disorders including Huntington's disease, multiple sclerosis, essential tremor, progressive myoclonus epilepsy, and Niemann–Pick disease. Cerebellar atrophy can also occur as a result of exposure to toxins including heavy metals or pharmaceutical or recreational drugs.

There is a general consensus that the cerebellum is involved in pain processing. The cerebellum receives pain input from both descending cortico-cerebellar pathways and ascending spino-cerebellar pathways, through the pontine nuclei and inferior olives. Some of this information is transferred to the motor system inducing a conscious motor avoidance of pain, graded according to pain intensity.

These direct pain inputs, as well as indirect inputs, are thought to induce long-term pain avoidance behavior that results in chronic posture changes and consequently, in functional and anatomical remodeling of vestibular and proprioceptive nuclei. As a result, chronic neuropathic pain can induce macroscopic anatomical remodeling of the hindbrain, including the cerebellum. The magnitude of this remodeling and the induction of neuron progenitor markers suggest the contribution of adult neurogenesis to these changes.

The circuits in the cerebellum are similar across all classes of vertebrates, including fish, reptiles, birds, and mammals. There is also an analogous brain structure in cephalopods with well-developed brains, such as octopuses. This has been taken as evidence that the cerebellum performs functions important to all animal species with a brain.

There is considerable variation in the size and shape of the cerebellum in different vertebrate species. In amphibians, it is little developed, and in lampreys, and hagfish, the cerebellum is barely distinguishable from the brain-stem. Although the spinocerebellum is present in these groups, the primary structures are small, paired-nuclei corresponding to the vestibulocerebellum. The cerebellum is a bit larger in reptiles, considerably larger in birds, and larger yet in mammals. The large paired and convoluted lobes found in humans are typical of mammals, but the cerebellum is, in general, a single median lobe in other groups, and is either smooth or only slightly grooved. In mammals, the neocerebellum is the major part of the cerebellum by mass, but, in other vertebrates, it is typically the spinocerebellum.

The cerebellum of cartilaginous and bony fishes is extraordinarily large and complex. In at least one important respect, it differs in internal structure from the mammalian cerebellum: The fish cerebellum does not contain discrete deep cerebellar nuclei. Instead, the primary targets of Purkinje cells are a distinct type of cell distributed across the cerebellar cortex, a type not seen in mammals. In mormyrid fish (a family of weakly electrosensitive freshwater fish), the cerebellum is considerably larger than the rest of the brain put together. The largest part of it is a special structure called the "valvula", which has an unusually regular architecture and receives much of its input from the electrosensory system.

The hallmark of the mammalian cerebellum is an expansion of the lateral lobes, whose main interactions are with the neocortex. As monkeys evolved into great apes, the expansion of the lateral lobes continued, in tandem with the expansion of the frontal lobes of the neocortex. In ancestral hominids, and in "Homo sapiens" until the middle Pleistocene period, the cerebellum continued to expand, but the frontal lobes expanded more rapidly. The most recent period of human evolution, however, may actually have been associated with an increase in the relative size of the cerebellum, as the neocortex reduced its size somewhat while the cerebellum expanded. The size of the human cerebellum, compared to the rest of the brain, has been increasing in size while the cerebrum decreased in size With both the development and implementation of motor tasks, visual-spatial skills and learning taking place in the cerebellum, the growth of the cerebellum is thought to have some form of correlation to greater human cognitive abilities. The lateral hemispheres of the cerebellum are now 2.7 times greater in both humans and apes than they are in monkeys. These changes in the cerebellum size cannot be explained by greater muscle mass. They show that either the development of the cerebellum is tightly linked to that of the rest of the brain or that neural activities taking place in the cerebellum were important during Hominidae evolution. Due to the cerebellum's role in cognitive functions, the increase in its size may have played a role in cognitive expansion.

Most vertebrate species have a cerebellum and one or more cerebellum-like structures, brain areas that resemble the cerebellum in terms of cytoarchitecture and neurochemistry. The only cerebellum-like structure found in mammals is the dorsal cochlear nucleus (DCN), one of the two primary sensory nuclei that receive input directly from the auditory nerve. The DCN is a layered structure, with the bottom layer containing granule cells similar to those of the cerebellum, giving rise to parallel fibers that rise to the superficial layer and travel across it horizontally. The superficial layer contains a set of GABAergic neurons called cartwheel cells that resemble Purkinje cells anatomically and chemically—they receive parallel fiber input, but do not have any inputs that resemble climbing fibers. The output neurons of the DCN are pyramidal cells. They are glutamatergic, but also resemble Purkinje cells in some respects—they have spiny, flattened superficial dendritic trees that receive parallel fiber input, but they also have basal dendrites that receive input from auditory nerve fibers, which travel across the DCN in a direction at right angles to the parallel fibers. The DCN is most highly developed in rodents and other small animals, and is considerably reduced in primates. Its function is not well understood; the most popular speculations relate it to spatial hearing in one way or another.

Most species of fish and amphibians possess a lateral line system that senses pressure waves in water. One of the brain areas that receives primary input from the lateral line organ, the medial octavolateral nucleus, has a cerebellum-like structure, with granule cells and parallel fibers. In electrosensitive fish, the input from the electrosensory system goes to the dorsal octavolateral nucleus, which also has a cerebellum-like structure. In ray-finned fishes (by far the largest group), the optic tectum has a layer—the marginal layer—that is cerebellum-like.

All of these cerebellum-like structures appear to be primarily sensory-related rather than motor-related. All of them have granule cells that give rise to parallel fibers that connect to Purkinje-like neurons with modifiable synapses, but none have climbing fibers comparable to those of the cerebellum—instead they receive direct input from peripheral sensory organs. None has a demonstrated function, but the most influential speculation is that they serve to transform sensory inputs in some sophisticated way, perhaps to compensate for changes in body posture. In fact, James M. Bower and others have argued, partly on the basis of these structures and partly on the basis of cerebellar studies, that the cerebellum itself is fundamentally a sensory structure, and that it contributes to motor control by moving the body in a way that controls the resulting sensory signals. Despite Bower's viewpoint, there is also strong evidence that the cerebellum directly influences motor output in mammals.

Even the earliest anatomists were able to recognize the cerebellum by its distinctive appearance. Aristotle and Herophilus (quoted in Galen) called it the παρεγκεφαλίς ("paregkephalis"), as opposed to the ἐγκέφαλος ("egkephalos") or brain proper. Galen's extensive description is the earliest that survives. He speculated that the cerebellum was the source of motor nerves.

Further significant developments did not come until the Renaissance. Vesalius discussed the cerebellum briefly, and the anatomy was described more thoroughly by Thomas Willis in 1664. More anatomical work was done during the 18th century, but it was not until early in the 19th century that the first insights into the function of the cerebellum were obtained. Luigi Rolando in 1809 established the key finding that damage to the cerebellum results in motor disturbances. Jean Pierre Flourens in the first half of the 19th century carried out detailed experimental work, which revealed that animals with cerebellar damage can still move, but with a loss of coordination (strange movements, awkward gait, and muscular weakness), and that recovery after the lesion can be nearly complete unless the lesion is very extensive. By the beginning of the 20th century, it was widely accepted that the primary function of the cerebellum relates to motor control; the first half of the 20th century produced several detailed descriptions of the clinical symptoms associated with cerebellar disease in humans.

The name "cerebellum" is a diminutive of "cerebrum" (brain); it can be translated literally as "little brain". The Latin name is a direct translation of the Ancient Greek παρεγκεφαλίς ("paregkephalis"), which was used in the works of Aristotle, the first known writer to describe the structure. No other name is used in the English-language literature, but historically a variety of Greek or Latin-derived names have been used, including "cerebrum parvum", "encephalion", "encranion", "cerebrum posterius", and "parencephalis".



</doc>
<doc id="50534" url="https://en.wikipedia.org/wiki?curid=50534" title="Caroline of Ansbach">
Caroline of Ansbach

Caroline of Brandenburg-Ansbach (Wilhelmina Charlotte Caroline; 1 March 1683 – 20 November 1737) was Queen of Great Britain as the wife of King George II.

Her father, Margrave John Frederick of Brandenburg-Ansbach, belonged to a branch of the House of Hohenzollern and was the ruler of a small German state, the Principality of Ansbach. Caroline was orphaned at a young age and moved to the enlightened court of her guardians, King Frederick I and Queen Sophia Charlotte of Prussia. At the Prussian court, her previously limited education was widened, and she adopted the liberal outlook possessed by Sophia Charlotte, who became her good friend and whose views influenced Caroline all her life.
As a young woman, Caroline was much sought-after as a bride. After rejecting the suit of the nominal King of Spain, Archduke Charles of Austria, she married George Augustus, the third-in-line to the British throne and heir apparent to the Electorate of Hanover. They had eight children, seven of whom grew to adulthood.

Caroline moved permanently to Britain in 1714 when her husband became Prince of Wales. As Princess of Wales, she joined her husband in rallying political opposition to his father King George I. In 1717, her husband was expelled from court after a family row. Caroline came to be associated with Robert Walpole, an opposition politician who was a former government minister. Walpole rejoined the government in 1720, and Caroline's husband and King George I reconciled publicly, on Walpole's advice. Over the next few years, Walpole rose to become the leading minister.

Caroline became queen and electress consort upon her husband's accession in 1727. Her eldest son, Frederick, became Prince of Wales. He was a focus for the opposition, like his father before him, and Caroline's relationship with him was strained. As princess and as queen, Caroline was known for her political influence, which she exercised through and for Walpole. Her tenure included four regencies during her husband's stays in Hanover, and she is credited with strengthening the House of Hanover's place in Britain during a period of political instability. Caroline was widely mourned following her death in 1737, not only by the public but also by the King, who refused to remarry.

Caroline was born on 1 March 1683 at Ansbach, the daughter of John Frederick, Margrave of Brandenburg-Ansbach, and his second wife, Princess Eleonore Erdmuthe of Saxe-Eisenach. Her father was the ruler of one of the smallest German states; he died of smallpox at the age of 32, when Caroline was three years old. Caroline and her only full sibling, her younger brother Margrave William Frederick, left Ansbach with their mother, who returned to her native Eisenach.
In 1692, Caroline's widowed mother was pushed into an unhappy marriage with the Elector of Saxony, and she and her two children moved to the Saxon court at Dresden. Eleonore Erdmuthe was widowed again two years later, after her unfaithful husband contracted smallpox from his mistress. Eleonore remained in Saxony for another two years, until her death in 1696. The orphaned Caroline and William Frederick returned to Ansbach to stay with their elder half-brother, Margrave George Frederick II. George Frederick was a youth with little interest in parenting a girl, and so Caroline soon moved to Lützenburg outside Berlin, where she entered into the care of her new guardians, Frederick, Elector of Brandenburg, and his wife, Sophia Charlotte, who had been a friend of Eleonore Erdmuthe.

Frederick and Sophia Charlotte became king and queen of Prussia in 1701. The queen was the daughter of Dowager Electress Sophia of Hanover, and the sister of George, Elector of Hanover. She was renowned for her intelligence and strong character, and her uncensored and liberal court attracted a great many scholars, including philosopher Gottfried Leibniz. Caroline was exposed to a lively intellectual environment quite different from anything she had experienced previously. Before she began her education under Sophia Charlotte's care, Caroline had received little formal education; her handwriting remained poor throughout her life. With her lively mind, Caroline developed into a scholar of considerable ability. She and Sophia Charlotte developed a strong relationship in which Caroline was treated as a surrogate daughter; the queen once declared Berlin was "a desert" without Caroline whenever she left temporarily for Ansbach.

An intelligent and attractive woman, Caroline was much sought-after as a bride. Dowager Electress Sophia called her "the most agreeable Princess in Germany". She was considered for the hand of Archduke Charles of Austria, who was a candidate for the throne of Spain and later became Holy Roman Emperor. Charles made official overtures to her in 1703, and the match was encouraged by King Frederick of Prussia. After some consideration, Caroline refused in 1704, as she would not convert from Lutheranism to Catholicism. Early in the following year, Queen Sophia Charlotte died on a visit to her native Hanover. Caroline was devastated, writing to Leibniz, "The calamity has overwhelmed me with grief and sickness, and it is only the hope that I may soon follow her that consoles me."

In June 1705, Queen Sophia Charlotte's nephew, Prince George Augustus of Hanover, visited the Ansbach court, supposedly incognito, to inspect Caroline, as his father the Elector did not want his son to enter into a loveless arranged marriage as he himself had. The nephew of three childless uncles, George Augustus was under pressure to marry and father an heir to prevent endangering the Hanoverian succession. He had heard reports of Caroline's "incomparable beauty and mental attributes". He immediately took a liking to her "good character" and the British envoy reported that George Augustus "would not think of anybody else after her". For her part, Caroline was not fooled by the prince's disguise, and found her suitor attractive. He was the heir apparent of his father's Electorate of Hanover and third-in-line to the British throne of his distant cousin Queen Anne, after his grandmother Dowager Electress Sophia and his father the Elector.

On 22 August 1705, Caroline arrived in Hanover for her wedding to George Augustus; they were married that evening in the palace chapel at Herrenhausen. By May of the following year, Caroline was pregnant, and her first child Prince Frederick was born on 20 January 1707. A few months after the birth, in July, Caroline fell seriously ill with smallpox followed by pneumonia. Her baby was kept away from her, but George Augustus remained at her side devotedly, and caught and survived the infection himself. Over the next seven years, Caroline had three more children, Anne, Amelia, and Caroline, all of whom were born in Hanover.

George Augustus and Caroline had a successful and loving marriage, though he continued to keep mistresses, as was customary for the time. Caroline was well aware of his infidelities, as they were well known and he told her about them. His two best-known mistresses were Henrietta Howard, later Countess of Suffolk, and, from 1735, Amalie von Wallmoden, Countess of Yarmouth. Howard was one of Caroline's Women of the Bedchamber and became Mistress of the Robes when her husband inherited a peerage in 1731; she retired in 1734. In contrast with her mother-in-law and husband, Caroline was known for her marital fidelity; she never made any embarrassing scenes nor did she take lovers. She preferred her husband's mistresses to be ladies-in-waiting, as that way she believed she could keep a closer eye on them.

The succession of her husband's family to the British throne was still insecure, as Queen Anne's half-brother James Stuart contested the Hanoverian claim, and Queen Anne and Caroline's grandmother-in-law Dowager Electress Sophia had fallen out. Anne refused permission for any of the Hanoverians to visit Britain in her lifetime. Caroline wrote to Leibniz, "I accept the comparison which you draw, though all too flattering, between me and Queen Elizabeth as a good omen. Like Elizabeth, the Electress's rights are denied her by a jealous sister [Queen Anne], and she will never be sure of the English crown until her accession to the throne." In June 1714, Dowager Electress Sophia died in Caroline's arms at the age of 83, and Caroline's father-in-law became heir presumptive to Queen Anne. Just weeks later, Anne died and the Elector of Hanover was proclaimed as her successor, becoming George I of Great Britain.

George Augustus sailed to England in September 1714, and Caroline and two of her daughters followed in October. Her journey across the North Sea from The Hague to Margate was the only sea voyage she took in her life. Their young son, Prince Frederick, remained in Hanover for the rest of George I's reign to be brought up by private tutors.

On the accession of George I in 1714, Caroline's husband automatically became Duke of Cornwall and Duke of Rothesay. Shortly afterwards, he was invested as Prince of Wales, whereupon she became Princess of Wales. Caroline was the first woman to receive the title at the same time as her husband received his. She was the first Princess of Wales for over two hundred years, the last one being Catherine of Aragon. As George I had repudiated his wife Sophia Dorothea of Celle in 1694 prior to his becoming King of Great Britain, there was no queen consort, and Caroline was the highest-ranking woman in the kingdom. George Augustus and Caroline made a concerted effort to "anglicise" by acquiring knowledge of England's language, people, politics and customs. Two separate courts developed with strong contrasts; the old king's had German courtiers and government ministers, while the Wales's court attracted English nobles out of favour with the King, and was considerably more popular with the British people. George Augustus and Caroline gradually became centres of the political opposition to the King.

Two years after their arrival in England, Caroline suffered a stillbirth, which her friend the Countess of Bückeburg blamed on the incompetence of English doctors, but the following year she had another son, Prince George William. At the baptism in November 1717, her husband fell out with his father over the choice of godparents, leading to the couple's placement under house arrest at St. James's Palace prior to their banishment from court. Caroline was originally allowed to stay with their children, but refused as she believed her place was with her husband. She and her husband moved into Leicester House, while their children remained in the care of the King. Caroline fell sick with worry, and fainted during a secret visit to her children made without the King's approval. By January, the King had relented and allowed Caroline unrestricted access. In February, Prince George William fell ill, and the King allowed both George Augustus and Caroline to see him at Kensington Palace without any conditions. When the baby died, a post-mortem was conducted to prove that the cause of death was disease (a polyp on the heart) rather than the separation from his mother. Further tragedy occurred in 1718, when Caroline miscarried at Richmond Lodge, her country residence. Over the next few years, Caroline had three more children: William, Mary and Louise.

Leicester House became a frequent meeting place for the ministry's political opponents. Caroline struck up a friendship with politician Sir Robert Walpole, a former minister in the Whig government who led a disgruntled faction of the party. In April 1720, Walpole's wing of the Whig party reconciled with the governing wing, and Walpole and Caroline helped to effect a reconciliation between the King and her husband for the sake of public unity. Caroline wanted to regain her three eldest daughters, who remained in the care of the King, and thought the reconciliation would lead to their return, but negotiations came to nothing. George Augustus came to believe that Walpole had tricked him into the reconciliation as part of a scheme to gain power. The prince was isolated politically when Walpole's Whigs joined the government, and Leicester House played host to literary figures and wits, such as John Arbuthnot and Jonathan Swift, rather than politicians. Arbuthnot told Swift that Caroline had enjoyed his "Gulliver's Travels", particularly the tale of the crown prince who wore one high-heel and one low-heel in a country where the King and his party wore low heels, and the opposition wore high ones: a barely veiled reference to the political leanings of the Prince of Wales.

Caroline's intellect far outstripped her husband's, and she read avidly. She established an extensive library at St. James's Palace. As a young woman, she corresponded with Gottfried Leibniz, the intellectual colossus who was courtier and factotum to the House of Hanover. She later facilitated the Leibniz-Clarke correspondence, arguably the most important philosophy of physics discussion of the 18th century. She helped to popularise the practice of variolation (an early type of immunisation), which had been witnessed by Lady Mary Wortley Montagu and Charles Maitland in Constantinople. At the direction of Caroline, six condemned prisoners were offered the chance to undergo variolation instead of execution: they all survived, as did six orphan children given the same treatment as a further test. Convinced of its medical value, Caroline had her children Amelia, Caroline and Frederick inoculated against smallpox in the same manner. In praising her support for smallpox inoculation, Voltaire wrote of her, "I must say that despite all her titles and crowns, this princess was born to encourage the arts and the well-being of mankind; even on the throne she is a benevolent philosopher; and she has never lost an opportunity to learn or to manifest her generosity."

Caroline became queen consort on the death of her father-in-law in 1727, and she was crowned alongside her husband at Westminster Abbey on 11 October that year. She was the first queen consort to be crowned since Anne of Denmark in 1603. Though George II denounced Walpole as a "rogue and rascal" over the terms of the reconciliation with his father, Caroline advised her husband to retain Walpole as the leading minister. Walpole commanded a substantial majority in Parliament and George II had little choice but to accept him or risk ministerial instability. Walpole secured a civil list payment of £100,000 a year for Caroline, and she was given both Somerset House and Richmond Lodge. Courtier Lord Hervey called Walpole "the Queen's minister" in recognition of their close relationship. For the next ten years, Caroline had immense influence. She persuaded the King to adopt policies at the behest of Walpole, and persuaded Walpole against taking inflammatory actions. Caroline had absorbed the liberal opinions of her mentor, Queen Sophia Charlotte of Prussia, and supported clemency for the Jacobites (supporters of the rival Stuart claim to the throne), freedom of the press, and freedom of speech in Parliament.

Over the next few years, she and her husband fought a constant battle against their eldest son, Frederick, Prince of Wales, who had been left behind in Germany when they came to England. He joined the family in 1728, by which time he was an adult, had mistresses and debts, and was fond of gambling and practical jokes. He opposed his father's political beliefs, and complained of his lack of influence in government. The Regency Act 1728 made Caroline rather than Frederick regent when her husband was in Hanover for five months from May 1729. During her regency, a diplomatic incident with Portugal (where a British ship had been seized on the Tagus) was defused, and the negotiation of the Treaty of Seville between Britain and Spain was concluded. From May 1732, she was regent for four months while George II was again away in Hanover. An investigation into the penal system uncovered widespread abuses, including cruel treatment and conspiracy in the escape of wealthy convicts. Caroline pressed Walpole for reform, largely unsuccessfully. In March 1733, Walpole introduced an unpopular excise bill to parliament, which the Queen supported, but it gathered such strong opposition that it was eventually dropped.

Caroline's entire life in Britain was spent in the South-East of England in or around London. As queen, she continued to surround herself with artists, writers and intellectuals. She collected jewellery, especially cameos and intaglios, acquired important portraits and miniatures, and enjoyed the visual arts. She commissioned works such as terracotta busts of the kings and queens of England from Michael Rysbrack, and supervised a more naturalistic design of the royal gardens by William Kent and Charles Bridgeman. In 1728, she rediscovered sets of sketches by Leonardo da Vinci and Hans Holbein that had been hidden in a drawer since the reign of William III.

Caroline's eldest daughter Anne married William IV of Orange in 1734, and moved with her husband to the Netherlands. Caroline wrote to her daughter of her "indescribable" sadness at the parting. Anne soon felt homesick, and travelled back to England when her husband went on campaign. Eventually, her husband and father commanded her to return to Holland.

In mid-1735, Frederick, Prince of Wales, was further dismayed when Caroline, rather than himself, again acted as regent while the King was absent in Hanover. The King and Queen arranged Frederick's marriage, in 1736, to Princess Augusta of Saxe-Gotha. Shortly after the wedding, George went to Hanover, and Caroline resumed her role as "Protector of the Realm". As regent, Caroline considered the reprieve of Captain John Porteous, who had been convicted of murder in Edinburgh. Before she could act, a mob stormed the jail where he was held and killed him. Caroline was appalled. The King's absences abroad were leading to unpopularity, and in late 1736 he made plans to return, but his ship was caught in poor weather, and it was rumoured that he had been lost at sea. Caroline was devastated, and disgusted by the insensitivity of her son, who hosted a grand dinner while the gale was blowing. During her regency, the Prince of Wales attempted to start a number of quarrels with his mother, whom he saw as a useful proxy to irritate the King. George eventually returned in January 1737.

Frederick applied to Parliament unsuccessfully for an increased financial allowance that had hitherto been denied him by the King, and public disagreement over the money drove a further wedge between parents and son. On the advice of Walpole, Frederick's allowance was raised in an attempt to mitigate further conflict, but by less than he had asked. In June 1737, Frederick informed his parents that Augusta was pregnant, and due to give birth in October. In fact, Augusta's due date was earlier and a peculiar episode followed in July in which the prince, on discovering that his wife had gone into labour, sneaked her out of Hampton Court Palace in the middle of the night, to ensure that the King and Queen could not be present at the birth. George and Caroline were horrified. Traditionally, royal births were witnessed by members of the family and senior courtiers to guard against supposititious children, and Augusta had been forced by her husband to ride in a rattling carriage for an hour and a half while heavily pregnant and in pain. With a party including two of her daughters and Lord Hervey, the Queen raced over to St. James's Palace, where Frederick had taken Augusta. Caroline was relieved to discover that Augusta had given birth to a "poor, ugly little she-mouse" rather than a "large, fat, healthy boy" as the pitiful nature of the baby made a supposititious child unlikely. The circumstances of the birth deepened the estrangement between mother and son. According to Lord Hervey, she once remarked after seeing Frederick, "Look, there he goes—that wretch!—that villain!—I wish the ground would open this moment and sink the monster to the lowest hole in hell!"

In the final years of her life, Caroline was troubled by gout in her feet, but more seriously she had suffered an umbilical hernia at the birth of her final child in 1724. On 9 November 1737, she felt an intense pain and, after struggling through a formal reception, took to her bed. Part of her small intestine had poked through the hernia opening. Over the next few days she was bled, purged, and operated on, without anaesthetic, but there was no improvement in her condition. The King refused Frederick permission to see his mother, a decision with which she complied; she sent her son a message of forgiveness through Walpole. She asked her husband to remarry after her death, which he rejected saying he would take only mistresses; she replied "Ah, mon Dieu, cela n'empêche pas" ("My God, that doesn't prevent it"). On 17 November, her strangulated bowel burst. She died on 20 November 1737 at St. James's Palace.

She was buried in Westminster Abbey on 17 December. Frederick was not invited to the funeral. George Frideric Handel composed an anthem for the occasion, "The Ways of Zion Do Mourn / Funeral Anthem for Queen Caroline". The King arranged for a pair of matching coffins with removable sides, so that when he followed her to the grave (23 years later), they could lie together again.

Caroline was widely mourned. The Protestants lauded her moral example, and even the Jacobites acknowledged her compassion, and her intervention on the side of mercy for their compatriots. During her lifetime her refusal to convert when offered the hand of Archduke Charles was used to portray her as a strong adherent to Protestantism. For example, John Gay wrote of Caroline in "A Letter to A Lady" (1714):

She was widely seen by both the public and the court as having great influence over her husband. A satirical verse of the period went:
The memoirs of the eighteenth century, particularly those of John, Lord Hervey, fed perceptions that Caroline and Walpole governed her husband. Peter Quennell wrote that Hervey was the "chronicler of this remarkable coalition" and that she was Hervey's "heroine". Using such sources, biographers of the nineteenth and twentieth centuries credit her with aiding the establishment of the House of Hanover in Britain, in the face of Jacobite opposition. R. L. Arkell wrote "by her acumen and geniality, [Caroline] ensured the dynasty's rooting itself in England", and W. H. Wilkins said her "gracious and dignified personality, her lofty ideals and pure life did much to counteract the unpopularity of her husband and father-in-law, and redeem the early Georgian era from utter grossness." Although modern historians tend to believe that Hervey, Wilkins and Arkell have overestimated her importance, it is nevertheless probable that Caroline of Ansbach was one of the most influential consorts in British history.


Caroline County in the British Colony of Virginia was named in her honour when it was formed in 1727.

The royal coat of arms of the United Kingdom are impaled with those of her father, John Frederick, Margrave of Brandenburg-Ansbach. The arms of her father were quarterly of fifteen, 1st, per fess gules and argent, within a bordure counter-changed of the same (for Magdeburg); 2nd, argent, an eagle displayed sable, crowned or; 3rd, or, a griffin segreant gules, crowned; 4th and 5th, argent, a griffin segreant gules; 6th, or, a griffin segreant sable; 7th, argent, an eagle displayed sable (for Crossen); 8th, per pale argent and gules within a bordure counter-changed of the same (for Halberstadt); 9th, argent, an eagle displayed sable; 10th, or, a lion rampant sable, crowned, within a bordure goboné argent and gules (for Nuremberg); 11th, gules, two keys in saltire or (for Minden); 12th, quarterly argent and sable (for Hohenzollern); 13th, the field gules, the figure argent; 14th, per fess gules and argent; 15th, plain field of gules (for right of regalia); overall an inescutcheon, argent, an eagle displayed gules (for Brandenburg).

Caroline's ten pregnancies resulted in eight live births, of whom one died in infancy, and seven lived to adulthood.




</doc>
<doc id="50603" url="https://en.wikipedia.org/wiki?curid=50603" title="Multiple sclerosis">
Multiple sclerosis

Multiple sclerosis (MS) is a demyelinating disease in which the insulating covers of nerve cells in the brain and spinal cord are damaged. This damage disrupts the ability of parts of the nervous system to communicate, resulting in a range of signs and symptoms, including physical, mental, and sometimes psychiatric problems. Specific symptoms can include double vision, blindness in one eye, muscle weakness, trouble with sensation, or trouble with coordination. MS takes several forms, with new symptoms either occurring in isolated attacks (relapsing forms) or building up over time (progressive forms). Between attacks, symptoms may disappear completely; however, permanent neurological problems often remain, especially as the disease advances.
While the cause is not clear, the underlying mechanism is thought to be either destruction by the immune system or failure of the myelin-producing cells. Proposed causes for this include genetics and environmental factors such as being triggered by a viral infection. MS is usually diagnosed based on the presenting signs and symptoms and the results of supporting medical tests.
There is no known cure for multiple sclerosis. Treatments attempt to improve function after an attack and prevent new attacks. Medications used to treat MS, while modestly effective, can have side effects and be poorly tolerated. Physical therapy can help with people's ability to function. Many people pursue alternative treatments, despite a lack of evidence of benefit. The long-term outcome is difficult to predict, with good outcomes more often seen in women, those who develop the disease early in life, those with a relapsing course, and those who initially experienced few attacks. Life expectancy is on average 5 to 10 years lower than that of an unaffected population.
Multiple sclerosis is the most common immune-mediated disorder affecting the central nervous system. In 2015, about 2.3 million people were affected globally with rates varying widely in different regions and among different populations. That year about 18,900 people died from MS, up from 12,000 in 1990. The disease usually begins between the ages of 20 and 50 and is twice as common in women as in men. MS was first described in 1868 by Jean-Martin Charcot. The name "multiple sclerosis" refers to the numerous scars (sclerae—better known as plaques or lesions) that develop on the white matter of the brain and spinal cord. A number of new treatments and diagnostic methods are under development.

A person with MS can have almost any neurological symptom or sign, with autonomic, visual, motor, and sensory problems being the most common. The specific symptoms are determined by the locations of the lesions within the nervous system, and may include loss of sensitivity or changes in sensation such as tingling, pins and needles or numbness, muscle weakness, blurred vision, very pronounced reflexes, muscle spasms, or difficulty in moving; difficulties with coordination and balance (ataxia); problems with speech or swallowing, visual problems (nystagmus, optic neuritis or double vision), feeling tired, acute or chronic pain, and bladder and bowel difficulties, among others. Difficulties thinking and emotional problems such as depression or unstable mood are also common. Uhthoff's phenomenon, a worsening of symptoms due to exposure to higher than usual temperatures, and Lhermitte's sign, an electrical sensation that runs down the back when bending the neck, are particularly characteristic of MS. The main measure of disability and severity is the expanded disability status scale (EDSS), with other measures such as the multiple sclerosis functional composite being increasingly used in research.

The condition begins in 85% of cases as a clinically isolated syndrome (CIS) over a number of days with 45% having motor or sensory problems, 20% having optic neuritis, and 10% having symptoms related to brainstem dysfunction, while the remaining 25% have more than one of the previous difficulties. The course of symptoms occurs in two main patterns initially: either as episodes of sudden worsening that last a few days to months (called relapses, exacerbations, bouts, attacks, or flare-ups) followed by improvement (85% of cases) or as a gradual worsening over time without periods of recovery (10–15% of cases). A combination of these two patterns may also occur or people may start in a relapsing and remitting course that then becomes progressive later on. Relapses are usually not predictable, occurring without warning. Exacerbations rarely occur more frequently than twice per year. Some relapses, however, are preceded by common triggers and they occur more frequently during spring and summer. Similarly, viral infections such as the common cold, influenza, or gastroenteritis increase their risk. Stress may also trigger an attack. Women with MS who become pregnant experience fewer relapses; however, during the first months after delivery the risk increases. Overall, pregnancy does not seem to influence long-term disability. Many events have been found not to affect relapse rates including vaccination, breast feeding, physical trauma, and Uhthoff's phenomenon.

The cause of MS is unknown; however, it is believed to occur as a result of some combination of genetic and environmental factors such as infectious agents. Theories try to combine the data into likely explanations, but none has proved definitive. While there are a number of environmental risk factors and although some are partly modifiable, further research is needed to determine whether their elimination can prevent MS.

MS is more common in people who live farther from the equator, although exceptions exist. These exceptions include ethnic groups that are at low risk far from the equator such as the Samis, Amerindians, Canadian Hutterites, New Zealand Māori, and Canada's Inuit, as well as groups that have a relatively high risk close to the equator such as Sardinians, inland Sicilians, Palestinians, and Parsi. The cause of this geographical pattern is not clear. While the north-south gradient of incidence is decreasing, as of 2010 it is still present.

MS is more common in regions with northern European populations and the geographic variation may simply reflect the global distribution of these high-risk populations. Decreased sunlight exposure resulting in decreased vitamin D production has also been put forward as an explanation. A relationship between season of birth and MS lends support to this idea, with fewer people born in the northern hemisphere in November as compared to May being affected later in life. Environmental factors may play a role during childhood, with several studies finding that people who move to a different region of the world before the age of 15 acquire the new region's risk to MS. If migration takes place after age 15, however, the person retains the risk of their home country. There is some evidence that the effect of moving may still apply to people older than 15.

MS is not considered a hereditary disease; however, a number of genetic variations have been shown to increase the risk. Some of these genes appear to have higher levels of expression in microglial cells than expected by chance. The probability of developing the disease is higher in relatives of an affected person, with a greater risk among those more closely related. In identical twins both are affected about 30% of the time, while around 5% for non-identical twins and 2.5% of siblings are affected with a lower percentage of half-siblings. If both parents are affected the risk in their children is 10 times that of the general population. MS is also more common in some ethnic groups than others.

Specific genes that have been linked with MS include differences in the human leukocyte antigen (HLA) system—a group of genes on chromosome 6 that serves as the major histocompatibility complex (MHC). That differences in the HLA region are related to susceptibility has been known since the 1980s, and this same region has also been implicated in the development of other autoimmune diseases such as diabetes type I and systemic lupus erythematosus. The most consistent finding is the association between multiple sclerosis and alleles of the MHC defined as DR15 and DQ6. Other loci have shown a protective effect, such as HLA-C554 and HLA-DRB1*11. Overall, it has been estimated that HLA differences account for between 20% and 60% of the genetic predisposition. Modern genetic methods (genome-wide association studies) have revealed at least twelve other genes outside the HLA locus that modestly increase the probability of MS.

Many microbes have been proposed as triggers of MS, but none have been confirmed. Moving at an early age from one location in the world to another alters a person's subsequent risk of MS. An explanation for this could be that some kind of infection, produced by a widespread microbe rather than a rare one, is related to the disease. Proposed mechanisms include the hygiene hypothesis and the prevalence hypothesis. The hygiene hypothesis proposes that exposure to certain infectious agents early in life is protective, the disease is a response to a late encounter with such agents. The prevalence hypothesis proposes that the disease is due to an infectious agent more common in regions where MS is common and where, in most individuals, it causes an ongoing infection without symptoms. Only in a few cases and after many years does it cause demyelination. The hygiene hypothesis has received more support than the prevalence hypothesis.

Evidence for a virus as a cause include the presence of oligoclonal bands in the brain and cerebrospinal fluid of most people with MS, the association of several viruses with human demyelination encephalomyelitis, and the occurrence of demyelination in animals caused by some viral infections. Human herpes viruses are a candidate group of viruses. Individuals having never been infected by the Epstein–Barr virus are at a reduced risk of getting MS, whereas those infected as young adults are at a greater risk than those having had it at a younger age. Although some consider that this goes against the hygiene hypothesis, since the non-infected have probably experienced a more hygienic upbringing, others believe that there is no contradiction, since it is a first encounter with the causative virus relatively late in life that is the trigger for the disease. Other diseases that may be related include measles, mumps and rubella.

Smoking has been shown to be an independent risk factor for MS. Stress may be a risk factor although the evidence to support this is weak. Association with occupational exposures and toxins—mainly solvents—has been evaluated, but no clear conclusions have been reached. Vaccinations were studied as causal factors; however, most studies show no association. Several other possible risk factors, such as diet and hormone intake, have been looked at; however, evidence on their relation with the disease is "sparse and unpersuasive". Gout occurs less than would be expected and lower levels of uric acid have been found in people with MS. This has led to the theory that uric acid is protective, although its exact importance remains unknown.

The three main characteristics of MS are the formation of lesions in the central nervous system (also called plaques), inflammation, and the destruction of myelin sheaths of neurons. These features interact in a complex and not yet fully understood manner to produce the breakdown of nerve tissue and in turn the signs and symptoms of the disease.
Cholesterol crystals are believed to both impair myelin repair and aggravate inflammation. MS is believed to be an immune-mediated disorder that develops from an interaction of the individual's genetics and as yet unidentified environmental causes. Damage is believed to be caused, at least in part, by attack on the nervous system by a person's own immune system.

The name "multiple sclerosis" refers to the scars (sclerae – better known as plaques or lesions) that form in the nervous system. These lesions most commonly affect the white matter in the optic nerve, brain stem, basal ganglia, and spinal cord, or white matter tracts close to the lateral ventricles. The function of white matter cells is to carry signals between grey matter areas, where the processing is done, and the rest of the body. The peripheral nervous system is rarely involved.

To be specific, MS involves the loss of oligodendrocytes, the cells responsible for creating and maintaining a fatty layer—known as the myelin sheath—which helps the neurons carry electrical signals (action potentials). This results in a thinning or complete loss of myelin and, as the disease advances, the breakdown of the axons of neurons. When the myelin is lost, a neuron can no longer effectively conduct electrical signals. A repair process, called remyelination, takes place in early phases of the disease, but the oligodendrocytes are unable to completely rebuild the cell's myelin sheath. Repeated attacks lead to successively less effective remyelinations, until a scar-like plaque is built up around the damaged axons. These scars are the origin of the symptoms and during an attack magnetic resonance imaging (MRI) often shows more than ten new plaques. This could indicate that there are a number of lesions below which the brain is capable of repairing itself without producing noticeable consequences. Another process involved in the creation of lesions is an abnormal increase in the number of astrocytes due to the destruction of nearby neurons. A number of lesion patterns have been described.

Apart from demyelination, the other sign of the disease is inflammation. Fitting with an immunological explanation, the inflammatory process is caused by T cells, a kind of lymphocyte that plays an important role in the body's defenses. T cells gain entry into the brain via disruptions in the blood–brain barrier. The T cells recognize myelin as foreign and attack it, explaining why these cells are also called "autoreactive lymphocytes".

The attack of myelin starts inflammatory processes, which triggers other immune cells and the release of soluble factors like cytokines and antibodies. A further breakdown of the blood-brain barrier, in turn, causes a number of other damaging effects such as swelling, activation of macrophages, and more activation of cytokines and other destructive proteins. Inflammation can potentially reduce transmission of information between neurons in at least three ways. The soluble factors released might stop neurotransmission by intact neurons. These factors could lead to or enhance the loss of myelin, or they may cause the axon to break down completely.

The blood–brain barrier (BBB) is a part of the capillary system that prevents the entry of T cells into the central nervous system. It may become permeable to these types of cells secondary to an infection by a virus or bacteria. After it repairs itself, typically once the infection has cleared, T cells may remain trapped inside the brain. Gadolinium cannot cross a normal BBB and, therefore, gadolinium-enhanced MRI is used to show BBB breakdowns.

Multiple sclerosis is typically diagnosed based on the presenting signs and symptoms, in combination with supporting medical imaging and laboratory testing. It can be difficult to confirm, especially early on, since the signs and symptoms may be similar to those of other medical problems. The McDonald criteria, which focus on clinical, laboratory, and radiologic evidence of lesions at different times and in different areas, is the most commonly used method of diagnosis with the Schumacher and Poser criteria being of mostly historical significance.

Clinical data alone may be sufficient for a diagnosis of MS if an individual has had separate episodes of neurological symptoms characteristic of the disease. In those who seek medical attention after only one attack, other testing is needed for the diagnosis. The most commonly used diagnostic tools are neuroimaging, analysis of cerebrospinal fluid and evoked potentials. Magnetic resonance imaging of the brain and spine may show areas of demyelination (lesions or plaques). Gadolinium can be administered intravenously as a contrast agent to highlight active plaques and, by elimination, demonstrate the existence of historical lesions not associated with symptoms at the moment of the evaluation. Testing of cerebrospinal fluid obtained from a lumbar puncture can provide evidence of chronic inflammation in the central nervous system. The cerebrospinal fluid is tested for oligoclonal bands of IgG on electrophoresis, which are inflammation markers found in 75–85% of people with MS. The nervous system in MS may respond less actively to stimulation of the optic nerve and sensory nerves due to demyelination of such pathways. These brain responses can be examined using visual- and sensory-evoked potentials.

While the above criteria allow for a non-invasive diagnosis, and even though some state that the only definitive proof is an autopsy or biopsy where lesions typical of MS are detected, currently, as of 2017, there is no single test (including biopsy) that can provide a definitive diagnosis of this disease.

Several phenotypes (commonly termed "types"), or patterns of progression, have been described. Phenotypes use the past course of the disease in an attempt to predict the future course. They are important not only for prognosis but also for treatment decisions. Currently, the United States National Multiple Sclerosis Society and the Multiple Sclerosis International Federation, describes four types of MS (revised in 2013):


Relapsing-remitting MS is characterized by unpredictable relapses followed by periods of months to years of relative quiet (remission) with no new signs of disease activity. Deficits that occur during attacks may either resolve or leave problems, the latter in about 40% of attacks and being more common the longer a person has had the disease. This describes the initial course of 80% of individuals with MS. When deficits always resolve between attacks, this is sometimes referred to as "benign MS", although people will still build up some degree of disability in the long term. On the other hand, the term "malignant multiple sclerosis" is used to describe people with MS having reached significant level of disability in a short period. The relapsing-remitting subtype usually begins with a clinically isolated syndrome (CIS). In CIS, a person has an attack suggestive of demyelination, but does not fulfill the criteria for multiple sclerosis. 30 to 70% of persons experiencing CIS later develop MS.

Primary progressive MS occurs in approximately 10–20% of individuals, with no remission after the initial symptoms. It is characterized by progression of disability from onset, with no, or only occasional and minor, remissions and improvements. The usual age of onset for the primary progressive subtype is later than of the relapsing-remitting subtype. It is similar to the age that secondary progressive usually begins in relapsing-remitting MS, around 40 years of age.

Secondary progressive MS occurs in around 65% of those with initial relapsing-remitting MS, who eventually have progressive neurologic decline between acute attacks without any definite periods of remission. Occasional relapses and minor remissions may appear. The most common length of time between disease onset and conversion from relapsing-remitting to secondary progressive MS is 19 years.

Other, unusual types of MS have been described; these include Devic's disease, Balo concentric sclerosis, Schilder's diffuse sclerosis, and Marburg multiple sclerosis. There is debate on whether they are MS variants or different diseases. Multiple sclerosis behaves differently in children, taking more time to reach the progressive stage. Nevertheless, they still reach it at a lower average age than adults usually do.

Although there is no known cure for multiple sclerosis, several therapies have proven helpful. The primary aims of therapy are returning function after an attack, preventing new attacks, and preventing disability. Starting medications is generally recommended in people after the first attack when more than two lesions are seen on MRI.

As with any medical treatment, medications used in the management of MS have several adverse effects. Alternative treatments are pursued by some people, despite the shortage of supporting evidence.

During symptomatic attacks, administration of high doses of intravenous corticosteroids, such as methylprednisolone, is the usual therapy, with oral corticosteroids seeming to have a similar efficacy and safety profile. Although, in general, effective in the short term for relieving symptoms, corticosteroid treatments do not appear to have a significant impact on long-term recovery. The consequences of severe attacks that do not respond to corticosteroids might be treatable by plasmapheresis.

As of 2017, ten disease-modifying medications are approved by regulatory agencies for relapsing-remitting multiple sclerosis (RRMS). They are interferon beta-1a, interferon beta-1b, glatiramer acetate, mitoxantrone, natalizumab, fingolimod, teriflunomide, dimethyl fumarate, alemtuzumab, and ocrelizumab.

Their cost effectiveness as of 2012 is unclear. In March 2017 the FDA approved ocrelizumab, a humanized anti-CD20 monoclonal antibody, as a treatment for RRMS, with requirements for several Phase IV clinical trials.

In RRMS they are modestly effective at decreasing the number of attacks. The interferons and glatiramer acetate are first-line treatments and are roughly equivalent, reducing relapses by approximately 30%. Early-initiated long-term therapy is safe and improves outcomes. Natalizumab reduces the relapse rate more than first-line agents; however, due to issues of adverse effects is a second-line agent reserved for those who do not respond to other treatments or with severe disease. Mitoxantrone, whose use is limited by severe adverse effects, is a third-line option for those who do not respond to other medications. Treatment of clinically isolated syndrome (CIS) with interferons decreases the chance of progressing to clinical MS. Efficacy of interferons and glatiramer acetate in children has been estimated to be roughly equivalent to that of adults. The role of some newer agents such as fingolimod, teriflunomide, and dimethyl fumarate, as of 2011, is not yet entirely clear.

As of 2017, rituximab was widely used off-label to treat RRMS.

As of 2017, rituximab has been widely used off-label to treat progressive primary MS. In March 2017 the FDA approved ocrelizumab as a treatment for primary progressive MS, the first drug to gain that approval, with requirements for several Phase IV clinical trials.

, only one medication, mitoxantrone, has been approved for secondary progressive MS. In this population tentative evidence supports mitoxantrone moderately slowing the progression of the disease and decreasing rates of relapses over two years.

The disease-modifying treatments have several adverse effects. One of the most common is irritation at the injection site for glatiramer acetate and the interferons (up to 90% with subcutaneous injections and 33% with intramuscular injections). Over time, a visible dent at the injection site, due to the local destruction of fat tissue, known as lipoatrophy, may develop. Interferons may produce flu-like symptoms; some people taking glatiramer experience a post-injection reaction with flushing, chest tightness, heart palpitations, and anxiety, which usually lasts less than thirty minutes. More dangerous but much less common are liver damage from interferons, systolic dysfunction (12%), infertility, and acute myeloid leukemia (0.8%) from mitoxantrone, and progressive multifocal leukoencephalopathy occurring with natalizumab (occurring in 1 in 600 people treated).

Fingolimod may give rise to hypertension and slowed heart rate, macular edema, elevated liver enzymes or a reduction in lymphocyte levels. Tentative evidence supports the short-term safety of teriflunomide, with common side effects including: headaches, fatigue, nausea, hair loss, and limb pain. There have also been reports of liver failure and PML with its use and it is dangerous for fetal development. Most common side effects of dimethyl fumarate are flushing and gastrointestinal problems. While dimethyl fumarate may lead to a reduction in the white blood cell count there were no reported cases of opportunistic infections during trials.

Both medications and neurorehabilitation have been shown to improve some symptoms, though neither changes the course of the disease. Some symptoms have a good response to medication, such as an unstable bladder and spasticity, while others are little changed. For neurologic problems, a multidisciplinary approach is important for improving quality of life; however, it is difficult to specify a 'core team' as many health services may be needed at different points in time. Multidisciplinary rehabilitation programs increase activity and participation of people with MS but do not influence impairment level. There is limited evidence for the overall efficacy of individual therapeutic disciplines, though there is good evidence that specific approaches, such as exercise, and psychological therapies are effective. Cognitive behavioral therapy has shown to be moderately effective for reducing MS fatigue.

Over 50% of people with MS may use complementary and alternative medicine, although percentages vary depending on how alternative medicine is defined. The evidence for the effectiveness for such treatments in most cases is weak or absent. Treatments of unproven benefit used by people with MS include dietary supplementation and regimens, vitamin D, relaxation techniques such as yoga, herbal medicine (including medical cannabis), hyperbaric oxygen therapy, self-infection with hookworms, reflexology, acupuncture, and mindfulness. Regarding the characteristics of users, they are more frequently women, have had MS for a longer time, tend to be more disabled and have lower levels of satisfaction with conventional healthcare.

The expected future course of the disease depends on the subtype of the disease; the individual's sex, age, and initial symptoms; and the degree of disability the person has. Female sex, relapsing-remitting subtype, optic neuritis or sensory symptoms at onset, few attacks in the initial years and especially early age at onset, are associated with a better course.

The average life expectancy is 30 years from the start of the disease, which is 5 to 10 years less than that of unaffected people. Almost 40% of people with MS reach the seventh decade of life. Nevertheless, two-thirds of the deaths are directly related to the consequences of the disease. Suicide is more common, while infections and other complications are especially dangerous for the more disabled. Although most people lose the ability to walk before death, 90% are capable of independent walking at 10 years from onset, and 75% at 15 years. 

MS is the most common autoimmune disorder of the central nervous system. As of 2010, the number of people with MS was 2–2.5 million (approximately 30 per 100,000) globally, with rates varying widely in different regions. It is estimated to have resulted in 18,000 deaths that year. In Africa rates are less than 0.5 per 100,000, while they are 2.8 per 100,000 in South East Asia, 8.3 per 100,000 in the Americas, and 80 per 100,000 in Europe. Rates surpass 200 per 100,000 in certain populations of Northern European descent. The number of new cases that develop per year is about 2.5 per 100,000.

Rates of MS appear to be increasing; this, however, may be explained simply by better diagnosis. Studies on populational and geographical patterns have been common and have led to a number of theories about the cause.

MS usually appears in adults in their late twenties or early thirties but it can rarely start in childhood and after 50 years of age. The primary progressive subtype is more common in people in their fifties. Similar to many autoimmune disorders, the disease is more common in women, and the trend may be increasing. As of 2008, globally it is about two times more common in women than in men. In children, it is even more common in females than males, while in people over fifty, it affects males and females almost equally.

Robert Carswell (1793–1857), a British professor of pathology, and Jean Cruveilhier (1791–1873), a French professor of pathologic anatomy, described and illustrated many of the disease's clinical details, but did not identify it as a separate disease. Specifically, Carswell described the injuries he found as "a remarkable lesion of the spinal cord accompanied with atrophy". Under the microscope, Swiss pathologist Georg Eduard Rindfleisch (1836–1908) noted in 1863 that the inflammation-associated lesions were distributed around blood vessels.

The French neurologist Jean-Martin Charcot (1825–1893) was the first person to recognize multiple sclerosis as a distinct disease in 1868. Summarizing previous reports and adding his own clinical and pathological observations, Charcot called the disease "sclerose en plaques".

The first attempt to establish a set of diagnostic criteria was also due to Charcot in 1868. He published what now is known as the "Charcot Triad", consisting in nystagmus, intention tremor, and telegraphic speech (scanning speech) Charcot also observed cognition changes, describing his patients as having a "marked enfeeblement of the memory" and "conceptions that formed slowly".

Diagnosis was based on Charcot triad and clinical observation until Schumacher made the first attempt to standardize criteria in 1965 by introducing some fundamental requirements: Dissemination of the lesions in time (DIT) and space (DIS), and that "signs and symptoms cannot be explained better by another disease process". Both requirements were later inherited by Poser criteria and McDonald criteria, whose 2010 version is currently in use.

During the 20th century, theories about the cause and pathogenesis were developed and effective treatments began to appear in the 1990s. Since the beginning of the 21st century, refinements of the concepts have taken place. The 2010 revision of the McDonald criteria allowed for the diagnosis of MS with only one proved lesion (CIS). Subsequently, three years later, the 2013 revision of the "phenotypes for the disease course" were forced to consider CIS as one of the phenotypes of MS, making obsolete some expressions like "conversion from CIS to MS".

There are several historical accounts of people who probably had MS and lived before or shortly after the disease was described by Charcot.

A young woman called Halldora who lived in Iceland around 1200 suddenly lost her vision and mobility but, after praying to the saints, recovered them seven days after. Saint Lidwina of Schiedam (1380–1433), a Dutch nun, may be one of the first clearly identifiable people with MS. From the age of 16 until her death at 53, she had intermittent pain, weakness of the legs, and vision loss—symptoms typical of MS. Both cases have led to the proposal of a "Viking gene" hypothesis for the dissemination of the disease.

Augustus Frederick d'Este (1794–1848), son of Prince Augustus Frederick, Duke of Sussex and Lady Augusta Murray and the grandson of George III of the United Kingdom, almost certainly had MS. D'Este left a detailed diary describing his 22 years living with the disease. His diary began in 1822 and ended in 1846, although it remained unknown until 1948. His symptoms began at age 28 with a sudden transient visual loss (amaurosis fugax) after the funeral of a friend. During his disease, he developed weakness of the legs, clumsiness of the hands, numbness, dizziness, bladder disturbances, and erectile dysfunction. In 1844, he began to use a wheelchair. Despite his illness, he kept an optimistic view of life. Another early account of MS was kept by the British diarist W. N. P. Barbellion, nom-de-plume of Bruce Frederick Cummings (1889–1919), who maintained a detailed log of his diagnosis and struggle. His diary was published in 1919 as "The Journal of a Disappointed Man".

There is ongoing research looking for more effective, convenient, and tolerable treatments for relapsing-remitting MS; creation of therapies for the progressive subtypes; neuroprotection strategies; and effective symptomatic treatments.

During the 2000s and 2010s, there has been approval of several oral drugs that are expected to gain in popularity and frequency of use. Several more oral drugs are under investigation, including ozanimod, laquinimod, and estriol. Laquinimod was announced in August 2012 and is in a third phase III trial after mixed results in the previous ones. Similarly, studies aimed to improve the efficacy and ease of use of already existing therapies are occurring. This includes the use of new preparations such as the PEGylated version of interferon-β-1a, which it is hoped may be given at less frequent doses with similar effects. Estriol, a female sex hormone found at high concentrations during late pregnancy, has been identified as a candidate therapy for women with relapsing-remitting MS and has progressed through Phase II trials. Request for approval of "peginterferon beta-1a" is expected during 2013.

Monoclonal antibodies have also raised high levels of interest. As of 2012 alemtuzumab, daclizumab, and CD20 monoclonal antibodies such as rituximab, ocrelizumab and ofatumumab had all shown some benefit and were under study as potential treatments, and the FDA approved ocrelizumab for relapsing and primary MS in March 2017. Their use has also been accompanied by the appearance of potentially dangerous adverse effects, the most important of which being opportunistic infections. Related to these investigations is the development of a test for JC virus antibodies, which might help to determine who is at greater risk of developing progressive multifocal leukoencephalopathy when taking natalizumab. While monoclonal antibodies will probably have some role in the treatment of the disease in the future, it is believed that it will be small due to the risks associated with them.

Another research strategy is to evaluate the combined effectiveness of two or more drugs. The main rationale for using a number of medications in MS is that the involved treatments target different mechanisms and, therefore, their use is not necessarily exclusive. Synergies, in which one drug improves the effect of another are also possible, but there can also be drawbacks such as the blocking of the action of the other or worsened side-effects. There have been several trials of combined therapy, yet none have shown positive enough results to be considered as a useful treatment for MS.

Research on neuroprotection and regenerative treatments, such as stem cell therapy, while of high importance, are in the early stages. Likewise, there are not any effective treatments for the progressive variants of the disease. Many of the newest drugs as well as those under development are probably going to be evaluated as therapies for PPMS or SPMS.

MS is a clinically defined entity with several atypical presentations. Some auto-antibodies have been found in atypical MS cases, giving birth to separate disease families and restricting the previously wider concept of MS.

First of all, anti-AQP4 autoantibodies were found in neuromyelitis optica (NMO), which was previously considered a MS variant. After that, a whole spectrum of diseases named NMOSD (NMO spectrum diseases) or anti-AQP4 diseases has been accepted.

Later, it was found that some cases of MS were presenting anti-MOG autoantibodies, mainly overlapping with the Marburg variant. Anti-MOG autoantibodies were found to be also present in ADEM, and now a second spectrum of separated diseases is being considered. At this moment, it is named inconsistently across different authors, but it is normally something similar to anti-MOG demyelinating diseases.

Finally, a third kind of auto-antibodies is accepted. They are several anti-neurofascin auto-antibodies which damage the Ranvier nodes of the neurones. These antibodies are more related to the peripheral nervous demyelination, but they were also found in chronic progressive PPMS and combined central and peripheral demyelination (CCPD, which is considered another atypical MS presentation). 

Besides all this autoantibodies found, four different patterns of demyelination have been reported in MS, opening the door to consider MS as an heterogeneous disease.

While diagnostic criteria are not expected to change in the near future, work to develop biomarkers that help with diagnosis and prediction of disease progression is ongoing. New diagnostic methods that are being investigated include work with anti-myelin antibodies, and studies with serum and cerebrospinal fluid, but none of them has yielded reliably positive results.

At the current time, there are no laboratory investigations that can predict prognosis. Several promising approaches have been proposed including: interleukin-6, nitric oxide and nitric oxide synthase, osteopontin, and fetuin-A. Since disease progression is the result of degeneration of neurons, the roles of proteins showing loss of nerve tissue such as neurofilaments, tau, and N-acetylaspartate are under investigation. Other effects include looking for biomarkers that distinguish between those who will and will not respond to medications.

Improvement in neuroimaging techniques such as positron emission tomography (PET) or magnetic resonance imaging (MRI) carry a promise for better diagnosis and prognosis predictions, although the effect of such improvements in daily medical practice may take several decades. Regarding MRI, there are several techniques that have already shown some usefulness in research settings and could be introduced into clinical practice, such as double-inversion recovery sequences, magnetization transfer, diffusion tensor, and functional magnetic resonance imaging. These techniques are more specific for the disease than existing ones, but still lack some standardization of acquisition protocols and the creation of normative values. There are other techniques under development that include contrast agents capable of measuring levels of peripheral macrophages, inflammation, or neuronal dysfunction, and techniques that measure iron deposition that could serve to determine the role of this feature in MS, or that of cerebral perfusion. Similarly, new PET radiotracers might serve as markers of altered processes such as brain inflammation, cortical pathology, apoptosis, or remylienation. Antibiodies against the Kir4.1 potassium channel may be related to MS.

In 2008, vascular surgeon Paolo Zamboni suggested that MS involves narrowing of the veins draining the brain, which he referred to as chronic cerebrospinal venous insufficiency (CCSVI). He found CCSVI in all patients with MS in his study, performed a surgical procedure, later called in the media the "liberation procedure" to correct it, and claimed that 73% of participants improved. This theory received significant attention in the media and among those with MS, especially in Canada. Concerns have been raised with Zamboni's research as it was neither blinded nor controlled, and its assumptions about the underlying cause of the disease are not backed by known data. Also, further studies have either not found a similar relationship or found one that is much less strong, raising serious objections to the hypothesis. The "liberation procedure" has been criticized for resulting in serious complications and deaths with unproven benefits. It is, thus, as of 2013 not recommended for the treatment of MS. Additional research investigating the CCSVI hypothesis are under way.




</doc>
<doc id="50693" url="https://en.wikipedia.org/wiki?curid=50693" title="Mortimer Wheeler">
Mortimer Wheeler

Sir Robert Eric Mortimer Wheeler (10 September 1890 – 22 July 1976) was a British archaeologist and officer in the British Army. Over the course of his career, he served as Director of both the National Museum of Wales and London Museum, Director-General of the Archaeological Survey of India, and the founder and Honorary Director of the Institute of Archaeology in London, in addition to writing twenty-four books on archaeological subjects.

Born in Glasgow to a middle-class family, Wheeler was raised largely in Yorkshire before relocating to London in his teenage years. After studying Classics at University College London (UCL), he began working professionally in archaeology, specializing in the Romano-British period. During World War I he volunteered for service in the Royal Artillery, being stationed on the Western Front, where he rose to the rank of major and was awarded the Military Cross. Returning to Britain, he obtained his doctorate from UCL before taking on a position at the National Museum of Wales, first as Keeper of Archaeology and then as Director, during which time he oversaw excavation at the Roman forts of Segontium, Y Gaer, and Isca Augusta with the aid of his first wife, Tessa Wheeler. Influenced by the archaeologist Augustus Pitt Rivers, Wheeler argued that excavation and the recording of stratigraphic context required an increasingly scientific and methodical approach, developing the "Wheeler Method". In 1926, he was appointed Keeper of the London Museum; there, he oversaw a reorganisation of the collection, successfully lobbied for increased funding, and began lecturing at UCL.

In 1934, he established the Institute of Archaeology as part of the federal University of London, adopting the position of Honorary Director. In this period, he oversaw excavations of the Roman sites at Lydney Park and Verulamium and the Iron Age hill fort of Maiden Castle. During World War II, he re-joined the Armed Forces and rose to the rank of brigadier, serving in the North African Campaign and then the Allied invasion of Italy. In 1944 he was appointed Director-General of the Archaeological Survey of India, through which he oversaw excavations of sites at Harappa, Arikamedu, and Brahmagiri, and implemented reforms to the subcontinent's archaeological establishment. Returning to Britain in 1948, he divided his time between lecturing for the Institute of Archaeology and acting as archaeological adviser to Pakistan's government. In later life, his popular books, cruise ship lectures, and appearances on radio and television, particularly the BBC series "Animal, Vegetable, Mineral?", helped to bring archaeology to a mass audience. Appointed Honorary Secretary of the British Academy, he raised large sums of money for archaeological projects, and was appointed British representative for several UNESCO projects.

Wheeler is recognised as one of the most important British archaeologists of the twentieth century, responsible for successfully encouraging British public interest in the discipline and advancing methodologies of excavation and recording. Furthermore, he is widely acclaimed as a major figure in the establishment of South Asian archaeology. However, many of his specific interpretations of archaeological sites have been discredited or reinterpreted and he was often criticised for bullying colleagues and sexually harassing young women.

Mortimer Wheeler was born on 10 September 1890 in the city of Glasgow, Scotland. He was the first child of the journalist Robert Mortimer Wheeler and his second wife Emily Wheeler ("née" Baynes). The son of a tea merchant based in Bristol, in youth Robert had considered becoming a Baptist minister, but instead became a staunch freethinker while studying at the University of Edinburgh. Initially working as a lecturer in English literature Robert turned to journalism after his first wife died in childbirth. His second wife, Emily, shared her husband's interest in English literature, and was the niece of Thomas Spencer Baynes, a Shakespearean scholar at St. Andrews University. Their marriage was emotionally strained, a situation exacerbated by their financial insecurity. Within two years of their son's birth, the family moved to Edinburgh, where a daughter named Amy was born. The couple gave their two children nicknames, with Mortimer being "Boberic" and Amy being "Totsy".

When Wheeler was four, his father was appointed chief leader writer for the "Bradford Observer". The family relocated to Saltaire, a village northwest of Bradford, a cosmopolitan city in Yorkshire, northeast England, which was then in the midst of the wool trade boom. Wheeler was inspired by the moors surrounding Saltaire and fascinated by the area's archaeology. He later wrote about discovering a late prehistoric cup-marked stone, searching for lithics on Ilkley Moor, and digging into a barrow on Baildon Moor. Although suffering from ill health, Emily Wheeler taught her two children with the help of a maid up to the age of seven or eight. Mortimer remained emotionally distant from his mother, instead being far closer to his father, whose company he favoured over that of other children. His father had a keen interest in natural history and a love of fishing and shooting, rural pursuits in which he encouraged Mortimer to take part. Robert acquired many books for his son, particularly on the subject of art history, with Wheeler loving to both read and paint.

In 1899, Wheeler joined Bradford Grammar School shortly before his ninth birthday, where he proceeded straight to the second form. In 1902 Robert and Emily had a second daughter, whom they named Betty; Mortimer showed little interest in this younger sister. In 1905, Robert agreed to take over as head of the London office of his newspaper, by then renamed the "Yorkshire Daily Observer", and so the family relocated to the southeast of the city in December, settling into a house named Carlton Lodge on South Croydon Road, West Dulwich. In 1908 they moved to 14 Rollescourt Avenue in nearby Herne Hill. Rather than being sent for a conventional education, when he was 15 Wheeler was instructed to educate himself by spending time in London, where he frequented the National Gallery and the Victoria and Albert Museum.

After passing the entrance exam on his second attempt, in 1907 Wheeler was awarded a scholarship to read classical studies at University College London (UCL), commuting daily from his parental home to the university campus in Bloomsbury, central London. At UCL, he was taught by the prominent classicist A. E. Housman. During his undergraduate studies, he became editor of the "Union Magazine", for which he produced a number of illustrated cartoons. Increasingly interested in art, he decided to switch from classical studies to a course at UCL's art school, the Slade School of Fine Art; he returned to his previous subject after coming to the opinion that – in his words – he never became more than "a conventionally accomplished picture maker". This interlude had adversely affected his classical studies, and he received a second class BA on graduating.

Wheeler began studying for a Master of Arts degree in classical studies, which he attained in 1912. During this period, he also gained employment as the personal secretary of the UCL Provost Gregory Foster, although he later criticised Foster for transforming the university from "a college in the truly academic sense [into] a hypertrophied monstrosity as little like a college as a plesiosaurus is like a man". It was also at this time of life that he met and began a relationship with Tessa Verney, a student then studying history at UCL, when they were both serving on the committee of the University College Literary Society.

During his studies, Wheeler had developed his love of archaeology, having joined an excavation of Viroconium Cornoviorum, a Romano-British settlement in Wroxeter, in 1913. Considering a profession in the discipline, he won a studentship that had been established jointly by the University of London and the Society of Antiquaries in memory of Augustus Wollaston Franks. The prominent archaeologist Sir Arthur Evans doubled the amount of money that went with the studentship. Wheeler's proposed project had been to analyse Romano-Rhenish pottery, and with the grant he funded a trip to the Rhineland in Germany, there studying the Roman pottery housed in local museums; his research into this subject was never published.

At this period, there were very few jobs available within British archaeology; as the later archaeologist Stuart Piggott related, "the young Wheeler was looking for a professional job where the profession had yet to be created." In 1913 Wheeler secured a position as junior investigator for the English Royal Commission on Historical Monuments, who were embarking on a project to assess the state of all structures in the nation that pre-dated 1714. As part of this, he was first sent to Stebbing in Essex to assess Late Medieval buildings, although once that was accomplished he focused on studying the Romano-British remains of that county. In summer 1914 he married Tessa in a low-key, secular wedding ceremony, before they moved into Wheeler's parental home in Herne Hill.

After the United Kingdom's entry into World War I in 1914, Wheeler volunteered for the armed forces. Although preferring solitary to group activities, Wheeler found that he greatly enjoyed soldiering, and on 9 November 1914 was commissioned a temporary second lieutenant in the University of London Officer Training Corps, serving with its artillery unit as an instructor. It was during this period, in January 1915, that a son was born to the Wheelers, and named Michael. Michael Wheeler was their only child, something that was a social anomaly at the time, although it is unknown if this was by choice or not. In May 1915, Wheeler transferred to the 1st Lowland Brigade of the Royal Field Artillery (Territorial Force), and was confirmed in his rank on 1 July, with a promotion to temporary lieutenant from the same date. Shortly thereafter, on 16 July, Wheeler was promoted to temporary captain. In this position he was stationed at various bases across Britain, often bringing his wife and child with him; his responsibility was as a battery commander, initially of field guns and later of howitzers.

In October 1917 Wheeler was posted to the 76th Army Field Artillery Brigade, one of the Royal Field Artillery brigades under the direct control of the General Officer Commanding, Third Army. The brigade was then stationed in Belgium, where it had been engaged in the Battle of Passchendaele against German troops along the Western Front. By now a substantive lieutenant (temporary captain), on 7 October he was appointed second-in-command of an artillery battery with the acting rank of captain, but within a fortnight became commander of a battery with the acting rank of major, replacing a major who had been poisoned by mustard gas. He was part of the Left Group of artillery covering the advancing Allied infantry in the battle. Throughout, he maintained correspondences with his wife, his sister Amy, and his parents. After the Allied victory in the battle, the brigade was transferred to Italy.

Wheeler and the brigade arrived in Italy on 20 November, and proceeded through the Italian Riviera to reach Caporetto, where it had been sent to bolster the Italian troops against a German and Austro-Hungarian advance. As the Russian Republic removed itself from the war, the German Army refocused its efforts on the Western Front, and so in March 1918 Wheeler's brigade was ordered to leave Italy, getting a train from Castelfranco to Vieux Rouen in France. Back on the Western Front, the brigade was assigned to the 2nd Division, again part of Julian Byng's Third Army, reaching a stable area of the front in April. Here, Wheeler was engaged in artillery fire for several months, before the British went on the offensive in August. On 24 August, in between the ruined villages of Achiet and Sapignies, he led an expedition which captured two German field guns while under heavy fire from a castle mound; he was later awarded the Military Cross for this action:

Wheeler continued as part of the British forces pushing westward until the German surrender in November 1918, receiving a mention in dispatches on 8 November. He was not demobilised for several months, instead being stationed at Pulheim in Germany until March; during this time he wrote up his earlier research on Romano-Rhenish pottery, making use of access to local museums, before returning to London in July 1919. Reverting to his permanent rank of lieutenant on 16 September, Wheeler was finally discharged from service on 30 September 1921, retaining the rank of major.

On returning to London, Wheeler moved into a top-floor flat near Gordon Square with his wife and child. He returned to working for the Royal Commission, examining and cataloguing the historic structures of Essex. In doing so, he produced his first publication, an academic paper on Colchester's Roman Balkerne Gate which was published in the "Transactions of the Essex Archaeological Society" in 1920. He soon followed this with two papers in the "Journal of Roman Studies"; the first offered a wider analysis of Roman Colchester, while the latter outlined his discovery of the vaulting for the city's Temple of Claudius which was destroyed by Boudica's revolt. In doing so, he developed a reputation as a Roman archaeologist in Britain. He then submitted his research on Romano-Rhenish pots to the University of London, on the basis of which he was awarded his Doctorate of Letters; thenceforth until his knighthood he styled himself as Dr Wheeler. He was unsatisfied with his job in the Commission, unhappy that he was receiving less pay and a lower status than he had had in the army, and so began to seek out alternative employment.

He obtained a post as the Keeper of Archaeology at the National Museum of Wales, a job that also entailed becoming a lecturer in archaeology at the University College of South Wales and Monmouthshire. Taking up this position, he moved to Cardiff with his family in August 1920, although he initially disliked the city. The museum was in disarray; prior to the war, construction had begun on a new purpose-built building to house the collections. This had ceased during the conflict and the edifice was left abandoned during Cardiff's post-war economic slump. Wheeler recognised that Wales was very divided regionally, with many Welsh people having little loyalty to Cardiff; thus, he made a point of touring the country, lecturing to local societies about archaeology. According to the later archaeologist Lydia C. Carr, the Wheelers' work for the cause of the museum was part of a wider "cultural-nationalist movement" linked to growing Welsh nationalism during this period; for instance, the Welsh nationalist party Plaid Cymru was founded in 1925.

Wheeler was impatient to start excavations, and in July 1921 started a six-week project to excavate at the Roman fort of Segontium; accompanied by his wife, he used up his holiday to oversee the project. A second season of excavation at the site followed in 1922. Greatly influenced by the writings of the archaeologist Augustus Pitt-Rivers, Wheeler emphasised the need for a strong, developed methodology when undertaking an archaeological excavation, believing in the need for strategic planning, or what he termed "controlled discovery", with clear objectives in mind for a project. Further emphasising the importance of prompt publication of research results, he wrote full seasonal reports for "Archaeologia Cambrensis" before publishing a full report, "Segontium and the Roman Occupation of Wales". Wheeler was keen on training new generations of archaeologists, and two of the most prominent students to excavate with him at Segontium were Victor Nash-Williams and Ian Richmond.

Over the field seasons of 1924 and 1925, Wheeler ran excavations of the Roman fort of Y Gaer near Brecon, a project aided by his wife and two archaeological students, Nowell Myres and Christopher Hawkes. During this project, he was visited by the prominent Egyptologist Sir Flinders Petrie and his wife Hilda Petrie; Wheeler greatly admired Petrie's emphasis on strong archaeological methodologies. Wheeler published the results of his excavation in "The Roman Fort Near Brecon". He then began excavations at Isca Augusta, a Roman site in Caerleon, where he focused on revealing the Roman amphitheatre. Intent on attracting press attention to both raise public awareness of archaeology and attract new sources of funding, he contacted the press and organised a sponsorship of the excavation by the middle-market newspaper the "Daily Mail". In doing so, he emphasised the folkloric and legendary associations that the site had with King Arthur. In 1925, Oxford University Press published Wheeler's first book for a general audience, "Prehistoric and Roman Wales"; he later expressed the opinion that it was not a good book.

In 1924, the Director of the National Museum of Wales, William Evans Hoyle, resigned amid ill health. Wheeler applied to take on the role of his replacement, providing supportive testimonials from Charles Reed Peers, Robert Bosanquet, and H. J. Fleure. Although he had no prior museum experience, he was successful in his application and was appointed Director. He then employed a close friend, Cyril Fox, to take on the vacated position of Keeper of Archaeology. Wheeler's proposed reforms included extending the institution's reach and influence throughout Wales by building affiliations with regional museums, and focusing on fundraising to finance the completion of the new museum premises. He obtained a £21,367 donation from the wealthy shipowner William Reardon Smith and appointed Smith to be the museum's treasurer, and also travelled to Whitehall, London, where he successfully urged the British Treasury to provide further funding for the museum. As a result, construction on the museum's new building was able to continue, and it was officially opened by King George V in 1927.

Upon the retirement of the Keeper of the London Museum, Harmon Oates, Wheeler was invited to fill the vacancy. He had been considering a return to London for some time and eagerly agreed, taking on the post, which was based at Lancaster House in the St James's area, in July 1926. In Wales, many felt that Wheeler had simply taken the directorship of the National Museum to advance his own career prospects, and that he had abandoned them when a better offer came along. Wheeler himself disagreed, believing that he had left Fox at the Museum as his obvious successor, and that the reforms he had implemented would therefore continue. The position initially provided Wheeler with an annual salary of £600, which resulted in a decline in living standards for his family, who moved into a flat near to Victoria Station.

Tessa's biographer L.C. Carr later commented that together, the Wheelers "professionalized the London Museum". Wheeler expressed his opinion that the museum "had to be cleaned, expurgated, and catalogued; in general, turned from a junk shop into a tolerably rational institution". Focusing on reorganising the exhibits and developing a more efficient method of cataloguing the artefacts, he also authored "A Short Guide to the Collections", before using the items in the museum to write three books: "London and the Vikings", "London and the Saxons", and "London and the Romans". Upon his arrival, the Treasury allocated the museum an annual budget of £5,000, which Wheeler deemed insufficient for its needs. In 1930, Wheeler persuaded them to increase that budget, as he highlighted increasing visitor numbers, publications, and acquisitions, as well as a rise in the number of educational projects. With this additional funding, he was able to employ more staff and increase his own annual salary to £900.

Soon after joining the museum, Wheeler was elected to the council of the Society of Antiquaries. Through the Society, he became involved in the debate as to who should finance archaeological supervision of building projects in Greater London; his argument was that the City of London Corporation should provide the funding, although in 1926 it was agreed that the Society itself would employ a director of excavation based in Lancaster House to take on the position. Also involved in the largely moribund Royal Archaeological Institute, Wheeler organised its relocation to Lancaster House. In 1927, Wheeler took on an unpaid lectureship at University College London, where he established a graduate diploma course on archaeology; one of the first to enroll was Stuart Piggott. In 1928, Wheeler curated an exhibit at UCL on "Recent Work in British Archaeology", for which he attracted much press attention.

Wheeler was keen to continue archaeological fieldwork outside London, undertaking excavations every year from 1926 to 1939. After completing his excavation of the Carlaeon amphitheatre in 1928, he began fieldwork at the Roman settlement and temple in Lydney Park, Gloucestershire, having been invited to do so by the aristocratic landowner, Charles Bathurst. It was during these investigations that Wheeler personally discovered the Lydney Hoard of coinage. Wheeler and his wife jointly published their excavation report in 1932 as "Report on the Excavation of the Prehistoric, Roman and Post-Roman Site in Lydney Park, Gloucestershire", which Piggott noted had "set the pattern" for all Wheeler's future excavation reports.

From there, Wheeler was invited to direct a Society of Antiquaries excavation at the Roman settlement of Verulamium, which existed on land recently acquired by the Corporation of St Albans. He took on this role for four seasons from 1930 to 1933, before leaving a fifth season of excavation under the control of the archaeologist Kathleen Kenyon and the architect A. W. G. Lowther. Wheeler enjoyed the opportunity to excavate at a civilian as opposed to military site, and also liked its proximity to his home in London. He was particularly interested in searching for a pre-Roman Iron Age oppidum at the site, noting that the existence of a nearby Catuvellauni settlement was attested to in both classical texts and numismatic evidence. With Wheeler focusing his attention on potential Iron Age evidence, Tessa concentrated on excavating the inside of the city walls; Wheeler had affairs with at least three assistants during the project. After Tessa wrote two interim reports, the final excavation report was finally published in 1936 as "Verulamium: A Belgic and Two Roman Cities", jointly written by Wheeler and his wife. The report resulted in the first major published criticism of Wheeler, produced by the young archaeologist Nowell Myres in a review for "Antiquity"; although stating that there was much to praise about the work, he critiqued Wheeler's selective excavation, dubious dating, and guesswork. Wheeler responded with a piece in which he defended his work and launched a personal attack on both Myres and Myres's employer, Christ Church, Oxford.

Wheeler had long desired to establish an academic institution devoted to archaeology that could be based in London. He hoped that it could become a centre in which to establish the professionalisation of archaeology as a discipline, with systematic training of students in methodological techniques of excavation and conservation and recognised professional standards; in his words, he hoped "to convert archaeology into a discipline worthy of that name in all senses". He further described his intention that the Institute should become "a laboratory: a laboratory of archaeological science".
Many archaeologists shared his hopes, and to this end Petrie had donated much of his collection of Near Eastern artefacts to Wheeler, in the hope that it would be included in such an institution. Wheeler was later able to persuade the University of London, a federation of institutions across the capital, to support the venture, and both he and Tessa began raising funds from wealthy backers. In 1934, the Institute of Archaeology was officially opened, albeit at this point without premises or academic staff; the first students to enroll were Rachel Clay and Barbara Parker, who went on to have careers in the discipline. While Wheeler – who was still Keeper of the London Museum – took on the role of Honorary Director of the Institute, he installed the archaeologist Kathleen Kenyon as secretary of the Management Committee, describing her as "a level-headed person, with useful experience".

After ending his work at Verulamium, Wheeler turned his attention to the late Iron Age hill-fort of Maiden Castle near to Dorchester, Dorset, where he excavated for four seasons from 1934 to 1937. Co-directed by Wheeler, Tessa, and the Curator of Dorset County Museum, Charles Drew, the project was carried out under the joint auspices of the Society of Antiquaries and the Dorset Field Club. With around 100 assistants each season, the dig constituted the largest excavation that had been conducted in Britain up to that point, with Wheeler organising weekly meetings with the press to inform them about any discoveries. He was keen to emphasise that his workforce consisted of many young people as well as both men and women, thus presenting the image of archaeology as a modern and advanced discipline. According to the later historian Adam Stout, the Maiden Castle excavation was "one of the most famous British archaeological investigations of the twentieth century. It was the classic 'Wheeler dig', both in terms of scale of operations and the publicity which it generated." 

Wheeler's excavation report was published in 1943 as "Maiden Castle, Dorset". The report's publication allowed further criticism to be voiced of Wheeler's approach and interpretations; in his review of the book, the archaeologist W. F. Grimes criticised the highly selective nature of the excavation, noting that Wheeler had not asked questions regarding the socio-economic issues of the community at Maiden Castle, aspects of past societies that had come to be of increasing interest to British archaeology. Over coming decades, as further excavations were carried out at the site and archaeologists developed a greater knowledge of Iron Age Britain, much of Wheeler's interpretation of the site and its development was shown to be wrong, in particular by the work of the archaeologist Niall Sharples.

In 1936, Wheeler embarked on a visit to the Near East, sailing from Marseilles to Port Said, where he visited the Old Kingdom tombs of Sakkara. From there he went via Sinai to Palestine, Lebanon, and Syria. During this trip, he visited various archaeological projects, but was dismayed by the quality of their excavations; in particular, he noted that the American-run excavation at Tel Megiddo was adopting standards that had been rejected in Britain twenty-five years previously. He was away for six weeks, and upon his return to Europe discovered that his wife Tessa had died of a pulmonary embolism after a minor operation on her toe. According to Tessa's biographer, for Wheeler this discovery was "the peak of mental misery, and marked the end of his ability to feel a certain kind of love". That winter, his father also died. By the summer of 1937, he had embarked on a new romance, with a young woman named Mavis de Vere Cole, widow of Horace de Vere Cole, who had first met Wheeler when visiting the Maiden Castle excavations with her then-lover, the painter Augustus John. After she eventually agreed to his repeated requests for marriage, the two were wedded early in 1939 in a ceremony held at Caxton Hall, with a reception at Shelley House. They proceeded on a honeymoon to the Middle East.

After a search that had taken several years, Wheeler was able to secure a premises for the Institute of Archaeology: St. John's Lodge in Regent's Park, central London. Left empty since its use as a hospital during the First World War, the building was owned by the Crown and was controlled by the First Commissioner of Works, William Ormsby-Gore; he was very sympathetic to archaeology, and leased the building to the Institute at a low rent. The St. John's Lodge premises were officially opened on 29 April 1937. During his speech at the ceremony, the University of London's Vice-Chancellor Charles Reed Peers made it clear that the building was only intended as a temporary home for the Institute, which it was hoped would be able to move to Bloomsbury, the city's academic hub. In his speech, the university's Chancellor, Alexander Cambridge, 1st Earl of Athlone, compared the new institution to both the Institute of Historical Research and the Courtauld Institute of Art.

Wheeler had also become President of the Museums Association, and in a presidential address given in Belfast talked on the topic of preserving museum collections in war time, believing that Britain's involvement in a second European conflict was imminent. In anticipation of this event, in August 1939 he arranged for the London Museum to place many of its most important collections into safe keeping. He was also awarded an honorary doctorate from Bristol University, and at the award ceremony met the Conservative Party politician Winston Churchill, who was then engaged in writing his multi-volume "A History of the English-Speaking Peoples"; Churchill asked Wheeler to aid him in writing about late prehistoric and early medieval Britain, to which the latter agreed.

After Maiden Castle, Wheeler turned his attention to France, where the archaeological investigation of Iron Age sites had lagged behind developments in Britain. There, he oversaw a series of surveys and excavations with the aid of Leslie Scott, beginning with a survey tour of Brittany in the winter of 1936–37. After this, Wheeler decided to excavate the oppidum at Camp d'Artus, near Huelgoat, Finistère. In addition to bringing many British archaeologists to work on the site, he hired six local Breton workmen to assist the project, coming to the belief that the oppidum had been erected by local Iron Age tribes to defend themselves from the Roman invasion led by Julius Caesar. Meanwhile, Scott had been placed in charge of an excavation at the smaller nearby hill fort of Kercaradec, near Quimper. In July 1939, the project focused its attention on Normandy, with excavations beginning at the Iron Age hill forts of Camp de Canada and Duclair. They were brought to an abrupt halt in September 1939 as the Second World War broke out in Europe, and the team evacuated back to Britain. Wheeler's excavation report, co-written with Katherine Richardson, was eventually published as "Hill-forts of Northern France" in 1957.

Wheeler had been expecting and openly hoping for war with Nazi Germany for a year prior to the outbreak of hostilities; he believed that the United Kingdom's involvement in the conflict would remedy the shame that he thought had been brought upon the country by its signing of the Munich Agreement in September 1938. Volunteering for the armed services, he was assigned to assemble the 48th Light Anti-Aircraft Battery at Enfield, where he set about recruiting volunteers, including his son Michael. As the 48th swelled in size, it was converted into the 42nd Mobile Light Anti-Aircraft Regiment in the Royal Artillery, which consisted of four batteries and was led by Wheeler – now promoted to the rank of colonel – as Commanding Officer. Given the nickname of "Flash Alf" by those serving under him, he was recognised by colleagues as a ruthless disciplinarian and was blamed by many for the death of one of his soldiers from influenza during training. Having been appointed secretary of the Society of Antiquaries in 1939 and then director in 1940, he travelled to London to deal with society affairs on various occasions. In 1941 Wheeler was awarded a Fellowship of the British Academy. Cole had meanwhile entered into an affair with a man named Clive Entwistle, who lambasted Wheeler as "that whiskered baboon". When Wheeler discovered Entwistle in bed with his wife, he initiated divorce proceedings that were finalised in March 1942.

In the summer of 1941, Wheeler and three of his batteries were assigned to fight against German and Italian forces in the North African Campaign. In September, they set sail from Glasgow aboard the RMS "Empress of Russia"; because the Mediterranean was controlled largely by enemy naval forces, they were forced to travel via the Cape of Good Hope, before taking shore leave in Durban. There, Wheeler visited the local kraals to compare them with the settlements of Iron Age Britain. The ship docked in Aden, where Wheeler and his men again took shore leave. They soon reached the British-controlled Suez, where they disembarked and were stationed on the shores of the Great Bitter Lake. There, Wheeler took a brief leave of absence to travel to Jerusalem, where he visited Petrie on his hospital deathbed. Back in Egypt, he gained permission to fly as a front gunner in a Wellington bomber on a bombing raid against Axis forces, to better understand what it was like for aircrew to be fired on by an anti-aircraft battery.

Serving with the Eighth Army, Wheeler was present in North Africa when the Axis armies pushed the Allies back to El Alamein. He was also part of the Allied counter-push, taking part in the Second Battle of El Alamein and the advance on Axis-held Tripoli. On the way he became concerned that the archaeological sites of North Africa were being threatened both by the fighting and the occupying forces. After the British secured control of Libya, Wheeler visited Tripoli and Leptis Magna, where he found that Roman remains had been damaged and vandalised by British troops; he brought about reforms to prevent this, lecturing to the troops on the importance of preserving archaeology, making many monuments out-of-bounds, and ensuring that the Royal Air Force changed its plans to construct a radar station in the midst of a Roman settlement. Aware that the British were planning to invade and occupy the Italian island of Sicily, he insisted that measures be introduced to preserve the historic and archaeological monuments on the island.

Promoted to the rank of brigadier, after the German surrender in North Africa, Wheeler was sent to Algiers where he was part of the staff committee planning the invasion of Italy. There, he learned that the India Office had requested that the army relieve him of his duties to permit him to be appointed Director General of Archaeology in India. Although he had never been to the country, he agreed that he would take the job on the condition that he be permitted to take part in the invasion of Italy first. As intended, Wheeler and his 12th Anti-Aircraft Brigade then took part in the invasion of Sicily and then mainland Italy, where they were ordered to use their anti-aircraft guns to protect the British 10th Corps. As the Allies advanced north through Italy, Wheeler spent time in Naples and then Capri, where he met various aristocrats who had anti-fascist sympathies.

Wheeler left Italy in November 1943 and returned to London. There, he resigned as the director of the London Museum and focused on organising the Institute of Archaeology, preparing it for its adoption of a new director, V. Gordon Childe, after the war. He also resigned as director of the Society of Antiquaries, but was appointed the group's representative to the newly formed Council for British Archaeology. He developed a relationship with a woman named Kim Collingridge, and asked her to marry him. As she was a devout Roman Catholic, he officially converted to the religion, something which shocked many of his friends, who believed that he was being dishonest because he did not genuinely believe in the doctrines of the faith. He then set sail for Bombay aboard a transport ship, the "City of Exeter", in February 1944.

Wheeler arrived in Bombay in the spring of 1944. There, he was welcomed by the city's governor, John Colville, before heading by train to Delhi and then Simla, where the headquarters of the Archaeological Survey of India were located. Wheeler had been suggested for the job by Archibald Wavell, the Viceroy of India, who had been acting on the recommendations of the archaeologist Leonard Woolley, who had authored a report lamenting the state of the archaeological establishment in the British-controlled subcontinent. Wheeler recognised this state of affairs, in a letter to a friend complaining about the lack of finances and equipment, commenting that "We're back in 1850". He initially found much to dislike in India, and in his letters to friends in Britain expressed derogatory and racist sentiments toward Indians: he stated that "they feed wrongly and think wrongly and live wrongly ... I already find myself regarding them as ill-made clockwork toys rather than as human beings, and I find myself bullying them most brutally." He expelled those staff members whom he deemed too idle, and physically beat others in an attempt to motivate them.

From the beginning of his tenure, he sought to distance himself from previous Directors-General and their administrations by criticising them in print and attempting to introduce new staff who had no loyalty to his predecessors. Assigned with a four-year contract, Wheeler attempted to recruit two archaeologists from Britain, Glyn Daniel and Stuart Piggott, to aid him in reforming the Archaeological Survey, although they declined the offer. He then toured the subcontinent, seeking to meet all of the Survey's staff members. He had drawn up a prospectus containing research questions that he wanted the Survey to focus on; these included understanding the period between the Bronze Age Indus Valley Civilization and the Achaemenid Empire, discerning the socio-cultural background to the Vedas, dating the Aryan invasion, and establishing a dating system for southern India prior to the sixth century CE. During his time in office he also achieved a 25 per cent budget increase for the Archaeological Survey, and convinced the government to agree to the construction of a National Museum of Archaeology, to be built in New Delhi.

In October 1944, he opened his six-month archaeological field school in Taxila, where he instructed various students from across India in the methodologies of the discipline. Wheeler became very fond of his students, with one of them, B. B. Lal, later commenting that "behind the gruff exterior, Sir Mortimer had a very kind and sympathetic heart". Throughout his period in India, his students were some of the only individuals to whom Wheeler warmed; more widely, he was annoyed by what he saw as the idleness, incompetence and corruption of Indian society. Initially focusing on the northwest of the subcontinent, Wheeler was particularly fascinated by the Bronze Age Indus Valley Civilization. On his initial inspection of the Indus Valley sites of Mohenjo-daro and Harappa, he organised a very brief excavation which revealed fortifications around both settlements. He later led a more detailed excavation at Harappa, where he exposed further fortifications and established a stratigraphy for the settlement.

Turning his attention to southern India, Wheeler discovered remnants of a Roman amphora in a museum, and began excavations at Arikamedu, revealing a port from the first century CE which had traded in goods from the Roman Empire. The excavation had been plagued by severe rains and tropical heat, although it was during the excavation that World War II ended; in celebration, Wheeler gave all his workers an extra rupee for the day. It has since been alleged that while Wheeler took credit for discovering the significance of this site, it had previously been established by A. Aiyappan, the Superintendent of the Government Museum in Madras, and the French archaeologist Jouveau Dubreuil, with Wheeler intentionally ignoring their contribution. He later undertook excavations of six megalithic tombs in Brahmagiri, Mysore, which enabled him to gain a chronology for the archaeology of much of southern India.

Wheeler established a new archaeological journal, "Ancient India", planning for it to be published twice a year. He had trouble securing printing paper and faced various delays; the first issue was released in January 1946, and he would release three further volumes during his stay. Wheeler married Kim Collingridge in Simla, before he and his wife took part in an Indian Cultural Mission to Iran. The Indian government had deemed Wheeler ideal to lead the group, which departed via train to Zahidan before visiting Persepolis, Tehran, Isfahan, Shiraz, Pasargadae, and Kashan. Wheeler enjoyed the trip, and was envious of Tehran's archaeological museum and library, which was far in advance of anything then found in India. Crossing into Iraq, in Baghdad the team caught a flight back to Delhi. In 1946, he was involved in a second cultural mission, this time to Afghanistan, where he expressed a particular interest in the kingdom of ancient Bactria and visited the archaeology of Balkh.

Wheeler was present during the 1947 Partition of India into the Dominion of Pakistan and the Union of India and the accompanying ethnic violence between Hindu and Muslim communities. He was unhappy with how these events had affected the Archaeological Survey, complaining that some of his finest students and staff were now citizens of Pakistan and no longer able to work for him. He was based in New Delhi when the city was rocked by sectarian violence, and attempted to help many of his Muslim staff members escape from the Hindu-majority city unharmed. He further helped smuggle Muslim families out of the city hospital, where they had taken refuge from a violent Hindu mob. As India neared independence from the British Empire, the political situation had changed significantly; by October 1947 he was one of the last British individuals in a high-up position within the country's governing establishment, and recognised that many Indian nationalists wanted him to also leave.

As their relationship had become increasingly strained, his wife had left and returned to Britain. Although hoping to leave his post in India several months early, he was concerned for his economic prospects, and desperately searched for a new position. Through friends in the British archaeological community, he was offered a job as the Secretary of the Royal Commission on the Ancient and Historical Monuments of Wales, although he was upset that this would mean a drop in his professional status and income and decided to turn it down. Instead, he agreed to take up a chair in the Archaeology of the Roman Provinces at the Institute of Archaeology. In addition, the Pakistani Minister of Education invited him to become the Archaeological Adviser to the Pakistani government; he agreed to also take up this position, on the condition that he would only spend several months in the country each year over the next three.

Returning to London, Wheeler moved into the Hallam Street flat where his son and daughter-in-law were living. Wheeler and the latter disliked each other, and so in summer 1950 he moved out and began renting an apartment in Mount Street. A year later he moved into his wife's house in Mallord Street, in an unsuccessful hope of reigniting their relationship.
Taking up his part-time professorship at the Institute of Archaeology, he began to lecture to students almost every day. There, he found that he developed a relationship of mutual respect with the director, Childe, despite their strong personal and professional differences. In April 1949, after the retirement of Cyril Fox, Wheeler was nominated for the Presidency of the Society of Antiquaries, but lost to James Mann; many archaeologists, including Childe and O. G. S. Crawford, resigned from the Society in protest, deeming Wheeler to have been a far more appropriate candidate for the position. Wheeler was nevertheless elected director of the Society. In 1950 he was awarded the Petrie Medal, and in 1952 was knighted. That same year he was invited to give the Norton lectures for the Archaeological Institute of America, and while in the United States was also awarded the Lucy Wharton Drexel medal at Pennsylvania. He nevertheless disliked the country, and in later life exhibited anti-Americanism.

Wheeler spent three months in the Dominion of Pakistan during early 1949, where he was engaged in organising the fledgling Pakistani Archaeological Department with the aid of former members of the Archaeological Survey and new students whom he recruited. The Minister of Education, Fazlur Rahman, was sympathetic to Wheeler's plans, and the government agreed to establish a National Museum of Pakistan in Karachi, which opened in April 1950. Wheeler himself was appointed the first President of the Pakistani Museums Association, and found himself as a mediator in the arguments between India and Pakistan over the redistribution of archaeological and historic artefacts following the partition. He also wrote a work of archaeological propaganda for the newly formed state, "Five Thousand Years of Pakistan" (1950).

To instruct new Pakistani students in the methods of archaeology, in early 1950 Wheeler ran a training excavation at Mohenjo-daro; there, he was joined by the British student Leslie Alcock, who spoke both Punjabi and Urdu and who was appointed a site supervisor by Wheeler. This excavation proved to be the only one for which Wheeler would not write and publish a full excavation report. Instead, he made reference to its findings in his book "The Indus Civilization", published as part of the series The Cambridge History of India. His relationship with the Pakistani government had become strained, and so he declined to return to work for them for a third year.

Wheeler had been keen to return to excavation in Britain. Based on the one he had organised in India, Wheeler developed an archaeological training course, which he ran at Verulamium in the summer of 1949 to instruct British students in the methodologies of excavation. In summer 1950, he was invited by the Royal Commission on Historical Monuments to direct a trial excavation at Bindon Hill in Dorset. It was a leisurely project which he treated as a seaside holiday. He was invited by the Ancient Monuments Department of the Ministry of Works to excavate the Stanwick Iron Age Fortifications in North Riding, Yorkshire, which he proceeded to do over the summers of 1951 and 1952. Aided by many old friends and colleagues from within the British archaeological scene, he was joined by Alcock and Alcock's wife, among others. Wheeler published his report on the site in 1954.

In 1949 Wheeler was appointed Honorary Secretary of the British Academy after Frederic G. Kenyon stepped down from the position. According to Piggott, the institution had "unhappily drifted into senility without the excuse of being venerable", and Wheeler devoted much time attempting to revitalise the organisation and ensured that Charles Webster was appointed President. Together, Wheeler and Webster sought to increase the number of younger members of the Academy, increasing the number of Fellows who were permitted to join and proposing that those over 75 years of age not be permitted to serve on the organisation's council; this latter measure was highly controversial, and though defeated in 1951, Wheeler and Webster were able to push it through in 1952. In doing so, Piggott stated, Wheeler helped rid the society of its "self-perpetuating gerontocracy". To aid him in these projects, Wheeler employed a personal assistant, Molly Myers, who remained with him for the rest of his life.

In 1956, Wheeler retired from his part-time professorship at the Institute of Archaeology. Childe was also retiring from his position of director that year, and Wheeler involved himself in the arguments surrounding who should replace him. Wheeler vocally opposed the nomination of W.F. Grimes, deeming his career undistinguished; instead, he championed Glyn Daniel as a candidate, although ultimately Grimes was selected. That year, Wheeler's marriage broke down, and he moved from his wife's house to a former brothel at 27 Whitcomb Street in central London. From 1954 to 1959, he served as the President of the Society of Antiquaries, and after resigning supported Ian Richmond as his replacement; however, Joan Evans was selected. From 1964 to 1966 he served as Chairman of the Ancient Monuments Board, stepping down when he concluded that he was too old for the role.
In December 1963, Wheeler underwent a prostate operation that went wrong, and was hospitalised for over a month.
In November 1967, Wheeler became a Companion of Honour, and in 1968 he became a Fellow of the Royal Society.

Wheeler became famous in Britain as "the embodiment of popular archaeology through the medium of television". In 1952, Wheeler was invited to be a panelist on the new BBC television series, "Animal, Vegetable, Mineral?". Based on the American quiz programme "What in the World?", the show was hosted by Glyn Daniel and featured three experts in archaeology, anthropology, and natural history being asked to identify artefacts which had been selected from various museums. However, Wheeler is alleged to have prepared for the show by checking beforehand which objects had been temporarily removed from display. The show proved popular with British audiences, and would air for six more years. It brought Wheeler to public attention, resulting in a Television Personality of the Year award for him in 1954. He also appeared in an episode of "Buried Treasure", an archaeology show also hosted by Daniel, in which the pair travelled to Denmark to discuss Tollund Man. In 1957, he appeared in a second episode of "Buried Treasure", for which he travelled to Pakistan to discuss that nation's archaeology, and in 1958 again appeared in an episode, this time on the site of Great Zimbabwe in Southern Rhodesia. In 1959 he presented his own three-part series on "The Grandeur That Was Rome", for which he travelled to Hadrian's Wall, Pompeii, and Leptis Magna; the show failed to secure high ratings, and was Wheeler's last major foray into television. Meanwhile, he also made appearances on BBC radio, initially featuring on the John Irving series "The Archaeologist", but later presenting his own eight-part series on Roman Britain and also appearing on the series "Asian Club", which was aimed primarily at newly arrived migrants from the Indian subcontinent.

From 1954 onward, Wheeler began to devote an increasing amount of his time to encouraging greater public interest in archaeology, and it was in that year that he obtained an agent. Oxford University Press also published two of his books in 1954. The first was a book on archaeological methodologies, "Archaeology from the Earth", which was translated into various languages. The second was "Rome Beyond the Imperial Frontier", discussing evidence for Roman activity at sites like Arikamedu and Segontium. In 1955 Wheeler released his episodic autobiography, "Still Digging", which had sold over 70,000 copies by the end of the year. In 1959, Wheeler wrote "Early India and Pakistan", which was published as part as Daniel's "Ancient Peoples and Places" series for Thames and Hudson; as with many earlier books, he was criticised for rushing to conclusions.

He authored the section entitled "Ancient India" for Piggott's edited volume "The Dawn of Civilisation", which was published by Thames and Hudson in 1961, before writing an introduction for Roger Wood's photography book "Roman Africa in Colour", which was also published by Thames and Hudson. He then agreed to edit a series for the publisher, known as "New Aspects of Antiquity", through which they released a variety of archaeological works. The rival publisher Weidenfeld & Nicolson had also persuaded Wheeler to work for them, securing him to write many sections of their book, "Splendours of the East". They also published his 1968 book "Flames Over Persepolis", in which Wheeler discussed Persepolis and the Persian Empire in the year that it was conquered by Alexander the Great.

In 1954, the tour company R.K. Swan invited Wheeler to provide lectures on the archaeology of ancient Greece aboard their Hellenic cruise line, which he did in 1955. In 1957, he then gave a guided tour of the archaeology of the Indian subcontinent for the rival tour company Fairways and Swinford. After Swans appointed him to the position of chairman of their Hellenic Cruise division, he made two fortnight tours a year, in spring and summer. In late 1969 he conducted the Swans tour to the Indian subcontinent, visiting the south and east of the republic as well as Ceylon. During this period, Wheeler had kept in contact with many of his friends and colleagues in India and Pakistan, helping to secure them work and funding where possible.

Wheeler had continued his archaeological investigations, and in 1954 led an expedition to the Somme and Pas de Calais where he sought to obtain more information on the French Iron Age to supplement that gathered in the late 1930s. Pakistan's Ministry of Education invited Wheeler to return to their country in October 1956. Here, he undertook test excavations at Charsada to determine a chronology of the site. In 1965, he agreed to take on the position of President of the Camelot Research Committee, which had been established to promote the findings of excavations at Cadbury Castle in Somerset run by his friends Ralegh Radford and Alcock; the project ended in 1970. He also agreed to sit as Chairman of the Archaeological Committee overseeing excavations at York Minster, work which occupied him into the 1970s. Wheeler had also continued his work with museums, campaigning for greater state funding for them. While he had become a trustee of the institution in 1963, he achieved publicity for vocally criticising the British Museum as "a mountainous corpse", lambasting it as being poorly managed and overcrowded with artefacts. The BBC staged a public debate with the museum director Frank Francis.

As Honorary Secretary of the British Academy, Wheeler focused on increasing the organisation's revenues, thus enabling it to expand its remit. He developed personal relationships with various employees at the British Treasury, and offered the Academy's services as an intermediary in dealing with the Egypt Exploration Society, the British School at Athens, the British School at Rome, the British School at Ankara, the British School in Iraq, and the British School at Jerusalem, all of which were then directly funded independently by the Treasury. Accepting this offer, the Treasury agreed to double its funding of the Academy to £5,000 a year. Approaching various charitable foundations, from 1955 Wheeler also secured funding from both the Pilgrim Trust and the Nuffield Foundation, and in 1957 then secured additional funding from the Rockefeller Foundation.

With this additional money, the Academy was able to organise a survey of the state of the humanities and social sciences in the United Kingdom, authoring a report that was published by Oxford University Press in 1961 as "Research in the Humanities and the Social Sciences". On the basis of this report, Wheeler was able to secure a dramatic rise in funding from the British Treasury; they increased their annual grant to £25,000, and promised that this would increase to £50,000 shortly after. According to his later biographer Jacquetta Hawkes, in doing so Wheeler raised the position of the Academy to that of "the main source of official patronage for the humanities" within the United Kingdom, while Piggott stated that he set the organisation upon its "modern course".

To improve Britain's cultural influence abroad, Wheeler had been urging the establishment of a British Institute of History and Archaeology in East Africa, touring East Africa itself in August 1955. In 1956 the Academy requested £6,000 from the Treasury to fund this new institution, to which they eventually agreed in 1959. The Institute was initially established in Dar es Salaam in 1961, although later relocated to Nairobi. Meanwhile, Wheeler had also been campaigning for the establishment of a British Institute of Persian Studies, a project which was supported by the British Embassy in Tehran; they hoped that it would rival the successful French Institute in the city. In 1960, the Treasury agreed, with the new institution being housed on the premises of the University of Tehran. He further campaigned for the establishment of a British Institute in Japan, although these ideas were scrapped amid the British financial crisis of 1967.

Wheeler retained an active interest in the running of these British institutions abroad; in 1967 he visited the British School in Jerusalem amid the Six-Day War between Israel and its Arab neighbours, and in January 1968 visited the Persian institute with the archaeologist Max Mallowan and Mallowan's wife Agatha Christie, there inspecting the excavations at Siraf. In 1969 he proceeded to the Italian city of Rome to inspect the British School there. That year, he resigned as Honorary Secretary of the Academy. The position became a salaried, professional one, with the numismatist Derek Allen taking on the position.

Recognising his stature within the archaeological establishment, the government appointed Wheeler as the British representative on a UNESCO project to undertake a programme of rescue archaeology in the Nile Valley ahead of the construction of the Aswan Dam, which was going to flood large areas of Egypt and Sudan. Personally securing UK funding for the project, he deemed it an issue of national and personal shame when he was unable to persuade the British government to supply additional funding for the relocation of the Abu Simbel temples. In October 1968, he took part in a UNESCO visit to Pakistan to assess the state of Mohenjo-daro, writing the project's report on how the archaeological site could best be preserved. His involvement with UNESCO continued for the rest of his life, and in March 1973 he was invited to the organisation's conference in Paris.

During his final years, Wheeler remained involved in various activities, for instance sitting on the advisory panel of the "Antiquity" journal and the Management Committee of the Royal Archaeological Institute. In March 1971, the archaeologist Barry Cunliffe and a number of his undergraduate students at the University of Southampton organised a conference on the subject of "The Iron Age and its Hillforts" to celebrate Wheeler's eightieth birthday. Wheeler attended the event, whose conference proceedings were published as a festschrift for the octogenarian. In spring 1973, Wheeler returned to BBC television for two episodes of the archaeology-themed series "Chronicle" in which he discussed his life and career. The episodes were well received, and Wheeler became a close friend of the show's producer, David Collison.

In the 1970s, Wheeler became increasingly forgetful and came to rely largely on his assistant, Molly Myres, to organise his affairs. Amid increasing ill health, in September 1973 he moved full-time into Myres's house in Leatherhead, Surrey, although he continued to use his central London flat during day-trips to the city. There, he authored a final book, "My Archaeological Mission to India and Pakistan", although much of the text was culled from his previous publications; it was published by Thames and Hudson in 1976. After suffering a stroke, Wheeler died at Myers' home on 22 July 1976. In memoriam, the British Academy, Royal Academy, and Royal Society flew their flags at half-mast. Wheeler's funeral was held with full military honours at a local crematorium, while a larger memorial service was held in St James's Church, Piccadilly in November.

Wheeler was known as "Rik" among friends. He divided opinion among those who knew him, with some loving and others despising him, and during his lifetime, he was often criticised on both scholarly and moral grounds. The archaeologist Max Mallowan asserted that he "was a delightful, light-hearted and amusing companion, but those close to him knew that he could be a dangerous opponent if threatened with frustration".
His charm offensives were often condemned as being insincere. During excavations, he was known as an authoritarian leader but favoured those who he thought exhibited bravery by standing up to his authority. Hence, he has been termed "a benevolent dictator". He was meticulous in his writings, and would repeatedly revise and rewrite both pieces for publication and personal letters. Throughout his life, he was a heavy smoker.

Wheeler expressed the view that he was "the least political of mortals". Despite not taking a strong interest in politics, Wheeler was described by his biographer as "a natural conservative"; for instance, during his youth he was strongly critical of the Suffragettes and their cause of greater legal rights for women. Nevertheless, he was "usually happy to advance young women professionally", something that may have been based largely on his sexual attraction toward them. He expressed little interest in his relatives; in later life, he saw no reason to have a social relationship with people purely on the basis of family ties.

Wheeler was married three times. In May 1914, Wheeler married Tessa Verney. Tessa became an accomplished archaeologist, and they collaborated until she died in 1936. Their only child, Michael Mortimer Wheeler, was born in January 1915; he became a barrister. Following Tessa's death, in 1939, Wheeler married Mavis de Vere Cole, widow of the prankster Horace de Vere Cole. Their relationship was strained; Cole's diaries revealed that Wheeler physically hit her when she annoyed him. In 1945 Mortimer Wheeler married his third wife, Margaret "Kim" Collingridge. Although they became estranged in 1956, Collingridge's Catholicism prevented divorce. Meanwhile, Wheeler was well known for his conspicuous promiscuity, favouring young women for one-night stands, many of whom were his students. He was further known for having casual sex in public places. That behaviour led to much emotional suffering among his various wives and mistresses of which he was aware. As a result of his behaviour, later archaeologist Gabriel Moshenska informed a reporter from the "Daily Mail" that Wheeler had developed a reputation as "a bit of a groper and a sex pest and an incredible bully as well".

Wheeler has been termed "the most famous British archaeologist of the twentieth century" by archaeologists Gabriel Moshenska and Tim Schadla-Hall. Highlighting his key role in encouraging interest in archaeology throughout British society, they stated that his "mastery of public archaeology was founded on his keen eye for value and a showman's willingness to package
and sell the past". This was an issue about which Wheeler felt very strongly; writing his obituary for the "Biographical Memoirs of Fellows of the Royal Society", the English archaeologist Stuart Piggott noted that Wheeler placed "great importance to the archaeologist's obligation to the public, on whose support the prosecution of his subject ultimately depended."

Piggott believed that Wheeler's greatest impact was as "the great innovator in field techniques", comparing him in this respect to Pitt-Rivers. Piggott stated that the "importance of Wheeler's contribution to archaeological technique, enormous and far-reaching, lies in the fact that in the early 1920s he not only appreciated and understood what Pitt-Rivers had done, but saw that his work could be used as a basis for adaptation, development and improvement." L. C. Carr stated that it was for his methodological developments, oft termed "the Wheeler Method", that Wheeler was best known; in this she contrasted him with those archaeologists who were best known for their associations with a specific archaeological site, such as Arthur Evans and Knossos or Leonard Woolley and Ur.

Wheeler was well known for his publications on archaeological matters; Carr stated that both Wheeler and his first wife emphasised "technical rigour and a full presentation of materials unearthed, as well as a literary discussion of their meaning calculated to appeal to a larger audience." Focusing on Wheeler's publications regarding South Asian archaeology, Sudeshna Guha noted that he "produced an assemblage of image-objects that embodied the precision he demanded from excavation photography."
Mallowan noted that "Immediate and swift presentation of results was more important to him than profound scholarship, although his critical sense made him conscious that it was necessary to maintain high standards and he would approve of nothing that was slipshod." Jacquetta Hawkes commented that he made errors in his interpretation of the archaeological evidence because he was "sometimes too sure of being right, too ready to accept his own authority". She asserted that while Wheeler was not an original thinker, he had "a vision of human history that enabled him to see each discovery of its traces, however small, in its widest significance."

Piggott claimed that Wheeler's appointment as Director-General of the Archaeological Survey of India represented "the most remarkable archaeological achievement of his career, an enormous challenge accepted and surmounted in the autocratic and authoritarian terms within which he could best deploy his powers as administrator and excavator. No other archaeologist of the time, it seems fair to remark, could have come near to attaining his command of incisive strategy and often ruthless tactics which won him the bewildered admiration and touching devotion of his Indian staff." The Indian archaeologist Dilip K. Chakrabarti later stated that Wheeler's accomplishments while in India were "considerable", particularly given the socio-political turmoil of independence and partition. Chakrabarti stated that Wheeler had contributed to South Asian archaeology in various ways: by establishing a "total view" of the region's development from the Palaeolithic onward, by introducing new archaeological techniques and methodologies to the subcontinent, and by encouraging Indian universities to begin archaeological research. Ultimately, Chakrabarti was of the opinion that Wheeler had "prepared the archaeology of the subcontinent for its transition to modernity in the post-Partition period." Similarly, Peter Johansen praised Wheeler for systematising and professionalising Indian archaeology and for "instituting a clearly defined body of techniques and methods for field and laboratory work and training."

On Wheeler's death, H.D. Sankalia of Deccan College, Pune, described him as "well known among Old World archaeologists in the United States", particularly for his book "Archaeology from the Earth" and his studies of the Indus Valley Civilisation. In its 2013 obituary of the English archaeologist Mick Aston, "British Archaeology" magazine – the publication of the Council for British Archaeology – described Aston as "the Mortimer Wheeler of our times" because despite the strong differences between their personalities, both had done much to bring archaeology to the British public. However, writing in 2011, Moshenska and Schadla-Hall asserted that Wheeler's reputation has not undergone significant revision among archaeologists, but that instead he had come to be remembered as "a cartoonish and slightly eccentric figure" whom they termed "Naughty Morty".
Carr described the Institute of Archaeology as "one of the [Wheeler] couple's most permanent memorials."

In 1960, Ronald William Clark published a biography titled "Sir Mortimer Wheeler". FitzRoy Somerset, 4th Baron Raglan reviewed the volume for the journal "Man", describing "this very readable little book" as being "adulatory" in tone, "but hardly more so than its subject deserves." In 1982, the archaeologist Jacquetta Hawkes published a second biography, "Mortimer Wheeler: Adventurer in Archaeology". Hawkes admitted she had developed "a very great liking" for Wheeler, having first met him when she was an archaeology student at the University of Cambridge. She believed that he had "a daemonic energy", with his accomplishments in India being "almost superhuman". Ultimately, she thought of him as being "an epic hero in an anti-heroic age" in which growing social egalitarianism had stifled and condemned aspects of his greatness.

In the 2000 film "Hey Ram", the lead character, Saket Ram (played by Kamal Haasan) and his friend, Amjad Khan (played by Shah Rukh Khan) are shown as employees of Wheeler, who was portrayed by Lewis K. Elbinger, before the 1947 Hindu-Muslim riots. In a 2003 volume of the "South Asian Studies" journal, Sudeshna Gusha published a research article examining Wheeler's use of photography in his excavations and publications in the Indian subcontinent.
In 2011, the academic journal "Public Archaeology" published a research paper by Moshenska and Schadla-Hall that analysed Wheeler's role in presenting archaeology to the British public. Two years later, the "Papers from the Institute of Archaeology" issued a short comic strip by Moshenska and Alex Salamunovich depicting Wheeler's activities in studying the archaeology of Libya during World War II.

A bibliography of Wheeler's published books was included by Piggott in his obituary, and again by Hawkes in her biography.




</doc>
<doc id="50838" url="https://en.wikipedia.org/wiki?curid=50838" title="Manta ray">
Manta ray

Manta rays are large rays belonging to the genus "Manta". The larger species, "M. birostris", reaches in width, while the smaller, "M. alfredi", reaches . Both have triangular pectoral fins, horn-shaped cephalic fins and large, forward-facing mouths. They are classified among the Myliobatiformes (stingrays and relatives) and are placed in the family Myliobatidae (eagle rays).

Mantas are found in warm temperate, subtropical and tropical waters. Both species are pelagic; "M. birostris" migrates across open oceans, singly or in groups, while "M. alfredi" tends to be resident and coastal. They are filter feeders and eat large quantities of zooplankton, which they gather with their open mouths as they swim. However, research suggests that the majority of their diet (73%) actually comes from mesopelagic sources; that is, they are actually deep sea predators, feeding on fish and other organisms that inhabit areas of the sea between below the surface.

Gestation lasts over a year, and mantas give birth to live pups. Mantas may visit cleaning stations for the removal of parasites. Like whales, they breach for unknown reasons.

Both species are listed as vulnerable by the International Union for Conservation of Nature. Anthropogenic threats include pollution, entanglement in fishing nets, and direct harvesting for their gill rakers for use in Chinese medicine. Their slow reproductive rate exacerbates these threats. They are protected in international waters by the Convention on Migratory Species of Wild Animals, but are more vulnerable closer to shore. Areas where mantas congregate are popular with tourists. Only a few public aquariums are large enough to house them.

The name "manta" is Portuguese and Spanish for mantle (cloak or blanket), a type of blanket-shaped trap traditionally used to catch rays. Mantas are known as "devilfish" because of their horn-shaped cephalic fins, which are imagined to give them an "evil" appearance.

Manta rays are members of the order Myliobatiformes which consists of stingrays and their relatives. The genus "Manta" is part of the eagle ray family Myliobatidae, where it is grouped in the subfamily Mobulinae along with the "Mobula" devil rays. In 2017, an analysis of DNA, and to a lesser degree, morphology, found that "Mobula" was paraphyletic with respect to the manta rays, and they recommended treating "Manta" as a junior synonym of "Mobula".

Mantas evolved from bottom-dwelling stingrays, eventually developing more wing-like pectoral fins. "M. birostris" still has a vestigial remnant of a sting barb in the form of a caudal spine. The mouths of most rays lie on the underside of the head, while in mantas, they are right at the front. Manta rays and devil rays are the only ray species that have evolved into filter feeders.

The scientific naming of mantas has had a convoluted history, during which several names were used for both the genus ("Ceratoptera", "Brachioptilon" "Daemomanta", and "Diabolicthys") and species (such as "vampyrus", "americana", "johnii", and "hamiltoni"). All were eventually treated as synonyms of the single species "Manta birostris". The genus name "Manta" was first published in 1829 by Dr Edward Nathaniel Bancroft of Jamaica. The specific name " birostris" is ascribed to Johann Julius Walbaum (1792) by some authorities and to Johann August Donndorff (1798) by others. The specific name "alfredi" was first used by Australian zoologist Gerard Krefft, who named the manta after Prince Alfred.

Authorities were still not in agreement and some argued that the black color morph was a different species from the mostly white morph. This proposal was discounted by a 2001 study of the mitochondrial DNA of both. A 2009 study analyzed the differences in morphology, including color, meristic variation, spine, dermal denticles (tooth-like scales), and teeth of different populations. Two distinct species emerged: the smaller "M. alfredi" found in the Indo-Pacific and tropical east Atlantic, and the larger "M. birostris" found throughout tropical, subtropical and warm temperate oceans. The former is more coastal, while the latter is more ocean-going and migratory. A 2010 study on mantas around Japan confirmed the morphological and genetic differences between "M. birostris" and "M. alfredi".

A third possible species, preliminarily called "Manta "sp. cf. "birostris", reaches at least in width, and inhabits the tropical west Atlantic, including the Caribbean. "M. birostris" an it occur in sympatry.

While some small teeth have been found, few fossilized skeletons of manta rays have been discovered. Their cartilaginous skeletons do not preserve well, as they lack the calcification of the bony fish. Only three sedimentary beds bearing manta ray fossils are known, one from the Oligocene in South Carolina and two from the Miocene and Pliocene in North Carolina. Remains of an extinct species have been found in the Chandler Bridge Formation of South Carolina. These were originally described as "Manta fragilis", but were later reclassified as "Paramobula fragilis".

Manta rays have broad heads, triangular pectoral fins, and horn-shaped cephalic fins located on either side of their mouths. They have horizontally flattened bodies with eyes on the sides of their heads behind the cephalic fins, and gill slits on their ventral surfaces. Their tails lack skeletal support and are shorter than their disc-like bodies. The dorsal fins are small and at the base of the tail. The largest mantas can reach . In both species, the width is about 2.2 times the length of the body; "M. birostris" reaches at least in width, while "M. alfredi" reaches about . Dorsally, mantas are typically black or dark in color with pale markings on their "shoulders". Ventrally, they are usually white or pale with distinctive dark markings by which individual mantas can be recognized. All-black color morphs are known to exist. The skin is covered in mucus which protects it from infection.
The two species of manta differ in color patterns, dermal denticles, and dentition. "M. birostris" has more angular shoulder markings, larger ventral dark spots on the abdominal region, charcoal-colored ventral outlines on the pectoral fins, and a dark colored mouth. The shoulder markings of "M. alfredi" are more rounded, while its ventral spots are located near the posterior end and between the gill slits, and the mouth is white or pale colored. The denticles have multiple cusps and overlap in "M. birostris", while those of "M. alfredi" are evenly spaced and lack cusps. Both species have small, square-shaped teeth on the lower jaw, but "M. birostris" also has enlarged teeth on the upper jaw. Unlike "M. alfredi", "M. birostris" has a caudal spine near its dorsal fin.

Mantas move through the water by the wing-like movements of their pectoral fins, which drive water backwards. Their large mouths are rectangular, and face forward as opposed to other ray and skate species with downward-facing mouths. The spiracles typical of rays are vestigial, and mantas must swim continuously to keep oxygenated water passing over their gills. The cephalic fins are usually spiralled, but flatten during foraging. The fish's gill arches have pallets of pinkish-brown spongy tissue that collect food particles. Mantas track down prey using visual and olfactory senses. They have one of the highest brain-to-body mass ratios and the largest brain size of all fish. Their brains have retia mirabilia which may serve to keep them warm. "M. alfredi" has been shown to dive to depths over , while their relative "Mobula tarapacana", which has a similar structure, dives to nearly ; the retia mirabilia probably serve to prevent their brains from being chilled during such dives into colder subsurface waters.

Mating takes place at different times of the year in different parts of the manta's range. Courtship is difficult to observe in this fast-swimming fish, although mating "trains" with multiple individuals swimming closely behind each other are sometimes seen in shallow water. The mating sequence may be triggered by a full moon and seems to be initiated by a male following closely behind a female while she travels at around . He makes repeated efforts to grasp her pectoral fin with his mouth, which may take 20 to 30 minutes. Once he has a tight grip, he turns upside-down and presses his ventral side against hers. He then inserts one of his claspers into her cloaca, where it remains for 60–90 seconds. The clasper forms a tube which channels sperm from the genital papilla; a siphon propels the seminal fluid into the oviduct. The male continues to grip the female's pectoral fin with his teeth for a further few minutes as both continue to swim, often followed by up to 20 other males. The pair then parts. For some reason, the male almost always grasps the left pectoral fin, and females often have scars that illustrate this.

The fertilized eggs develop within the female's oviduct. At first, they are enclosed in an egg case while the developing embryos absorb the yolk. After hatching, the pups remain in the oviduct and receive additional nutrition from milky secretions. With no umbilical cord or placenta, the unborn pup relies on buccal pumping to obtain oxygen. Brood size is usually one or occasionally two. The gestation period is thought to be 12–13 months. When fully developed, the pup resembles a miniature adult and is expelled from the oviduct with no further parental care. In wild populations, an interval of two years between births may be normal, but a few individuals become pregnant in consecutive years, demonstrating an annual ovulatory cycle. The Okinawa Churaumi Aquarium has had some success in breeding "M. alfredi", with one female giving birth in three successive years. In one of these pregnancies, the gestation period was 372 days and at birth the pup had a width of and weight of . In southern Africa, "M. birostris" males mature at while females reach maturity slightly over that. In Indonesia, "M. birostris" males appear to mature at , while female mature around . In southern Africa, "M. alfredi" matures at widths of for males and for females. In the Maldives, males of "M. alfredi" mature at a width of , while females mature at . In Hawaii, "M. alfredi" matures at a width of for males and for females. Female mantas appear to mature at 8–10 years. Manta rays may live as long as 50 years.

Swimming behavior in mantas differs across habitats: when travelling over deep water, they swim at a constant rate in a straight line, while further inshore, they usually bask or swim idly around. Mantas may travel alone or in groups up to 50. They may associate with other fish species, as well as sea birds and marine mammals. Mantas sometimes breach, leaping partially or entirely out of the water. Individuals in a group may make aerial jumps one after the other. These leaps occur in three forms - forward leaps where the fish lands head first, similar jumps with a tail first re-entry, or somersaults. The reason for breaching is not known; possible explanations include mating rituals, birthing, communication, or the removal of parasites and commensal remoras (suckerfish).
Manta rays are filter feeders as well as predators. 

“We studied the giant manta rays’ diet using biochemical tests, such as stable isotope analysis,” said [Queensland University biologist Katherine] Burgess. “These tests can determine what animals have been eating by examining a piece of tissue from a muscle biopsy from a free-swimming animal.”

These skins tests showed that an average of 27 percent of the giant manta ray’s diet came from surface plankton, and 73 percent came from what scientists call “mesopelagic” sources—a fancy term for fish and other organisms that inhabit areas of the sea between 650 to 3,330 feet (200 to 1,000 meters) below the surface.

They consume large quantities of zooplankton in the form of shrimp, krill, and planktonic crabs. An individual manta eats about 13% of its body weight each week. When foraging, it slowly swims around its prey, herding it into a tight "ball", and then speeds through the bunched organisms with a wide-open mouth. If a ball is particularly dense, a manta may somersault through it. While feeding, mantas flatten their cephalic fins to channel food into their mouths and the small particles are collected by the tissue between the gill arches. As many as 50 individual fish may gather at a single, plankton-rich feeding site. 

Mantas are themselves preyed upon by large sharks and by killer whales. They may also be bitten by cookiecutter sharks, and harbor parasitic copepods.

Mantas visit cleaning stations on coral reefs for the removal of external parasites. The ray adopts a near-stationary position close to the coral surface for several minutes while the cleaner fish consume the attached organisms. Such visits most frequently occur when the tide is high. In Hawaii, wrasses provide the cleaning; some species feed around the manta's mouth and gill slits, while others address the rest of the body surface. In Mozambique, sergeant major fish clean the mouth, while butterflyfishes concentrate on bite wounds. "M. alfredi"
visits cleaning stations more often than "M. birostris". Individual mantas may revisit the same cleaning station or feeding area repeatedly and appear to have cognitive maps of their environment.

Mantas are found in tropical and subtropical waters in all the world's major oceans, and also venture into temperate seas. The furthest from the equator they have been recorded is North Carolina in the United States (31°N) and the North Island of New Zealand (36°S). They prefer water temperatures above and "M. alfredi" is predominantly found in tropical areas. Both species are pelagic. "M. birostris" lives mostly in the open ocean, travelling with the currents and migrating to areas where upwellings of nutrient-rich water increase prey concentrations.

Fish that have been fitted with radio transmitters have travelled as far as from where they were caught, and descended to depths of at least . "M. alfredi" is a more resident and coastal species. Seasonal migrations do occur, but they are shorter than those of "M. birostris". Mantas are common around coasts from spring to fall, but travel further offshore during the winter. They keep close to the surface and in shallow water in daytime, while at night they swim at greater depths.

The greatest threat to manta rays is overfishing. "M. birostris" is not evenly distributed over the oceans, but is concentrated in areas that provide the food resources it requires, while "M. alfredi" is even more localized. Their distributions are thus fragmented, with little evidence of intermingling of subpopulations. Because of their long lifespans and low reproductive rate, overfishing can severely reduce local populations with little likelihood that individuals from elsewhere will replace them.

Both commercial and artisanal fisheries have targeted mantas for their meat and products. They are typically caught with nets, trawls, and harpoons. Mantas were once captured by fisheries in California and Australia for their liver oil and skin; the latter were used as abrasives. Their flesh is edible and is consumed in some countries, but is unattractive compared to other fish. Demand for their gill rakers, the cartilaginous structures protecting the gills, has recently entered Chinese medicine. To fill the growing demand in Asia for gill rakers, targeted fisheries have developed in the Philippines, Indonesia, Mozambique, Madagascar, India, Pakistan, Sri Lanka, Brazil, and Tanzania. Each year, thousands of manta rays, primarily "M. birostris", are caught and killed purely for their gill rakers. A fisheries study in Sri Lanka and India estimated that over 1000 were being sold in the country's fish markets each year. By comparison, "M. birostris" populations at most of the key aggregation sites around the world are estimated to have significantly fewer than 1000 individuals. Targeted fisheries for manta rays in the Gulf of California, the west coast of Mexico, India, Sri Lanka, Indonesia, and the Philippines have reduced populations in these areas dramatically.

Manta rays are subject to other anthropogenic threats. Because mantas must swim constantly to flush oxygen-rich water over their gills, they are vulnerable to entanglement and subsequent suffocation. Mantas cannot swim backwards, and because of their protruding cephalic fins, are prone to entanglement in fishing lines, nets, ghost nets, and even loose mooring lines. When snared, mantas often attempt to free themselves by somersaulting, tangling themselves further. Loose, trailing line can wrap around and cut its way into its flesh, resulting in irreversible injury. Similarly, mantas become entangled in gill nets designed for smaller fish. Some mantas are injured by collision with boats, especially in areas where they congregate and are easily observed. Other threats or factors that may affect manta numbers are climate change, tourism, pollution from oil spills, and the ingestion of microplastics.

In 2011, mantas became strictly protected in international waters because of their inclusion in the Convention on Migratory Species of Wild Animals. The CMS is an international treaty organization concerned with conserving migratory species and habitats on a global scale. Although individual nations were already protecting manta rays, the fish often migrate through unregulated waters, putting them at increased risk from overfishing. The IUCN declared "M. birostris" to be 'vulnerable with an elevated risk of extinction' in November 2011.

In the same year, "M. alfredi" was also classified as vulnerable with local populations of fewer than 1000 individuals and little or no interchange between subpopulations. The Manta Trust is a UK-based charity dedicated to research and conservation efforts for manta rays. The organization's website is also an information resource for manta conservation and biology.

Besides these international initiatives, some countries are taking their own actions. New Zealand has banned the taking of manta rays since the introduction of the Wildlife Act in 1953. In June 1995, the Maldives banned the export of all ray species and their body parts, effectively putting a stop to manta fishing, as there had not previously been a fishery for local consumption. The government reinforced this in 2009 with the introduction of two marine protected areas. In the Philippines, the taking of mantas was banned in 1998, but this was overturned in 1999 under pressure from local fishermen. Fish stocks were surveyed in 2002, and the ban was reintroduced. The taking or killing of mantas in Mexican waters was prohibited in 2007. This ban may not be strictly enforced, but laws are being more rigidly applied at Isla Holbox, an island off the Yucatán Peninsula, where manta rays are used to attract tourists.

In 2009, Hawaii became the first of the United States to introduce a ban on the killing or capturing of manta rays. Previously, no fishery for mantas existed in the state, but migratory fish that pass the islands are now protected. In 2010, Ecuador introduced a law prohibiting all fishing for manta and other rays, their retention as bycatch, and their sale.

The ancient Peruvian Moche people worshipped the sea and its animals. Their art often depicts manta rays. Historically, mantas were feared for their size and power. Sailors believed that they ate fish and could sink boats by pulling on the anchors. This attitude changed around 1978, when divers around the Gulf of California found them to be placid and that they could interact with the animals. Several divers photographed themselves with mantas, including "Jaws" author Peter Benchley.

The Tampa Bay Rays baseball team derives its name from the animals.

Due to their size, mantas are rarely kept in captivity and few aquaria currently display them. One notable individual is "Nandi", a manta ray which was accidentally caught in shark nets off Durban, South Africa, in 2007. Rehabilitated and outgrowing her aquarium at uShaka Marine World, Nandi was moved to the larger Georgia Aquarium in August 2008, where she resides in its 23,848-m (6,300,000-US gal) "Ocean Voyager" exhibit. A second manta ray joined that aquarium's collection in September 2009, and a third was added in 2010.

The Atlantis resort on Paradise Island, Bahamas, hosted a manta named "Zeus" that was used as a research subject for 3 years until it was released in 2008. The Okinawa Churaumi Aquarium also houses manta rays in the "Kuroshio Sea" tank, one of the largest aquarium tanks in the world. The first manta ray birth in captivity took place there in 2007. Although this pup did not survive, the aquarium has since had the birth of three more manta rays in 2008, 2009, and 2010.

Sites at which manta rays congregate attract tourists, and manta viewing generates substantial annual revenue for local communities. Tourist sites exist in the Bahamas, the Cayman Islands, Spain, the Fiji Islands, Thailand, Indonesia, Hawaii, Western Australia and the Maldives. Mantas are popular because of their enormous size and because they are easily habituated to humans. Scuba divers may get a chance to watch mantas visiting cleaning stations and night dives enable viewers to see mantas feeding on plankton attracted by the lights.

Ray tourism benefits locals and visitors by raising awareness of natural resource management and educating them about the animals. It can also provide funds for research and conservation. Constant unregulated interactions with tourists can negatively affect the fish by disrupting ecological relationships and increasing disease transmission. At Bora Bora, an excessive number of swimmers, boaters, and jet skiers caused the local manta ray population to abandon the area.

In 2014, Indonesia brought in a fishing and export ban, as it has realized that manta ray tourism is more economically beneficial than allowing the fish to be killed. A dead manta is worth $40 to $500, while manta ray tourism can bring in $1 million during the life of a single manta ray. Indonesia has 5.8 million m (2.2 million mi) of ocean, and this is now the world's largest sanctuary for manta rays.




</doc>
<doc id="51106" url="https://en.wikipedia.org/wiki?curid=51106" title="Francis Walsingham">
Francis Walsingham

Sir Francis Walsingham ( 1532 – 6 April 1590) was principal secretary to Queen Elizabeth I of England from 20 December 1573 until his death and is popularly remembered as her "spymaster".

Born to a well-connected family of gentry, Walsingham attended Cambridge University and travelled in continental Europe before embarking on a career in law at the age of twenty. A committed Protestant, during the reign of the Catholic Queen Mary I of England he joined other expatriates in exile in Switzerland and northern Italy until Mary's death and the accession of her Protestant half-sister, Elizabeth.

Walsingham rose from relative obscurity to become one of the small coterie who directed the Elizabethan state, overseeing foreign, domestic and religious policy. He served as English ambassador to France in the early 1570s and witnessed the St. Bartholomew's Day massacre. As principal secretary, he supported exploration, colonization, the use of England's maritime strength and the plantation of Ireland. He worked to bring Scotland and England together. Overall, his foreign policy demonstrated a new understanding of the role of England as a maritime Protestant power with intercontinental trading ties. He oversaw operations that penetrated Spanish military preparation, gathered intelligence from across Europe, disrupted a range of plots against Elizabeth and secured the execution of Mary, Queen of Scots.

Francis Walsingham was born in or about 1532, probably at Foots Cray, near Chislehurst, Kent. His parents were William and Joyce Walsingham. William was a successful, well-connected and wealthy London lawyer who died in 1534 and Joyce was the daughter of courtier Sir Edmund Denny and the sister of Sir Anthony Denny, who was the principal gentleman of King Henry VIII's privy chamber. William Walsingham served as a member of the commission that was appointed to investigate the estates of Cardinal Thomas Wolsey in 1530, and his elder brother, Sir Edmund Walsingham, was Lieutenant of the Tower of London. After William's death, Joyce married the courtier Sir John Carey in 1538. Carey's brother William was the husband of Mary Boleyn, Anne Boleyn's elder sister. Of Francis Walsingham's five sisters, Mary married Sir Walter Mildmay, who was Chancellor of the Exchequer for over 20 years, and Elizabeth married the parliamentarian Peter Wentworth.

Francis Walsingham matriculated at King's College, Cambridge, in 1548 with many other Protestants but as an undergraduate of high social status did not sit for a degree. From 1550 or 1551, he travelled in continental Europe, returning to England by 1552 to enrol at Gray's Inn, one of the qualifying bodies for English lawyers.

Upon the death in 1553 of Henry VIII's successor, Edward VI, Edward's Catholic half-sister Mary I became queen. Many wealthy Protestants, such as John Foxe and John Cheke, fled England, and Walsingham was among them. He continued his studies in law at the universities of Basel and Padua, where he was elected to the governing body by his fellow students in 1555.

Mary I died in 1558 and was succeeded by her Protestant half-sister Elizabeth I. Walsingham returned to England and through the support of one of his fellow former exiles, Francis Russell, 2nd Earl of Bedford, he was elected to Elizabeth's first parliament as the member for Bossiney, Cornwall, in 1559. At the subsequent election in 1563, he was returned for both Lyme Regis, Dorset, another constituency under Bedford's influence, and Banbury, Oxfordshire. He chose to sit for Lyme Regis. In January 1562 he married Anne, daughter of Sir George Barne, Lord Mayor of London in 1552–3, and widow of wine merchant Alexander Carleill. Anne died two years later leaving her son Christopher Carleill in Walsingham's care. In 1566, Walsingham married Ursula St. Barbe, widow of Sir Richard Worsley, and Walsingham acquired her estates of Appuldurcombe and Carisbrooke Priory on the Isle of Wight. The following year, she bore him a daughter, Frances. Walsingham's other two stepsons, Ursula's sons John and George, were killed in a gunpowder accident at Appuldurcombe in 1567.

In the following years, Walsingham became active in soliciting support for the Huguenots in France and developed a friendly and close working relationship with Nicholas Throckmorton, his predecessor as MP for Lyme Regis and a former ambassador to France. By 1569, Walsingham was working with William Cecil to counteract plots against Elizabeth. He was instrumental in the collapse of the Ridolfi plot, which hoped to replace Elizabeth with the Catholic Mary, Queen of Scots. He is credited with writing propaganda decrying a conspiratorial marriage between Mary and Thomas Howard, 4th Duke of Norfolk, and Roberto di Ridolfi, after whom the plot was named, was interrogated at Walsingham's house.

In 1570, the Queen chose Walsingham to support the Huguenots in their negotiations with Charles IX of France. Later that year, he succeeded Sir Henry Norris as English ambassador in Paris. One of his duties was to continue negotiations for a marriage between Elizabeth and Charles IX's younger brother Henry, Duke of Anjou. The marriage plan was eventually dropped on the grounds of Henry's Catholicism. A substitute match with the next youngest brother, Francis, Duke of Alençon, was proposed but Walsingham considered him ugly and "void of a good humour". Elizabeth was 20 years older than Alençon, and was concerned that the age difference would be seen as absurd. Walsingham believed that it would serve England better to seek a military alliance with France against Spanish interests. The defensive Treaty of Blois was concluded between France and England in 1572, but the treaty made no provision for a royal marriage and left the question of Elizabeth's successor open.

The Huguenots and other European Protestant interests supported the nascent revolt in the Spanish Netherlands, which were provinces of Habsburg Spain. When Catholic opposition to this course in France resulted in the death of Huguenot leader Gaspard de Coligny and the St. Bartholomew's Day massacre, Walsingham's house in Paris became a temporary sanctuary for Protestant refugees, including Philip Sidney. Ursula, who was pregnant, escaped to England with their four-year-old daughter. She gave birth to a second girl, Mary, in January 1573 while Walsingham was still in France. He returned to England in April 1573, having established himself as a competent official whom the Queen and Cecil could trust. He cultivated contacts throughout Europe, and a century later his dispatches would be published as "The Complete Ambassador".

In the December following his return, Walsingham was appointed to the Privy Council of England and was made joint principal secretary (the position which later became "Secretary of State") with Sir Thomas Smith. Smith retired in 1576, leaving Walsingham in effective control of the privy seal, though he was not formally invested as Lord Privy Seal. Walsingham acquired a Surrey county seat in Parliament from 1572 that he retained until his death, but he was not a major parliamentarian. He was knighted on 1 December 1577, and held the sinecure posts of Recorder of Colchester, "custos rotulorum" of Hampshire, and High Steward of Salisbury, Ipswich and Winchester. He was appointed Chancellor of the Order of the Garter from 22 April 1578 until succeeded by Sir Amias Paulet in June 1587, when he became Chancellor of the Duchy of Lancaster in addition to principal secretary.

The duties of the principal secretary were not defined formally, but as he handled all royal correspondence and determined the agenda of council meetings, he could wield great influence in all matters of policy and in every field of government, both foreign and domestic. During his term of office, Walsingham supported the use of England's maritime power to open new trade routes and explore the New World, and was at the heart of international affairs. He was involved directly with English policy towards Spain, the Netherlands, Scotland, Ireland and France, and embarked on several diplomatic missions to neighbouring European states.

Closely linked to the mercantile community, he actively supported trade promotion schemes and invested in the Muscovy Company and the Levant Company. He supported the attempts of John Davis and Martin Frobisher to discover the Northwest Passage and exploit the mineral resources of Labrador, and encouraged Humphrey Gilbert's exploration of Newfoundland. Gilbert's voyage was largely financed by recusant Catholics and Walsingham favoured the scheme as a potential means of removing Catholics from England by encouraging emigration to the New World. Walsingham was among the promoters of Francis Drake's profitable 1578–1581 circumnavigation of the world, correctly judging that Spanish possessions in the Pacific were vulnerable to attack. The venture was calculated to promote the Protestant interest by embarrassing and weakening the Spanish, as well as to seize Spanish treasure. The first edition of Richard Hakluyt's "Principal Navigation, Voyages and Discoveries of the English Nation" was dedicated to Walsingham.

Walsingham advocated direct intervention in the Netherlands in support of the Protestant revolt against Spain, on the grounds that although wars of conquest were unjust, wars in defence of religious liberty and freedom were not. Cecil was more circumspect and advised a policy of mediation, a policy that Elizabeth endorsed. Walsingham was sent on a special embassy to the Netherlands in 1578, to sound out a potential peace deal and gather military intelligence.

Charles IX died in 1574 and the Duke of Anjou inherited the French throne as Henry III. Between 1578 and 1581 the Queen resurrected attempts to negotiate a marriage with Henry III's youngest brother, the Duke of Alençon, who had put himself forward as a protector of the Huguenots and a potential leader of the Dutch. Walsingham was sent to France in mid-1581 to discuss an Anglo-French alliance, but the French wanted the marriage agreed first and Walsingham was under instruction to obtain a treaty before committing to the marriage. He returned to England without an agreement. Personally, Walsingham opposed the marriage, perhaps to the point of encouraging public opposition. Alençon was a Catholic and as his elder brother, Henry III, was childless, he was heir to the French throne. Elizabeth was past the age of childbearing and had no clear successor. If she died while married to the French heir, her realms could fall under French control. By comparing the match of Elizabeth and Alençon with the match of the Protestant Henry of Navarre and the Catholic Margaret of Valois, which occurred in the week before the St. Bartholomew's Day massacre, the "most horrible spectacle" he had ever witnessed, Walsingham raised the spectre of religious riots in England in the event of the marriage proceeding. Elizabeth put up with his blunt, often unwelcome, advice, and acknowledged his strong beliefs in a letter, in which she called him "her Moor [who] cannot change his colour".

These were years of tension in policy towards France, with Walsingham sceptical of the unpredictable Henry III and distrustful of the English ambassador in Paris, Edward Stafford. Stafford, who was compromised by his gambling debts, was in the pay of the Spanish and passed vital information to Spain. Walsingham may have been aware of Stafford's duplicity, as he fed the ambassador false information, presumably in the hope of fooling or confusing the Spanish.

The pro-English Regent of Scotland James Douglas, 4th Earl of Morton, whom Walsingham had supported, was overthrown in 1578. After the collapse of the Raid of Ruthven, another initiative to secure a pro-English government in Scotland, Walsingham reluctantly visited the Scottish court in August 1583, knowing that his diplomatic mission was unlikely to succeed. James VI dismissed Walsingham's advice on domestic policy saying he was an "absolute King" in Scotland. Walsingham replied with a discourse on the topic that "young princes were many times carried into great errors upon an opinion of the absoluteness of their royal authority and do not consider, that when they transgress the bounds and limits of the law, they leave to be kings and become tyrants." A mutual defence pact was eventually agreed in the Treaty of Berwick of 1586.

Walsingham's cousin Edward Denny fought in Ireland during the rebellion of the Earl of Desmond and was one of the English settlers granted land in Munster confiscated from Desmond. Walsingham's stepson Christopher Carleill commanded the garrisons at Coleraine and Carrickfergus. Walsingham thought Irish farmland was underdeveloped and hoped that plantation would improve the productivity of estates. Tensions between the native Irish and the English settlers had lasting effects on the history of Ireland.

Walsingham's younger daughter Mary died aged seven in July 1580; his elder daughter, Frances, married Sir Philip Sidney on 21 September 1583, despite the Queen's initial objections to the match (for unknown reasons) earlier in the year. As part of the marriage agreement, Walsingham agreed to pay £1,500 of Sidney's debts and gave his daughter and son-in-law the use of his manor at Barn Elms in Surrey. A granddaughter born in November 1585 was named Elizabeth after the Queen, who was one of two godparents along with Sidney's uncle, Robert Dudley, 1st Earl of Leicester. The following year, Sidney was killed fighting the Spanish in the Netherlands and Walsingham was faced with paying off more of Sidney's extensive debts. His widowed daughter gave birth, in a difficult delivery, to a second child shortly afterward, but the baby, a girl, was stillborn.

Walsingham was driven by Protestant zeal to counter Catholicism, and sanctioned the use of torture against Catholic priests and suspected conspirators. Edmund Campion was among those tortured and found guilty on the basis of extracted evidence; he was hanged, drawn and quartered at Tyburn in 1581. Walsingham could never forget the atrocities against Protestants he had witnessed in France during the Bartholomew's Day massacre and believed a similar slaughter would occur in England in the event of a Catholic resurgence. Walsingham's brother-in-law Robert Beale, who was in Paris with Walsingham at the time of the massacre, encapsulated Walsingham's view: "I think it time and more than time for us to awake out of our dead sleep, and take heed lest like mischief as has already overwhelmed the brethren and neighbours in France and Flanders embrace us which be left in such sort as we shall not be able to escape." Walsingham tracked down Catholic priests in England and supposed conspirators by employing informers, and intercepting correspondence. Walsingham's staff in England included the cryptographer Thomas Phelippes, who was an expert in deciphering letters and forgery, and Arthur Gregory, who was skilled at breaking and repairing seals without detection.

In May 1582, letters from the Spanish ambassador in England, Bernardino de Mendoza, to contacts in Scotland were found on a messenger by Sir John Forster, who forwarded them to Walsingham. The letters indicated a conspiracy among the Catholic powers to invade England and displace Elizabeth with Mary, Queen of Scots. By April 1583, Walsingham had a spy, identified as Giordano Bruno by author John Bossy, deployed in the French embassy in London. Walsingham's contact reported that Francis Throckmorton, a nephew of Walsingham's old friend Nicholas Throckmorton, had visited the ambassador, Michel de Castelnau. In November 1583, after six months of surveillance, Walsingham had Throckmorton arrested and then tortured to secure a confession—an admission of guilt that clearly implicated Mendoza. The Throckmorton plot called for an invasion of England along with a domestic uprising to liberate Mary, Queen of Scots, and depose Elizabeth. Throckmorton was executed in 1584 and Mendoza was expelled from England.

After the assassination in mid-1584 of William the Silent, the leader of the Dutch revolt against Spain, English military intervention in the Low Countries was agreed in the Treaties of Nonsuch of 1585. The murder of William the Silent also reinforced fears for Queen Elizabeth's safety. Walsingham helped create the Bond of Association, the signatories of which promised to hunt down and kill anyone who conspired against Elizabeth. The Act for the Surety of the Queen's Person, passed by Parliament in March 1585, set up a legal process for trying any claimant to the throne implicated in plots against the Queen. The following month Mary, Queen of Scots, was placed in the strict custody of Sir Amias Paulet, a friend of Walsingham. At Christmas, she was moved to a moated manor house at Chartley. Walsingham instructed Paulet to open, read and pass to Mary unsealed any letters that she received, and to block any potential route for clandestine correspondence. In a successful attempt to entrap her, Walsingham arranged a single exception: a covert means for Mary's letters to be smuggled in and out of Chartley in a beer keg. Mary was misled into thinking these secret letters were secure, while in reality they were deciphered and read by Walsingham's agents. In July 1586, Anthony Babington wrote to Mary about an impending plot to free her and kill Elizabeth. Mary's reply was clearly encouraging and sanctioned Babington's plans. Walsingham had Babington and his associates rounded up; fourteen were executed in September 1586. In October, Mary was put on trial under the Act for the Surety of the Queen's Person in front of 36 commissioners, including Walsingham.

During the presentation of evidence against her, Mary broke down and pointed accusingly at Walsingham saying, "all of this is the work of Monsieur de Walsingham for my destruction", to which he replied, "God is my witness that as a private person I have done nothing unworthy of an honest man, and as Secretary of State, nothing unbefitting my duty." Mary was found guilty and the warrant for her execution was drafted, but Elizabeth hesitated to sign it, despite pressure from Walsingham. Walsingham wrote to Paulet urging him to find "some way to shorten the life" of Mary to relieve Elizabeth of the burden, to which Paulet replied indignantly, "God forbid that I should make so foul a shipwreck of my conscience, or leave so great a blot to my poor posterity, to shed blood without law or warrant." Walsingham made arrangements for Mary's execution; Elizabeth signed the warrant on 1 February 1587 and entrusted it to William Davison, who had been appointed as junior Secretary of State in late September 1586. Davison passed the warrant to Cecil and a privy council convened by Cecil without Elizabeth's knowledge agreed to carry out the sentence as soon as was practical. Within a week, Mary was beheaded. On hearing of the execution, Elizabeth claimed not to have sanctioned the action and that she had not meant Davison to part with the warrant. Davison was arrested and imprisoned in the Tower of London. Walsingham's share of Elizabeth's displeasure was small because he was absent from court, at home ill, in the weeks just before and after the execution. Davison was eventually released in October 1588, on the orders of Cecil and Walsingham.

From 1586, Walsingham received many dispatches from his agents in mercantile communities and foreign courts detailing Spanish preparations for an invasion of England. Walsingham's recruitment of Anthony Standen, a friend of the Tuscan ambassador to Madrid, was an exceptional intelligence triumph and Standen's dispatches were deeply revealing. Walsingham worked to prepare England for a potential war with Spain, in particular by supervising the substantial rebuilding of Dover Harbour, and encouraging a more aggressive strategy. On Walsingham's instructions, the English ambassador in Turkey, William Harborne, attempted unsuccessfully to persuade the Ottoman Sultan to attack Spanish possessions in the Mediterranean in the hope of distracting Spanish forces. Walsingham supported Francis Drake's raid of Cadiz in 1587, which wrought havoc with Spanish logistics. The Spanish Armada sailed for England in July 1588. Walsingham received regular dispatches from the English naval forces, and raised his own troop of 260 men as part of the land defences. On 18 August 1588, after the dispersal of the armada, naval commander Lord Henry Seymour wrote to Walsingham, "you have fought more with your pen than many have in our English navy fought with their enemies".

In foreign intelligence, Walsingham's extensive network of "intelligencers", who passed on general news as well as secrets, spanned Europe and the Mediterranean. While foreign intelligence was a normal part of the principal secretary's activities, Walsingham brought to it flair and ambition, and large sums of his own money. He cast his net more widely than others had done previously: expanding and exploiting links across the continent as well as in Constantinople and Algiers, and building and inserting contacts among Catholic exiles. Among his spies may have been the playwright Christopher Marlowe; Marlowe was in France in the mid-1580s and was acquainted with Walsingham's kinsman Thomas Walsingham.

From 1571 onwards, Walsingham complained of ill health and often retired to his country estate for periods of recuperation. He complained of "sundry carnosities", pains in his head, stomach and back, and difficulty in passing water. Suggested diagnoses include cancer, kidney stones, urinary infection, and diabetes. He died on 6 April 1590, at his house in Seething Lane. Historian William Camden wrote that Walsingham died from "a carnosity growing "intra testium sunctas" [testicular cancer]". He was buried privately in a simple ceremony at 10 pm on the following day, beside his son-in-law, in Old St Paul's Cathedral. The grave and monument were destroyed in the Great Fire of London in 1666. His name appears on a modern monument in the crypt listing the important graves lost.
In his will, dated 12 December 1589, Walsingham complained of "the greatness of my debts and the mean state [I] shall leave my wife and heirs in", but the true state of his finances is unclear. He received grants of land from the Queen, grants for the export of cloth and leases of customs in the northern and western ports. His primary residences, apart from the court, were in Seething Lane by the Tower of London (now the site of a Victorian office building called Walsingham House), at Barn Elms in Surrey and at Odiham in Hampshire. Nothing remains of any of his houses. He spent much of his own money on espionage in the service of the Queen and the Protestant cause. In 1586, he funded a lectureship in theology at Oxford University for the Puritan John Rainolds. He had underwritten the debts of his son-in-law, Sir Philip Sidney, had pursued the Sidney estate for recompense unsuccessfully and had carried out major land transactions in his later years. After his death, his friends reflected that poor bookkeeping had left him further in the Crown's debt than was fair. In 1611, the Crown's debts to him were calculated at over £48,000, but his debts to the Crown were calculated at over £43,000 and a judge, Sir Julius Caesar, ordered both sets of debts cancelled "quid pro quo". Walsingham's surviving daughter Frances received a £300 annuity, and married the Earl of Essex. Ursula, Lady Walsingham, continued to live at Barn Elms with a staff of servants until her death in 1602.

Protestants lauded Walsingham as "a sound pillar of our commonwealth and chief patron of virtue, learning and chivalry". He was part of a Protestant intelligentsia that included Philip Sidney, Edmund Spenser and John Dee: men who promoted an expansionist and nationalist English Renaissance. Spenser included a dedicatory sonnet to Walsingham in the "Faerie Queene", likening him to Maecenas who introduced Virgil to the Emperor Augustus. After Walsingham's death, Sir John Davies composed an acrostic poem in his memory and Watson wrote an elegy, "Meliboeus", in Latin. On the other hand, Jesuit Robert Persons thought Walsingham "cruel and inhumane" in his persecution of Catholics. Catholic sources portray a ruthless, devious man driven by religious intolerance and an excessive love for intrigue. Walsingham attracts controversy still. Although he was ruthless, his opponents on the Catholic side were no less so; the treatment of prisoners and suspects by Tudor authorities was typical of European governments of the time. Walsingham's personal, as opposed to his public, character is elusive; his public papers were seized by the government while many of his private papers, which might have revealed much, were lost. The fragments that do survive demonstrate his personal interest in gardening and falconry.

Fictional portrayals of Walsingham tend to follow Catholic interpretations, depicting him as sinister and Machiavellian. He features in conspiracy theories surrounding the death of Christopher Marlowe, whom he predeceased. Charles Nicholl examined (and rejected) such theories in "The Reckoning: The Murder of Christopher Marlowe" (1992), which was used as a source by Anthony Burgess for his novel "A Dead Man in Deptford" (1993).

The 1998 film "Elizabeth" gives considerable, although sometimes historically inaccurate, prominence to Walsingham (portrayed by Geoffrey Rush). It fictionalizes him as irreligious and sexually ambiguous, merges chronologically distant events, and inaccurately suggests that he murdered Mary of Guise. Rush reprised the role in the 2007 sequel, "". Both Stephen Murray in the 1970 BBC series "Elizabeth R" and Patrick Malahide in the 2005 Channel Four miniseries "Elizabeth I" play him as a dour official.



</doc>
<doc id="51275" url="https://en.wikipedia.org/wiki?curid=51275" title="Kenneth Horne">
Kenneth Horne

Charles Kenneth Horne, generally known as Kenneth Horne, (27 February 1907 – 14 February 1969) was an English comedian and businessman. He is perhaps best remembered for his work on three BBC Radio series: "Much-Binding-in-the-Marsh" (1944–54), "Beyond Our Ken" (1958–64) and "Round the Horne" (1965–68).

The son of a clergyman who was also a politician, Horne had a burgeoning business career with Triplex Safety Glass, which was interrupted by service with the Royal Air Force during the Second World War. While serving in a barrage balloon unit, he was asked to broadcast as a quizmaster on the BBC radio show "Ack-Ack, Beer-Beer". The experience brought him into contact with the more established entertainer Richard Murdoch, and the two wrote and starred in the comedy series "Much-Binding-in-the-Marsh". After demobilisation Horne returned to his business career, and kept his broadcasting as a sideline. His career in industry flourished, and he later became the chairman and managing director of toy manufacturers Chad Valley.

In 1958 Horne suffered a stroke and gave up his business dealings to focus on his entertainment work. He was the anchor figure in "Beyond Our Ken", which also featured Kenneth Williams, Hugh Paddick, Betty Marsden and Bill Pertwee. When the programme came to an end in 1964, the same cast recorded four series of the comedy "Round the Horne".

Before the planned fifth series of "Round the Horne" began recording, Horne died of a heart attack while hosting the annual Guild of Television Producers' and Directors' Awards; "Round the Horne" could not continue without him and was withdrawn. The series has been regularly re-broadcast since his death. A 2002 BBC radio survey to find listeners' favourite British comedian placed Horne third, behind Tony Hancock and Spike Milligan.

Kenneth Horne was born Charles Kenneth Horne on 27 February 1907 at Ampthill Square, London. He was the seventh and youngest child of Silvester Horne and his wife, Katherine Maria "" Cozens-Hardy. Katherine's father was Herbert Cozens-Hardy, the Liberal MP for North Norfolk who became the Master of the Rolls in 1907 and Baron Cozens-Hardy on 1 July 1914. Silvester, a powerful orator, was a leading light in the Congregationalist movement, as minister at the Whitefield's Tabernacle, Tottenham Court Road from 1903 and, from 1910, chairman of the Congregational Union of England and Wales. Between 1910 and 1914 he was the Liberal MP for Ipswich.

By 1913 Silvester was suffering from continual poor health. He resigned his position at the tabernacle on medical advice in January 1914, and intended to resign his parliamentary seat. On a speaking tour of the US and Canada he lectured at Yale University, and then took the ferry to Toronto; as it entered the harbour, he collapsed and died, aged 49; Horne was aged seven at the time. From September that year Horne attended St George's School, Harpenden as a boarder—the seventh of the Horne children to attend the school. Although he was not strong academically, he developed into a good sportsman, representing the school in rugby and cricket, and during the summer holidays took part in the Public Schoolboys Lawn Tennis Championship at Queen's Club; in his final appearance in 1925 he was knocked out by the future Wimbledon finalist Bunny Austin.

Horne enrolled at the London School of Economics in October 1925, where his tutors included Hugh Dalton and Stephen Leacock; he was dissatisfied with his time at the university and called Leacock "one of the most boring lecturers I ever came across". During the general strike in 1926 volunteers were asked to enlist at the Organisation for the Maintenance of Supplies to take over the essential services; Horne joined, and spent two days driving a London bus before the strike was called off. Through the influence and generosity of an uncle, Austin Pilkington of the Pilkington glassmaking family of St Helens, he was able to enrol at Magdalene College, Cambridge in October 1926. He committed himself to the sporting side of life and represented the college at rugby, and in the relay team alongside the future Olympic gold medallist Lord Burghley. He also played tennis for the university, partnering Bunny Austin. Distracted by his athletic exploits, he neglected his studies and was sent down in December 1927.

Austin Pilkington was aggrieved at Horne's failure to make the most of the opportunity he had provided, and decided against offering the young man a post in the family firm. Despite the disappointment, through his contacts within the industry, he secured for the young Horne an interview with the Triplex Safety Glass Company at King's Norton, a district of Birmingham. Horne's sporting record commended him to the manager of the Triplex factory, and he was taken on as a management trainee on a modest salary. In September 1930, despite his unimpressive finances, he married Lady Mary Pelham-Clinton-Hope, daughter of the 8th Duke of Newcastle. The marriage was happy at first, but had broken down by 1932. Mary applied for an annulment in November 1932; she declared the reason was "the incapacity of the respondent [Charles Kenneth Horne] to consummate the marriage", which was dissolved in 1933, although the two remained on friendly terms thereafter.

When Horne's first marriage was dissolved, he was sought out by a former girlfriend, Joan Burgess, daughter of a neighbour at King's Norton. Unlike his first wife, she had much in common with him, including a liking for squash, tennis, golf and dancing. A month before her 21st birthday they were married, in September 1936. Joan became pregnant soon after the wedding, and in July 1937 a baby boy was delivered; he was stillborn.

In 1938 Horne enlisted in the Royal Air Force Volunteer Reserve on a part-time training scheme. He was commissioned as an acting pilot officer in No. 911 (County of Warwick) Squadron, a barrage balloon unit in Sutton Coldfield, and was called up into the RAF full-time on the outbreak of war. In the initial months of the conflict—the Phoney War—Horne's duties were undemanding, and he formed a concert party from his friends and colleagues. In November 1940 he was promoted to flight lieutenant, and to squadron leader a year later. In early 1942, the BBC producer Bill McLurg asked whether the RAF station at which Horne was based could put on an edition of his programme "Ack-Ack, Beer-Beer". Horne was ordered to put on the show, and he made his broadcasting debut on 16 April 1942, as the compere. Although the standard of the talent on the show was not high, McLurg was impressed with Horne's presentation, especially the way he hosted the programme's quiz; he invited Horne to be the programme's regular quizmaster, a role the latter fulfilled on over fifty "Ack-Ack, Beer-Beer" quizzes over the next two years. In January 1943 he became one of the show's regular comperes and presented the entire show for the first time.

In March 1943 Horne was posted to the Air Ministry in London, with the rank of wing commander. Continuing to broadcast on "Ack-Ack, Beer-Beer", he also began to write sketches for the programme, and make broadcasts on other shows, including the Overseas Recorded Broadcasting Service (ORBS), to be transmitted to British forces in the Middle East. His work with ORBS brought him into contact with Flight Lieutenant Richard Murdoch, who he jokingly introduced in one broadcast as "the station commander of Much-Binding-in-the-Marsh"; with a great deal in common in their backgrounds and a similar sense of humour, the pair quickly formed a friendship. Horne informed Murdoch of a squadron leader vacancy in his section at the Ministry, and Murdoch became his colleague. Murdoch, a professional actor and entertainer for 12 years before the war, recognised Horne's talent as a performer, and used his contacts to secure him more broadcasting work.
"Ack-Ack, Beer-Beer" came to an end in February 1944 when the BBC decided to direct their programming at the general armed forces, rather than the barrage balloon crews. A month later Horne and Murdoch had expanded the idea of the remote and fictitious Royal Air Force station, Much-Binding-in-the-Marsh. The pair took the idea to the BBC producer Leslie Bridgemont who was responsible for the show "Merry-go-Round", which featured, in weekly rotation, shows based on the Army, Navy and RAF. Bridgemont included a "Much-Binding-in-the-Marsh" section in "Merry-go-Round" on 31 March 1944; Horne played "an officer so dim that even the other officers noticed", with Murdoch as his harassed second-in-command and Sam Costa as an "amiable chump who always got things wrong".

During 1944 Horne met and fell in love with Marjorie Thomas, a war widow with a young daughter. He was divorced in early 1945, and he and Thomas were married in November that year, three months after he had been demobilised.

On his return to civilian life, Horne resumed working at Triplex, and was promoted to the position of sales director. Despite his subsequent joint career in broadcasting and business, his commercial activities always took precedence. He declared that his work on radio was only a hobby, and that he would give it up before his business career. He combined his two roles by working full-time, and writing scripts with Murdoch at weekends.
"Much-Binding-in-the-Marsh" had gained sufficient popularity over its run of 20 "Merry-go-Round" episodes to be given its own 39-week series beginning in January 1947. With the coming of peace, the supposed RAF station became a civil airport, and the show continued much as before, written by and starring Horne and Murdoch, with Sam Costa. Maurice Denham—described by Murdoch as a vocal chameleon—joined the cast and played over 60 roles. The programme became popular, with audiences of 20 million, and ran for four series until September 1950.

In March 1948 Horne appeared with Murdoch in six episodes of the BBC Television comedy series "Kaleidoscope". In June that year he and Murdoch again appeared on television in a one-off sitcom, "At Home", which they wrote. The following year Horne began his connection with "Twenty Questions", an association that lasted, on-and-off, for 20 years. By the fourth series of "Much-Binding" in 1950, the listener figures had declined to a level that concerned the BBC and they decided against a fifth series. Rather than wait to see what other offers of work would come in from the Corporation, Horne and Murdoch signed the comedy to a 35-programme series on Radio Luxembourg between October 1950 and June 1951. The programme was poorly received on the commercial channel: Murdoch observed that "it wasn't really a great success—even my mother said it was rotten, and she was my greatest fan". After one series, the show returned to the BBC in 1951–52, although renamed as "Over to You". Murdoch and Horne again appeared together, in April 1952, on "Desert Island Discs".

In 1954, after nine years in his senior position at Triplex, and 27 years at the company, Horne accepted the position of managing director of the British Industries Fair, a government-backed organisation promoting British goods worldwide; he took up his position in July 1955. Much of the work involved liaising with foreign buyers and delegations, and he accompanied the Queen and Duke of Edinburgh on visits to the annual fair. In 1956 the government withdrew its funding and the BIF closed. Horne received several attractive job offers, and chose the post of chairman and managing director of the toy manufacturers Chad Valley, where he was a success. In September that year he and Murdoch appeared in a one-off television programme "Show for the Telly".

In January 1957 Horne appeared as the compere on the popular Saturday evening comedy and music radio show "Variety Playhouse", initially for a run of four months, but soon extended until the end of June. He also began to write a weekly column for the women's magazine "She", and to appear in an increasing number of other programmes. After his work on "Variety Playhouse" had finished, he and the programme's writers Eric Merriman and Barry Took prepared a script for a pilot episode of a new show, "Beyond Our Ken". The show, in which Horne was joined by Kenneth Williams, Ron Moody, Hugh Paddick and Betty Marsden, was broadcast in October 1957.

The pilot episode of "Beyond Our Ken" was well received by the BBC, and they commissioned a series to start in April 1958. On 27 February that year—his 51st birthday—Horne suffered a debilitating stroke and was totally paralysed down his left-hand side and lost the power of speech. He underwent a course of intensive physiotherapy and was able to return home after two weeks. His voice returned when, during heavy massage on his left thigh, a sharp pain led to him shouting "You bugger!" at the physiotherapist. His doctor told him that the stroke was caused by the stress of combining a full-time business post with his broadcasting work. He also told Horne that when he had recovered he would never be fit enough to continue as before. Horne considered that it was not the physical problem of combining his two careers, but the mental strain of problems in his business life; accordingly he decided to give up commerce and concentrate on a career in entertainment. Because of the stroke, plans for "Beyond Our Ken" were suspended.

In April 1958 Horne eased himself back into broadcasting as chairman of "Twenty Questions". This evidence of his recovery was sufficient for the BBC to begin recording "Beyond Our Ken" in June, in preparation for the broadcast of the first series between July and November. "Beyond Our Ken" was written around the imperturbable establishment figure of Horne, while the other performers played a "spectrum of characters never before heard on the radio", including the exaggeratedly upper class Rodney and Charles, the genteel pensioners Ambrose and Felicity, the cook Fanny Haddock—a parody of popular TV cook Fanny Cradock—and the gardener Arthur Fallowfield. The first episode was not well received by a sample audience, but the BBC decided to back Horne and his team, and the initial six-week contract was extended to 21 weeks. Before the series came to an end, a second had been commissioned to run the following year. After the first series Moody was succeeded by Bill Pertwee; Took left after the second series, leaving Merriman to write the remaining programmes on his own.

The second series of "Beyond Our Ken" followed in 1959; a third in 1960. Horne also continued his work in television, hosting his own series, "Trader Horne", and appearing on a number of other programmes. In April 1961 he made his second appearance on "Desert Island Discs", this time unaccompanied by Murdoch. In October that year—three weeks after the fifth series of "Beyond Our Ken" began recording—Horne appeared as the anchorman on a new BBC television series, "Let's Imagine", a discussion programme which ran for 20 editions over 18 months. He was the subject of "This Is Your Life" in February 1962, hosted by Eamonn Andrews, in which guests included friends and colleagues from his connections in business and entertainment. In June 1963 he began "Ken's Column", a series of 15-minute one-man programmes for Anglia Television.

The seventh series of "Beyond Our Ken" finished in February 1964, with an average audience of ten million listeners per programme. In September that year Horne returned from holiday and was scheduled to appear in a number of programmes; Eric Merriman objected to Horne's activities, saying that Horne had been made into a star by the writer, and that "no other comedy series should be allowed to use him". When the BBC refused to withdraw Horne from the second programme, "Down with Women", Merriman resigned from writing "Beyond Our Ken" and the show came to an end. After some pressure from Horne to keep the remainder of the team together, the BBC commissioned a replacement series, "Round the Horne", on similar lines. They turned to one of the original writers of "Beyond Our Ken", Barry Took and his new writing partner, Marty Feldman. Horne remained the genial and unflappable focal figure, and the writers invented several new and eccentric characters to revolve round him. They included J. Peasemold Gruntfuttock, the walking slum; the Noël Coward parodies Charles and Fiona; the incompetent villain Dr. Chou En Ginsberg; the folk singer Rambling Syd Rumpo and the "outrageously camp" Julian and Sandy. The resulting programme was described by radio historians Andy Foster and Steve Furst as "one of the seminal comedies to come out of the BBC", while "The Spectator" described it as "one of the great radio successes". The first series of "Round the Horne", consisting of 16 episodes, ran from March to June 1965. Horne's role was to provide "the perfect foil to the inspired lunacy happening all around him":

On 7 October 1966, at the age of 59, Horne suffered a major heart attack. He was much weakened, and was unfit to work for three months. As a result, he did not appear in the "Round the Horne" Christmas special. He returned to work in January 1967 to record the third series.

"Round the Horne" ran to four series, broadcast in successive years, and finished in June 1968. Three weeks after the fourth series finished, the first episode of "Horne A'Plenty" was broadcast on ITV. In a sketch show format, and with Barry Took as script editor (and later producer), this was an attempt to translate the spirit of "Round the Horne" to television, although with different actors supporting Horne: Graham Stark, for example, substituted for Kenneth Williams and Sheila Steafel for Betty Marsden. The first six-part series ran from 22 June to 27 July 1968, the second (by which time ABC had become Thames Television) from 27 November to 1 January 1969.

Because of his heart condition, Horne had been prescribed an anticoagulant, but had stopped taking it on the advice of a faith healer. Horne died of a heart attack on 14 February 1969, while hosting the annual Guild of Television Producers' and Directors' Awards at the Dorchester hotel in London. Presenting the awards was Earl Mountbatten of Burma; an award had gone to Barry Took and Marty Feldman for their TV series "Marty", and Horne had just urged viewers to tune into the fifth series of "Round the Horne" (which was due to start on 16 March) when he fell from the podium. The televised recording of the event omitted the incident, with announcer Michael Aspel explaining, "Mr Horne was taken ill at this point and has since died." A memorial service was held at St Martin-in-the-Fields in March that year.

After his death, Horne was eulogised in "The Times" as "a master of the scandalous double-meaning delivered with shining innocence", while "The Sunday Mirror" called him "one of the few personalities who bridged the generation gap" and "perhaps the last of the truly great radio comics." In the December 1970 issue of "The Listener", Barry Took recalled "Round the Horne" and said of its star:

"He was an unselfish performer, but it was still always "his" show. You just knew it. A Martian would have known it. His warmth tempered the sharpness of the writing ... To say that everyone loved him sounds like every obituary ever written – nonetheless it's true ... Horne was one of the few great men I have met, and his generosity of spirit and gesture have, in my experience, never been surpassed. I mourn him still."

On hearing the news Kenneth Williams wrote in his diary that "I loved that man. His unselfish nature, his kindness, tolerance and gentleness were an example to everyone". In "The Sunday Times" in February 1969, Paul Jennings wrote of him: "If I ever knew a gentleman, it was Kenneth Horne. ... He gave you his whole attention, his whole courtesy. And what a courtesy it was! ... I knew him in the context of panel games, to which his marvellous unforced humour, spontaneous but beautifully timed, always added sparkle."

Horne's friend, Barry Took, considered that "Horne's rich, fruity voice and warm patrician manner made him the ideal link man and that, coupled with a mischievous sense of humour, ensured that any programme in which he was involved was the better for his presence". Horne attributed his voice and delivery "to 'the Grace of God', his grandfather Lord Cozens-Hardy, the former Master of the Rolls, and the hard training of being 'a jovial chap among the golf and motoring fraternity'."

The obituarist for "The Times" highlighted Horne's "remarkably skilful but very personal comic technique" of playing "a friendly good-natured old buffer who was simply doing his best, apparently lost in wonder, at the glossier, more spectacular talents of those among whom he found himself". The media analysts Frank Krutnik and Steve Neale see a similar role, and consider that "Horne functioned, like [Jack] Benny, [Fred] Allen and [Tommy] Handley before him, as a 'stooge' rather than a joke-wielder, frequently switching roles between announcer and in-sketch performer".

In "Round the Horne", as well as acting as link man, Horne also played other character roles in the film and melodrama spoofs, but always sounded exactly like Kenneth Horne. Referring to his ability with voices, he commented that "between them Betty, Ken W., Hugh and Bill Pertwee can provide at least 100 voices, and if you take me into account the figure leaps to 101." Williams reported that Horne had a card index mind, "in which there seemed to be stored every funny voice, every dialect, every comedy trick, which he knew that each member of the cast was capable of", and would suggest a change in approach if a line did not work during rehearsals.

Graham Ball, writing in the "Sunday Express" observed that Horne "didn't tell jokes in the usual manner, didn't have a catchphrase and never resorted to blue comedy". Ball also identified that Horne's "stage character, that of a slightly bufferish English gent, was adored by middle- and working-class audiences alike. His humour was original, almost underplayed, but the effortless delivery and uncanny timing concealed an almost anarchic sense of mischief."

By 24 February 1969 it had been decided that "Round the Horne" could not continue without its star. As a result, the scripts for Series Five (which Horne had jokingly suggested should be subtitled 'The First All-Nude Radio Show') were hastily adapted into a new series for Kenneth Williams called "Stop Messing About", which ran for two series but was widely judged a failure and discontinued in 1970. On the first day of recording the new show, Williams wrote in his diary that "I miss [Horne] dreadfully. I could weep for all that "goodness" gone from our atmosphere at the show".

A successful stage show called "Round the Horne ... Revisited" opened in London in October 2003, compiled by Series Four co-writer Brian Cooke from original scripts and featuring Jonathan Rigby as Horne. It ran until April 2005, and also generated three nationwide tours and a BBC television film. Horne was played by Stephen Critchlow, who also played him in the 2006 BBC television drama "".

On 27 February 2007 (Horne's centenary), BBC Radio 4 broadcast a half-hour documentary tribute entitled "Sound the Horne", hosted by Jimmy Carr. The following year, on 18 September, another Radio 4 documentary was broadcast, "Thoroughly Modest Mollie", which focused on Horne's frequent ghost-writer, Mollie Millest. Rigby reprised the role yet again in a new stage show, devised by Barry Took's widow Lyn, called "Round the Horne – Unseen and Uncut", which toured in 2008 and 2009. in The same year an unbroadcast pilot script written by Horne and Millest in 1966 was produced by Radio 4. Called "Twice Ken is Plenty" and intended as a two-man showcase for Horne and Kenneth Williams, the 21st-century version was performed by Rigby and, as Williams, Robin Sebastian.

Horne has been the subject of two biographies, Norman Hackforth's "Solo for Horne" in 1976 and Barry Johnston's "Round Mr Horne: The Life of Kenneth Horne" in 2006. In 1998 Ernie Wise unveiled a blue plaque to Horne at BBC Broadcasting House. Editions of "Beyond Our Ken" and "Round the Horne" are regularly broadcast on the digital radio service BBC Radio 4 Extra, and by 2006 over half a million copies of tapes and CDs of "Round the Horne" had been sold by the BBC. In a 2002 survey conducted by the BBC to find listeners' favourite British comedian, Horne appeared third, behind Tony Hancock and Spike Milligan.




</doc>
<doc id="52110" url="https://en.wikipedia.org/wiki?curid=52110" title="John C. Calhoun">
John C. Calhoun

John Caldwell Calhoun (; March 18, 1782March 31, 1850) was an American statesman and political theorist from South Carolina who served as the seventh vice president of the United States from 1825 to 1832. He is remembered for strongly defending slavery and for advancing the concept of minority rights in politics, which he did in the context of protecting the interests of the white South when it was outnumbered by Northerners. He began his political career as a nationalist, modernizer, and proponent of a strong national government and protective tariffs. In the late 1820s, his views changed radically and he became a leading proponent of states' rights, limited government, nullification, and opposition to high tariffs—he saw Northern acceptance of these policies as the only way to keep the South in the Union. His beliefs and warnings heavily influenced the South's secession from the Union in 1860–1861.

Calhoun began his political career with election to the House of Representatives in 1810. As a prominent leader of the war hawk faction, Calhoun strongly supported the War of 1812 to defend American honor against British infractions of American independence and neutrality during the Napoleonic Wars. He then served as Secretary of War under President James Monroe, and in this position reorganized and modernized the War Department. Calhoun was a candidate for the presidency in the 1824 election. After failing to gain support, he let his name be put forth as a candidate for vice president. The Electoral College elected Calhoun for vice president by an overwhelming majority. He served under John Quincy Adams and continued under Andrew Jackson, who defeated Adams in the election of 1828.

Calhoun had a difficult relationship with Jackson primarily due to the Nullification Crisis and the Petticoat affair. In contrast with his previous nationalism, Calhoun vigorously supported South Carolina's right to nullify federal tariff legislation he believed unfairly favored the North, putting him into conflict with unionists such as Jackson. In 1832, with only a few months remaining in his second term, he resigned as vice president and entered the Senate. He sought the Democratic nomination for the presidency in 1844, but lost to surprise nominee James K. Polk, who went on to become president. Calhoun served as Secretary of State under John Tyler from 1844 to 1845. As Secretary of State, he supported the annexation of Texas as a means to extend the slave power, and helped settle the Oregon boundary dispute with Britain. He then returned to the Senate, where he opposed the Mexican–American War, the Wilmot Proviso, and the Compromise of 1850 before his death in 1850. Calhoun often served as a virtual party-independent who variously aligned as needed with Democrats and Whigs.

Later in life, Calhoun became known as the "cast-iron man" for his rigid defense of white Southern beliefs and practices. His concept of republicanism emphasized approval of slavery and minority rights, as particularly embodied by the Southern states. His concept of minority rights did not extend to slaves; he owned dozens of slaves in Fort Hill, South Carolina. Calhoun asserted that slavery, rather than being a "necessary evil," was a "positive good," benefiting both slaves and slave owners. To protect minority rights against majority rule, he called for a concurrent majority whereby the minority could sometimes block proposals that it felt infringed on their liberties. To this end, Calhoun supported states' rights and nullification, through which states could declare null and void federal laws that they viewed as unconstitutional. Calhoun was one of the "Great Triumvirate" or the "Immortal Trio" of Congressional leaders, along with his Congressional colleagues Daniel Webster and Henry Clay. In 1957, a Senate Committee headed by Senator John F. Kennedy selected Calhoun as one of the five greatest United States Senators of all time.

John Caldwell Calhoun was born in Abbeville District, South Carolina on March 18, 1782, the fourth child of Patrick Calhoun (1727–1796) and his wife Martha Caldwell. Patrick's father, also named Patrick Calhoun, had joined the Scotch-Irish immigration movement from County Donegal to southwestern Pennsylvania. After the death of the elder Patrick in 1741, the family moved to southwestern Virginia. Following the defeat of British General Edward Braddock at the Battle of the Monongahela in 1755, the family, fearing Indian attacks, moved to South Carolina in 1756. Patrick Calhoun belonged to the Calhoun clan in the tight-knit Scotch-Irish community on the Southern frontier. He was known as an Indian fighter and an ambitious surveyor, farmer, planter and politician, being a member of the South Carolina Legislature. As a Presbyterian, he stood opposed to the Anglican elite based in Charleston. He was a Patriot in the American Revolution, and opposed ratification of the federal Constitution on grounds of states' rights and personal liberties. Calhoun would eventually adopt his father's states' rights beliefs.

Young Calhoun showed scholastic talent, and although schools were scarce on the Carolina frontier, he was enrolled briefly in an academy in Appling, Georgia, which soon closed. He continued his studies privately. When his father died, his brothers were away starting business careers and so the 14-year old Calhoun took over management of the family farm and five other farms. For four years he simultaneously kept up his reading and his hunting and fishing. The family decided he should continue his education, and so he resumed studies at the Academy after it reopened.

With financing from his brothers, he went to Yale College in Connecticut in 1802. For the first time in his life, Calhoun encountered serious, advanced, well-organized intellectual dialogue that could shape his mind. Yale was dominated by President Timothy Dwight, a Federalist who became his mentor. Dwight's brilliance entranced (and sometimes repelled) Calhoun. Biographer John Niven says:

Dwight repeatedly denounced Jeffersonian democracy, and Calhoun challenged him in class. Dwight could not shake Calhoun's commitment to republicanism. "Young man," retorted Dwight, "your talents are of a high order and might justify you for any station, but I deeply regret that you do not love sound principles better than sophistry—you seem to possess a most unfortunate bias for error." Dwight also expounded on the strategy of secession from the Union as a legitimate solution for New England's disagreements with the national government.

Calhoun made friends easily, read widely, and was a noted member of the debating society of Brothers in Unity. He graduated as valedictorian in 1804. He studied law at the nation's only real law school, Tapping Reeve Law School in Litchfield, Connecticut, where he worked with Tapping Reeve and James Gould. He was admitted to the South Carolina bar in 1807. Biographer Margaret Coit argues that:

In January 1811, Calhoun married Floride Bonneau Colhoun, a first cousin once removed. She was the daughter of wealthy United States Senator and lawyer John E. Colhoun, a leader of Charleston high society. The couple had 10 children over 18 years: Andrew Pickens Calhoun, Floride Pure Calhoun, Jane Calhoun, Anna Maria Calhoun, Elizabeth Calhoun, Patrick Calhoun, John Caldwell Calhoun Jr., Martha Cornelia Calhoun, James Edward Calhoun, and William Lowndes Calhoun. Three of them, Floride Pure, Jane, and Elizabeth, died in infancy. Calhoun's fourth child, Anna Maria, married Thomas Green Clemson, founder of Clemson University in South Carolina.

Calhoun was not openly religious. He was raised Calvinist but was attracted to Southern varieties of Unitarianism of the sort that attracted Jefferson. Southern Unitarianism was generally less organized than the variety popular in New England. He was generally not outspoken about his religious beliefs. After his marriage, Calhoun and his wife attended the Episcopal Church, of which she was a member. In 1821 he became a founding member of All Souls Unitarian Church in Washington, D.C.

Historian Merrill Peterson describes Calhoun: "Intensely serious and severe, he could never write a love poem, though he often tried, because every line began with 'whereas' ..."

With a base among the Irish and Scotch Irish, Calhoun won election to the House of Representatives in 1810. He immediately became a leader of the War Hawks, along with Speaker Henry Clay of Kentucky and South Carolina congressmen William Lowndes and Langdon Cheves. Brushing aside the vehement objections of both anti-war New Englanders and arch-conservative Jeffersonians led by John Randolph of Roanoke, they demanded war against Britain to preserve American honor and republican values, which had been violated by the British refusal to recognize American shipping rights. As a member, and later acting chairman, of the Committee on Foreign Affairs, Calhoun played a major role in drafting two key documents in the push for war, the Report on Foreign Relations and the War Report of 1812. Drawing on the linguistic tradition of the Declaration of Independence, Calhoun's committee called for a declaration of war in ringing phrases, denouncing Britain's "lust for power", "unbounded tyranny", and "mad ambition". Historian James Roark says, "These were fighting words in a war that was in large measure about insult and honor." The United States declared war on Britain on June 18, inaugurating the War of 1812. The opening phase involved multiple disasters for American arms, as well as a financial crisis when the Treasury could barely pay the bills. The conflict caused economic hardship for the Americans, as the Royal Navy blockaded the ports and cut off imports, exports and the coastal trade. Several attempted invasions of Canada were fiascos, but the U.S. in 1813 seized control of Lake Erie and broke the power of hostile Indians in battles such as the Battle of the Thames in Canada in 1813 and the Battle of Horseshoe Bend in Alabama in 1814. These Indians had, in many cases, cooperated with the British or Spanish in opposing American interests.

Calhoun labored to raise troops, provide funds, speed logistics, rescue the currency, and regulate commerce to aid the war effort. One colleague hailed him as "the young Hercules who carried the war on his shoulders." Disasters on the battlefield made him double his legislative efforts to overcome the obstructionism of John Randolph, Daniel Webster, and other opponents of the war. In December 1814, with the armies of Napoleon Bonaparte apparently defeated, and the British invasions of New York and Baltimore thwarted, British and American diplomats signed the Treaty of Ghent. It called for a return to the borders of 1812 with no gains or losses. Before the treaty reached the Senate for ratification, and even before news of its signing reached New Orleans, a massive British invasion force was utterly defeated in January 1815 at the Battle of New Orleans, making a national hero of General Andrew Jackson. Americans celebrated what they called a "second war of independence" against Britain. This led to the beginning of the "Era of Good Feelings", an era marked by the formal demise of the Federalist Party and increased nationalism.

Despite American successes, the mismanagement of the Army during the war distressed Calhoun, and he resolved to strengthen and centralize the War Department. The militia had proven itself quite unreliable during the war and Calhoun saw the need for a permanent and professional military force. In 1816 he called for building an effective navy, including steam frigates, as well as a standing army of adequate size. The British blockade of the coast had underscored the necessity of rapid means of internal transportation; Calhoun proposed a system of "great permanent roads". The blockade had cut off the import of manufactured items, so he emphasized the need to encourage more domestic manufacture, fully realizing that industry was based in the Northeast. The dependence of the old financial system on import duties was devastated when the blockade cut off imports. Calhoun called for a system of internal taxation that would not collapse from a war-time shrinkage of maritime trade, as the tariffs had done. The expiration of the charter of the First Bank of the United States had also distressed the Treasury, so to reinvigorate and modernize the economy Calhoun called for a new national bank. A new bank was chartered as the Second Bank of the United States by Congress and approved by President James Madison in 1816. Through his proposals, Calhoun emphasized a national footing and downplayed sectionalism and states rights. Historian Ulrich B. Phillips says that at this stage of Calhoun's career, "The word "nation" was often on his lips, and his conviction was to enhance national unity which he identified with national power."

Regarding his career in the House of Representatives, an observer commented that Calhoun was "the most elegant speaker that sits in the House ... His gestures are easy and graceful, his manner forcible, and language elegant; but above all, he confines himself closely to the subject, which he always understands, and enlightens everyone within hearing."

His talent for public speaking required systematic self-discipline and practice. A later critic noted the sharp contrast between his hesitant conversations and his fluent speaking styles, adding that Calhoun "had so carefully cultivated his naturally poor voice as to make his utterance clear, full, and distinct in speaking and while not at all musical it yet fell pleasantly on the ear". Calhoun was "a high-strung man of ultra intellectual cast". As such, Calhoun was not known for charisma. He was often seen as harsh and aggressive with other representatives. But he was a brilliant intellectual orator and strong organizer. Historian Russell Kirk says, "That zeal which flared like Greek fire in Randolph burned in Calhoun, too; but it was contained in the Cast-iron Man as in a furnace, and Calhoun's passion glowed out only through his eyes. No man was more stately, more reserved."

John Quincy Adams concluded in 1821 that "Calhoun is a man of fair and candid mind, of honorable principles, of clear and quick understanding, of cool self-possession, of enlarged philosophical views, and of ardent patriotism. He is above all sectional and factious prejudices more than any other statesman of this Union with whom I have ever acted." Historian Charles Wiltse noted Calhoun's evolution, "Though he is known today primarily for his sectionalism, Calhoun was the last of the great political leaders of his time to take a sectional position—later than Daniel Webster, later than Henry Clay, later than Adams himself."

In 1817, the deplorable state of the War Department led four men to decline offers from President James Monroe to accept the office of Secretary of War before Calhoun finally assumed the role. Calhoun took office on December 8 and served until 1825. He continued his role as a leading nationalist during the Era of Good Feelings. He proposed an elaborate program of national reforms to the infrastructure that he believed would speed economic modernization. His first priority was an effective navy, including steam frigates, and in the second place a standing army of adequate size—and as further preparation for emergency, "great permanent roads", "a certain encouragement" to manufactures, and a system of internal taxation that would not collapse from a war-time shrinkage of maritime trade, like customs duties.

After the war ended in 1815 the "Old Republicans" in Congress, with their Jeffersonian ideology for economy in the federal government, sought to reduce the operations and finances of the War Department. Calhoun's political rivalry with William H. Crawford, the Secretary of the Treasury, over the pursuit of the presidency in the 1824 election, complicated Calhoun's tenure as War Secretary. The general lack of military action following the war meant that a large army, such as that preferred by Calhoun, was no longer considered necessary. The "Radicals", a group of strong states' rights supporters who mostly favored Crawford for president in the coming election, were inherently suspicious of large armies. Some allegedly also wanted to hinder Calhoun's own presidential aspirations for that election. Thus, on March 2, 1821, Congress passed the Reduction Act, which reduced the number of enlisted men of the army by half, from 11,709 to 5,586, and the number of the officer corps by a fifth, from 680 to 540. Calhoun, though concerned, offered little protest. Later, to provide the army with a more organized command structure, which had been severely lacking during the War of 1812, he appointed Major General Jacob Brown to a position that would later become known as "Commanding General of the United States Army".

As secretary, Calhoun had responsibility for management of Indian affairs. He promoted a plan, adopted by Monroe in 1825, to preserve the sovereignty of eastern Indians by relocating them to western reservations they could control without interference from state governments. In over seven years Calhoun supervised the negotiation and ratification of 40 treaties with Indian tribes. Calhoun opposed the invasion of Florida launched in 1818 by General Jackson during the First Seminole War, which was done without direct authorization from Calhoun or President Monroe, and in private with other cabinet members, advocated censorship of Jackson as punishment. The United States annexed Florida from Spain in 1819 through the Adams–Onís Treaty. A reform-minded modernizer, he attempted to institute centralization and efficiency in the Indian Department and in the Army by establishing new coastal and frontier fortifications and building military roads, but Congress either failed to respond to his reforms or responded with hostility. Calhoun's frustration with congressional inaction, political rivalries, and ideological differences spurred him to create the Bureau of Indian Affairs in 1824. The responsibilities of the bureau were to manage treaty negotiations, schools, and trade with Indians, in addition to handling all expenditures and correspondence concerning Indian affairs. Thomas McKenney was appointed as the first head of the bureau.

Calhoun's tenure as Secretary of War witnessed the outbreak of the Missouri crisis in December 1818, when a petition arrived from Missouri settlers seeking admission into the Union as a slave state. In response, Representative James Tallmadge Jr. of New York proposed two amendments to the bill designed to restrict the spread of slavery into what would become the new state. This amendments touched off an intense debate between North and South that had some talking openly of disunion. In February 1820, Calhoun predicted to Secretary of State John Quincy Adams, a New Englander, that the Missouri issue "would not produce a dissolution" of the Union. "But if it should," Calhoun went on, "the South would of necessity be compelled to form an alliance with...Great Britain." "I said that would be returning to the colonial state," Adams recalled saying afterwards. According to Adams, "He said, yes, pretty much, but it would be forced upon them."

Calhoun was initially a candidate for President of the United States in the election of 1824. Four other men also sought the presidency: Andrew Jackson, Adams, Crawford, and Henry Clay. Calhoun failed to win the endorsement of the South Carolina legislature, and his supporters in Pennsylvania decided to abandon his candidacy in favor of Jackson's, and instead supported him for vice president. Other states soon followed, and Calhoun therefore allowed himself to become a candidate for vice president rather than president. The Electoral College elected Calhoun vice president by a landslide. He won 182 votes out of 261 electoral votes, while five other men received the remaining votes. No presidential candidate received a majority in the Electoral College, and the election was ultimately resolved by the House of Representatives, where Adams was declared the winner over Crawford and Jackson, who in the election had led Adams in both popular vote and electoral vote. After Clay, the Speaker of the House, was appointed Secretary of State by Adams, Jackson's supporters denounced what they considered a "corrupt bargain" between Adams and Clay to give Adams the presidency in exchange for Clay receiving the office of Secretary of State, the holder of which had traditionally become the next president. Calhoun also expressed some concerns, which caused friction between him and Adams.

Calhoun also opposed President Adams' plan to send a delegation to observe a meeting of South and Central American leaders in Panama, believing that the United States should stay out of foreign affairs. Calhoun became disillusioned with Adams' high tariff policies and increased centralization of government through a network of "internal improvements", which he now saw as a threat to the rights of the states. Calhoun wrote to Jackson on June 4, 1826, informing him that he would support Jackson's second campaign for the presidency in 1828. The two were never particularly close friends. Calhoun never fully trusted Jackson, a frontiersman and popular war hero, but hoped that his election would bring some reprieve from Adams's anti-states' rights policies. Jackson selected Calhoun as his running mate, and together they defeated Adams and his running mate Richard Rush. Calhoun thus became the second of two vice presidents to serve under two different presidents. The only other man who accomplished this feat was George Clinton, who served as Vice President from 1805 to 1812 under Thomas Jefferson and James Madison.

During the election, Jackson's aide James Alexander Hamilton attempted a rapprochement between Jackson and Crawford, whom Jackson resented partially due to the belief that it was he, not Calhoun, who had opposed the invasion of Florida. Hamilton spoke about this prospect with Governor John Forsyth of Georgia, who acted as a mediator between the Jackson campaign and Crawford. Forsyth wrote a letter back to Hamilton in which he claimed that, after speaking with Crawford, Crawford stated that it was Calhoun, not him, who supported censuring Jackson for his invasion of Florida. Knowing that the letter could destroy the partnership between Jackson and Calhoun, Hamilton and fellow-Jackson aide William B. Lewis allowed it to remain in Hamilton's possession without informing Jackson or the public of its existence.

Early in Jackson's administration, Floride Calhoun organized Cabinet wives (hence the term "petticoats") against Peggy Eaton, wife of Secretary of War John Eaton, and refused to associate with her. They alleged that John and Peggy Eaton had engaged in an adulterous affair while she was still legally married to her first husband, and that her recent behavior was unladylike. The allegations of scandal created an intolerable situation for Jackson. The Petticoat affair ended friendly relations between Calhoun and Jackson.

Jackson sided with the Eatons. He and his late wife Rachel Donelson had undergone similar political attacks stemming from their marriage in 1791. The two had married in 1791 not knowing that Rachel's first husband, Lewis Robards, had failed to finalize the expected divorce. Once the divorce was finalized, they married legally in 1794, but the episode caused a major controversy, and was used against him in the 1828 campaign. Jackson saw attacks on Eaton stemming ultimately from the political opposition of Calhoun, who had failed to silence his wife's criticisms. The Calhouns were widely regarded as the chief instigators. Jackson, who loved to personalize disputes, also saw the Petticoat affair as a direct challenge to his authority, because it involved lower-ranking executive officials and their wives seeming to contest his ability to choose whomever he wanted for his cabinet. Secretary of State Martin Van Buren, a widower, took Jackson's side and defended the Eatons. Van Buren was a northerner and a supporter of the 1828 tariff (which Calhoun bitterly opposed). Calhoun and Van Buren were the main contenders for who would be nominated as vice president in the ensuing election and who would then, presumably, be the party's choice to succeed Jackson. The fact that Van Buren sided with the Eatons, in addition to disagreements between Jackson and Calhoun on a number of other issues, mainly the Nullification Crisis, marked him as Calhoun's likely vice presidential successor.

Some historians, including Jackson biographers Richard B. Latner and Robert V. Remini, believe that the hostility towards the Eatons was rooted less in questions of proper behavior than in politics. Eaton had been in favor of the Tariff of Abominations. He was also politically close to Van Buren. Calhoun may have wanted to expel Eaton from the cabinet as a way of boosting his anti-tariff agenda and increasing his standing in the Democratic Party. Many cabinet members were southern and could be expected to sympathize with such concerns, especially Treasury Secretary Samuel D. Ingham, who was allied with Calhoun and believed that he, not Van Buren, should succeed Jackson as president.

In 1830, reports had emerged accurately stating that Calhoun, as Secretary of War, had favored censuring Jackson for his 1818 invasion of Florida. These infuriated Jackson. Eventually, Lewis decided to reveal the existence of Forsyth's letter, and on April 30, Crawford wrote a second letter, this time to Forsyth, repeating the charge Forsyth represented him as having previously made. Jackson received the letter on May 12, which confirmed his suspicions. He claimed that Calhoun had "betrayed" him. Eaton took his revenge on Calhoun. For reasons unclear, Calhoun asked Eaton to approach Jackson about the possibility of Calhoun publishing his correspondence with Jackson at the time of the Seminole War. Eaton did nothing. This caused Calhoun to believe that Jackson had approved the publication of the letters. Calhoun published them in the "United States Telegraph," a newspaper edited by a Calhoun protégé, Duff Green. This gave the appearance of Calhoun trying to justify himself against a conspiracy to damage him, and further enraged the President.

Finally in the spring of 1831, at the suggestion of Van Buren, who, like Jackson, supported the Eatons, Jackson replaced all but one of his Cabinet members, thereby limiting Calhoun's influence. Van Buren began the process by resigning as Secretary of State, facilitating Jackson's removal of others. Van Buren thereby grew in favor with Jackson, while the rift between the President and Calhoun was widened. Later, in 1832, Calhoun, as vice president, cast a tie-breaking vote against Jackson's nomination of Van Buren as Minister to Great Britain in a failed attempt to end Van Buren's political career. Missouri Senator Thomas Hart Benton, a staunch supporter of Jackson, then stated that Calhoun had "elected a Vice President", as Van Buren was able to move past his failed nomination as Minister to Great Britain and instead gain the Democratic Party's vice presidential nomination in the 1832 election, in which he and Jackson were victorious.

Calhoun had begun to oppose increases in protective tariffs, as they generally benefitted Northerners more than Southerners. While he was Vice President in the Adams administration, Jackson's supporters devised a high tariff legislation that placed duties on imports that were also made in New England. Calhoun had been assured that the northeastern interests would reject the Tariff of 1828, exposing pro-Adams New England congressmen to charges that they selfishly opposed legislation popular among Jacksonian Democrats in the west and Mid-Atlantic States. The southern legislators miscalculated and the so-called "Tariff of Abominations" passed and was signed into law by President Adams. Frustrated, Calhoun returned to his South Carolina plantation, where he anonymously composed "South Carolina Exposition and Protest", an essay rejecting the centralization philosophy and supporting the principle of nullification as a means to prevent tyranny of a central government.

Calhoun supported the idea of nullification through a concurrent majority. Nullification is a legal theory that a state has the right to nullify, or invalidate, any federal law it deems unconstitutional. In Calhoun's words, it is "... the right of a State to interpose, in the last resort, in order to arrest an unconstitutional act of the General Government, within its limits." Nullification can be traced back to arguments by Jefferson and Madison in writing the Kentucky and Virginia Resolutions of 1798 against the Alien and Sedition Acts. Madison expressed the hope that the states would declare the acts unconstitutional, while Jefferson explicitly endorsed nullification. Calhoun openly argued for a state's right to secede from the Union, as a last resort to protect its liberty and sovereignty. In his later years, Madison rebuked supporters of nullification, stating that no state had the right to nullify federal law.

In "South Carolina Exposition and Protest", Calhoun argued that a state could veto any federal law that went beyond the enumerated powers and encroached upon the residual powers of the State. President Jackson, meanwhile, generally supported states' rights, but opposed nullification and secession. At the 1830 Jefferson Day dinner at Jesse Brown's Indian Queen Hotel, Jackson proposed a toast and proclaimed, "Our federal Union, it must be preserved." Calhoun replied, "The Union, next to our liberty, the most dear. May we all remember that it can only be preserved by respecting the rights of the states, and distributing equally the benefit and burden of the Union." Calhoun's publication of letters from the Seminole in the "Telegraph" caused his relationship with Jackson to deteriorate further, thus contributing to the Nullification crisis. Jackson and Calhoun began an angry correspondence that lasted until Jackson stopped it in July.

On July 14, 1832, Jackson signed into law the Tariff of 1832. It was designed to placate the nullifiers by lowering tariff rates, but the nullifiers in South Carolina remained unsatisfied. On November 24, the South Carolina legislature officially nullified both the Tariff of 1832 and the Tariff of 1828, to be null and void as of February 1, 1833. In response, Jackson sent U.S. Navy warships to Charleston harbor, and threatened to hang Calhoun or any man who worked to support nullification or secession. After joining the Senate, Calhoun began to work with Clay on a new compromise tariff. A bill sponsored by the administration had been introduced by Representative Gulian C. Verplanck of New York, but it lowered rates more sharply than Clay and other protectionists desired. Clay managed to get Calhoun to agree to a bill with higher rates in exchange for Clay's opposition to Jackson's military threats and, perhaps, with the hope that he could win some Southern votes in his next bid for the presidency. On the same day, Congress passed the Force Bill, which empowered the President of the United States to use military force to ensure state compliance with federal law. South Carolina accepted the tariff, but in a final show of defiance, nullified the Force Bill. In Calhoun's speech against the Force Bill, delivered on February 5, 1833, no longer as vice president, he strongly endorsed nullification, at one point saying:
In his three-volume biography of Jackson, James Parton summed up Calhoun's role in the Nullification crisis: "Calhoun began it. Calhoun continued it. Calhoun stopped it."

As tensions over nullification escalated, South Carolina Senator Robert Y. Hayne was considered less capable than Calhoun to represent South Carolina in the Senate debates, so in late 1832 Hayne resigned to become governor; the South Carolina legislature elected Calhoun as his replacement. On December 28, Calhoun resigned as vice president to become a senator, with a voice in the debates. Van Buren had already been elected as Jackson's new vice president, meaning that Calhoun had less than three months left on his term anyway. The South Carolina newspaper "City Gazette" commented on the change:
Biographer John Niven argues "that these moves were part of a well-thought-out plan whereby Hayne would restrain the hotheads in the state legislature and Calhoun would defend his brainchild, nullification, in Washington against administration stalwarts and the likes of Daniel Webster, the new apostle of northern nationalism." Calhoun was the first of two vice presidents to resign, the second being Spiro Agnew in 1973. During his terms as vice president, he made a record of 31 tie-breaking votes in Congress.

When Calhoun took his seat in the Senate on December 29, 1832, his chances of becoming President were considered poor due to his involvement in the Nullification Crisis, which left him without connections to a major national party. After implementation of the Compromise Tariff of 1833, which helped solve the Nullification Crisis, the Nullifier Party, along with other anti-Jackson politicians, formed a coalition known as the Whig Party. Calhoun sometimes affiliated with the Whigs, but chose to remain a virtual independent due to the Whig promotion of federally subsidized "internal improvements."

From 1833 to 1834, Jackson was engaged in removing federal funds from the Second Bank of the United States during the Bank War. Calhoun opposed this action, considering it a dangerous expansion of executive power. He called the men of the Jackson administration "artful, cunning, and corrupt politicians, and not fearless warriors." He accused Jackson of being ignorant on financial matters. As evidence, he cited the economic panic caused by Nicholas Biddle as a means to stop Jackson from destroying the Bank. On March 28, 1834, Calhoun voted with the Whig senators on a successful motion to censure Jackson for his removal of the funds. In 1837, he refused to attend the inauguration of Jackson's chosen successor, Van Buren, even as other powerful senators who opposed the administration, such as Webster and Clay, did witness the inauguration. However, by 1837 Calhoun generally had realigned himself with most of the Democrats' policies.

To restore his national stature, Calhoun cooperated with Van Buren. Democrats were hostile to national banks, and the country's bankers had joined the Whig Party. The Democratic replacement, meant to help combat the Panic of 1837, was the Independent Treasury system, which Calhoun supported and which went into effect. Calhoun, like Jackson and Van Buren, attacked finance capitalism and opposed what he saw as encroachment by government and big business. For this reason, he opposed the candidacy of Whig William Henry Harrison in the 1840 presidential election, believing that Harrison would institute high tariffs and therefore place an undue burden on the Southern economy. Calhoun resigned from the Senate on March 3, 1843, four years before the expiration of his term, and returned to Fort Hill to prepare an attempt to win the Democratic nomination for the 1844 presidential election. He gained little support, even from the South, and quit.

When Harrison died in 1841 after a month in office, Vice President John Tyler succeeded him. Tyler, a former Democrat, was expelled from the Whig Party after vetoing bills passed by the Whig congressional majority to reestablish a national bank and raise tariffs. He named Calhoun Secretary of State on April 10, 1844, following the death of Abel P. Upshur in the USS "Princeton" disaster.
Upshur's loss was a severe blow to the Tyler administration. When Calhoun was nominated as Upshur's replacement, the White House was well-advanced towards securing a treaty of annexation with Texas. The State Department's secret negotiations with the Texas republic had proceeded despite explicit threats from a suspicious Mexican government that an unauthorized seizure of its northern district of Coahuila y Tejas would be equivalent to an act of war. Both the negotiations with Texas envoys and the garnering of support from the U.S. Senate had been spearheaded aggressively by Secretary Upshur, a strong pro-slavery partisan. Tyler looked to its ratification by the Senate as the "sine qua non" to his ambition for another term in office. Tyler "planned to outflank the Whigs by gaining support from the Democratic Party or possibly creating a new party of [discontented] Northern Democrats and Southern Whigs."

Calhoun, though as avid a proponent for Texas acquisition as Upshur, posed a political liability to Tyler's aims. As secretary of state, Calhoun's political objective was to see that the presidency was placed in the hands of a southern extremist, who would put the expansion of slavery at the center of national policy.

Tyler and his allies had, since 1843, devised and encouraged national propaganda promoting Texas annexation, which understated Southern slaveholder's aspirations regarding the future of Texas. Instead, Tyler chose to portray the annexation of Texas as something that would prove economically beneficial to the nation as a whole. The further introduction of slavery into the vast expanses of Texas and beyond, they argued, would "diffuse" rather than concentrate slavery regionally, ultimately weakening white attachment and dependence on slave labor. This theory was yoked to the growing enthusiasm among Americans for Manifest Destiny, a desire to see the social, economic and moral precepts of republicanism spread across the continent. Moreover, Tyler declared that national security was at stake: If foreign powers—Great Britain in particular—were to gain influence in Texas, it would be reduced to a British cotton-producing reserve and a base to exert geostrategic influence over North America. Texas might be coerced into relinquishing slavery, inducing slave uprisings in adjoining slave states and deepening sectional conflicts between American free-soil and slave-soil interests. The appointment of Calhoun, with his southern states' rights reputation—which some believed was "synonymous with slavery"—threatened to cast doubt on Tyler's carefully crafted reputation as a nationalist. Tyler, though ambivalent, felt obliged to enlist Calhoun as Secretary of State, because Tyler's closest confidantes had, in haste, offered the position to the South Carolinian statesman in the immediate aftermath of the "Princeton" disaster. Calhoun would be confirmed by Congress by unanimous vote.

In advance of Calhoun's arrival in Washington, D.C., Tyler attempted to quickly finalize the treaty negotiations. Sam Houston, President of the Texas Republic, fearing Mexican retaliation, insisted on a tangible demonstration of U.S. commitments to the security of Texas. When key Texas diplomats failed to appear on schedule, the delay compelled Tyler to bring his new Secretary of State directly into negotiations. Secretary Calhoun was directed to honor former Secretary Upshur's verbal assurances of protection now offered by Calhoun in writing, to provide for U.S. military intervention in the event that Mexico used force to hold Texas. Tyler deployed U.S. Navy vessels to the Gulf of Mexico and ordered army units mobilized, entirely paid for with $100,000 of executive branch contingency funds. The move side-stepped constitutional requirements that Congress authorize appropriations for war.

On April 22, 1844, Secretary Calhoun signed the treaty of annexation and ten days later delivered it to the Senate for consideration in secret session. The details of the treaty negotiations and supporting documents were leaked to the press by Senator Benjamin Tappan of Ohio. Tappan, a Democrat, was an opponent of annexation and of slavery. The terms of the Tyler–Texas treaty and the release of Calhoun's letter to British ambassador Richard Pakenham exposed the annexation campaign as a program to expand and preserve slavery. In the Pakenham letter, Calhoun alleged that the institution of slavery contributed to the physical and mental well-being of Southern slaves. The U.S. Senate was compelled to open its debates on ratification to public scrutiny, and hopes for its passage by the two-thirds majority required by the Constitution were abandoned by administration supporters. In linking Texas annexation to the expansion of slavery, Calhoun had alienated many who might previously have supported the treaty.

On June 8, 1844, after fierce partisan struggles, the Senate rejected the Tyler–Texas treaty by a vote of 35–16, a margin of more than two-to-one. The vote went largely along party lines: Whigs had opposed it almost unanimously (1–27), while Democrats split, but voted largely in favor (15–8). Nevertheless, the disclosure of the treaty placed the issue of Texas annexation at the center of the 1844 general election.

At the Democratic Convention in Baltimore, Maryland in May 1844, Calhoun's supporters, with Calhoun in attendance, threatened to bolt the proceedings and shift support to Tyler's third party ticket if the delegates failed to produce a pro-Texas nominee. Calhoun's Pakenham letter, and its identification with proslavery extremism, moved the presumptive Democratic Party nominee, the northerner Martin Van Buren, into denouncing annexation. Therefore, Van Buren, already not widely popular in the South, saw his support from that region crippled. As a result, James K. Polk, a pro-Texas Jacksonian and Tennessee politician, won the nomination. Historian Daniel Walker Howe says that Calhoun's Pakenham letter was a deliberate attempt to influence the outcome of the 1844 election, writing:
In the general election, Calhoun offered his endorsement to Polk on condition that he support the annexation of Texas, oppose the Tariff of 1842, and dissolve the "Washington Globe," the semi-official propaganda organ of the Democratic Party headed by Francis Preston Blair. He received these assurances and enthusiastically supported Polk's candidacy. Polk narrowly defeated Clay, who opposed annexation. Lame-duck President Tyler organized a joint House–Senate vote on the Texas treaty which passed, requiring only a simple majority. He signed a bill of annexation on March 1, With President Polk's support, the Texas annexation treaty was approved by the Texas Republic in 1845. A bill to admit Texas as the 28th state of the Union was signed by Polk on December 29, 1845.

Calhoun was reelected to the Senate in 1845 following the resignation of Daniel Elliott Huger. He soon became vocally opposed to the Mexican–American War. He believed that it would distort the national character by undermining republicanism in favor of empire and by bringing non-white persons into the country. (See § The evils of war and political parties.) When Congress declared war against Mexico on May 13, he abstained from voting on the measure. Calhoun also vigorously opposed the Wilmot Proviso, an 1846 proposal by Pennsylvania Representative David Wilmot to ban slavery in all newly acquired territories. The House of Representatives, through its Northern majority, passed the provision several times. However, the Senate, where non-slave and slave states had more equal representation, never passed the measure.

A major crisis emerged from the persistent Oregon boundary dispute between Great Britain and the United States, due to an increasing number of American migrants. The territory included most of present-day British Columbia, Washington, Oregon, and Idaho. American expansionists used the slogan "54–40 or fight" in reference to the Northern boundary coordinates of the Oregon territory. The parties compromised, ending the war threat, by splitting the area down the middle at the 49th parallel, with the British acquiring British Columbia and the Americans accepting Washington and Oregon. Calhoun, along with President Polk and Secretary of State James Buchanan, continued work on the treaty while he was a senator, and it was ratified by a vote of 41–14 on June 18, 1846.

The Compromise of 1850, devised by Clay and Stephen A. Douglas, a first-term Democratic senator from Illinois, was designed to solve the controversy over the status of slavery in the vast new territories acquired from Mexico. Many pro-slavery Southerners opposed it, and Calhoun helped organize preparations for the Nashville Convention, which would meet that summer to discuss the possibility of Southern secession. The nearly 68-year old Calhoun had suffered periodic bouts of tuberculosis throughout his life. In March 1850, the disease reached a critical stage. Weeks from death and too feeble to speak, Calhoun wrote a blistering attack on the compromise that would become likely his most famous speech. On March 4, a friend, Senator James Mason of Virginia, read the remarks. Calhoun affirmed the right of the South to leave the Union in response to Northern subjugation. He warned that the day "the balance between the two sections" was destroyed would be a day not far removed from disunion, anarchy, and civil war. Calhoun queried how the Union might be preserved in light of subjugation by the "stronger" party against the "weaker" one. He maintained that the responsibility of solving the question lay entirely on the North—as the stronger section, to allow the Southern minority an equal share in governance and to cease the agitation. He added:
Calhoun died soon afterwards, and although the compromise measures did eventually pass, Calhoun's ideas about states' rights attracted increasing attention across the South. Historian William Barney argues that Calhoun's ideas proved "appealing to Southerners concerned with preserving slavery ... Southern radicals known as 'Fire-Eaters' pushed the doctrine of states rights to its logical extreme by upholding the constitutional right of the state to secede."

Calhoun died at the Old Brick Capitol boarding house in Washington, D.C., on March 31, 1850, of tuberculosis, at the age of 68. He was interred at the St. Philip's Churchyard in Charleston, South Carolina. During the Civil War, a group of Calhoun's friends were concerned about the possible desecration of his grave by Federal troops and, during the night, removed his coffin to a hiding place under the stairs of the church. The next night, his coffin was buried in an unmarked grave near the church, where it remained until 1871, when it was again exhumed and returned to its original place.

After Calhoun had died, an associate suggested that Senator Thomas Hart Benton give a eulogy in honor of Calhoun on the floor of the Senate. Benton, a devoted Unionist, declined, saying: "He is not dead, sir—he is not dead. There may be no vitality in his body, but there is in his doctrines."

The Clemson University campus in South Carolina occupies the site of Calhoun's Fort Hill plantation, which he bequeathed to his wife and daughter. They sold it and its 50 slaves to a relative. When that owner died, Thomas Green Clemson foreclosed the mortgage. He later bequeathed the property to the state for use as an agricultural college to be named after him.

Calhoun's widow, Floride, died on July 25, 1866, and was buried in St. Paul's Episcopal Church Cemetery in Pendleton, South Carolina, near their children, but apart from her husband.

Historian LeeH. Cheek, Jr., distinguishes between two strands of American republicanism: the puritan tradition, based in New England, and the agrarian or South Atlantic tradition, which Cheek argues was espoused by Calhoun. While the New England tradition stressed a politically centralized enforcement of moral and religious norms to secure civic virtue, the South Atlantic tradition relied on a decentralized moral and religious order based on the idea of subsidiarity (or localism). Cheek maintains that the "Kentucky and Virginia Resolutions" (1798), written by Jefferson and Madison, were the cornerstone of Calhoun's republicanism. Calhoun emphasized the primacy of subsidiarity—holding that popular rule is best expressed in local communities that are nearly autonomous while serving as units of a larger society.

Calhoun led the pro-slavery faction in the Senate, opposing both total abolitionism and attempts such as the Wilmot Proviso to limit the expansion of slavery into the western territories.

Calhoun's father, Patrick Calhoun, helped shape his son's political views. He was a staunch supporter of slavery who taught his son that social standing depended not merely on a commitment to the ideal of popular self-government but also on the ownership of a substantial number of slaves. Flourishing in a world in which slaveholding was a hallmark of civilization, Calhoun saw little reason to question its morality as an adult. He further believed that slavery instilled in the remaining whites a code of honor that blunted the disruptive potential of private gain and fostered the civic-mindedness that lay near the core of the republican creed. From such a standpoint, the expansion of slavery decreased the likelihood for social conflict and postponed the declension when money would become the only measure of self-worth, as had happened in New England. Calhoun was thus firmly convinced that slavery was the key to the success of the American dream.

Whereas other Southern politicians had excused slavery as a "necessary evil," in a famous on February 6, 1837, Calhoun asserted that slavery was a "positive good." He rooted this claim on two grounds: white supremacy and paternalism. All societies, Calhoun claimed, are ruled by an elite group that enjoys the fruits of the labor of a less-exceptional group. Senator William Cabell Rives of Virginia earlier had referred to slavery as an evil that might become a "lesser evil" in some circumstances. Calhoun believed that conceded too much to the abolitionists:

Calhoun's treatment of his own slaves includes this incident in 1831, when his slave Alick ran away when threatened with a severe whipping. Calhoun wrote to the captor:
Calhoun rejected the belief of Southern leaders such as Henry Clay that all Americans could agree on the "opinion and feeling" that slavery was wrong, although they might disagree on the most practicable way to respond to that great wrong. Calhoun's constitutional ideas acted as a viable conservative alternative to Northern appeals to democracy, majority rule, and natural rights.

As well as providing the intellectual justification of slavery, Calhoun played a central role in devising the South's overall political strategy. According to Phillips:

Shortly after delivering his speech against the Compromise of 1850, Calhoun predicted the destruction of the Union over the slavery issue. Speaking to Senator Mason, he said:

Calhoun was consistently opposed to the War with Mexico, arguing that an enlarged military effort would only feed the alarming and growing lust of the public for empire regardless of its constitutional dangers, bloat executive powers and patronage, and saddle the republic with a soaring debt that would disrupt finances and encourage speculation. Calhoun feared, moreover, that Southern slave owners would be shut out of any conquered Mexican territories, as nearly happened with the Wilmot Proviso. He argued that the war would detrimentally lead to the annexation of all of Mexico, which would bring Mexicans into the country, whom he considered deficient in moral and intellectual terms. He said, in a speech on January 4, 1848:

Anti-slavery Northerners denounced the war as a Southern conspiracy to expand slavery; Calhoun in turn perceived a connivance of Yankees to destroy the South. By 1847 he decided the Union was threatened by a totally corrupt party system. He believed that in their lust for office, patronage and spoils, politicians in the North pandered to the anti-slavery vote, especially during presidential campaigns, and politicians in the slave states sacrificed Southern rights in an effort to placate the Northern wings of their parties. Thus, the essential first step in any successful assertion of Southern rights had to be the jettisoning of all party ties. In 1848–49, Calhoun tried to give substance to his call for Southern unity. He was the driving force behind the drafting and publication of the "Address of the Southern Delegates in Congress, to Their Constituents." It alleged Northern violations of the constitutional rights of the South, then warned Southern voters to expect forced emancipation of slaves in the near future, followed by their complete subjugation by an unholy alliance of unprincipled Northerners and blacks. Whites would flee and the South would "become the permanent abode of disorder, anarchy, poverty, misery, and wretchedness." Only the immediate and unflinching unity of Southern whites could prevent such a disaster. Such unity would either bring the North to its senses or lay the foundation for an independent South. But the spirit of union was still strong in the region and fewer than 40% of the Southern congressmen signed the address, and only one Whig.

Many Southerners believed his warnings and read every political news story from the North as further evidence of the planned destruction of the white southern way of life. The climax came a decade after Calhoun's death with the election of Republican Abraham Lincoln in 1860, which led to the secession of South Carolina, followed by six other Southern states. They formed the new Confederate States, which, in accordance with Calhoun's theory, did not have any organized political parties.

Calhoun's basic concern for protecting the diversity of minority interests is expressed in his chief contribution to political science—the idea of a concurrent majority across different groups as distinguished from a numerical majority. A concurrent majority is a system in which a minority group is permitted to exercise a sort of veto power over actions of a majority that are believed to infringe upon the minority's rights.

According to the principle of a numerical majority, the will of the more numerous citizens should always rule, regardless of the burdens on the minority. Such a principle tends toward a consolidation of power in which the interests of the absolute majority always prevail over those of the minority. Calhoun believed that the great achievement of the American constitution was in checking the tyranny of a numerical majority through institutional procedures that required a concurrent majority, such that each important interest in the community must consent to the actions of government. To secure a concurrent majority, those interests that have a numerical majority must compromise with the interests that are in the minority. A concurrent majority requires a unanimous consent of all the major interests in a community, which is the only sure way of preventing tyranny of the majority. This idea supported Calhoun's doctrine of interposition or nullification, in which the state governments could refuse to enforce or comply with a policy of the Federal government that threatened the vital interests of the states.

Historian Richard Hofstadter (1948) emphasizes that Calhoun's conception of "minority" was very different from the minorities of a century later:

Unlike Jefferson, Calhoun rejected attempts at economic, social, or political leveling, claiming that true equality could not be achieved if all classes were given equal rights and responsibilities. Rather, to ensure true prosperity, it was necessary for a stronger group to provide protection and care for the weaker one. This meant that the two groups should not be equal before the law. For Calhoun, "protection" (order) was more important than freedom. Individual rights were something to be earned, not something bestowed by nature or God. Calhoun was concerned with protecting the interests of the Southern States (which he identified with the interests of their slaveholding elites) as a distinct and beleaguered minority among the members of the federal Union; his idea of a concurrent majority as a protection for minority rights has gained some acceptance in American political thought. Political scientist Malcolm Jewell argues, "The decision-making process in this country resembles John Calhoun's 'concurrent majority': A large number of groups both within and outside the government must, in practice, approve any major policy."

Calhoun's ideas on the concurrent majority are illustrated in "A Disquisition on Government". The "Disquisition" is a 100-page essay on Calhoun's definitive and comprehensive ideas on government, which he worked on intermittently for six years until its 1849 completion. It systematically presents his arguments that a numerical majority in any government will typically impose a despotism over a minority unless some way is devised to secure the assent of all classes, sections, and interests and, similarly, that innate human depravity would debase government in a democracy.

In the 1840s three interpretations of the constitutional powers of Congress to deal with slavery in territories emerged: the "free-soil doctrine," the "popular sovereignty position," and the "Calhoun doctrine." The Free Soilers stated that Congress had the power to outlaw slavery in the territories. The popular sovereignty position argued that the voters living there should decide. The Calhoun doctrine said that Congress and the citizens of the territories could never outlaw slavery in the territories.

In what historian Robert R. Russell calls the "Calhoun Doctrine," Calhoun argued that the Federal Government's role in the territories was only that of the trustee or agent of the several sovereign states: it was obliged not to discriminate among the states and hence was incapable of forbidding the bringing into any territory of anything that was legal property in any state. Calhoun argued that citizens from every state had the right to take their property to any territory. Congress and local voters, he asserted, had no authority to place restrictions on slavery in the territories. In a February 1847 speech before the senate, Calhoun declared that "the enacthment of any law which should directly, or by its effects, deprive the citizens of any of the States of this Union from emigrating, with their property, in to any of the territories of the United States, will make such discrimination and would therefore be a violation of the Constitution." Enslavers therefore had a fundamental right to take their property wherever they wished. As constitutional historian Hermann von Holst noted, "Calhoun's doctrine made it a solemn constitutional duty of the United States government and of the American people to act as if the existence or non-existence of slavery in the Territories did not concern them in the least." The Calhoun Doctrine was opposed by the Free Soil forces, which merged into the new Republican Party around 1854. Chief Justice Roger B. Taney based his decision in the 1857 Supreme Court case "Dred Scott v. Sandford," in which he ruled that the federal government could not prohibit slavery in any of the territories, upon Calhoun's arguments. Moderates rejected these beliefs, and Taney's decision became a major point of partisan attack by the Republican Party.

Many different places, streets, and schools were named after Calhoun, as may be seen on the list linked above. Some, such as Springfield, Illinois (1832) and Jackson County, Kansas (1859), were subsequently renamed. The "Immortal Trio" (Calhoun, Daniel Webster, and Henry Clay) were memorialized with streets in Uptown New Orleans.

The Confederate government honored Calhoun on a 1¢ postage stamp, which was printed in 1862 but was never officially released.

In 1887, a monument to Calhoun was erected in Marion Square in Charleston, South Carolina; it was not well-liked by the residents and was replaced in 1896 by a different monument that still stands.

The USS "JohnC. Calhoun", in commission from 1963 to 1994, was a Fleet Ballistic Missile nuclear submarine.

In 1817, surveyors sent by Secretary of War Calhoun to map the area around Fort Snelling named the largest lake in what became Minneapolis, Minnesota, for him. Two centuries later, the city of Minneapolis renamed the lake with the Dakota language name Bde Maka Ska, meaning "White Earth Lake" or "White Banks Lake". The Calhoun-Isles Community Band in the Uptown district of Minneapolis changed its name to City of Lakes Community Band in November 2018 to distance itself from Calhoun's pro-slavery legacy following the renaming of the lake.

Calhoun was portrayed by actor Arliss Howard in the 1997 film "Amistad". The film depicts the controversy and legal battle surrounding the status of slaves who in 1839 rebelled against their transporters on "La Amistad" slave ship.

Calhoun was despised by Jackson and his supporters for his alleged attempts to subvert the unity of the nation for his own political gain. On his deathbed, Jackson regretted that he had not had Calhoun executed for treason. "My country," he declared, "would have sustained me in the act, and his fate would have been a warning to traitors in all time to come." Even after his death, Calhoun's reputation among Jacksonians remained poor. They disparaged him by portraying him as a man thirsty for power, who when he failed to attain it, sought to tear down his country with him. According to Parton, writing in 1860:
Calhoun is often remembered for his defense of minority rights, in the context of defending white Southern interests from perceived Northern threats, by use of the "concurrent majority." He is also noted and criticized for his strong defense of slavery. These positions played an enormous role in influencing Southern secessionist leaders by strengthening the trend of sectionalism, thus contributing to the Civil War. During his lifetime and after, Calhoun was seen as one of the Senate's most important figures. In 1957, a Senate committee chaired by John F. Kennedy selected Calhoun as one of the five greatest United States senators in history. Biographer Irving Bartlett wrote:
Calhoun is often held in high regard by the Southern Lost Cause historians, who hold a romanticized view of the old Southern way of life and its cause during the Civil War. Historians such as Charles M. Wiltse, Margaret Coit, and Clyde N. Wilson have, in their writings, portrayed Calhoun as a sympathetic or heroic figure.

John Niven paints a portrait of Calhoun that is both sympathetic and tragic. He says that Calhoun's ambition and personal desires "were often thwarted by lesser men than he." Niven identifies Calhoun as a "driven man and a tragic figure." He argues that Calhoun was motivated by the near disaster of the War of 1812, of which he was a "thoughtless advocate," to work towards fighting for the freedoms and securities of the white Southern people against any kind of threat. Ultimately, Niven says, he "... would overcompensate and in the end would more than any other individual destroy the culture he sought to preserve, perpetuating for several generations the very insecurity that had shaped his public career."

Recently, Calhoun's reputation has suffered particularly due to his defense of slavery. The racially motivated Charleston church shooting in South Carolina in June 2015 reinvigorated demands for the removal of monuments dedicated to prominent pro-slavery and Confederate States figures. That month, the monument to Calhoun in Charleston was found vandalized, with spray-painted denunciations of Calhoun as a racist and a defender of slavery.

In response to decades of requests, Yale President Peter Salovey announced that the university's Calhoun College will be renamed in 2017 to honor Grace Murray Hopper, a pioneering computer programmer, mathematician and Navy rear admiral who graduated from Yale. Calhoun is commemorated elsewhere on the campus, including the exterior of Harkness Tower, a prominent campus landmark, as one of Yale's "Eight Worthies."

In Calhoun's defense, paleoconservative commentator Clyde Wilson, editor of the multivolume "The Papers of John C. Calhoun" and a Distinguished Chair of the Abbeville Institute, argued:







</doc>
<doc id="52707" url="https://en.wikipedia.org/wiki?curid=52707" title="Kate Winslet">
Kate Winslet

Kate Elizabeth Winslet, (born 5 October 1975) is an English actress. She is particularly known for her work in period dramas and tragedies, and is often drawn to portraying troubled women. Winslet is the recipient of several accolades, including three British Academy Film Awards, and is among the few performers to have won Academy, Emmy, and Grammy Awards.

Born in Reading, Berkshire, Winslet studied drama at the Redroofs Theatre School. Her first screen appearance, at the age of 15, was in the British television series "Dark Season" (1991). She made her film debut playing a teenage murderess in "Heavenly Creatures" (1994), and received her first BAFTA Award for playing Marianne Dashwood in "Sense and Sensibility" (1995). Global stardom followed soon after with her leading role in the epic romance "Titanic" (1997). It was the highest-grossing film of all time to that point, after which she eschewed parts in blockbusters in favour of critically acclaimed period pieces, including "Quills" (2000) and "Iris" (2001), which were not widely seen.

The science fiction romance "Eternal Sunshine of the Spotless Mind" (2004), in which Winslet was cast against type in a contemporary setting, proved to be a turning point in her career, and she gained further recognition for her performances in "Finding Neverland" (2004), "Little Children" (2006), "Revolutionary Road" (2008), and "The Reader" (2008). For playing a Nazi camp guard in the last of these, she won the BAFTA Award and Academy Award for Best Actress. In the 2010s, Winslet played a single mother in 1930s America in the miniseries "Mildred Pierce" (2011), joined the "Divergent" film series, and portrayed Joanna Hoffman in "Steve Jobs" (2015). She won a Primetime Emmy Award for the former and a third BAFTA Award for the latter.

For her narration of a short story in the audiobook "Listen to the Storyteller" (1999), Winslet won a Grammy Award. She performed the song "What If" for the soundtrack of her film "" (2001). A co-founder of the charity Golden Hat Foundation, which aims to create autism awareness, she has written a book on the topic, "The Golden Hat: Talking Back to Autism" (2010). "Time" magazine named her one of the 100 most influential people in the world in 2009, and in 2012, she was appointed Commander of the Order of the British Empire (CBE). Divorced from the film directors Jim Threapleton and Sam Mendes, Winslet has been married to the businessman Ned Rocknroll since 2012. She has a child from each marriage.

Kate Winslet was born on 5 October 1975 in Reading, Berkshire, England, to Sally Anne (née Bridges) and Roger John Winslet. Her mother worked as a nanny and waitress, and her father, a struggling actor, took labouring jobs to support the family. Her maternal grandparents were both actors and ran the Reading Repertory Theatre Company. Winslet has two sisters, Anna and Beth, both of whom are actresses, and a younger brother, Joss. The family had limited financial means; they lived on free meal benefits and were supported by a charity named the Actor's Charitable Trust. When Winslet was 10, her father severely injured his foot in a boating accident and found it harder to work, leading to more financial hardships for the family. Winslet has said that her parents always made them feel cared for and that they were a supportive family.

Winslet attended St Mary and All Saints' Church of England primary school. Living in a family of actors inspired her to pursue acting from a young age. She and her sisters participated in amateur stage shows at school and at a local youth theatre, named Foundations. When she was five, Winslet made her first stage appearance as Mary in her school's production of the Nativity. She has described herself as an overweight child; she was nicknamed "blubber" by her schoolmates and was bullied for the way she looked. She said that she did not let this defeat her. At 11, Winslet was accepted into the Redroofs Theatre School in Maidenhead. The school also functioned as an agency and took students to London to audition for acting jobs. She appeared in a Sugar Puffs commercial and dubbed for foreign films. At school, she was made head girl and took part in productions of "Alice's Adventures in Wonderland" and "The Lion, the Witch and the Wardrobe", and played the lead role of Wendy Darling in "Peter Pan". She worked simultaneously with the Starmaker Theatre Company in Reading. She participated in over 20 of their stage productions, but was rarely selected as the lead due to her weight. Nonetheless, she played key roles as Miss Agatha Hannigan in "Annie", the Mother Wolf in "The Jungle Book", and Lena Marelli in "Bugsy Malone".

In 1991, within two weeks of finishing her GCSE examinations, Winslet made her screen debut as one of the main cast members of the BBC science fiction television series "Dark Season". Her part was that of Reet, a schoolgirl who helps her classmates fight against a sinister man distributing free computers to her school. She did not earn much from the job, and at 16, a lack of funds forced Winslet to leave Redroofs. To support herself, she worked at a delicatessen. In 1992, she had a small part in the television film "Anglo-Saxon Attitudes", an adaptation of Angus Wilson's satirical novel. Winslet, who weighed at the time, played the daughter of an obese woman in it. While filming, an off-hand comment from the director Diarmuid Lawrence about the likeness between her and the actress who played her mother prompted Winslet to lose weight. She next took on the role of the young daughter of a bankrupt middle-aged man (played by Ray Winstone) in the television sitcom "Get Back" (1992–93). She also had a guest role in a 1993 episode of the medical drama series "Casualty".

Winslet was among 175 girls to audition for Peter Jackson's psychological drama "Heavenly Creatures" (1994), and was cast after impressing Jackson with the intensity she brought to her part. The New Zealand-based production is based on the Parker–Hulme murder case of 1954, in which Winslet played Juliet Hulme, a teenager who assists her friend, Pauline Parker (played by Melanie Lynskey), in the murder of Pauline's mother. She prepared for the part by reading the transcripts of the girls' murder trial, their letters and diaries, and interacted with their acquaintances. She has said that she learnt tremendously from the job. Jackson filmed in the real murder locations, and the experience left Winslet traumatised. She found it difficult to detach herself from her character, and said that after returning home, she often cried. The film was a critical breakthrough for Winslet; "The Washington Post" writer Desson Thomson called her "a bright-eyed ball of fire, lighting up every scene she's in". Winslet recorded "Juliet's Aria" for the film's soundtrack. Also that year, she appeared as Geraldine Barclay, a prospective secretary, in the Royal Exchange Theatre production of Joe Orton's farce "What the Butler Saw".

While promoting "Heavenly Creatures" in Los Angeles, Winslet auditioned for the minor part of Lucy Steele for a 1995 film adaptation of Jane Austen's novel "Sense and Sensibility", starring and written by Emma Thompson. Impressed by her reading, Thompson cast her for the much larger part of the recklessly romantic teenager Marianne Dashwood. The director Ang Lee wanted Winslet to play the part with grace and restraintaspects that he felt were missing from her performance in "Heavenly Creatures"and thus asked her to practice tai chi, read gothic literature, and learn to play the piano. David Parkinson of the "Radio Times" found Winslet to be a standout among the ensemble cast, and Mick LaSalle of the "San Francisco Chronicle" took note of how well she had portrayed her character's growth and maturity. The film grossed over US$134 million worldwide. She won the Screen Actors Guild and British Academy Film Award for Best Supporting Actress, and received nominations for the Academy Award and Golden Globe Award in the same category. Also in 1995, Winslet featured in the poorly received Disney film "A Kid in King Arthur's Court".

Winslet had roles in two period dramas of 1996"Jude" and "Hamlet". As with her "Heavenly Creatures" part, Winslet's roles in these films were those of women with a "mad edge". In Michael Winterbottom's "Jude", based on the novel "Jude the Obscure" by Thomas Hardy, she played Sue Bridehead, a young woman with suffragette leanings who falls in love with her cousin, Jude (played by Christopher Eccleston). Roger Ebert believed that the part allowed Winslet to display her acting range, and praised her for the defiance she brought to the role. After unsuccessfully auditioning for Kenneth Branagh's 1994 film "Mary Shelley's Frankenstein", Winslet was cast for the part of Ophelia, the doomed lover of the title character, in Branagh's adaptation of William Shakespeare's tragedy "Hamlet". Winslet, at 20 years old, was intimidated by the experience of performing Shakespeare with established actors such as Branagh and Julie Christie, saying that the job required a level of intellect that she thought she did not possess. Mike Jeffries of "Empire" believed that she had played the part "well beyond her years". Despite the acclaim, "Jude" and "Hamlet" earned little at the box office.

Winslet was keen on playing Rose DeWitt Bukater, a free-spirited socialite aboard the ill-fated RMS "Titanic", in James Cameron's epic romance "Titanic" (1997). Cameron was initially reluctant to cast her, preferring the stars Claire Danes or Gwyneth Paltrow, but Winslet pleaded with him, "You don't understand! I am Rose! I don't know why you're even seeing anyone else!" Her persistence led Cameron to hire her. Leonardo DiCaprio featured as her love interest, Jack. "Titanic" had a production budget of US$200 million, and its arduous principal photography was held at Rosarito Beach where a replica of the ship was created. Filming proved to be taxing for Winslet. She nearly drowned, caught influenza, suffered from hypothermia on being submerged in freezing water, and had bruises on her arms and knees. The workload allowed her only four hours of sleep per day and she felt drained by the experience. David Ansen, writing for "Newsweek", praised Winslet for capturing her character's zeal with delicacy, and Mike Clark of "USA Today" considered her to be the film's prime asset. Against expectations, "Titanic" went on to become the highest-grossing film of all time to that point, earning over US$2 billion in box-office receipts worldwide, and established Winslet as a global star. The film won 11 Academy Awardsthe most for any filmincluding Best Picture and gained Winslet a Best Actress nomination.

Winslet did not view "Titanic" as a platform for bigger salaries. She eschewed parts in blockbuster films in favour of independent productions that were not widely seen, believing that she "still had a lot to learn" and was unprepared to be a star. She later said that her decision ensured career longevity. "Hideous Kinky", a low-budget drama shot before the release of "Titanic", was Winslet's sole film release of 1998. She turned down offers to star in "Shakespeare in Love" (1998) and "Anna and the King" (1999) to do the film. Based on the semi-autobiographical novel by Esther Freud, "Hideous Kinky" tells the story of a single British mother yearning for a new life in 1970s Morocco. Writing for "The New York Times", the critic Janet Maslin commended Winslet's decision to follow-up "Titanic" with such an offbeat project, and took note of how well she had captured her character's "obliviousness and optimism".

Jane Campion's psychological drama "Holy Smoke!" (1999) featured Winslet as an Australian woman who joins an Indian religious cult. She found the script brave and was challenged by the idea of portraying an unlikable, manipulative woman. She learnt an Australian accent and worked closely with Campion to justify her character's vileness. The film required her to perform explicit sex scenes with her co-star Harvey Keitel, and featured a scene in which her character appears stark naked and urinates on herself. David Rooney of "Variety" wrote, "Showing the kind of courage few young thesps would be capable of and an extraordinary range [...] from animal cunning to unhinged desperation, [Winslet] holds nothing back." That same year, she voiced a fairy for the animated film "Faeries", and won the Grammy Award for Best Spoken Word Album for Children for narrating the short story "The Face in the Lake" for the children's audiobook "Listen to the Storyteller".

In "Quills" (2000), a biopic of the erratic Marquis de Sade, starring Geoffrey Rush and Joaquin Phoenix, Winslet played the supporting part of a sexually repressed laundress working in a mental asylum. Hailing her as the "most daring actress working today", James Greenberg of "Los Angeles" magazine praised Winslet for "continuing to explore the bounds of sexual liberation". She received a SAG Award nomination for Best Supporting Actress. The following year, she played a fictitious mathematician involved in the cracking of the Enigma ciphers in Michael Apted's espionage thriller "Enigma". Winslet's character was vastly expanded from a subsidiary love-interest in the novel it was based on to a prominent code-breaker in the film. She was pregnant while filming, and to prevent this from showing, she wore corsets under her costume.

The biopic "Iris" (2001) featured Winslet and Judi Dench as the novelist Iris Murdoch at different ages. The director Richard Eyre cast the two actresses after finding a "correspondence of spirit between them". Winslet was drawn to the idea of playing an intellectual and zesty female lead, and in research, she read Murdoch's novels, studied her husband's memoir "Elegy for Iris", and watched televised interviews of Murdoch. The project was filmed over four weeks and allowed Winslet to bring her daughter, who was six months old at the time, on set. Writing for "The Guardian", Martin Amis commented that "the seriousness and steadiness of [Winslet's] gaze effectively suggest the dawning amplitude of the Murdoch imagination". Winslet received her third Oscar nomination for "Iris", in addition to BAFTA and Golden Globe nominations for Best Supporting Actress.

Winslet's third film release of 2001 was the animated film "", based on Charles Dickens' novel. For the film's soundtrack she sang "What If", which proved to be a commercial hit; she donated her earnings from it to children's charities. After a year-long absence from the screen, Winslet starred as a headstrong journalist interviewing a professor on death row in the thriller "The Life of David Gale" (2003). She agreed to the project to work with the director Alan Parker, whom she admired, and believed that the film raised pertinent questions about capital punishment. Mick LaSalle thought that the film had muddled the subject and disliked both the film and Winslet's performance.

To avoid typecasting in historical dramas, Winslet sought out films set in contemporary times. She found it in the science fiction romance "Eternal Sunshine of the Spotless Mind" (2004), in which she played the neurotic and impetuous Clementine, a woman who decides to erase memories of her ex-boyfriend (played by Jim Carrey). Unlike her previous assignments, the role allowed her to display the quirky side to her personality. Gondry encouraged Carrey and Winslet to improvise on set, and to keep herself agile she practised kickboxing. "Eternal Sunshine of the Spotless Mind" proved to be a modest financial success and several critics regarded it as one of the best films of the 21st century. Peter Travers of "Rolling Stone" considered it to be a "uniquely funny, unpredictably tender and unapologetically twisted romance" and thought that Winslet was "electrifying and bruisingly vulnerable" in it. A journalist for "Premiere" magazine credited her for abandoning her "corseted English rose persona", and featured it as the 81st greatest film performance of all time. Winslet considers it to be a favourite among her roles, and she received Best Actress nominations at the Oscar and BAFTA award ceremonies. She has said that the film marked a turning point in her career and prompted directors to offer her a wide variety of parts.

Winslet was paid £6 million to star in her next release of the year, the drama "Finding Neverland". It is about the relationship between J. M. Barrie (played by Johnny Depp) and the Llewelyn Davies boys, which inspired Barrie to write Peter Pan; Winslet played the boys' mother, Sylvia. Despite her reluctance to star in another period piece, Winslet agreed to the project after empathising with Sylvia's love for her children. Ella Taylor of "LA Weekly" found Winslet to be "radiant and earthy as ever" and CNN's Paul Clinton thought that she was "exceptional in a delicate and finely tuned performance". She received a second Best Actress nomination at that year's BAFTA Award ceremony. With a box office gross of US$116 million, "Finding Neverland" became her most widely seen film since "Titanic".

In 2005, Winslet took on a guest role in an episode of the British comedy sitcom "Extras", starring Ricky Gervais and Stephen Merchant. She played a satirical version of herself in itan actress, who in an effort to win an Oscar, takes the role of a nun in a Holocaust film. She received a Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series nomination. Within three months of giving birth to her second child, Winslet returned to work on "Romance & Cigarettes", a musical romantic comedy directed by John Turturro, in which she played Tula, a promiscuous and foulmouthed woman. The part required her to sing and dance, and it helped her lose weight gained during her pregnancy. She twisted her ankle while filming one of the dance sequences. Derek Elley of "Variety" wrote that despite her limited screen time, Winslet had "the showiest role and filthiest one-liners". Winslet declined an offer from Woody Allen to star in "Match Point" (2005) to spend more time with her children.

Winslet had four film releases in 2006. She first appeared in "All the King's Men", a political thriller set in 1940s Louisiana, featuring Sean Penn and Jude Law. She played the supporting part of the love interest to Law's character. The film received negative reviews for its lack of political insight and narrative cohesiveness, and failed to recoup its US$55 million investment. Her next release, the drama "Little Children", was better received. Based on the novel of the same name, the film tells the story of Sarah Pierce, an unhappy housewife who has an affair with a married neighbour (played by Patrick Wilson). Winslet was challenged by the role of an uncaring mother, as she did not understand or respect her character's actions. Scenes requiring her to be hostile towards the child actress playing her daughter proved upsetting for her. Having given birth to two children, Winslet was anxious about the sex scenes in which she had to be nude; she took on the challenge to present a positive image for women with imperfect bodies. A. O. Scott of "The New York Times" wrote that Winslet successfully "registers every flicker of Sarah's pride, self-doubt and desire, inspiring a mixture of recognition, pity and concern". With another Academy Award for Best Actress nomination, Winslet, at 31, became the youngest performer to accrue five Oscar nominations.

After "Little Children", Winslet played a part she found more sympathetic in Nancy Meyers' romantic comedy "The Holiday". She played Iris, a Briton who temporarily exchanges homes with an American (played by Cameron Diaz) during the Christmas holiday season. The film became Winslet's biggest commercial success in nine years, grossing over US$205 million worldwide. The critic Justin Chang thought the film was formulaic yet pleasing, and took note of Winslet's radiance and charm. In her final release of the year, Winslet voiced Rita, a scavenging sewer rat, in the animated film "Flushed Away". Winslet's sole project of 2007 was as the narrator for the English version of the French children's film "The Fox and the Child".

Winslet had two critically acclaimed roles in 2008. After reading Justin Haythe's screenplay for "Revolutionary Road", an adaptation of Richard Yates's debut novel, Winslet recommended the project to her husband at the time, the director Sam Mendes, and her "Titanic" co-star Leonardo DiCaprio. The film traces the tribulations of a young married couple in 1950s suburban America. Winslet was attracted to the idea of playing a woman whose aspirations had not been met, and she read "The Feminine Mystique" to understand the psychology of unhappy housewives from the era. Mendes encouraged DiCaprio and Winslet to spend time together, and she believed that the small set they used helped them to develop their character's strained relationship. Hailing Winslet as "the best English-speaking film actress of her generation", David Edelstein of "New York" magazine wrote that "there isn't a banal moment in Winslet's performance—not a gesture, not a word".

To avoid a scheduling conflict with "Revolutionary Road", Winslet turned down an offer to star in "The Reader". After her replacement Nicole Kidman left the project due to her pregnancy, Winslet was signed to it. Directed by Stephen Daldry, "The Reader" is based on Bernhard Schlink's novel "Der Vorleser" and is about Hanna Schmitz, an illiterate Nazi concentration camp guard (Winslet), who has an affair with a teenage boy. Winslet researched the Holocaust and the SS guards. To educate herself on the stigma of illiteracy, she spent time with students at the Literacy Partners, an organisation that teaches adults to read and write. Winslet was unable to sympathise with Schmitz and struggled to play the part honestly without humanising her actions. Despite this, some historians criticised the film for making Schmitz an object of the audience's sympathy and accused the filmmakers of Holocaust revisionism. Todd McCarthy commended her for supplying "a haunting shell to this internally decimated woman", and writing for "The Daily Telegraph", Sukhdev Sandhu considered her to be "absolutely fearless here, not just in her willingness to expose herself physically, but her refusal to expose her character psychologically".

Winslet received significant awards attention for her performances in "Revolutionary Road" and "The Reader". She won a Golden Globe Award for each of these films, and for the latter, she was awarded the Academy Award and BAFTA Award for Best Actress. At age 33, she surpassed her own record as the youngest performer to garner six Oscar nominations. She also became the third actress in history to win two Golden Globe Awards at the same ceremony. Exhausted by the media attention during this period, Winslet took two years off work until she was ready to creatively engage again.

Winslet returned to acting with the five-part HBO miniseries "Mildred Pierce" (2011), an adaptation of James M. Cain's novel from the director Todd Haynes. It is about the titular heroine (Winslet), a divorcée during the Great Depression struggling to establish a restaurant business while yearning for the respect of her narcissistic daughter (played by Evan Rachel Wood). Winslet, who had recently divorced Mendes, believed that certain aspects of her character's life mirrored her own. She was intimidated by the scope of the production, as she featured in every scene of the 280-page script. She was disturbed and upset by the story, and was particularly fascinated by the complex relationship between the mother-daughter pair. She collaborated closely with the production and costume designers, and learnt to bake pies and prepare chickens. The broadcast received a limited audience but gained positive reviews. Matt Zoller Seitz of Salon called the series a "quiet, heartbreaking masterpiece" and described Winslet's performance as "terrific—intelligent, focused and seemingly devoid of ego". She received the Primetime Emmy Award for Outstanding Lead Actress, in addition to Golden Globe and SAG Award wins.

The ensemble thriller "Contagion" from Steven Soderbergh was Winslet's first film release of 2011. She was cast as a disease detective for the CDC, and she modelled her role on Anne Schuchat, the director of the NCIRD. "Contagion" was a commercial success, and David Denby of "The New Yorker" praised Winslet for capturing the essence of an exasperated woman. Her next project was the Roman Polanski-directed "Carnage", adapted from the play "God of Carnage" by Yasmina Reza. Set entirely inside an apartment, the black comedy follows two sets of parents feuding over their respective children. Jodie Foster, John C. Reilly, and Christoph Waltz co-starred. The cast rehearsed the script like a play for two weeks, and Winslet brought her children with her to Paris for the eight weeks of filming. Critics found the adaptation to be less compelling than the play, but praised Foster and Winslet's work. They both received Golden Globe nominations for it.

Winslet said that her workload of 2011 helped her get over heartbreak from her divorce, and after finishing work on "Carnage" she took a break from acting to focus on her children. A short part that she had filmed four years earlier for the anthology film "Movie 43" was her sole screen appearance of 2012, and it received the worst reviews of her career. Winslet also performed an audiobook recording of Émile Zola's novel "Thérèse Raquin". She was reluctant to accept Jason Reitman's offer to star in his 2013 film adaptation of Joyce Maynard's novel "Labor Day", but agreed after Reitman postponed the production for a year to accommodate Winslet's commitment to her children. Set over a Labor Day weekend, it tells the story of Adele (Winslet), an agoraphobic single mother, who falls in love with an escaped convict. Describing Adele's character as having "more vulnerability than strength", Winslet found her to be a departure from the strong-willed women she typically played. A scene in the film required her to make a pie, for which she drew on her baking experience from "Mildred Pierce". Critical reception for the film was negative, but it earned Winslet her tenth Golden Globe nomination. Chris Nashawaty of "Entertainment Weekly" criticised it as "mawkish and melodramatic" but commended Winslet for adding layers to her passive role.

The novelty of playing a villainous part drew Winslet to Jeanine Matthews in the science fiction film "Divergent" (2014). Set in a dystopian future, the adaptation of Veronica Roth's young adult novel stars Shailene Woodley as a heroine fighting an oppressive regime headed by Winslet's character. Winslet was pregnant with her third child while filming, and her tight-fitting costumes had to be altered to accommodate the pregnancy. To maintain her character's intimidating persona, Winslet remained aloof from her co-stars for much of the filming. Richard Lawson of "Vanity Fair" compared the film unfavourably to the "Hunger Games" film series, and thought that Winslet was underutilised in it. The film earned US$288 million worldwide. "A Little Chaos" marked Winslet's return to the period film genre. Directed by Alan Rickman, it is about a rivalry among gardeners commissioned to create a fountain at the Palace of Versailles. Winslet's role was that of the fictitious architect Sabine de Barra, someone she believed had overcome extreme grief and hardship like herself. Catherine Shoard of "The Guardian" took note of the "emotional honesty" Winslet brought to her part, but criticised the implausibility of her role. Also that year, she read audiobooks of Roald Dahl's children's novels "Matilda" and "The Magic Finger".

In 2015, Winslet reprised the role of Jeanine Matthews in the second instalment of the "Divergent" series, subtitled "", which despite negative reviews earned US$297 million worldwide. Her next film, an adaptation of the Australian gothic novel "The Dressmaker", was described by the director Jocelyn Moorhouse as being reminiscent of the western "Unforgiven" (1992). Winslet starred as the femme fatale Tilly Dunnage, a seamstress who returns to her hometown years after she was accused of murder. She learned to sew for the part and designed some of her own costumes. The production was filmed in the Australian desert and Winslet found it difficult to wear couture dresses in the harsh weather. Despite disliking the film, Robert Abele of "Los Angeles Times" commended Winslet for underplaying her over-the-top part. The film emerged as one of the highest-grossing Australian films of all time, but earned little elsewhere. Winslet won an AACTA Award for Best Actress.

While filming "The Dressmaker", Winslet became aware of an upcoming biopic of Steve Jobs by Aaron Sorkin and directed by Danny Boyle. Keen to play Jobs' marketing chief and confidant Joanna Hoffman, she sent a picture of herself dressed as Hoffman to the film's producer. "Steve Jobs", starring Michael Fassbender in the title role, is told in three acts, each depicting a key milestone in Jobs' career. In preparation, Winslet spent time with Hoffman, and worked with a dialect coach to adopt Hoffman's accent, a mixture of Armenian and Polish, which she considered to be the hardest one of her career. The cast rehearsed each act like a play and filmed it in sequence. Winslet collaborated closely with Fassbender, and their off-screen relationship mirrored the collegial dynamic between Jobs and Hoffman. The film received some of the best reviews of Winslet's career but was a box office flop. Peter Howell of the "Toronto Star" commended her for finding "strength and grace" in her part, and Gregory Ellwood of HitFix thought that she had improved on Hoffman's characterisation. She won Golden Globe and BAFTA Awards for Best Supporting Actress, and received her seventh Oscar nomination for it.

John Hillcoat's ensemble crime-thriller "Triple 9" (2016) featured Winslet as Irina Vlaslov, a ruthless Russian-Israeli gangster. The critic Ann Hornaday of "The Washington Post" felt that Winslet had failed to effectively portray her. Her next release of the year, "Collateral Beauty", about a man (played by Will Smith) struggling with the death of his daughter, was panned by critics. Writing for "New York" magazine, Emily Yoshida criticised it as a vacuous remake of "A Christmas Carol" and wrote that Winslet had "never looked more painted and tired". It was a modest earner at the box office. Winslet agreed to the romantic disaster film "The Mountain Between Us" (2017) to take on the challenge of a role requiring physical exertion. It featured Idris Elba and her as two strangers who crash land on an icy, isolated mountain range. They filmed in the mountains of Western Canada at 10,000 feet above sea level where the temperature was well below freezing. Winslet performed her own stunts and described it as the most physically gruelling experience of her career. "The Atlantic"s Megan Garber praised the chemistry between Elba and Winslet, and Moira Macdonald of "The Seattle Times" thought that their charisma had enhanced a mediocre picture.

Woody Allen's "Wonder Wheel", a drama set in 1950s Coney Island, was Winslet's final release of 2017. She played Ginny, a temperamental housewife having an affair with a lifeguard (Justin Timberlake). She described Ginny as permanently dissatisfied and uneasy, and playing her proved difficult for Winslet, who suffered from anxiety. Manohla Dargis of "The New York Times" disliked Allen's writing but credited Winslet for filling her "shabby character with feverish life". When asked about her decision to work with Allen despite an allegation of child sexual abuse against him, Winslet chose not to comment on Allen's personal life but said that she was pleased with the collaboration. Several journalists criticised her decision.

"Avatar 2", a science fiction sequel to James Cameron's 2009 film, which required Winslet to work with motion capture technology, will be released in 2020. She learned freediving for her part and was able to hold her breath underwater for seven minutes. Winslet will appear alongside Susan Sarandon and Mia Wasikowska in "Blackbird", a remake of the Danish film "Silent Heart" (2014), directed by Roger Michell, and in Wes Anderson's ensemble drama "The French Dispatch". She will return to HBO to star in and executive produce "Mare of Easttown", a limited series about a troubled police detective, and will portray Mary Anning in "Ammonite", co-starring Saoirse Ronan, which is a romance between two women in 1840s England. She has also committed to portray the model and photographer Lee Miller in an upcoming biopic, and will provide her voice for "Moominvalley", an animated television series about the Moomins.

Winslet supports several charities and causes. An orange top she wore in "Eternal Sunshine of the Spotless Mind" was auctioned in 2004 for a fund-raising event at Helen & Douglas House, a hospice in Oxford. In 2006, she became a patron of a Gloucester-based charity, the Family Haven, which provides counselling services to vulnerable families. The same year, hand-made envelopes designed by Winslet were auctioned for the "Pushing the Envelope" campaign created by the National Literacy Trust. Winslet was one of the celebrities to participate in a 2007 auction to raise funds for the Afghanistan Relief Organization. In 2009, she contributed to the Butterfly Book, a compilation of doodles made by several celebrities, to raise money for leukaemia research.

In 2009, Winslet narrated the English version of an Icelandic documentary named "A Mother's Courage: Talking Back to Autism", about Margret Ericsdottir, whose child Keli Thorsteinsson has non-verbal autism. Inspired by the story, she teamed with Ericsdottir in 2010 to form an NGO named the Golden Hat Foundation. The organisation aims to create autism awareness and was named after a poem written by Thorsteinsson. As the brand ambassador of the luxury brands Lancôme and Longines, Winslet partnered with these companies to raise awareness and funds for the foundation. She created a make-up collection for Lancôme in 2011 and, in 2017, she designed a new watch for Longines.

In 2012, Winslet wrote a book about autism, entitled "The Golden Hat: Talking Back to Autism", which was published by Simon & Schuster. It contains correspondence between Winslet and Ericsdottir, personal statements from various celebrities, and contributions from Thorsteinsson. A reviewer for "Publishers Weekly" praised the book for its "warmth and sincerity". The United Nations featured the book during a ceremony on the World Autism Awareness Day of 2012. For her work with the Golden Hat Foundation, Winslet received Spain's Yo Dona award for Best Humanitarian Work.

Winslet narrated a video for PETA in 2010 that showed animal cruelty in the production of foie gras. She encouraged chefs to remove the item from their menu and urged consumers to boycott it. In 2015, Winslet lent her support to the UNICEF campaign World's Largest Lesson, which creates awareness among children about sustainable development and global citizenship. Teased as a child for her weight, Winslet takes a stand against body-shaming and bullying. She narrated an Australian animated short film named "Daisy Chain" (2015), about a victim of cyber-bullying. In 2017, Winslet teamed with Leonardo DiCaprio's environmental foundation for a fundraiser on global warming. Also that year, DiCaprio and Winslet auctioned off a private dinner with themselves to raise money for a British woman's cancer treatment. Winslet teamed with Lancôme and the National Literacy Trust in 2018 to launch a programme that aims to educate underprivileged women in the UK.

While filming "Dark Season", Winslet, aged 15, began a romantic relationship with the actor-writer Stephen Tredre, who was 12 years her senior. She considered him to be a major influence in her life and cohabited with him in London. The couple separated in 1995, but they remained close until he died of bone cancer two years later. Winslet missed the premiere of "Titanic" to attend his funeral. In a 2008 interview, she said that she had never gotten over his death.

A year after Tredre's death, Winslet met Jim Threapleton on the set of "Hideous Kinky", in which he was an assistant director. They married in November 1998 at her primary school in Reading, and she gave birth to their daughter, Mia, in 2000. Describing her marriage to Threapleton as a "mess", she later said that she had lost control of her instincts during this period. They divorced in 2001. Soon after separating from Threapleton, Winslet met the director Sam Mendes when he offered her a part in a play. She refused the offer but began dating him. Disillusioned by the way the British tabloids portrayed her personal life, Winslet moved to New York. She wed Mendes in May 2003 on the island of Anguilla. Their son, Joe, was born later that year. The family divided their time in New York with frequent visits to their estate in the Cotswolds in England. Amid intense media speculation of an affair between Mendes and the actress Rebecca Hall, the couple announced their separation in 2010 and were divorced a year later. Winslet admitted to being heartbroken by the split, but affirmed her determination to look after her children in spite of her marital breakups.

While holidaying at Richard Branson's estate on Necker Island in 2011, Winslet met her third husband, Ned Rocknroll (born Edward Abel Smith; he is the nephew of Branson and works for Virgin Galactic), during a house fire. The couple married in New York in December 2012, and their son, Bear, was born the next year. After moving back to England, Winslet purchased a property worth £3.25 million by the sea in West Wittering, Sussex, where () she lives with Rocknroll and her children. In a 2015 interview, she described how much she enjoyed living in the countryside.

Winslet has said that despite her three marriages and a family structure that might be perceived as "unconventional" by some, she does not consider it to be any "less of a family". She refuses jobs that take her away from her children for long, and likes to schedule her film commitments to coincide with their school holidays. Discussing her parenting style, Winslet remarked that she enjoys packing lunches and doing the school run.

Several journalists consider Winslet to be among the best actresses of her generation. Despite achieving stardom early in her career with the top-grossing "Titanic", Winslet has rarely acted in commerce-driven pictures. A journalist for "Elle" believes that her choices reflect the "soul and attitude of a jobbing actress, trapped in the body of a movie star". Tom Perrotta, the author of "Little Children", has said that Winslet "gravitates toward troubling roles in smaller films", typically those of "thorny, potentially unsympathetic" women. The journalist Mark Harris writes that she specialises in "unsentimentalized, restless, troubled, discontented, disconcerted, difficult women" and John Hiscock of "The Daily Telegraph" has identified a theme of characters who are free-spirited with a sexual edge to them. Stephen Whitty of NJ.com associates Winslet with "serious, almost despairing material", although he finds it hard to pigeonhole her as an actress.

Leonardo DiCaprio, who starred with Winslet in "Titanic" and "Revolutionary Road", considers her to be "the most prepared and well-researched actor on set", and Jude Law, her co-star in "The Holiday", believes that despite her seriousness she remains "very calm and good-natured". Her "Steve Jobs" director Danny Boyle has identified a willingness in Winslet to avoid typecasting and has said that she takes an effort "to reposition directors' and producers' perspective on her" to allow herself to be challenged as an artist.

Winslet has said that she is interested in parts of "angst-ridden women" with strong dispositions masking flaws and insecurities. She connects with "women who are either finding their way out of a situation, looking for love, having some struggle within love, or questioning the big things in life". Drawn to parts that are in tandem with her personal struggles at certain points in her life, she finds it difficult to detach herself from her roles, saying that "you have to confront your true feelings every single day. And that's pretty exhausting. Then you have to go home and make dinner". Even so, she finds it therapeutic to perform. Winslet is known for her willingness to perform nude scenes, having done so in 12 of her films, although she considers its contribution to the narrative before agreeing to it. She believes that these scenes promote a positive body image among women.

In a 2015 article for "Elle", Sally Holmes described Winslet's ability to establish rapport with her manner. Jo Ellison of "Vogue" writes that she has an "authoritative, almost ambassadorial aura", and Kira Cochrane of "The Guardian" considers her to be "articulate, sophisticated, [with] a definite hint of grandeur". Describing Winslet as plain-spoken, Krista Smith of "Vanity Fair" believes that despite her stardom she is unpretentious.

Winslet's weight fluctuations over the years have been well documented by the media. She has been outspoken about her refusal to allow Hollywood to dictate her weight. In 2003, the British edition of "GQ" magazine published photographs of Winslet that had been digitally altered to make her look taller and thinner. Winslet stated that the alterations were made without her consent. "GQ" subsequently issued an apology. In 2007, she won a defamation case against "Grazia" magazine after it claimed that she had visited a dietitian. She claimed £10,000 in damages, and donated the amount to an eating disorder charity. She won another case in 2009 against the British tabloid "Daily Mail" after it claimed she had lied about her exercise regimen. She received an apology and a payout of £25,000.

Winslet was named one of the most beautiful people in the world by "People" magazine in 2005. Her beauty and sex appeal have been picked up by several other publications, including "Harper's Bazaar", "Who", and "Empire" magazines. She has said that she does not subscribe to the beauty ideal of Hollywood, and uses her celebrity to empower women to accept their appearance with pride. She has spoken against Botox and plastic surgery. In an effort to encourage natural ageing, she formed the British Anti-Cosmetic Surgery League, alongside the actresses Emma Thompson and Rachel Weisz. She instructs magazines and brands not to digitally smooth her wrinkles in photographs. Winslet is reluctant to discuss the gender pay gap in Hollywood as she dislikes talking about her salary in public. In 2009, "Forbes" reported her annual salary to be US$2 million, a majority of that stemming from her endorsement deals. Also that year, the UK Film Council calculated that she had earned £20 million from her acting roles since 1995.

"Time" magazine named Winslet one of the 100 most influential people in the world in 2009. Madame Tussauds in London unveiled a wax statue of Winslet in 2011. The following year, she received an Honorary César award, and in 2014, she received a star on the Hollywood Walk of Fame. Winslet was appointed Commander of the Order of the British Empire (CBE) in the 2012 Birthday Honours for her services to drama.

Prolific in film since 1994, Winslet's most acclaimed and highest-grossing films, according to the online portal Box Office Mojo and the review aggregate site Rotten Tomatoes, include "Heavenly Creatures" (1994), "Sense and Sensibility" (1995), "Hamlet" (1996), "Titanic" (1997), "Eternal Sunshine of the Spotless Mind" (2004), "The Holiday" (2006), "Divergent" (2014), "" (2015), and "Steve Jobs" (2015). Her television projects include the miniseries "Mildred Pierce" (2011).

Winslet has been recognised by the Academy of Motion Picture Arts and Sciences for the following performances:

Winslet has won three BAFTA Awards: Best Actress for "The Reader" (2008); and Best Supporting Actress for "Sense and Sensibility" (1995) and "Steve Jobs" (2016). She has also won the Primetime Emmy Award for Outstanding Lead Actress in a Miniseries for "Mildred Pierce" (2011), and the Grammy Award for Best Spoken Word Album for Children for narrating the children's audiobook "Listen to the Storyteller" (1999). She is among the few actresses to win three of the four major American entertainment awards (EGOT).



</doc>
<doc id="52720" url="https://en.wikipedia.org/wiki?curid=52720" title="Final Fantasy Tactics">
Final Fantasy Tactics

"Final Fantasy Tactics" is set in a fictional medieval-inspired kingdom called Ivalice, created by Yasumi Matsuno. The game's story follows Ramza Beoulve, a highborn cadet who finds himself thrust into the middle of an intricate military conflict known as The Lion War, where two opposing noble factions are coveting the throne of the kingdom. As the story progresses, Ramza and his allies discover a sinister plot behind the war.

The game received extremely positive reviews from gaming magazines and websites and has become a cult classic since its release. It has been cited as one of the greatest video games of all time.

A spin-off title, "Final Fantasy Tactics Advance", was released for the Nintendo Game Boy Advance in 2003 and a sequel to that title, "", was released in 2007 for the Nintendo DS. Various other games have also utilized the Ivalice setting, including "Vagrant Story" for the PlayStation and "Final Fantasy XII" for the PlayStation 2. An enhanced port of "Final Fantasy Tactics", "", was released in 2007 as part of Square Enix's "Ivalice Alliance" project. 

The gameplay of "Final Fantasy Tactics" differs in several key areas from other titles in the "Final Fantasy" series. Instead of a generic battle screen, with the player's characters on one side and the enemies on the other, encounters take place on three-dimensional, isometric fields. Characters move on a battlefield composed of square tiles; movement and action ranges are determined by the character's statistics and job class. Battles are turn-based; a unit may act when its CT (Charge Time) reaches 100. Charge time is increased once every CT unit (a measure of time in battles) by an amount equal to the unit's speed statistic. When CT reaches 100 or greater, the unit may act. During battle, whenever a unit performs an action successfully, it gains Experience Points (EXP) and Job Points (JP).
Another difference is the manner in which random battles are encountered. Like other "Final Fantasy" games, random battles occur on the world map. However, in "Final Fantasy Tactics", random battles only occur in pre-set locations, marked in green on the world map. Passing over one of these spots may result in a random encounter. Another major aspect of battles is magical attacks. Certain magical attacks cause area of effect damage, and many of the more powerful magical attacks require several turns of charging. Hit Points of enemy units are also visible to the player (except in the case of certain bosses), allowing the player to know exactly how much damage they still have to inflict on a particular unit.

Movement on the world map is limited to predefined paths connecting the towns and battle points. When the character icon is over a town, a menu can be opened with several options: "Bar" for taking sidequest job offers, "Shop" for buying supplies and equipment, and "Soldier Office" for recruiting new characters. Later in the game, some towns contain "Fur Shops" for obtaining items by way of poaching monsters.
Like several installments in the series, "Final Fantasy Tactics" features a character class system, which allows players to customize characters into various roles. The game makes extensive use of most of the original character classes seen in earlier "Final Fantasy" games, including Summoners, Wizards (Black Mages), Priests (White Mages), Monks, Lancers (Dragoons), and Thieves. New recruits start out as either a Squire or a Chemist, the base classes for warrior and magician jobs, respectively. The game features twenty jobs accessible by normal characters.

Throughout the game, unique characters also join the party. As well, some characters join as "guests", which are computer-controlled characters that fight on your side. Many of the unique characters have custom classes that replace the base squire class. It's also possible to recruit monsters into the party. Monsters have unique abilities, but cannot change jobs. Monsters can be captured from battles or bred from existing monsters.

In battle, JP are rewarded for every successful action. JP are used to learn new abilities within each job class. Accumulating enough JP results in a job level up; new jobs are unlocked by attaining a certain level in the current job class (for instance, to become a Priest or Wizard, the unit must first attain Job Level 2 as a Chemist), which also allows the character to gain more JP in that class in battles. Once all the abilities of a job class have been learned, the class is "Mastered". A soldier in a specific Job always has its innate skill equipped (Wizards always have "Black Magic," Knights always have "Battle Skill") but a second job-skill slot and several other ability slots (Reaction, Support, and Movement) can be filled with any skill the particular soldier has learned from any job class. This deep level of customization and flexibility grants nigh-infinite replayability, contributing to the game's unusually enduring popularity.

The story takes place in the kingdom of Ivalice, located in a peninsula surrounded by sea on the north, west and south, with a headland south of the landmass. Its geography features ranging landscapes, from plains to mountains ranges to deserts and forests. It is heavily populated by human beings, although intelligent monsters can be found living in less populated areas. Magic is predominant in the land, although ruins and artifacts indicated that past populace had relied on machinery, such as airships and robots.

Ivalice is a kingdom of seven territories; Fovoham, Gallione, Limberry, Lionel, Zeltennia, the Holy Territory of Murond (Mullonde in later versions), and the Royal Capital of Lesalie (Lesalia in later versions), Ivalice's neighbors are the kingdom of Ordalia in the east and Romanda, a military nation to the north, across the Rhana Strait. While the three nations share common royal bloodlines, major wars have taken place between them. An influential religious institution known as the Glabados Church heads the dominant faith, centering around a religious figure known as Saint Ajora.

The story takes place after Ivalice ended its war with the two nations in what is known as the Fifty Years War, and is facing economic problems and political strife. Adding to its problems is the recent death of the king, whose heir is only an infant. A regent is needed to rule in place of the prince, and the kingdom is split between Prince Goltana, represented by the Black Lion, and Prince , symbolized by the White Lion. The conflict leads to what is known in the game as the Lion War. Behind this backdrop is a revelation by the game's historian Alazlam J. Durai, who seeks to reveal the story of an unknown character whose role in the Lion War was major but was covered up by the kingdom's church. The setting is based around this character, named by default as Ramza, and revolves around his early life and the future conflicts he faced while the events that changed the kingdom unfold.

Central to the plot of the game are two main characters, Ramza Beoulve and Delita Heiral. The two characters are childhood friends, and while both are born of differing social classes; Ramza a noble and Delita a commoner, both disregarded this fact and grew up together believing in justice and honor, as taught by Ramza's father Barbaneth (called Balbanes in earlier version). However, as the story progresses, the two characters faced many conflicts that changed their viewpoint on life; Delita seeks to manipulate the upper class to achieve his dreams, while Ramza believes in justice and honor regardless of name and class.

The game's plot is then portrayed through the eyes of Ramza Beoulve, who is the player character of the story. His exploits in the war introduced him to a number of characters; each with their own roles and agenda concerning the war and the fictional world, Ivalice, that they inhabit. The most prominent factions at the beginning of the story are those of Prince Goltana and Prince , both are nobles seeking to obtain control of the throne by being the guardian to the monarch's young heir and were thus engaged in a war. The story progresses to include characters from the Glabados Church, which have been controlling Ivalice silently and engineering the war in question.

As the game progresses, players are able to recruit generic player characters and customize them using the Job system of the Final Fantasy series. Several battles also feature "Guest" characters that are controlled via the game's A.I., which may be recruited later in the game according to the story proper. Aside from original characters, the developers have also incorporated cameo roles from other Square games. The characters were designed by Akihiko Yoshida, who was also in charge of the illustration and character designs of games such as "Tactics Ogre", "Final Fantasy Tactics Advance", "Final Fantasy XII", and "Vagrant Story".

"Final Fantasy Tactics" begins with Ivalice just recovering from the Fifty Year War against Ordalia. The power vacuum caused by the death of its ruler, King Omdoria, soon sparks another conflict. Princess Ovelia and the younger Prince Orinas are both candidates to the throne, with the former supported by Prince Goltana of the Black Lion, and the latter by Queen Ruvelia and her brother, Prince of the White Lion. This erupts into a full-scale war known as the "Lion War", with either side using whatever means possible to secure their place in the throne. This includes bearing an illegitimate child, killing other possible heirs, betrayal, assassination and false identities.

Throughout the game, nobles regard commoners and peasants as animals, and many commoners try to take revenge on the nobles, who abandoned them after the war. Most joined the so-called Corpse Brigade (a.k.a. Death Corps) to fight against the nobles' soldiers, and many die in vain. Ramza, part of the noble Beoulve family of knights, and Delita, his childhood friend who was an ordinary commoner, are witnesses to this phenomenon. Events such as meeting an arrogant noble named Argath (a.k.a. Algus), as well as the negligent killing of Delita's sister Tietra (a.k.a. Teta) during an uprising, cause Delita and Ramza to abandon their ties to the nobility, both going separate ways.

Ramza joins a mercenary group, led by Gafgarion, who protects Princess Ovelia from being hunted by both sides. Delita joins Prince Goltana's forces to rise up through the ranks and gain control over his own destiny. Ramza and Delita are reunited when Gafgarion attempts to take Ovelia to Prince , though this proves futile. Agrias suggests visiting Cardinal Delacroix (a.k.a. Draclau) of the Glabados Church to protect Ovelia, while Delita continues to work in the shadows, working with multiple sides to realize his ambitions. Along the way to Lionel Castle, Ramza meets Mustadio, a machinist in possession of a holy relic called the Zodiac Stone. Hunted by a trading company for the power it contains, Mustadio also seeks Delacroix’s intervention.

However, soon after the encounter with Cardinal Delacroix, Ramza discovers that an elaborate plot was set by the Glabados Church. In their desire to control Ivalice, the Church, particularly the High Confessor Marcel Funebris (a.k.a. High Priest Marge Funeral), uses the legend of the so-called holy Zodiac Braves to gather the Zodiac Stones, and fuels the Lion War between and Goltana. To stave off Ramza's interference, Delacroix uses the stone to transform into a legendary Lucavi demon, and Ramza has no choice but to slay him/it. As a result, Ramza is regarded a heretic of the Church, and he is approached by the Confessor Zalmour (a.k.a. Heretic Examiner Zalmo) at Lesalia Imperial Capital.

While noble in name, the Beoulve family is susceptible to corruption, due to ambition. Dycedarg, the eldest sibling, conspires with and the Church to ensure that the Beoulve family remains in power. However, his younger brother Zalbag is unaware of his dealings. Alma, Ramza's younger sister, remains in church, unaffected by the situation until Ramza is branded a heretic in front of her. Ramza seeks to rescue her after her capture while helping Ramza escape the Confessors/Heresy Examiners. Only Ramza and Alma share their father's sense of justice.

Ramza is chased throughout the story by the Knights Templar (a.k.a. Shrine Knights), the soldiers of the Church who are hunting the Zodiac Stones, although he gains allies, either by saving their lives, or by showing them the truth. Some individuals with knowledge of the Zodiac Stones attempt to conspire with the Knights Templar for its power, though most fail. Ramza also acquires proof of the Church's lies about Saint Ajora, a central figure in the religion, and attempts to use it along with the Zodiac Stone to reveal the organization's plot.

During the course of the story, the two sides face off in a major battle that sees the deaths of many soldiers, including their leaders and Goltana. Ramza manages to stop the bloodshed from continuing and rescues the general, Count Cidolfus Orlandeau (a.k.a. Cidolfas Orlandu), though the Church succeeds in eliminating the two Lions to secure its power over Ivalice. Deeper into the story, Ramza discovers that the Knights Templar are in reality possessed by the Lucavi, who are the real conspirators behind the Church's plot. The Lucavi are seeking to resurrect their leader Ultima (a.k.a. Altima), who in the past was Saint Ajora, and they need much bloodshed and a suitable body to complete the resurrection. Alma is to serve as the host for Ultima's incarnation. While racing off to find her, Ramza encounters Dycedarg - now a Lucavi demon - and witnesses Zalbag's death. Zalbag is then risen and converted into an undead servant, and frequently begs for death during the encounter.

At the end of the story, though Ultima is resurrected, Ramza and his allies succeed in destroying her. Their final fates are unknown, although Orran Durai (a.k.a. Olan), a witness who had many encounters with Ramza, does witness Ramza and Alma riding away from the kingdom on Chocobos at the end of the game. In the epilogue, Delita marries Ovelia and becomes the King of Ivalice. However, he fails to find true satisfaction as even Ovelia distrusts him, leading her to stab Delita. Ovelia in turn is stabbed by the agonizing Delita and dies. Delita then sorrowfully cries out to Ramza, asking if what they have done was worth what they received (vilification for Ramza, and ostracization for Delita). Orran attempts to reveal the Church's evil plot with the "Durai Report." However, his papers are confiscated and he is burned at the stake for heresy. The story ends many centuries later with the historian Arazlam J. Durai (a.k.a. Alazlam) intent on revealing the truth of the Lion War and the Durai Report.

"Final Fantasy Tactics" was produced mostly by the team that made "" and "", and was Yasumi Matsuno's first project with Square following his departure from Quest in 1995. In an interview with Akito Inoue, an assistant professor at the International University of Japan, Inoue mentions that "Final Fantasy Tactics" was made because of how casual gamers are usually put off by games with branching storylines found in other Matsuno's titles such as "Tactics Ogre".

Several historical and mythological references were altered by translators: for instance, the Norse World Tree, Yggdrasil, makes an appearance as Yugodorasil; the word "breath" is consistently rendered as "bracelet" in attack names; and Wiegraf's name is nearly homonymous with a character from "Beowulf" but rendered differently. The in-game tutorial function also shows examples of Engrish - poorly translated English - including lines such as "This was the darkened Items won't appear."

The game also includes references to several Final Fantasy specific characters, places, and situations from earlier games in the "Final Fantasy" series — "Final Fantasy VII"<nowiki>'</nowiki>s Cloud Strife is a playable character, and through the "Proposition" system in bars scattered around the world map, treasures and lost areas such as "Matoya Cave" (a reference to the first "Final Fantasy") and various colors of materia can be found. To keep with tradition, Olan's adoptive father, Cidolfas Orlandu, is nicknamed "T.G. Cid", and chocobos are present in the game as well. Additionally, most of the monsters appear in one "Final Fantasy" game or another, although the Lucavi are entirely new monsters altogether.

The original score for "Final Fantasy Tactics" was composed, arranged, and produced by Hitoshi Sakimoto and Masaharu Iwata. Matsuno approached his longtime friends Sakimoto and Iwata to compose the music soon after the initial release of "Final Fantasy VII" in January 1997. Sakimoto composed 47 tracks for the game, and Iwata was left to compose the other 24. The orchestral timbres of the game's music were synthesized, with performance by Katsutoshi Kashiwabara and sound programming by Hidenori Suzuki. The album was first released on two Compact Discs by now-defunct DigiCube on June 21, 1997, bearing the catalog number "SSCX-10008", and was re-released by Square Enix on March 24, 2006, with the catalog number "SQEX-10066/7". It spans two discs and 71 tracks, covering a duration of 2:31:03.

Some reviewers made comparison with Nobuo Uematsu's "Final Fantasy" compositions, though the soundtrack received positive reviews from critics. Chudah's Corner summarized its review by stating that the soundtrack is an "astoundingly memorable classic of videogame music". This is also supported by other professional reviews, such as by an RPGFan reviewer that "don't believe that any other soundtrack known to man surpasses it", and a VGM World review who quotes that "the orchestral music is beautiful nonetheless".

"Final Fantasy Tactics" sold nearly 825,000 copies in Japan in the first half of 1997, and ended the year at almost 1.24 million copies sold. Since then, the total number of copies sold in Japan has reached approximately 1.35 million. In the United States it reached an estimated sale of 750,000 units as of year 2004. As of March 31, 2003, the game had shipped 2.27 million copies worldwide, with 1.36 million of those copies being shipped in Japan and 910,000 abroad. Since its release, rumors were circulated that the game was to be re-released by Sony as a Greatest Hits title, the tentative date being around July 30, 2001. As of August, 2011, the game had sold over 2.4 million copies worldwide.

"Final Fantasy Tactics" received universal acclaim upon its release, and critical opinion of the game has improved further over time. Magazines such as "Electronic Gaming Monthly" acknowledged it as "Square's first attempt into the strategy RPG genre"; though being "uneven", it is worthy of being called "a classic". "Game Informer" called it "the most impressive strategy RPG yet." Gaming websites such as GameSpot lauded the game's battle sequences as challenging, requiring more strategic planning than ordinary RPGs. IGN noted that the plot was the strength of the game, being in-depth and with numerous plot twists. During battle sequences, the story unfolds to create a serious atmosphere of the plot, even with simple and "cute" character design. The spells and summoning visuals were compared with "Final Fantasy VII" 's detailed graphics.

Criticism is made on gameplay, plot and the localization effort. One of the reviews of RPGFan criticized the difficulty of the game as being inconsistent with each encounter against enemy units. The factors that influence the difficulty of the game include overpowered enemy units or party members, and time had to be taken to level up before any progress can be made. Though in-depth, IGN also noted that the game's plot was confusing at times, and that the item system was repetitive. The game's localization effort was criticized by reviewers as poorly written, being rife with grammatical mistakes that almost stopped players from enjoying the storyline. General RPGFan review noted that the battlefield area was too small, hindering any possibilities for better strategy. The gameplay is summarized by one of the reviews as "strength vs. strength and proper spacing of troops when fighting magic users".

IGN awarded the game the Editor's Choice Award on 1998, praising the in-game graphics as "amazing" and the battle environments with its extra details as being "extremely well designed". GameSpot has named "Final Fantasy Tactics" as one of its Greatest Games of All Time—the first "Final Fantasy" game to receive such an honour. However, its legacy remains fairly obscure compared to "Final Fantasy VII", also released for the PlayStation that year. The game still entered many "best games of all time" lists, receiving 84th place in the "Top 100 Favorite Games of All Time" poll by Japanese magazine "Famitsu" during March 2006, 19th in a 2005 list by GameFAQs users, 45th in "Game Informer"<nowiki>'</nowiki>s list, 43rd in "Electronic Gaming Monthly"<nowiki>'</nowiki>s, and 38th in IGN's. Since its release, "Final Fantasy Tactics" has attracted a cult following. Fan communities dedicated to modding and balancing the game have appeared on the internet. These communities experience member activity as of 2011, fourteen years after "Final Fantasy Tactics"<nowiki>'</nowiki> original release.

"Final Fantasy Tactics" saw several re-releases. "Final Fantasy Tactics" was re-released as part of the Square's Millennium Collection. This series of games was only released in Japan, and each title is bought with a set of related merchandise. "Final Fantasy Tactics" was sold on June 29, 2000 along with titles such as "Saga Frontier, Saga Frontier 2, Brave Fencer Musashi, Front Mission 3, Ehrgeiz" and "Legend of Mana".

Four years after its release in 1997, "Final Fantasy Tactics" was selected as part of the Sony Greatest Hits line of rereleases. Games released as Sony Greatest Hits were sold at a lower price. "Final Fantasy Tactics" also became part of Ultimate Hits, Square Enix's main budget range available in Japan.

A PlayStation Portable version of "Final Fantasy Tactics", entitled "Final Fantasy Tactics: The War of the Lions" was released on May 10, 2007, in Japan; and is now released across all regions. It is the second game announced as part of the Ivalice Alliance. The game features an updated version of "Final Fantasy Tactics", along with new features including in-game cutscenes, new characters, and multiplayer capability. The updated mechanics contain a 16:9 widescreen support, new items, new jobs, and cel-shaded full motion videos. The English version contains full voice acting during the cinematic cut scenes, whereas the Japanese version does not.

The world of "Final Fantasy Tactics" has been featured in several other Square video games. After the game's release, the development staff went on to develop "Vagrant Story", which featured several subtle references to "Final Fantasy Tactics". In an interview with the French video game magazine "Joypad", Matsuno stated that both titles are set in the same fictional world of Ivalice. During the development of "Vagrant Story", Matsuno and Sakaguchi initiated a sequel to "Tactics", which would have used 2D graphics due to issues with 3D development at the time. Due to the team's committent to "Vagrant Story", the project was outsourced to an unspecified developer, but was cancelled for unspecified reasons.

Square released "Final Fantasy Tactics Advance" for the Nintendo Game Boy Advance in 2003. The game setting and engine are similar to the ones of its predecessor, however the characters and plot are notably different; the cast of characters is considerably smaller, and the plot is considerably simpler. Additionally, "Final Fantasy Tactics Advance" has a shorter main campaign, but more side missions and a secret campaign at the end of the game.

In 2006, "Final Fantasy XII" was released, also set in the world of Ivalice. Square Enix announced at the end of the same year the "Ivalice Alliance", a new series of games set in the world of Ivalice, during a Tokyo press conference. The first title released was "". An indirect sequel to "Final Fantasy Tactics Advance", titled "", was released in Japan in 2007 and in the rest of the world in 2008. It is also one of the titles released under the Ivalice Alliance game series, and takes place in the Ivalice universe. Ramza also appears as a playable character in the fighting game "Dissidia Final Fantasy NT".

In 2017, the MMORPG "" portrayed a version of the events of "Final Fantasy Tactics" as a fairy tale, with Ivalice being a mythical realm. It also introduced a version of Ramza and Alma as characters within the setting.

The remixed song from the game, "Ovelia & Delita", was nominated for "Best Game Music Cover/Remix" at the 16th Annual Game Audio Network Guild Awards.



</doc>
<doc id="52756" url="https://en.wikipedia.org/wiki?curid=52756" title="Final Fantasy VII">
Final Fantasy VII

Final Fantasy VII is a 1997 role-playing video game developed by Square for the PlayStation console. It is the seventh main installment in the "Final Fantasy" series. Published in Japan by Square, it was released in other regions by Sony Computer Entertainment and became the first in the main series to see a PAL release. The game's story follows Cloud Strife, a mercenary who joins an eco-terrorist organization to stop a world-controlling megacorporation from using the planet's life essence as an energy source. Events send Cloud and his allies in pursuit of Sephiroth, a superhuman intent on destroying their planet. During the journey, Cloud builds close friendships with his party members, including Aerith Gainsborough, who holds the secret to saving their world.

Development began in 1994, originally for the Super Nintendo Entertainment System. After delays and technical difficulties from experimenting on several platforms, Square moved production to the PlayStation, largely due to the advantages of the format. Veteran "Final Fantasy" staff returned, including series creator and producer Hironobu Sakaguchi, director Yoshinori Kitase, and composer Nobuo Uematsu. The title became the first in the series to use full motion video and 3D computer graphics, which featured 3D character models superimposed over 2D pre-rendered backgrounds. Although the gameplay systems remained mostly unchanged from previous entries, "Final Fantasy VII" introduced more widespread science fiction elements and a more realistic presentation. The game had a staff of over 100, with a combined development and marketing budget of around 80 million.

Assisted by a large promotional campaign, "Final Fantasy VII" received widespread commercial and critical success and remains widely regarded as a landmark title and one of the greatest video games of all time. The title won numerous Game of the Year awards and was acknowledged for boosting the sales of the PlayStation and popularizing Japanese role-playing games worldwide. Critics praised its graphics, gameplay, music, and story, although some criticism was directed towards its English localization. Its success has led to enhanced ports on various platforms, a multimedia subseries called the "Compilation of Final Fantasy VII" and an upcoming high-definition remake for the PlayStation 4.

The gameplay of "Final Fantasy VII" is mostly comparable to earlier "Final Fantasy" titles and Japanese role-playing games. The game features three modes of play: the world map, the field, and the battle screen. At its grandest scale, players explore the entire world of "Final Fantasy VII" on a 3D world map. The world map is littered with representations of areas for the player to enter, including towns, environments, and ruins. Natural barriers—such as mountains, deserts, and bodies of water—block access by foot to some areas; as the game progresses, the player receives vehicles that help traverse these obstacles. Chocobos can be found in certain spots on the map, and if caught, can be ridden to areas inaccessible by foot or vehicle. In field mode, the player navigates fully scaled versions of the areas represented on the world map. For the first time in the series, this mode is represented in three-dimensional space. The player can explore the environment, talk with characters, advance the story, and initiate event games in this mode. Event games are short minigames that use special control functions and are often tied into the story. While in field mode, the player may also find shops and inns. Shops provide an opportunity to buy and sell items that can aid Cloud and his party, such as weapons, armor, and accessories. If the characters rest at an inn, their hit points and mana points will be restored, along with any abnormalities contracted during battles.
At random intervals on the world map and in field mode, and at specific moments in the story, the game will enter the battle screen. This screen places the player characters on one side, the enemies on the other, and employs an "Active Time Battle" (ATB) system in which the characters exchange moves until one side is defeated. The damage (or healing) dealt by either side is quantified on screen. Characters have many statistics that determine their effectiveness in battle; for example, hit points determine how much damage they can take, and magic determines how much damage they can inflict with spells. Each character on the screen has a time gauge; when a character's gauge is full, the player can input a command for that character. The commands change as the game progresses, and are dependent on the characters in the player's party and their equipment. Commands may include attacking with a weapon, casting magic, using items, summoning monsters, and other actions that either damage the enemy or aid the player characters. "Final Fantasy VII" also features powerful, character-specific commands called Limit Breaks, which can be used only after a special gauge is charged by enemy attacks. After being attacked, characters may be afflicted by one or more abnormal "statuses", such as poison or paralysis. These statuses and their adverse effects can be removed by special items or abilities. When all the enemies are defeated, the battle ends and the player may be rewarded with money, items, and experience points. If the player is defeated, it is game over and the game must be loaded to the last save point.

When not in battle, the player can use the menu screen. On this screen, the player can review each character's status and statistics, use items and abilities, change equipment, save the game (when on the world map or at a save point), and manage orbs called Materia. The main method of customizing characters in "Final Fantasy VII", Materia may be added to equipment to provide characters with new magic spells, monsters to summon, commands, statistical upgrades, and other benefits. Materia levels up with their own experience point system and can be combined to create different effects.

"Final Fantasy VII" takes place on a world referred to in-game as the "Planet", though it has been retroactively named "Gaia". The planet's lifeforce, called the Lifestream, is a flow of spiritual energy that gives life to everything on the Planet. Its processed form is known as "Mako". On a societal and technological level, the game has been defined as an industrial or post-industrial science fiction milieu. During "Final Fantasy VII", the Planet's Lifestream is being drained for energy by the Shinra Electric Power Company, a world-dominating megacorporation headquartered in the city of Midgar. Shinra's actions are weakening the Planet, threatening its existence and all life. Significant factions within the game include AVALANCHE, an eco-terrorist group seeking Shinra's downfall so the Planet can recover; the Turks, a covert branch of Shinra's security forces; SOLDIER, an elite Shinra fighting force created by enhancing humans with Mako; and the Cetra, a near-extinct human tribe which maintains a strong connection to the Planet and the Lifestream.

The central protagonist is Cloud Strife, an unsociable mercenary who claims to be a former 1st Class SOLDIER. Early on, he works with two members of AVALANCHE: Barret Wallace, its brazen but fatherly leader; and Tifa Lockhart, a shy yet nurturing martial artist and childhood friend of Cloud. On their journey, they meet Aerith Gainsborough, a carefree flower merchant and one of the last surviving Cetra; Red XIII, an intelligent quadruped from a tribe that protects the planet; Cait Sith, a fortune-telling robotic cat controlled by repentant Shinra staff member Reeve; and Cid Highwind, a pilot whose dream of being the first human in outer space was not realized. The group can also recruit Yuffie Kisaragi, a young ninja and skilled Materia thief; and Vincent Valentine, a former Turk, and victim of Shinra experiments. The game's main antagonists are Rufus Shinra, son of President Shinra and later leader of the Shinra Corporation; Sephiroth, a former SOLDIER who reappears several years after he was thought dead; and Jenova, a hostile extraterrestrial life-form imprisoned by the Cetra 2000 years before. A key character in Cloud's backstory is Zack Fair, a member of SOLDIER and Aerith's first love.

AVALANCHE destroys a Shinra Mako reactor in Midgar; an attack on another reactor goes wrong, and Cloud falls into the city slums. There, he meets Aerith and protects her from Shinra. Meanwhile, Shinra finds AVALANCHE and collapses part of the upper city, killing most of AVALANCHE along with the slum population below. Aerith is also captured; as a Cetra, she can potentially reveal the "Promised Land", which Shinra believes is overflowing with exploitable Lifestream energy. Cloud, Barret, and Tifa rescue Aerith; during their escape from Midgar, they discover that President Shinra was murdered by Sephiroth, who was presumed dead five years earlier. The party pursues Sephiroth across the Planet, with now-President Rufus on their trail; they are soon joined by the rest of the playable characters.

At a Cetra temple, Sephiroth reveals his intentions to use the Black Materia to summon "Meteor", a spell that will hit the Planet with a devastating impact. Sephiroth will absorb the Lifestream as it attempts to heal the wound, becoming a god-like being. The party retrieves the Black Materia, but Sephiroth manipulates Cloud into surrendering it. Aerith departs alone to stop Sephiroth, following him to an abandoned Cetra city. During her prayer to the Planet for help, Sephiroth attempts to force Cloud to kill her; failing, he kills her himself before fleeing and leaving the Black Materia behind. The party then learns of Jenova, a hostile alien lifeform whose remains were unearthed by Shinra scientists decades earlier; at Nibelheim, Jenova's cells were used to create Sephiroth. Five years before the game, Sephiroth and Cloud visited Nibelheim, where Sephiroth learned of his origins. Driven insane by this, he murdered the townspeople, then vanished when confronted by Cloud.

At the Northern Crater, the party learns that the "Sephiroths" they have encountered are Jenova clones created by the insane Shinra scientist Hojo. Confronting the real Sephiroth as he is killing his clones to reunite Jenova's cells, Cloud is again manipulated into delivering the Black Materia. Sephiroth then taunts Cloud by showing another SOLDIER in Cloud's place in his memories of Nibelheim, suggesting that Cloud is a failed Sephiroth clone. Sephiroth summons Meteor and seals the Crater; Cloud falls into the Lifestream and the party is captured by Rufus.

Escaping Shinra, the party discovers Cloud at an island hospital in a catatonic state from Mako poisoning; Tifa stays as his caretaker. When the island is attacked by a planetary defense force called Weapon, the two fall into the Lifestream, where Tifa helps Cloud reconstruct his memories. Cloud was a mere infantryman who was never accepted into SOLDIER; the SOLDIER in his memories was his friend Zack. At Nibelheim, Cloud surprised and wounded Sephiroth after the latter's mental breakdown, but Jenova preserved Sephiroth's life. Hojo experimented on Cloud and Zack for four years, injecting them with Jenova's cells and Mako; they escaped, but Zack was eventually killed. The combined trauma of these events triggered an identity crisis in Cloud; he constructed a false persona around Zack's stories and his own fantasies. Cloud accepts his past and reunites with the party, who learn that Aerith's prayer to the Planet had been successful: the Planet had attempted to summon Holy to prevent Meteor's impact, but Sephiroth blocked Holy.

Shinra fails to destroy Meteor but manages to defeat a Weapon and puncture the Northern Crater, costing the lives of Rufus and other personnel. After killing Hojo, who is revealed to be Sephiroth's biological father, the party descends to the Planet's core through the opening in the Northern Crater and defeats both Jenova and Sephiroth. The party escapes and Holy is summoned, which destroys Meteor with the help of the Lifestream. Five hundred years later, Red XIII is seen with two cubs looking out over the ruins of Midgar, which are now covered in greenery, showing the planet has healed.

Initial concept talks for "Final Fantasy VII" began in 1994 at "Final Fantasy" developer Square, following the completion of "Final Fantasy VI". As with the previous installment, series creator Hironobu Sakaguchi reduced his role to producer and granted others a more active role in development: these included Yoshinori Kitase, one of the directors of "Final Fantasy VI". The next installment was planned as a 2D game for Nintendo's Super Nintendo Entertainment System (Super NES). After creating an early 2D prototype of it, the team postponed development to help finish "Chrono Trigger". Once "Chrono Trigger" was completed, the team resumed discussions for "Final Fantasy VII" in 1995.

The team discussed continuing the 2D strategy, which would have been the safe and immediate path just prior to the imminent industry shift toward 3D gaming; such a change would require radical new development models. The team decided to take the riskier option and make a 3D game on new generation hardware but had yet to choose between the cartridge-based Nintendo 64 or the CD-ROM-based PlayStation from Sony Computer Entertainment. The team also considered the Sega Saturn console and Microsoft Windows. Their decision was influenced by two factors: a highly successful tech demo based on "Final Fantasy VI" using the new Softimage 3D software, and the escalating price of cartridge-based games, which was limiting Square's audience. Tests were made for a Nintendo 64 version, which would use the planned 64DD peripheral despite the lack of 64DD development kits and the prototype device's changing hardware specifications. This version was discarded during early testing, as the 2000 polygons needed to render the Behemoth monster placed excessive strain on the Nintendo 64 hardware, causing a low frame rate. It would have required an estimated thirty 64DD discs to run "Final Fantasy VII" properly with the data compression methods of the day. Faced with both technical and economic issues on Nintendo's current hardware, and impressed by the increased storage capacity of CD-ROM when compared to the Nintendo 64 cartridge, Square shifted development of "Final Fantasy VII", and all other planned projects, onto the PlayStation.

In contrast to the visuals and audio, the overall gameplay system remained mostly unchanged from "Final Fantasy V" and "VI", but with an emphasis on player control. The initial decision was for battles to feature shifting camera angles. Battle arenas had a lower polygon count than field areas, which made creating distinctive features more difficult. The summon sequences benefited strongly from the switch to the cinematic style, as the team had struggled to portray their scale using 2D graphics. In his role as producer, Sakaguchi placed much of his effort into developing the battle system. He proposed the Materia system as a way to provide more character customization than previous "Final Fantasy" games: battles no longer revolved around characters with innate skills and roles in battle, as Materia could be reconfigured between battles. Artist Tetsuya Nomura also contributed to the gameplay; he designed the Limit Break system as an evolution of the Desperation Attacks used in "Final Fantasy VI". The Limit Breaks served a purpose in gameplay while also evoking each character's personality in battle.

Square retained the passion-based game development approach from their earlier projects, but now had the resources and ambition to create the game they wanted. This was because they had extensive capital from their earlier commercial successes, which meant they could focus on quality and scale rather than obsessing over and working around their budget. "Final Fantasy VII" was at the time one of the most expensive video game projects ever, costing an estimated 40 million, which adjusted for inflation came to $61 million in 2017. Development of the final version took a staff of between 100 and 150 people just over a year to complete. As video game development teams were usually only 20 people, the game had what was described as the largest development team of any game up to that point. The development team was split between both Square's Japanese offices and its new American office in Los Angeles; the American team worked primarily on city backgrounds.

The game's art director was Yusuke Naora, who had worked as a designer for "Final Fantasy VI". With the switch into 3D, Naora realized that he needed to relearn drawing, as 3D visuals require a very different approach than 2D. With the massive scale and scope of the project, Naora was granted a team devoted entirely to the game's visual design. The department's duties included illustration, modeling of 3D characters, texturing, the creation of environments, visual effects, and animation. Naora later defined the art style of "Final Fantasy VII" as "dark" and "weird". The Shinra logo, which incorporated a kanji symbol, was drawn by Naora personally. Promotional artwork, in addition to the logo artwork, was created by Yoshitaka Amano, an artist whose association with the series went back to its inception. While he had taken a prominent role in earlier entries, Amano was unable to do so for "Final Fantasy VII", due to commitments at overseas exhibitions. His logo artwork was based on Meteor: when he saw images of Meteor, he was not sure how to turn it into suitable artwork. In the end, he created multiple variations of the image and asked staff to choose which they preferred. The green coloring represents the predominant lighting in Midgar and the color of the Lifestream, while the blue reflected the ecological themes present in the story. Its coloring directly influenced the general coloring of the game's environments.

Another prominent artist was Nomura. Having impressed Sakaguchi with his proposed ideas, which were handwritten and illustrated rather than simply typed on a PC, Nomura was brought on as main character designer. Nomura stated that when he was brought on, the main scenario had not been completed, but he "went along like, 'I guess first off you need a hero and a heroine', and from there drew the designs while thinking up details about the characters. After [he'd] done the hero and heroine, [he] carried on drawing by thinking what kind of characters would be interesting to have. When [he] handed over the designs [he'd] tell people the character details [he'd] thought up, or write them down on a separate sheet of paper". Something that could not be carried over from earlier titles was the chibi sprite art, as that would not fit with the new graphical direction. Naora, in his role as an assistant character designer and art director, helped adjust each character's appearance so the actions they performed were believable. When designing Cloud and Sephiroth, Nomura was influenced by his view of their rivalry mirroring the legendary animosity between Miyamoto Musashi and Sasaki Kojirō, with Cloud and Sephiroth being Musashi and Kojirō respectively. Sephiroth's look was defined as ""kakkoii"", a Japanese term combining good looks with coolness. Several of Nomura's designs evolved substantially during development. Cloud's original design of slicked-back black hair with no spikes was intended to save polygons and contrast with Sephiroth's long, flowing silver hair. However, Nomura feared that such masculinity could prove unpopular with fans, so he redesigned Cloud to feature a shock of spiky, bright blond hair. Vincent's occupation changed from researcher to detective to chemist, and finally to a former Turk with a tragic past.

Sakaguchi was responsible for writing the initial plot, which was substantially different from the final version. In this draft for the planned SNES version, the game's setting was envisioned as New York City in 1999. Similar to the final story, the main characters were part of an organization trying to destroy Mako reactors, but they were pursued by a hot-blooded detective named Joe. The main characters would eventually blow up the city. An early version of the Lifestream concept was present at this stage. According to Sakaguchi, his mother had died while "Final Fantasy VI" was being developed, and choosing life as a theme helped him cope with her passing in a rational and analytical manner. Square eventually used the New York setting in "Parasite Eve" (1998). While the planned concept was dropped, "Final Fantasy VII" still marked a drastic shift in setting from previous entries, dropping the Medieval fantasy elements in favor of a world that was "ambiguously futuristic".
When Kitase was put in charge of "Final Fantasy VII", he and Nomura reworked the entire initial plot. Scenario writer Kazushige Nojima joined the team after finishing work on "Bahamut Lagoon". While "Final Fantasy VI" featured an ensemble cast of numerous playable characters that were equally important, the team soon decided to develop a central protagonist for "Final Fantasy VII". The pursuit of Sephiroth that comprised most of the main narrative was suggested by Nomura, as nothing similar had been done in the series before. Kitase and Nojima conceived AVALANCHE and Shinra as opposing organizations and created Cloud's backstory as well as his relationship to Sephiroth. Among Nojima's biggest contributions to the plot were Cloud's memories and split personality; this included the eventual conclusion involving his newly created character of Zack. The crew helped Kitase adjust the specifics of Sakaguchi's original Lifestream concept.

Regarding the overall theme of the game, Sakaguchi said it was "not enough to make 'life' the theme, you need to depict living and dying. In any event, you need to portray death". Consequently, Nomura proposed killing off the heroine. Aerith had been the only heroine, but the death of a female protagonist would necessitate a second; this led to the creation of Tifa. The developers decided to kill Aerith, as her death would be the most devastating and consequential. Kitase wanted to depict it as very sudden and unexpected, leaving "not a dramatic feeling but great emptiness", "feelings of reality and not Hollywood". The script for the scene was written by Nojima. Kitase and Nojima then planned that most of the main cast would die shortly before the final battle; Nomura vetoed the idea because he felt it would undermine the impact of Aerith's death. Several character relations and statuses underwent changes during development. Aerith was to be Sephiroth's sister, which influenced the design of her hair. The team then made Sephiroth a previous love interest of hers to deepen her backstory, but later swapped him with Zack. Vincent and Yuffie were to be part of the main narrative, but due to time constraints, they were nearly cut and eventually relegated to being optional characters.

Nojima was charged with writing the scenario and unifying the team's ideas into a cohesive narrative, as Kitase was impressed with his earlier work on the mystery-like "Heracles no Eikō III: Kamigami no Chinmoku", an entry in the "Glory of Heracles" series. To make the characters more realistic, Nojima wrote scenes in which they would occasionally argue and raise objections: while this inevitably slowed down the pace of the story, it added depth to the characters. The graphical improvements allowed even relatively bland lines of dialogue to be enhanced with reactions and poses from the 3D character models. Voice acting would have led to significant load times, so it was omitted. Masato Kato wrote several late-game scenes, including the Lifestream sequence and Cloud and Tifa's conversation before the final battle. Initially unaffiliated with the project, Kato was called on to help flesh out less important story scenes. He wrote his scenes to his own tastes without outside consultation, something he later regretted.

With the shift from the SNES to the next generation consoles, "Final Fantasy VII" became the first project in the series to use 3D computer graphics. Developers initially considered overlaying 2D sprites on 3D backgrounds but decided to forgo pixel art entirely in favor of polygonal models. Aside from the story, "Final Fantasy VI" had many details undecided when development began; most design elements were hashed out along the way. In contrast, with "Final Fantasy VII", the developers knew from the outset it was going to be "a real 3D game", so from the earliest planning stage, detailed designs were in existence. The script was also finalized, and the image for the graphics had been fleshed out. This meant that when actual development work began, storyboards for the game were already in place. The shift from cartridge ROM to CD-ROM posed some problems: according to lead programmer Ken Narita, the CD-ROM had a slower access speed, delaying some actions during the game, so the team needed to overcome this issue. Certain tricks were used to conceal load times, such as offering animations to keep players from getting bored. When it was decided to use 3D graphics, there was a discussion among the staff whether to use sprite-based character models or 3D polygonal models. While sprites proved more popular with the staff, the polygon models were chosen as they could better express emotion. This decision was influenced by the team's exposure to the 3D character models used in "Alone in the Dark". Sakaguchi decided to use deformed models for field navigation and real-time event scenes, for better expression of emotion, while realistically proportioned models would be used in battles. The team purchased Silicon Graphics Onyx supercomputers and related workstations, and accompanying software including Softimage 3D, PowerAnimator, and N-World for an estimated total of $21 million. Many team members had never seen the technology before.
The transition from 2D graphics to 3D environments overlaid on pre-rendered backgrounds was accompanied by a focus on a more realistic presentation. In previous entries, the sizes for characters and environments were fixed, and the player saw things from a scrolling perspective. This changed with "Final Fantasy VII"; environments shifted with camera angles, and character model sizes shifted depending on both their place in the environment and their distance from the camera, giving a sense of scale. The choice of this highly cinematic style of storytelling, contrasting directly with Square's previous games, was attributed to Kitase, who was a fan of films and had an interest in the parallels between film and video game narrative. Character movement during in-game events was done by the character designers in the planning group. While designers normally cooperate with a motion specialist for such animations, the designers taught themselves motion work, resulting in each character's movements differing depending on their creators—some designers liked exaggerated movements, while others went for subtlety. Much of the time was spent on each character's day-to-day, routine animations. Motion specialists were brought in for the game's battle animations. The first characters the team worked with were Cloud and Barret. Some of the real-time effects, such as an explosion near the opening, were hand-drawn rather than computer animated.

The main creative force behind the overall 3D presentation was Kazuyuki Hashimoto, the general supervisor for these sequences. Being experienced in the new technology the team had brought on board, he accepted the post at Square as the team aligned with his own creative spirit. One of the major events in development was when the real-time graphics were synchronized to computer-generated full motion video (FMV) cutscenes for some story sequences, notably an early sequence where a real-time model of Cloud jumps onto an FMV-rendered moving train. The backgrounds were created by overlaying two 2D graphic layers and changing the motion speed of each to simulate depth perception. While this was not a new technique, the increased power of the PlayStation enabled a more elaborate version of this effect. The biggest issue with the 3D graphics was the large memory storage gap between the development hardware and the console: while the early 3D tech demo had been developed on a machine with over 400 megabytes of total memory, the PlayStation only had two megabytes of system memory and 500 kilobytes for texture memory. The team needed to figure out how to shrink the amount of data while preserving the desired effects. This was aided with reluctant help from Sony, who had hoped to keep Square's direct involvement limited to a standard API package, but they eventually relented and allowed the team direct access to the hardware specifications.

"Final Fantasy VII" features two types of cutscenes: real-time cutscenes featuring polygon models on pre-rendered backgrounds, and FMV cutscenes. The game's computer-generated imagery (CGI) FMVs were produced by Visual Works, a then-new subsidiary of Square that specialized in computer graphics and FMVs creation. Visual Works had created the initial movie concept for a 3D game project. The FMVs were created by an international team, covering both Japan and North America and involving talent from the gaming and film industry; Western contributors included artists and staff who had worked on the "Star Wars" film series, "Jurassic Park", "", and "True Lies". The team tried to create additional optional CGI content which would bring optional characters Vincent and Yuffie into the ending. As this would have further increased the number of discs the game needed, the idea was discarded. Kazuyuki Ikumori, a future key figure at Visual Works, helped with the creation of the CGI cutscenes, in addition to general background design. The CGI FMV sequences total around 40 minutes of footage, something only possible with the PlayStation's extra memory space and graphical power. This innovation brought with it the added difficulty of ensuring that the inferiority of the in-game graphics in comparison to the FMV sequences was not too obvious. Kitase has described the process of making the in-game environments as detailed as possible to be "a daunting task".

The musical score of "Final Fantasy VII" was composed, arranged, and produced by Nobuo Uematsu, who had served as the sole composer for the six previous "Final Fantasy" games. Originally, Uematsu had planned to use CD quality music with vocal performances to take advantage of the console's audio capabilities but found that it resulted in the game having much longer loading times for each area. Uematsu then decided that the higher quality audio was not worth the trade-off with performance, and opted instead to use MIDI-like sounds produced by the console's internal sound sequencer, similar to how his soundtracks for the previous games in the series on the Super NES were implemented. While the Super NES only had eight sound channels to work with, the PlayStation had twenty-four. Eight were reserved for sound effects, leaving sixteen available for the music. Uematsu's approach to composing the game's music was to treat it like a film soundtrack and compose music that reflected the mood of the scenes, rather than trying to make strong melodies to "define the game", as he felt that approach would come across too strong when placed alongside the game's new 3D visuals. As an example, he composed the track intended for the scene in the game where Aerith Gainsborough is killed to be "sad but beautiful", rather than more overtly emotional, creating what he felt was a more understated feeling. Uematsu additionally said that the soundtrack had a feel of "realism", which also prevented him from using "exorbitant, crazy music".

The first piece that Uematsu composed for the game was the opening theme; game director Yoshinori Kitase showed him the opening cinematic and asked him to begin the project there. The track was well received in the company, which gave Uematsu "a sense that it was going to be a really good project". "Final Fantasy VII" was the first game in the series to include a track with high-quality digitized vocals, "One-Winged Angel", which accompanies a section of the final battle of the game. The track has been called Uematsu's "most recognizable contribution" to the music of the "Final Fantasy" series, which Uematsu agrees with. Inspired by "The Rite of Spring" by Igor Stravinsky to make a more "classical" track, and by rock and roll music from the late 1960s and early 1970s to make an orchestral track with a "destructive impact", he spent two weeks composing short unconnected musical phrases, and then arranged them together into "One-Winged Angel", an approach he had never used before.

Music from the game has been released in several albums. Square released the main soundtrack album, "Final Fantasy VII Original Soundtrack", on four Compact Discs through its DigiCube subsidiary in 1997. A limited edition release was also produced, containing illustrated liner notes. The regular edition of the album reached third on the Japan Oricon charts, while the limited edition reached #19. Overall, the album had sold nearly 150,000 copies by January 2010. A single-disc album of selected tracks from the original soundtrack, along with three arranged pieces, titled "Final Fantasy VII Reunion Tracks", was also released by DigiCube in 1997, reaching #20 on the Japan Oricon charts. A third album, "Piano Collections Final Fantasy VII", was released by DigiCube in 2003, and contains one disc of piano arrangements of tracks from the game. It was arranged by Shirō Hamaguchi and performed by Seiji Honda, and reached #228 on the Oricon charts.

"Final Fantasy VII" was announced in February 1996. Square president and chief executive officer Tomoyuki Takechi were fairly confident about Japanese players making the game a commercial success despite it being on a new platform. A playable demo was included on a disc giveaway at the 1996 Tokyo Game Show, dubbed "Square's Preview Extra: Final Fantasy VII & Siggraph '95 Works". The disc also included the early test footage Square created using characters from "Final Fantasy VI". The initial release date was at some point in 1996, but to properly realize their vision, Square postponed the release date almost a full year. "Final Fantasy VII" was released on January 31, 1997. It was published in Japan by Square. A re-release of the game based on its Western version, titled "Final Fantasy VII International", was released on October 2, 1997. This improved "International" version would kickstart the trend for Square to create an updated version for the Japanese release, based on the enhanced Western versions. The "International" version was re-released as a physical disc as part of the "Final Fantasy 25th Anniversary Ultimate Box" Japanese package on December 18, 2012.

While its success in Japan had been taken for granted by Square executives, North America and Europe were another matter, as up to that time the Japanese role-playing genre was still a niche market in Western territories. Sony, due to the PlayStation's struggles against Nintendo and Sega's home consoles, lobbied for the publishing rights in North America and Europe following "Final Fantasy VII"s transfer to PlayStation—to further persuade Square, Sony offered a lucrative royalties deal with profits potentially equaling those Square would get by self-publishing the game. Square accepted Sony's offer as Square itself lacked Western publishing experience. Square was uncertain about the game's success, as other JRPGs including "Final Fantasy VI" had met with poor sales outside Japan. To help with promoting the title overseas, Square dissolved their original Washington offices and hired new staff for fresh offices in Costa Mesa. It was first exhibited to the Western public at Electronic Entertainment Expo 1996 (E3).

To promote the game overseas, Square and Sony launched a widespread three-month advertising campaign in August 1997. Beginning with a television commercial that ran alongside popular shows such as "Saturday Night Live" and "The Simpsons" by TBWA\Chiat\Day, the campaign included numerous articles in both gaming and general interest magazines, advertisements in comics from publishers such as DC Comics and Marvel, a special collaboration with Pepsi, media events, sample discs, and merchandise. According to estimations by Takechi, the total worldwide marketing budget came to USD$40 million; $10 million had been spent in Japan, $10 million in Europe, and $20 million in North America. Unlike its predecessors, "Final Fantasy VII" did not have its numeral adjusted to account for the lack of a Western release for "Final Fantasy II", "III", and "V"—while only the fourth "Final Fantasy" released outside Japan, its Japanese title was retained. It released in North America on September 7, 1997. The game released in Europe on November 17, becoming the first "Final Fantasy" game to be released in Europe. The Western version included additional elements and alterations, such as streamlining of the menu and Materia system, reducing the health of enemies, new visual cues to help with navigation across the world map, and additional cutscenes relating to Cloud's past.

A version for PC was developed by Square's Costa Mesa offices. Square invested in a PC version to reach as wide a player base as possible; many Western consumers did not own a PlayStation, and Square's deal with Sony did not prohibit such a port. Having never released a title for PC, Square decided to treat the port as a sales experiment. The port was handled by a team of 15 to 20 people, mostly from Costa Mesa but with help from Tokyo. Square did not begin the port until the console version was finished. The team needed to rewrite an estimated 80% of the game's code, due to the need to unify what had been a custom build for a console written by multiple staff members. Consequently, programmers faced problems such as having to unify the original PlayStation version's five different game engines, leading to delays. The PC version came with a license for Yamaha Corporation's software synthesizer S-YXG70, allowing high-quality sequenced music despite varying sound hardware setups on different user computers. The conversion of the nearly 100 original musical pieces to XG format files was done by Yamaha.

To maximize their chances of success, Square searched for a Western company to assist with releasing the PC version. Eidos Interactive, whose release of "Tomb Raider" had turned them into a publishing giant, agreed to market and publish the port. The port was announced in December 1997, along with Eidos' exclusivity deal for North America and Europe at the time. To help the product stand out in stores, Eidos chose a triangular shape for the cover and box. They agreed on a contract price of $1.8 million, making initial sales forecasts of 100,000 units based on that outlay. The PC version was released in North America and Europe on June 25, 1998; the port was not released in Japan. Within one month, sales of the port exceeded the initial forecasts. The PC version would end up providing the source code for subsequent ports.

Localization of "Final Fantasy VII" was handled internally by Square. The English localization, led by Seth Luisi, was completed by a team of about fifty people and faced a variety of problems. According to Luisi, the biggest hurdle was making "the direct Japanese-to-English text translation read correctly in English. The sentence structure and grammar rules for the Japanese language is very different from English", making it difficult for the translation to read like native English without distorting the meaning. Michael Basket was the sole translator for the project, though he received the help of native Japanese speakers from the Tokyo office. The localization was taxing for the team due to their inexperience, lack of professional editors, and poor communication between the North American and Japanese offices. A result of this disconnect was the original localization of Aerith's name—which was intended as a conflation of "air" and "earth"—as "Aeris" due to a lack of communication between localization staff and the Q&A team.

The team also faced several technical issues, such as dealing with a fixed font size and having to add symbols using language input keys so the code would function. Consequently, the text was still read as Japanese by the word processor; the computer's spellcheck could not be used, and mistakes had to be caught manually. To complicate matters, the Japanese text used obscure kanji symbols carried over from Chinese writing. Swear words were used frequently in the localization to help convey the original Japanese meaning, though most profanities were censored in a manner described by Square employee Richard Honeywood as the "old comic book '@#$%!'-type replacement". The European release was described as being in a worse condition, as the translations into multiple European languages were outsourced by Sony to another company, further hindering communication. For the PC port, Square attempted to fix translation and grammar mistakes for the North American and European versions but did not have the time and budget to retranslate all the text. According to Honeywood, the success of "Final Fantasy VII" in the West encouraged Square to focus more on localization quality; on future games, Square hired additional translators and editors, while also streamlining communication between the development and localization teams.

Some months prior to the game's North American release, Sony publicly stated that it was considering cutting the scene at the Honey Bee Inn due to the salacious content, prompting numerous online petitions and letters of protest from RPG fans. Square subsequently stated that it would never allow Sony to localize the game in any way.

The "International" version of "Final Fantasy VII" was released on PlayStation Network (PSN) as a PSOne Classic in Japan on April 10, 2009. This version was compatible with both PlayStation 3 and PlayStation Portable with support for PlayStation Vita and PlayStation TV coming later. "Final Fantasy VII" was later released as a PSOne Classic in North America, Europe, and Australia on June 2. The PC version was updated by DotEmu for use on modern operating systems and released via Square Enix's North American and European online stores on August 14, 2012. It included high-resolution support, cloud saves, achievements and a character booster. It would later be released via Steam on July 4, 2013, replacing the version available on Square Enix's North American and European online stores. The PC version would be released in Japan for the first time on May 16, 2013, exclusively via Square Enix's Japanese online store with the "International" version title. It has features unavailable in the western version including high-speed mode, no random encounters mode, and a max stats command. A release for iOS, based on the PC version and adjusted for mobile devices by D4 Enterprise, was released on August 19, 2015, with an auto-save feature. After being announced at PlayStation Experience 2014, the PC version was released for PlayStation 4 on December 5, 2015. DotEmu developed the PS4 version. A version for Android was released on July 7, 2016. A version for the PlayStation Classic was released on December 3, 2018. A version for the Nintendo Switch and Xbox One will be released worldwide on March 26, 2019.

Within three days of its release in Japan, "Final Fantasy VII" sold over two million copies. This popularity inspired thousands of retailers in North America to break street dates in September to meet public demand for the title. In the game's debut weekend in North America, it sold 330,000 copies, and had reached sales of 500,000 copies in less than three weeks. The momentum established in the game's opening weeks continued for several months; Sony announced the game had sold one million copies in North America by early December, prompting business analyst Edward Williams from Monness, Crespi, Hardt & Co. to comment, "Sony redefined the role-playing game (RPG) category and expanded the conventional audience with the launch of "Final Fantasy VII"." By the end of 2005, the game had sold over 9.8 million copies worldwide, making it the highest-selling game in the "Final Fantasy" series. By the end of 2006, "The Best", the bargain reissue of the game, had sold over 158,000 copies in Japan. "Final Fantasy VII" is credited as "the game that sold the PlayStation", as well as allowing Japanese role-playing games to find a place in markets outside Japan. By May 2010, it had sold over 10 million copies worldwide, making it the most popular title in the series in terms of units sold. The original PC version surpassed Eidos' expectations: while initially forecast to sell 100,000 units, it quickly exceeded sales of one million units, garnering royalties of over $2 million for Square. As of August 2015, all physical and digital versions have sold over 11 million units worldwide.

The game received widespread acclaim from critics upon release. It was referred to by "GameFan" as "quite possibly the greatest game ever made", a quote selected for the back cover of the game's jewel case. GameSpot commented that "never before have technology, playability, and narrative combined as well as in "Final Fantasy VII"," expressing particular favor toward the game's graphics, audio, and story. IGN's Jay Boor insisted the game's graphics were "light years beyond anything ever seen on the PlayStation", and regarded its battle system as its strongest point. "Computer and Video Games"s Alex C praised the dramatic story and well-developed characters. "Edge" noted that "Final Fantasy VII" had come close to being an interactive movie in playable form, praising its combination of a complex story that went against Western graphic adventures trends and "excellently orchestrated chip music". RPGamer praised the game's soundtrack, both in variety and sheer volume, stating that "Uematsu has done his work exceptionally well" and saying that it was potentially his best work. "Final Fantasy VII" has received some negative criticism. "Official U.S. PlayStation Magazine" ("OPM") and GameSpot questioned the game's linear progression. "OPM" considered the game's translation "a bit muddy" and felt the summon animations were "repetitive." RPGamer cited its translation as "packed with typos and other errors which further obscure what is already a very confusing plot." "GamePro" also considered the Japanese-to-English translation a significant weakness in the game, and IGN regarded the ability to use only three characters at a time as "the game's only shortcoming."

Reviewers gave similar praise to the PC version but criticized its various technical faults. "Computer Games Magazine" said that no other recent game had the same "tendency to fail to work in any capacity on multiple [computers]." "Computer Gaming World" complained that the music quality suffered on PC sound cards," and "Next Generation Magazine" found the game's pre-rendered backgrounds significantly less impressive than those of the PlayStation version. However, the latter magazine found the higher-resolution battle visuals "absolutely stunning," and "Computer Games Magazine" said that they showed off the potential graphical power of PCs. All three magazines concluded by praising the game despite its technical flaws, and "PC Gamer" summarized that, while "Square apparently did only what was required to get its PlayStation game running under Windows," "Final Fantasy VII" is "still a winner on the PC."

"Final Fantasy VII" was given numerous Game of the Year awards in 1997. It won in the Academy of Interactive Arts & Sciences' first annual Interactive Achievement Awards in the categories "Console Adventure Game of the Year" and "Console Role Playing Game of the Year" (it was also nominated in the categories "Interactive Title of the Year", "Outstanding Achievement in Art/Graphics" and "Outstanding Achievement in Interactive Design"). In the Origins Award, it won in the category "Best Roleplaying Computer Game of 1997." It was also awarded the "Readers' Choice All Systems Game of the Year", "Readers' Choice PlayStation Game of the Year" and "Readers' Choice Role-Playing Game of the Year" by "EGM", which also gave it other awards for "Hottest Video Game Babe" (for Tifa Lockhart), "Most Hyper for a Game", "Best Ending" and "Best Print Ad".

Since 1997, it has been selected by many game magazines as one of the top video games of all time, listed as 91st in "EGM"s 2001 "100 Best Games of All Time", and as fourth in "Retro Gamer"s "Top 100 Games" in 2004. In 2018, it was ranked 99th in IGN's "Top 100 Games of All Time" and as third in PALGN's "The Greatest 100 Games Ever". "Final Fantasy VII" was included in "The Greatest Games of All Time" list by GameSpot in 2006, and ranked as second in "Empire"'s 2006 "100 Greatest Games of All Time", as third in "Stuff"s "100 Greatest Games" in 2008 and as 15th in "Game Informer"s 2009 "Top 200 Games of All Time" (down five places from its previous best games of all time list). GameSpot placed it as the second most influential game ever made in 2002; in 2007, GamePro ranked it 14th on the list of the most important games of all time, and in 2009 it finished in the same place on their list of the most innovative games of all time. In 2012, "Time" named it one of their "All-TIME 100 Video Games". In March 2018, Game Informers "Readers Choice Top 300 Games of All Time", Final Fantasy ranked in 7th place. In March 2018, Gamesradar+ rated “The 25 best PS1 games of all time”, Final Fantasy VII was ranked in 12th place.

It has also appeared in numerous other greatest game lists. In 2007, "Dengeki PlayStation" gave it the "Best Story", "Best RPG" and "Best Overall Game" retrospective awards for games on the original PlayStation. GamePro named it the best RPG title of all time in 2008, and featured it in their 2010 article "The 30 Best PSN Games." In 2012, GamesRadar also ranked it as the sixth saddest game ever. On the other hand, GameSpy ranked it seventh on their 2003 list of the most overrated games.

"Final Fantasy VII" has often placed at or near the top of many reader polls of all-time best games. It was voted the "Reader's Choice Game of the Century" in an IGN poll in 2000, and placed second in the "Top 100 Favorite Games of All Time" by Japanese magazine "Famitsu" in 2006 (it was also voted as ninth in "Famitsu"'s 2011 poll of most tear-inducing games of all time). Users of GameFAQs voted it the "Best Game Ever" in 2004 and in 2005, and placed it second in 2009. In 2008, readers of "Dengeki" magazine voted it the best game ever made, as well as the ninth most tear-inducing game of all time. 

The game inspired an unofficial version for the NES by Chinese company Shenzhen Nanjing Technology. This port features the "Final Fantasy VII" game scaled back to 2D, with some of the side quests removed. The game's popularity and open-ended nature also led director Kitase and scenario writer Nojima to establish a plot-related connection between "Final Fantasy VII" and "Final Fantasy X-2". The character Shinra from "Final Fantasy X-2" proposes the concept of extracting the life energy from within the planet Spira. Nojima has stated that Shinra and his proposal are a deliberate nod to the Shinra Company and that he envisioned the events of "Final Fantasy X-2" as a prequel to those in "Final Fantasy VII". The advances in technology used to create the FMV sequences and computer graphics for "Final Fantasy VII" allowed Sakaguchi to begin production on the first "Final Fantasy" film, "". The game introduced a particular aesthetic to the series—fantasy suffused with modern-to-advanced technology—that was explored further in "Final Fantasy VIII", "The Spirits Within", and "Final Fantasy XV". Re-releases of Square games in Japan with bonus features would occur frequently after the release of "Final Fantasy VII International". Later titles that would be re-released as international versions include "Final Fantasy X" and other follow-ups from the franchise, as well as the "Kingdom Hearts" series.

Several characters from "Final Fantasy VII" have made cameo appearances in other Square Enix titles, most notably the fighting game "Ehrgeiz" and the popular "Final-Fantasy"-to-Disney crossover series "Kingdom Hearts". Additionally, fighting video game "Dissidia Final Fantasy" includes "Final Fantasy VII" characters such as Cloud and Sephiroth, and allows players to fight with characters from throughout the "Final Fantasy" series, and its follow-up, "Dissidia 012 Final Fantasy", included Tifa as well. Cloud is also a playable character in Final Fantasy Tactics. In 2015, Cloud was released as a downloadable content character for the Nintendo fighting game "Super Smash Bros. for Nintendo 3DS and Wii U", along with a stage based on Midgar and returned in "Super Smash Bros. Ultimate". Aerith's death in the game has often been referred to as one of the most significant moments from any video game.

The world of "Final Fantasy VII" is explored further in the "Compilation of Final Fantasy VII", a series of games, animated features, and short stories. The first title in the "Compilation" is the mobile game "", a prequel focusing on the Turks' activities six years before the original game. The CGI film sequel "", set two years after the game, was the first title announced but the second to be released. Special DVD editions of the film included "", an original video animation that recounts the destruction of Nibelheim. "" and its mobile phone counterpart, "", are third-person shooters set one year after "Advent Children". "Dirge" focuses on the backstory of Vincent Valentine, whose history was left mostly untold in "Final Fantasy VII". The most recent title is the PlayStation Portable game "", an action role-playing game that centers on Zack's past.

Releases not under the "Compilation" label include "Maiden Who Travels the Planet", which follows Aerith's journey in the Lifestream after her death, taking place concurrently with the second half of the original game. In 1998, the "Official Final Fantasy VII Strategy Guide" was licensed by Square Soft and published by Brady Games. "Final Fantasy VII Snowboarding" is a mobile port of the snowboard minigame featured in the original game, featuring different courses for the player to tackle. The game is downloadable on V Cast-compatible mobile phones and was first made available in 2005 in Japan and North America. In September 2009 Jason P. Blahuta, Michel S. Beaulieu Wiley created “Final Fantasy and Physiology: The Ultimate Walkthrough”. This eBook is a philosophical guide as to why and how players use certain characters in the Final Fantasy series, explains how gamer's perception of a character’s weapon and clothing designs can change how they use them. The writers inform the reader that this eBook will give them an in-depth understanding of themselves and the game which will enhance their gaming experience. 

"Final Fantasy VII G-Bike" is a mobile game released for iOS and Android in December 2014, based on the motorbike minigame featured in the original game. In September 2007, Square Enix published “Final Fantasy VII 10th Anniversary Ultimania”. This book is an in-depth complication of FFVII story-line and artwork. The Universal Studios Theme Park in Japan is developing a "Final Fantasy VII" themed virtual reality attraction.

With the announcement and development of the "Compilation of Final Fantasy VII", speculation spread that an enhanced remake of the original "Final Fantasy VII" would be released for the PlayStation 3. This conjecture was sparked at the 2005 E3 convention by the release of a video featuring the opening sequence of "Final Fantasy VII" recreated using the PlayStation 3's graphical capabilities. Throughout the lifespan of the PS3, SquareEnix stated that such a game was not in development. A high definition remake was eventually announced at E3 2015 for the PlayStation 4. The game will be more than a high definition remaster, with director Nomura stating that the game will have changes made to its story and combat system.



</doc>
<doc id="52757" url="https://en.wikipedia.org/wiki?curid=52757" title="Final Fantasy VIII">
Final Fantasy VIII

Final Fantasy VIII is a role-playing video game developed and published by Square for the PlayStation console. Released in 1999, it is the eighth main installment in the "Final Fantasy" series. Set on an unnamed fantasy world with science fiction elements, the game follows a group of young mercenaries, led by Squall Leonhart, as they are drawn into a conflict sparked by Ultimecia, a sorceress from the future who wishes to compress time. During the quest to defeat Ultimecia, Squall struggles with his role as leader and develops a romance with one of his comrades, Rinoa Heartilly.

Development began in 1997, during the English localization of "Final Fantasy VII". The game builds on the visual changes brought to the series by "Final Fantasy VII", including the use of 3D graphics and pre-rendered backgrounds, while also departing from many "Final Fantasy" traditions. It is the first "Final Fantasy" to use realistically proportioned characters consistently, feature a vocal piece as its theme music, and forgo the use of magic points for spellcasting.

"Final Fantasy VIII" was mostly well received by critics, who praised its originality and visuals while criticizing some of its gameplay elements. It was voted the 22nd-best game of all time in 2006 by readers of the Japanese magazine "Famitsu". The game was a commercial success; it earned more than US$50 million in sales during its first 13 weeks of release, making it the fastest-selling "Final Fantasy" title until "Final Fantasy XIII", a multi-platform release. A Microsoft Windows port followed in 2000, with the addition of the "Chocobo World" minigame. "Final Fantasy VIII" was re-released worldwide as a PSOne Classic on the PlayStation Store in 2009, for PlayStation 3 and PlayStation Portable, with support for PlayStation Vita in 2012. It was re-released via Steam in 2013 and in Japan in 2014. As of December 2013, it has sold more than 8.5 million copies worldwide.

Like the "Final Fantasy" games before it, "Final Fantasy VIII" consists of three main modes of play: the world map, the field map, and the battle screen. The world map is a 3D display in which the player may navigate freely across a small-scale rendering of the game world. Characters travel across the world map in a variety of ways, including by foot, car, Chocobo, train, and airship. The field map consists of controllable 3D characters overlaid on one or more 2D pre-rendered backgrounds, which represent environmental locations such as towns or forests. The battle screen is a 3D model of a location such as a street or room, where turn-based fights between playable characters and CPU-controlled enemies take place. The interface is menu-driven, as in previous titles, but with the typical weapon and armor systems removed and new features present, such as the Junction system. Also featured is a collectible card-based minigame called "Triple Triad".
For "Final Fantasy VIII", Hiroyuki Ito designed a battle system based on summoned monsters, called "Guardian Forces", abbreviated in-game as "GF". Assigning ("junctioning") a GF onto a character allows the player to use battle commands beyond "Attack" with the main weapon, such as Magic, GF (to have a junctioned GF perform an action), and Item. Previous "Final Fantasy" titles provided each character with a limited pool of magic points that were consumed by each spell; in "Final Fantasy VIII", spells are acquired ("drawn") either from enemies in battle, Draw Points distributed throughout the environments, or by refining items and cards. Spells are then stocked on characters as quantified inventory (up to 100 per spell and limited to 32 distinct spells per character) and are consumed one by one when used. Characters can also junction (equip) these spells onto their statistics—such as Strength, Vitality, and Luck—for various bonuses, provided the character has junctioned a Guardian Force. The junction system's flexibility affords the player a wide range of customization.

These expanded mechanics for summons were a departure for the series; in previous titles, summons were relegated to a single action during battle. The junction system also acts as a substitute for armor and accessories, which were used in earlier games to modify character statistics. Moreover, where earlier titles required weapons to be equipped and tailored to the character, each major character in "Final Fantasy VIII" features a unique weapon which can be upgraded, affecting its appearance, power, and Limit Break.

As in "Final Fantasy VII", characters in "Final Fantasy VIII" have unique abilities called "Limit Breaks", which range from powerful attacks to support spells. While the characters in "Final Fantasy VII" receive Limit Breaks after incurring significant damage, in "Final Fantasy VIII", Limit Breaks become available only at low health (hit points) under normal circumstances. The magic spell "Aura" increases the probability of Limit Breaks appearing, regardless of a character's remaining hit points, while various status afflictions can prevent Limit Breaks. They are similar to the Desperation Attacks of "Final Fantasy VI", albeit more frequent. "Final Fantasy VIII" also introduced interactive elements to complement Limit Break animations. These interactive sequences, which vary between character, weapon, and Limit Break, range from randomly selected magic spells to precisely timed button inputs. Successfully completing an interactive sequence increases the potency of the Limit Break.
"Final Fantasy VIII" features an experience point (EXP) system quite different from previous titles. The essentials remain unchanged: characters gain EXP after defeating enemies, which are typically encountered randomly throughout the game's environments. Earning a set amount of EXP causes the character to gain a level, which increases their overall statistics. While previous titles feature an EXP curve that increases with each level (e.g. getting to level 2 requires 200 EXP, level 3 requires 400, etc.), characters in "Final Fantasy VIII" gain a level after accumulating a flat rate of 1000 points. Enemy levels are based on the party's average level; in most RPGs, enemy levels remain stagnant. Some bosses have level caps to prevent the main quest from becoming too difficult. Higher-level enemies are capable of inflicting and withstanding significantly more damage, may have additional special attacks, and carry additional magic spells, allowing for Junctioning bonuses which themselves far exceed the bonuses imparted by level-gain. The game's unique EXP and level system allows a player to grind to maximum Level 100 before even beginning the plot, though this will result in far more powerful enemies.

In addition to gaining levels, Guardian Forces earn Ability Points (AP) after battles, which are automatically allocated to special abilities that Guardian Forces can learn. When a Guardian Force has learned an ability, that ability becomes available for any character or the character party, as is the case with field abilities. These abilities allow characters to attack more efficiently, refine magic spells from items, receive stat bonuses upon leveling up, access shops remotely, and use additional battle commands.

Most of "Final Fantasy VIII" is set on an unnamed fantasy world. The setting is highly European in design and features a blend of modern and futuristic locales. The planet contains five major landmasses, with Esthar, the largest, covering most of the eastern portion of the map. Galbadia, the second-largest continent, lies to the west, and contains many of the game's locations. The northernmost landmass is Trabia, an Arctic region. Positioned roughly in the middle of the world map lies Balamb, the smallest continent, the island on which the game begins. The remaining landmass is small and mostly desolate, riddled with rough, rocky terrain caused by the impact of a "Lunar Cry", an event where monsters from the moon fall to the planet. The southernmost landmass includes an archipelago of broken sections of land that have drifted apart. Islands and marine structures flesh out the game world, and a handful of off-world locations round out the playable areas.

The six main protagonists of "Final Fantasy VIII" are: Squall Leonhart, a loner who avoids vulnerability by focusing on his duty; Rinoa Heartilly, an outspoken and passionate young woman who follows her heart; Quistis Trepe, an instructor with a serious, patient attitude; Zell Dincht, an energetic martial artist with a fondness for hot dogs; Selphie Tilmitt, a cheerful girl who loves trains and pilots the airship "Ragnarok"; and Irvine Kinneas, a marksman and consummate ladies' man. All but Rinoa are members of "SeeD", an elite military force based out of futuristic installations called Gardens. Temporarily playable characters include Laguna Loire, Kiros Seagill, and Ward Zabac, who appear in "flashback" sequences; SeeD cadet-turned-antagonist Seifer Almasy; and sorceress Edea Kramer. The main antagonist is Ultimecia, a sorceress from the future who wishes to compress time.

Squall and Seifer scar each other while training outside Balamb Garden. Meanwhile, the Republic of Galbadia invades the Dollet Dukedom, forcing Dollet to hire SeeD. The school uses the mission as a final exam for its cadets; with the help of his instructor, Quistis, Squall passes the mission's prerequisite and is grouped with Seifer and Zell. Selphie replaces Seifer mid-mission when the latter disobeys orders and abandons his team. SeeD halts the Galbadian advance; Squall, Zell, and Selphie graduate to SeeD status, but Seifer is disciplined for his disobedience. During the graduation party, Squall meets Rinoa, whose personality is the opposite of his. When assigned with Zell and Selphie to help Rinoa's resistance in Galbadian-occupied Timber, Squall learns that Sorceress Edea is behind Galbadia's recent hostilities. Under orders from Garden, Squall and his comrades—joined by Rinoa, Quistis, and Irvine—attempt to assassinate Edea. During the attempt, Squall's party also learns that Seifer has left Garden to become Edea's second-in-command. Edea survives the attempt, stabs Squall in the shoulder with an ice shard, and detains the party.

After Squall's party escapes, Edea destroys Trabia Garden in a retaliatory missile strike and prepares to attack Balamb Garden. Selphie delays the launch while Squall's team returns to Balamb Garden and activates its mobile functions to evade the missiles. Garden cannot be controlled, however, and it crashes into the docks at Fishermans' Horizon. While Garden is being repaired, Galbadia invades the town in search of a girl named Ellone, who had been staying at Balamb Garden. Before leaving, Ellone reveals that she has been "sending" Squall and his team into flashbacks set 17 years earlier in a vain effort to alter history. The scenes center on Laguna and his friends as he evolves from Galbadian soldier to village protector to leader of an Estharian resistance against Sorceress Adel. Ellone eventually escapes to Esthar, the world's technological superpower.

Meanwhile, Squall confronts his personal anxieties fueled by ongoing developments, such as Headmaster Cid appointing him as SeeD's new leader, and his increasing attraction to Rinoa. Squall and his comrades learn that they, along with Seifer and Ellone, were all raised (except for Rinoa) in an orphanage run by Edea; after eventual separation, they later developed amnesia due to their use of Guardian Forces. Cid and Edea had established Garden and SeeD primarily to defeat corrupt sorceresses. After these revelations, the forces of Balamb Garden defeat the Galbadian Army, led by Seifer, aboard Galbadia Garden. Edea is also defeated by SeeD; however, the party learns that Edea is merely an unwilling host for Ultimecia, who planned to use Ellone to help achieve time compression. Ultimecia transfers her powers to Rinoa; Edea survives, but Rinoa enters a coma. Squall travels to Esthar to find Ellone, as he believes that she can help save Rinoa.

While Rinoa is being treated on Esthar's space station, Ultimecia uses her to free Adel from an orbital prison. Ultimecia then orders Seifer to activate the Lunatic Pandora facility, inciting a Lunar Cry that sends Adel's containment device to the planet. Having selected Adel as her next host, Ultimecia abandons Rinoa in outer space. Squall rescues her, and they return to the planet on a derelict starship and share a romantic moment; Ellone is captured by Galbadia shortly thereafter. After landing, the party encounters Laguna, now President of Esthar; he reveals Dr. Odine's plan to allow Ultimecia to cast time compression on their terms so that Ellone can send SeeD into Ultimecia's time period. At Lunatic Pandora, Squall's team defeats Seifer, rescues Ellone, and kills Adel; Ultimecia possesses Rinoa and begins casting time compression. Ellone sends Squall's team into Ultimecia's era, where she is defeated before time compression can be fully achieved.

A dying Ultimecia travels back in time to pass her powers to Edea, but Squall inadvertently informs Edea of the concepts of Garden and SeeD that she will create. Squall is nearly lost in the flow of time as he witnesses the origins of the game's story, sporadic apparitions of Rinoa, and a faceless portrait of himself. With Rinoa's help, he recollects his memories and returns to the present. A repentant Seifer reunites with Raijin and Fujin, Laguna and his friends visit his lover's grave, the SeeD celebrate at Balamb Garden, and Squall and Rinoa share a kiss under the moonlight.

Development of "Final Fantasy VIII" began in 1997, during the English-language translation of "Final Fantasy VII". As with much of the production of "Final Fantasy VII", series creator and veteran Hironobu Sakaguchi served as the executive producer, working primarily on the development of "" and leaving direction of "Final Fantasy VIII" to Yoshinori Kitase. Shinji Hashimoto was assigned to be the producer in Sakaguchi's place, while the game and battle system were designed by Kitase and Hiroyuki Ito, respectively. One of the development difficulties encountered was having three real-time characters exploring an environment at the same time. The card game Triple Triad was conceived and implemented by programmer Kentarow Yasui. The concept was derived from trading cards which is a popular hobby in some parts of Japan. Triple Triad was meant to keep the player's interest during long stretches without cutscenes. Originally, it was simply about collecting cards but Yasui considered this too disconnected from the main game and "begged" for the inclusion of an ability to transform cards into items. The game's total development costs were approximately ¥3 billion, which equated to around $16 million without inflation. The staff consisted of about 180 people. American localization specialist Alexander O. Smith stated due to a lack of communication with the development team, they were surprised that an IT employee used a GameShark to access text files for localizing to Western audiences.

From the beginning, Kitase knew he wanted a thematic combination of fantasy and realism. To this end, he aimed to include a cast of characters who appeared to be ordinary people. Character designer and battle visual director Tetsuya Nomura and art director Yusuke Naora strove to achieve this impression through the inclusion of realistically proportioned characters—a departure from the super deformed designs used in the previous title. Additionally, Naora attempted to enhance the realism of the world through predominantly bright lighting effects with shadows distributed as appropriate. Other measures taken included implementing rental cars for travel in-game, and the use of motion capture technology to give the game's characters lifelike movements in the game's full motion video sequences. The FMV sequences were created by a team of roughly 35 people, with the total cinematic run-time being estimated at over an hour, approximately 20 minutes longer than the FMV sequences in "VII". Motion capture was used to give a general realism to character movement, but the team favored manual animation over relying on motion capture. A major challenge was the technical advances made since the release of "VII", and the aim for more realistic characters. A major issue with the cutscenes was having real-time character models moving across environments within an FMV.

In an interview with "Famitsu", Naora described that the game was generally designed to be a "bright, fresh "Final Fantasy"." The main reason was that the team had dealt extensively with dark and "weird" imagery with "VII". The designers felt a need to invert the atmosphere of previous games in the series, which had feelings of "light emerging from darkness". This decision was easy for the developers to make, because most of them had worked on "Final Fantasy VII" and felt that a new direction was acceptable. The world designs were also developed with the knowledge that most of the staff were now used to computer graphics, which was not the case with "Final Fantasy VII". The developers also noted that with "Final Fantasy VIII", they attempted to "mix future, real life and fantasy." As part of a theme desired by Kitase to give the game a foreign atmosphere, various designs were given to its locations using the style of internationally familiar places, while also maintaining a fantasy atmosphere. Inspiration ranged from ancient Egyptian and Greek architecture, to the city of Paris, France, to an idealized futuristic European society. Flags were also given to some factions, their designs based on the group's history and culture.

To maintain a foreign atmosphere, the characters of the game were designed to have predominantly European appearances. The first "Final Fantasy VIII" character created was Squall. Desiring to add a unique angle to Squall's appearance and emphasize his role as the central character, Nomura gave him a scar across his brow and the bridge of his nose. As there was not yet a detailed history conceived for the character, Nomura left the explanation for Squall's scar to scenario writer Kazushige Nojima. Squall was given a gunblade, a fictional revolver–sword hybrid that functions primarily as a sword, with an added damaging vibration feature activated by use of its gun mechanism, similar to a vibroblade. His character design was complemented by a fur lining along the collar of his jacket, incorporated by Nomura as a challenge for the game's full motion video designers. Additionally, some designs Nomura had previously drawn, but had not yet used in a "Final Fantasy" game, were incorporated into "Final Fantasy VIII". These were the designs of Edea, Fujin and Raijin. The latter two had originally been designed for use in "Final Fantasy VII", but with the inclusion of the Turks characters in that game, it was felt that Fujin and Raijin were unnecessary. Nomura had designed Edea before the development of "Final Fantasy VII", based on the style of Yoshitaka Amano. For the Guardian Forces, Nomura felt they should be unique beings, without clothes or other human-like concepts. This was problematic, as he did not want them to "become the actual monsters", so he took great care in their design. Leviathan was the first GF, created as a test and included in a game demo. After it received a positive reaction from players, Nomura decided to create the remaining sequences in a similar fashion.

The plot of "Final Fantasy VIII" was conceived by Kitase, with the stories for the characters provided by Nomura and the actual scenario written by Nojima. During the game's pre-production, Nomura suggested the game be given a "school days" feel. Nojima already had a story in mind in which the main characters were the same age; their ideas meshed, taking form as the "Garden" military academies. Nojima planned that the two playable parties featured in the game (Squall's present day group and Laguna's group from the past) would be highly contrasted with one another. This idea was conveyed through the age and experience of Laguna's group, versus the youth and naïveté of Squall's group. Nojima has expressed that the dynamic of players' relationships with the protagonist is important to him. Both "Final Fantasy VII" and "Final Fantasy VIII" feature reserved, quiet protagonists in the form of Cloud Strife and Squall. With "Final Fantasy VIII", however, Nojima worked to give players actual insight into what the character was thinking; a direct contrast with his handling of "Final Fantasy VII", which encouraged the player to speculate.

In March 1999, one month after the game's release, "Final Fantasy VIII Ultimania" was published, a book that features an in-depth guide to "Final Fantasy VIII" and interviews with the developers. An origami book was released in November 1999. On September 22, 1999, a CD-ROM titled "Final Fantasy VIII Desktop Accessories" was released. It contains desktop icons, computer wallpapers, screensavers, and an e-mail application. It additionally features a stand-alone edition of the Triple Triad minigame, which allowed players to compete against one another via a local area network.

Also in 1999, the ballroom dance scene of "Final Fantasy VIII" was featured as a technical demo for the PlayStation 2. In 2000, a PC version was released for Windows. This port featured smoother graphics, enhanced audio, and the inclusion of "Chocobo World", a minigame starring Boko, a Chocobo featured in one of the side-quests in "Final Fantasy VIII". For most North American and European players, the PC version of the game was the only means of playing "Chocobo World", as the game was originally designed to be played via the PocketStation, a handheld console never released outside Japan. In 2009, "Final Fantasy VIII" was added to the PlayStation Store on the PlayStation Network.

On December 18, 2012, the game was re-released as part of the "Final Fantasy 25th Anniversary Ultimate Box" Japanese package. An upscaled PC version was announced May 17, 2013, and was released on Steam December 5, 2013.

Regular series composer Nobuo Uematsu wrote the soundtrack for "Final Fantasy VIII". He tried to base the songs on the emotional content of the scenes in which they would be played, asserting that expressing the emotions he desired was more important than improving skills: "I think it will be a shame if we won't be able to cry as we play our own game". He could not determine a character's emotions solely based on the plot, instead using images of appearance and attire—"It's important to know when their emotions are at their height, but it usually takes until a month before release for them to finish the ending dialog...!" When IGN Music stated that the music of "Final Fantasy VIII" was very dark and perhaps influenced by the plot of the game, Uematsu said "the atmosphere of music varies depending on story line, of course, but it's also my intention to put various types of music into one game". The absence of character themes found in the previous two games was due to Uematsu finding those of "Final Fantasy VI" and "Final Fantasy VII" ineffective. Uematsu considers it reasonable to have character themes if each character has a "highlight" in the game, but he found "Final Fantasy VIII" only focused on Squall Leonhart and Rinoa Heartilly as a couple, resulting in the "Eyes on Me" theme.

The original soundtrack was released on four compact discs by DigiCube in Japan on March 10, 1999, and by Square EA in North America as "Final Fantasy VIII Music Collection" in January 2000. It was republished worldwide by Square Enix on May 10, 2004. An album of orchestral arrangements of selected tracks from the game was released under the title "Fithos Lusec Wecos Vinosec Final Fantasy VIII" on November 19, 1999, by DigiCube, and subsequently published on July 22, 2004, by Square Enix. The pieces were arranged and conducted by Shirō Hamaguchi for a live orchestra. A collection of piano arrangements performed by Shinko Ogata was released under the title "Piano Collections: Final Fantasy VIII" by DigiCube on January 21, 2000, and subsequently re-published by Square Enix on July 22, 2004.

The score is best known for two songs: "Liberi Fatali", a Latin choral piece that is played during the introduction to the game, and "Eyes On Me", a pop song serving as the game's theme, performed by Chinese singer Faye Wong. Near the end of the production of "Final Fantasy VII", the developers suggested to use a singer, but abandoned the idea due to a lack of reasoning based on the game's theme and storyline. However, Nobuo Uematsu thought a ballad would closely relate to the theme and characters of "Final Fantasy VIII". This resulted in the game's developers sharing "countless" artists, eventually deciding on Wong. Uematsu claims "her voice and mood seem to match my image of the song exactly", and that her ethnicity "fits the international image of Final Fantasy". After negotiations were made, "Eyes on Me" was recorded in Hong Kong with an orchestra. The song was released as a CD single in Japan and sold over 400,000 copies, setting the record for highest-selling video game music disc ever released in that country at the time. "Liberi Fatali" was played during the 2004 Summer Olympics in Athens during the women's synchronized swimming event.

The music of "Final Fantasy VIII" has appeared in various official "Final Fantasy" concerts. These include 2002's "20020220 Music from FINAL FANTASY", in which the Tokyo Philharmonic Orchestra played "Liberi Fatali", "Don't Be Afraid", "Love Grows", and "The Man with the Machine Gun", the 2004 "Tour de Japon" series, which featured "The Oath", the "Dear Friends" series that began that same year and included "Liberi Fatali" and "Love Grows", and the 2005 "More Friends" concert, which included "Maybe I'm a Lion". More recent concerts include the "Voices – Music from Final Fantasy" 2006 concert showcasing "Liberi Fatali", "Fisherman's Horizon", and "Eyes on Me" and the international "Distant Worlds" concert tour that continues to date, which includes "Liberi Fatali", "Fisherman's Horizon", "Man with the Machine Gun", and "Love Grows". Several of these concerts have produced live albums as well. Music from the game has also been played in non "Final Fantasy"-specific concerts such as the "Play! A Video Game Symphony" world tour from 2006 onwards, for which Nobuo Uematsu composed the opening fanfare that accompanies each performance.

"Final Fantasy VIII" received critical acclaim. Within two days of its North American release on September 9, 1999, "Final Fantasy VIII" became the top-selling video game in the United States, a position it held for more than three weeks. It was also a bestseller in Japan and the UK. It grossed a total of more than $50 million in the 13 weeks to follow, making it the fastest-selling "Final Fantasy" title. In Japan, it sold roughly 2.5 million units within the first four days of release. More than 6 million units were sold in total by the end of 1999. As of March 31, 2003, the game had shipped 8.15 million copies worldwide: 3.7 million in Japan and 4.45 million abroad. The opening cut scene in "Final Fantasy VIII" was ranked second on "Game Informer"s list of "Top 10 Video Game Openings", and first by IGN. GameSpy listed it as the 15th best cinematic moment in video games. IGN named the game's ending the third best of any game for the PlayStation, while UGO.com named it one of the series' best and most memorable moments. "Final Fantasy VIII" was voted by readers of Japanese magazine "Famitsu" as the 22nd best game of all time in 2006, and named one of the 20 essential Japanese role-playing games by Gamasutra, stating "[t]here's a lot that "Final Fantasy VIII" does wrong, but there's even more that it does right". As of December 2013, it has sold more than 8.5 million copies worldwide. According to Steam Spy, another 703,000 copies of the PC version were sold by April 2018.

Reviews of the gameplay have been mixed. IGN felt that it was the weakest aspect of the game, citing its Guardian Force attack sequences as "incredibly cinematic" but tedious, sentiments echoed by "Electronic Gaming Monthly". They also regarded the battle system as intensely complicated, yet refreshingly innovative and something that "RPG fanatics love to obsess over". "Official U.S. PlayStation Magazine" claims that the game's Junction system is a major flaw due to repetitive stocking of spells, while the video game magazine "Edge" commented that the battle system consists of a "bewildering" number of intricate options and techniques that "most gamers will [...] relish". GameSpot praised the game's battle system, commenting that the "possibilities for customization [with the Junction system] are immense".

In general, "Final Fantasy VIII" has been compared favorably to its predecessors. Though questioning the game's lack of voice overs for its characters, Game Revolution praised its storyline and ending. For their part, "Edge" labeled "Final Fantasy VIII" "a far more accomplished game than "FFVII"". On the other hand, the magazine also felt that the game's length left its story unable to "offer consistently strong dialogue and sub-plots". Additionally, it found some of the story's plot twists "not... suitably manipulated and prepared", leaving it "hard not to greet such... moments with anything but indifference". Overall, "Edge" considered "Final Fantasy VIII" to be "yet another outstanding edition of SquareSoft's far-from-final fantasies", summarizing it as "aesthetically astonishing, rarely less than compelling, and near peerless in scope and execution". "Electronic Gaming Monthly" offered similar comments, stating that the game's character development "is the best of any RPG's" and that ""Final Fantasy VIII" is the pinnacle of its genre." UGO.com stated that while no other game in the series had stirred the controversy that "Final Fantasy VIII" had and that it was flawed, "Final Fantasy VIII" was a "daring, groundbreaking game [...] decidedly the most original console-style RPG ever created". In 2002, IGN named it the seventh best title for the PlayStation, placing higher on the list than "Final Fantasy VII"; the publication felt that "Final Fantasy VIII" improved on the strengths of its predecessor.

The PC port received mixed reception. "Maximum PC" praised the full motion video sequences as "phenomenal", adding that while the gameplay took getting used to, they enjoyed the teamwork emphasized by it, and that the game's visual presentation added to its appeal. GameSpy stated that while the game was not a "huge leap forward" from the previous title, its gameplay and visual appeal worked for its benefit, though that on a computer the pre-rendered backgrounds appeared blurry and the controls at time difficult with a keyboard. GameSpot criticized the game for not taking advantage of the capabilities afforded to computers at the time, describing the PlayStation version as both looking and sounding superior, and recommending that the title was "not worth buying period" for the PC. UGO.com also described the port as inferior to its original counterpart, adding that its presentation was in turn detrimental to the reception the game received as a whole. "Computer Gaming World" praised some of the changes made to the game in light of previous titles and the inclusion of the Triple Triad sub-game, though heavily criticized the port as "lazy" and "disappointing", stating that it only served to emphasize the original game's flaws. Despite their complaints, they named it the twentieth best game of 2000.



</doc>
<doc id="52758" url="https://en.wikipedia.org/wiki?curid=52758" title="Final Fantasy IX">
Final Fantasy IX

Final Fantasy IX is a 2000 role-playing video game developed and published by Squaresoft for the PlayStation video game console. It is the ninth game in the main "Final Fantasy" series and the last to debut on the original PlayStation. The plot centers on the consequences of a war between nations in a medieval fantasy world called Gaia. Players follow bandit Zidane Tribal, who kidnaps Alexandrian princess Garnet Til Alexandros XVII as part of a gambit by the neighboring nation of Lindblum. He joins Garnet and a growing cast of characters on a quest to take down her mother, Queen Brahne of Alexandria, who started the war. The plot shifts when the player learns that Brahne is a pawn of a more menacing threat, Kuja, who shares a mysterious history with Zidane spanning two worlds.

The game was developed alongside "Final Fantasy VIII". Envisioned by developers as a retrospective for the series, it departed from the futuristic settings of "Final Fantasy VII" and "Final Fantasy VIII" by returning to the medieval style of the first six installments. Consequently, it was influenced heavily by the original "Final Fantasy", and features allusions to the rest of the games. Despite this approach, the game did introduce new features to the series, such as "Active Time Event" cutscenes, "Mognet", and skill systems.

"Final Fantasy IX" was released to critical acclaim. It is often cited by critics and fans as one of the best "Final Fantasy" games, and holds the highest Metacritic score of the series. "Final Fantasy IX" was commercially successful, selling more than 5.5 million copies worldwide by February 2016. It was re-released in 2010 as a PS1 Classic on the PlayStation Store; this version was compatible with PlayStation 3 and PlayStation Portable, and PlayStation Vita support arrived in 2012. Ports featuring minor gameplay and graphical enhancements were released for various other platforms in the late 2010s.

In "Final Fantasy IX", the player navigates a character throughout the game world, exploring areas and interacting with non-player characters. Most of the game occurs on "field screens" consisting of pre-rendered backgrounds representing towns and dungeons. To aid exploration on the field screen, "Final Fantasy IX" introduces the "field icon", an exclamation mark appearing over their lead character's head, signalling an item or sign is nearby. Players speak with moogles to record their progress, restore life energy with a tent and purchase items—a deviation from previous installments, which used a save point to perform these functions. Moogles can also be contacted from the world map; with an item called a "Moogle flute". Moogles may request the playable character deliver letters to other Moogles via Mognet, playable characters might also receive letters from non-playable characters.

Players journey between field screen locations on the world map, a three dimensional, downsized representation of "Final Fantasy IX"s world presented from a top-down perspective. Players can freely navigate around the world map screen unless restricted by terrain like bodies of water or mountain ranges. To overcome geographical limitations, players can ride chocobos, sail on a boat or pilot airships. Like previous "Final Fantasy" installments, travel across the world map screen and hostile field screen locations is interrupted by random enemy encounters.

"Final Fantasy IX" offers a new approach to town exploration with the introduction of Active Time Events (ATE). These allow the player to view events unfolding at different locations, providing character development, special items and prompts for key story-altering decisions. ATE are occasionally used to simultaneously control two teams when the party is divided to solve puzzles and navigate mazes.

Whenever the playable character encounters an enemy, the map changes to the "battle screen". On the battle screen, the enemy appears on the opposite side of the characters; each battle uses the familiar Active Time Battle system that was first featured in "Final Fantasy IV". The character's command list is presented in a window opposite the ATB gauge list; while all characters can physically attack the enemy or use an item from the player's inventory, they also possess unique abilities. For example, the thief Zidane can steal items from the enemy, Eiko and Garnet can summon "eidolons" to aid the party and Vivi can use black magic to damage the opposition.

These character-specific commands change when the player goes into "Trance mode", which is activated for a short duration when an uncontrollable gauge fills as character sustains damage in a style similar to the Limit Breaks used in "Final Fantasy VII". When the gauge is full, the character's strength is amplified, and the player can select special attack commands. Zidane's "Skill" command list, for example, changes to "Dyne", allowing him to execute powerful attacks; Vivi's "Black Magic" command evolves into "Double Black", allowing him to cast two magic spells simultaneously. Through the Configuration screen, the player can change the Battle Style from Normal to Custom, which allows two players to control any combination of characters during battle. However, two controllers must be plugged into the PlayStation.

A character's performance in battle is determined by numerical values ("statistics") for categories like speed, strength and magical power. Character statistics are driven by experience; when players win battles, they are awarded "experience points", which accumulate until characters gain "experience levels". When characters "level up", the statistics for their attributes permanently increase, which may also be amplified by the types of equipment the character is wearing. Winning battles also awards the player money (Gil), Tetra Master playing cards, items and ability points (AP).

"Final Fantasy IX" deviates from the style of customisable characters featured in the last two games by reviving the character class concept, which designates a character to a certain role in battle. For example, Vivi is designated as a black mage and is the only character who can use black magic, and Steiner is a knight and is the only character who can use sword skills.

The basic function of equipment in "Final Fantasy" games is to increase character attributes; arming Zidane with a Mythril Vest, for example, increases his base defense statistic. In "Final Fantasy IX", weapons and armor include special character abilities, which the character may use when the item is equipped (permitting the ability matches their class). Once the character accumulates enough ability points in battle, the ability becomes usable without having to keep the item equipped. In addition to granting abilities the equipment in "Final Fantasy IX" determines the statistical growth of the characters at the time of level up. Armor not only raises base defense or evasion statistics but raises defense and/or other statistics at level up.

Abilities are classified into action and support categories. Action abilities consume magic points (MP) and include magic spells and special moves that are used in battle. Support abilities provide functions that remain in effect indefinitely and must be equipped with magic stones to be functional. The maximum number of these stones increases as the characters level up.

"Tetra Master" is a card-based minigame that can be initiated with various non-playable characters in the field. Players assemble a deck of five cards, which can be obtained via chests, given as a reward, or earned from fighting monsters. Each card has various arrows which point to the four sides and four corners of the card, and various stats that vary between cards, with rarer cards being more powerful. Players take it in turns to strategically place cards on a 4x4 playing grid based on the available directions. Battles can occur when players place a card next to another card, depending on where the player places it. If the defending card has no arrows whilst the attacking card has an arrow pointing towards it, that card is placed under the player's control. When two arrows meet with each other, the cards do battle based on their point values, with the losing card coming under the winning player's control, sometimes triggering combos that put multiple cards in the winner's control. After all cards are played, the winner is the player who has the most cards under their control, with a draw occurring if they have the same number of cards. The winning player may choose a card from their opponent's deck out of the ones they put under their control. If the winning player scores a perfect win however, in which all ten cards are put under their control, they will win all five cards from the opponent's deck.

"Final Fantasy IX" takes place primarily on a world named Gaia. Most of Gaia's population reside on the Mist Continent, named after the thick Mist that blankets the lowlands. Large mountain ranges act as natural borders that separate its four nations: Alexandria, Lindblum, Burmecia, and Cleyra. Alexandria is a warmongering monarchy that controls the eastern half of the continent. One of its cities is Treno, a cultural nexus under perpetual starlight that is home to many aristocrats and paupers alike. The technologically advanced Lindblum, a hub of airship travel, is nestled on a plateau to the southwest. Both countries are populated by a mix of humans, humanoids, and anthropomorphic animals. Burmecia, a kingdom showered by endless rain, is in the northwest; Cleyra, a neighboring region that seceded from Burmecia due to the latter's appreciation for war, hails from a giant tree in the desert and is protected by a powerful sandstorm. Both are inhabited by anthropomorphic rats with a fondness for dance and spear fighting.

Players eventually explore the Outer, Lost, and Forgotten Continents as well. Civilizations on the Outer Continent include Conde Petie, home of the dwarves; Black Mage Village, a hidden settlement of sentient magician drones; and Madain Sari, once home to a near-extinct race of horned humanoid summoners who conjure magical entities called eidolons. Also on the Outer Continent is the Iifa Tree, which disperses the Mist to other continents through its roots. This Mist stimulates the fighting instinct in humanoids and contributes to Gaia's bloody history. The Lost and Forgotten continents are littered mostly with ancient ruins. Scattered throughout the marshes of Gaia are the Qu: large, frog-eating, and seemingly androgynous humanoids who are considered great gourmands. Late in the game, players briefly travel to the parallel world of Terra and the dream realm of Memoria.

The main playable characters are: Zidane Tribal, a member of a group of bandits called Tantalus who are masquerading as a theater troupe; Garnet Til Alexandros XVII (alias Dagger), the Princess of Alexandria who is actually from Madain Sari; Vivi Ornitier, a young, timid, and kind black mage with an existential crisis; Adelbert Steiner, a brash Alexandrian knight captain and loyal servant of Princess Garnet; Freya Crescent, a Burmecian dragoon searching for her lost love; Quina Quen, a Qu whose master wants him/her to travel the world so that s/he will learn about cuisine; Eiko Carol, a young girl living in Madain Sari, and, along with Garnet, one of the last two summoners; and Amarant Coral, a bounty hunter hired to return Garnet to Alexandria. Other important characters include Cid Fabool, the charismatic Regent of Lindblum; Brahne, Garnet's mother and the power-hungry Queen of Alexandria; General Beatrix, the powerful leader of the female knights of Alexandria; Garland, an elderly Terran male tasked with saving his world; and antagonist Kuja, an arms dealer and pawn of Garland with his own existential crisis.

In Alexandria, Zidane and Tantalus kidnap Princess Garnet by order of Cid. Garnet does not resist, for she was already planning to flee and warn Cid of Queen Brahne's increasingly erratic behavior. Vivi and Steiner join the party during the escape. En route to Lindblum, the group discovers that Brahne is using a village to manufacture soulless black mage soldiers that look similar to Vivi. In Lindblum, Cid confirms that he hired the group to protect Garnet from Brahne's newfound aggression. After learning that Alexandria has invaded Burmecia with the black mages, Zidane and Vivi team up with Freya to investigate, while Garnet and Steiner secretly return to Alexandria to reason with Brahne. 

Zidane's team finds that the Alexandrian forces, headed by Beatrix, the head of Brahne's knights, conquered Burmecia with help from Kuja, and the refugees have fled to Cleyra. Brahne imprisons Garnet and extracts her eidolons; she uses one to destroy Cleyra while Zidane's group is defending the city. The party escapes on Brahne's airship, rendezvous with Steiner, and rescues Garnet. Meanwhile, Brahne cripples Lindblum with another eidolon. Cid explains that Kuja is supplying Brahne with the black mages and knowledge to use eidolons. The party befriends Quina and tracks Kuja to the Outer Continent, a land mostly devoid of Mist and thus inaccessible by airship. Brahne hires a pair of bounty hunters, Lani and Amarant, to follow the party and bring Garnet back to Alexandria. On the Outer Continent, the party defeats Lani and meets Eiko, a summoner who lives with a group moogles in the otherwise empty summoner village of Madain Sari. Eiko leads the Zidane and the others to the Iifa Tree. Inside, they learn that Kuja uses Mist to create the black mages, and that Vivi was a prototype. The party defeats the source of the Mist within the Tree, and the substance clears from the Mist Continent. While waiting for Kuja's reprisal at Madain Sari, Lani and Amarant attempt to kidnap Eiko but are foiled by Zidane and the moogles. Amarant then challenges Zidane to a duel to Zidane and loses. He then joins the party, and Garnet learns of her summoner heritage. Kuja arrives at the Tree, but Brahne also appears and attempts to kill Kuja with an eidolon so she can rule unopposed; he takes control of it and destroys her and her army.

After Garnet's coronation, Kuja attacks Alexandria castle. Garnet and Eiko summon an extremely powerful eidolon in defense; Kuja attempts to steal the eidolon as a means to kill his master, Garland, but the latter arrives and destroys it. Seeking to stop the quarreling villains, the party chases Kuja on an airship from Cid that runs on steam rather than the now-cleared Mist. They eventually unlock a portal to Terra, where the goals of the antagonists are revealed. The Terrans created Garland to merge the dying world with Gaia; Garland, in turn, created self-aware, soulless vessels called Genomes. For millennia, Garland has been using the Iifa Tree to replace deceased Gaian souls with the hibernating Terran souls, turning the former into Mist in the process; this will allow the Terrans to be reborn into the Genomes after the planetary merge. Kuja and Zidane are Genomes created to accelerate this process by bringing war and chaos to Gaia. Kuja had betrayed Garland to avoid becoming occupied by a Terran soul. Kuja defeats Garland, who reveals before dying that the former has a limited lifespan anyhow: Zidane was designed to be his replacement. Enraged, Kuja destroys Terra and escapes to the Iifa Tree.

At the Iifa tree, the party enters Memoria and reaches the origin of the universe: the Crystal World. They defeat Kuja, preventing him from destroying the original crystal of life and thus the universe. After destroying Necron, a force of death, the Tree is destroyed; the party flees, while Zidane stays behind to rescue Kuja. One year later, the cast's fate is revealed: Tantalus arrives in Alexandria to put on a performance; Vivi's lifespan has expired, but he has left behind a number of 'sons'; Freya and Fratley are rebuilding Burmecia; Eiko has been adopted by Cid; Quina works in the castle's kitchen; Amarant and Lani are travelling together; and Garnet presides as queen of Alexandria, with Steiner and Beatrix as her guards. In the climax of Tantalus's performance, the lead actor reveals himself as Zidane in disguise and is reunited with Queen Garnet.

Development of "Final Fantasy IX" began before Square had finished development on "Final Fantasy VIII". The game was developed in Hawaii as a compromise to developers living in the United States. As the series' last game on the PlayStation, Sakaguchi envisioned a "reflection" on the older games of the series. Leading up to its release, Sakaguchi called "Final Fantasy IX" his favorite "Final Fantasy" game as "it's closest to [his] ideal view of what "Final Fantasy" should be". This shift was also a response to demands from fans and other developers. Additionally, the team wanted to create an understandable story with deep character development; this led to the creation of Active Time Events. The scenario for the game was written by Sakaguchi. He began early planning on it around July 1998. Director Hiroyuki Ito had the idea to make the protagonist Zidane flirtatious towards women.
In the game's conceptual stage, the developers made it clear that the title would not necessarily be "Final Fantasy IX", as its break from the realism of "Final Fantasy VII" and "Final Fantasy VIII" may have alienated audiences. This led fans to speculate that it would be released as a "gaiden" (side story) to the main series. By late 1999, however, Square had confirmed that the game would indeed be published as "Final Fantasy IX", and by early 2000, the game was nearly finished. The developers made several adjustments to the game, such as changing the ending seven times. Director Ito had designed the battle system used in the game.

The game's developers sought to make the game's environment more "fantasy-oriented" than its PlayStation predecessors. Since the creators wanted to prevent the series from following a redundant setting, "Final Fantasy IX" distinctly breaks from the futuristic styles of "Final Fantasy VII" and "Final Fantasy VIII" by reintroducing a medieval setting. In the game, steam technology is just beginning to become widely available; the population relies on hydropower or wind power for energy sources, but sometimes harness Mist or steam to power more advanced engines. Continuing with the medieval theme, the game's setting is inspired by Norse and Northern European mythology. According to Ito, "[The development team is] attracted to European history and mythology because of its depth and its drama". The main "Final Fantasy IX" website says the development of the game's world serves as a culmination of the series by blending the "successful elements of the past, such as a return to the fantasy roots," with newer elements. The creators made the characters a high priority. The return to the series' roots also affected the characters' designs, which resulted in characters with "comic-like looks". Composer Nobuo Uematsu commented that the design staff attempted to give the characters realism while still appearing comic-like. To accomplish this, and to satisfy fans who had become used to the realistic designs of "Final Fantasy VIII", the designers stressed creating characters with whom the player could easily relate.

The music of "Final Fantasy IX" was written by series regular Nobuo Uematsu. In early discussions with game director Hiroyuki Ito he was asked to compose themes for the eight main characters along with "an exciting battle track, a gloomy, danger-evoking piece, and around ten other tracks." Uematsu spent an estimated year composing and producing "around 160" pieces for "Final Fantasy IX", with 140 ultimately appearing in the game.

During writing sessions he was given a travel break in Europe for inspiration where he spent time admiring ancient architecture in places like Germany. Uematsu has cited medieval music as a major influence on the score of "Final Fantasy IX". He aimed for a "simple" & "warm" atmosphere and incorporated uncommon instruments like the kazoo and dulcimer. Unlike the stark realism of its predecessors", VII" and "VIII", the high fantasy undertones of "Final Fantasy IX" allowed for a wider spectrum of musical styles and moods. Uematsu composed primarily with a piano and used two contrasting methods: "I create music that fits the events in the game, but sometimes, the [developers] will adjust a game event to fit the music I've already written." 

Uematsu incorporated several motifs from older "Final Fantasy" games into the score, such as the original battle music intro, a reworked "Volcano Theme" from "Final Fantasy" and the Pandemonium theme from "Final Fantasy II". Tantalus' band is also heard playing "Rufus' Welcoming Ceremony" from "Final Fantasy VII" near the beginning of the game.

Uematsu has stated on several occasions that "Final Fantasy IX" is his favorite score. "Melodies of Life" is the theme song of "Final Fantasy IX", and shares its main melody with pieces frequently used in the game itself, such as the overworld theme, and a lullaby that is sung by Dagger. It was performed by Emiko Shiratori in both the Japanese and English versions and arranged by Shirō Hamaguchi.

"Final Fantasy IX"s release was delayed to avoid a concurrent release with then rival Enix's "Dragon Quest VII". On October 7, 2000, a demo day for the North American version of "Final Fantasy IX" was held at the Metreon in San Francisco, California. The first American release of the game was also at the Metreon; limited edition merchandise was included with the game, and fans cosplayed as "Final Fantasy" characters in celebration of the release. In Canada, a production error left copies of "Final Fantasy IX" without an English version of the instruction manual, prompting Square to ship copies of the English manual to Canadian stores several days later.

The game was heavily promoted both before and after its release. Starting on March 6, 2000, "Final Fantasy IX" characters were used in a line of computer-generated Coca-Cola commercials. Figurines of several characters were also used as prizes in Coca-Cola's marketing campaign. That same year, IGN awarded "Final Fantasy" dolls and figurines for prizes in several of their contests.

"Final Fantasy IX" was also the benchmark of Square's interactive PlayOnline service. PlayOnline was originally developed to interact with "Final Fantasy X", but when those plans fell through it became a strategy site for "Final Fantasy IX". The site was designed to complement BradyGames' and Piggyback Interactive's official strategy guides for the game, where players who bought the print guide had access to "keywords" that could be searched for on PlayOnline's site for extra tips and information. This caused fury among buyers of the guide, as they felt cheated for the expensive print guide. The blunder made GameSpy's "Top 5 Dumbest Moments in Gaming" list, and Square dropped the idea for "Final Fantasy X", which was under development at the time.

On December 18, 2012, the game was re-released as part of the "Final Fantasy 25th Anniversary Ultimate Box" Japanese package. On February 10, 2016, a remaster was released for iOS and Android.<ref name="iOS/Android"></ref> The remaster features HD movies and character models, an auto-save feature, 7 game boosters and achievements. A port for Microsoft Windows was released on April 14, 2016. In September 2017, the Windows port was released on PlayStation 4. It was also released on the Nintendo Switch, Xbox One, and Windows 10 in North America on February 13, 2019, and in other regions the following day.

"Final Fantasy IX" sold over 2.65 million copies in Japan by the end of 2000, making it the second-highest selling game of the year there. Although it was a top-seller in Japan and America, "Final Fantasy IX" did not sell as well as "Final Fantasy VII" or "Final Fantasy VIII" in either Japan or the United States. In 2001, the game received a "Gold" certification from the Verband der Unterhaltungssoftware Deutschland (VUD), for sales of at least 100,000 units across Germany, Austria and Switzerland. As of March 31, 2003, "Final Fantasy IX" had sold 5.30 million copies worldwide. The game was voted the 24th-best game of all time by readers of the Japanese magazine "Famitsu". It sold over 5.5 million copies worldwide by February 2016.

"Final Fantasy IX" was released to critical acclaim both in Japan and the US. On the review aggregator Metacritic it has achieved a score of 94%, the highest score for a "Final Fantasy" game on the site. On GameRankings it has received a score of 93%, the second highest of any Final Fantasy game, behind "Final Fantasy VI" for the Super NES.

Across the reviews, praise was given to the graphics and nostalgic elements. Critics pointed out the strength of the game within its gameplay, character development, and visual representation. GameSpot noted that the learning curve is easily grasped, and that the ability system is not as complex as in "Final Fantasy VII" or "Final Fantasy VIII". Each player character possesses unique abilities, which hinders the development of an over-powered character. GameSpot describes the battle system as having a tactical nature and notes that the expanded party allows for more interaction between players and between enemies. Nevertheless, IGN disliked the lengthy combat pace and the repeated battles, describing it as "aggravating", and RPGFan felt the Trance system to be ineffective as the meter buildup is slow and unpredictable, with characters Trancing just before the enemy is killed.

The characters and graphics received positive reviews. Although IGN felt that the in-depth character traits in "Final Fantasy IX" could be generally found in other "Final Fantasy" games, it still found the characters to be engaging and sympathetic. GameSpot found the characters, up to their dialogue and traits, amusing and full of humor. IGN also noted that the Active Time Event system helps to expand the player's understanding of the characters' personalities as they question many ideas and emotions. Their semi-deformed appearance, which also covers monsters of every size, contain detailed animation and design. They gave praise to the pre-rendered backgrounds, noting the careful attention given to the artwork, movement in animations and character interactivity. The movies are seen as emotive and compelling, and the seamless transition and incorporation to the in-game graphics helped to move the plot well.

Critics acknowledged that the overall storyline was mainly built upon elements found in previous "Final Fantasy" installments, such as evil empires and enigmatic villains. The main villain, although considered by GameSpot to be the least threatening in the series, was seen by IGN as an impeccable combination of "Kefka's cackling villainy" and "plenty of the bishonenosity that made Sephiroth such a hit with the ladies". Mixed reactions were given to the audio aspects of the game. Some reviewers, such as RPGFan felt that the music was "uninspired and dull" whereas "GamePro" praised the audio for evoking "emotions throughout the story, from battles to heartbreak to comedy". Some criticism was leveled on composer Nobuo Uematsu who reused some tracks from past iterations of the series. Still, reviewers have come to agree that this and many other elements are part of the overall effort to create a nostalgic game for fans of the older "Final Fantasy" games.

The strategy guide also received criticism; it urged buyers to log onto an online site to gain the information, instead of providing it within the actual guide. The book's given links are no longer accessible on the PlayOnline website. Tetra Master was seen by GameSpot as inferior and confusing compared to "Final Fantasy VIII"s minigame Triple Triad, as the rules for it were only vaguely explained in the game and there were very few rewards earned from playing it despite its expansive nature.
Notes



</doc>
<doc id="52759" url="https://en.wikipedia.org/wiki?curid=52759" title="Final Fantasy X">
Final Fantasy X

Set in the fantasy world of Spira, a setting influenced by the South Pacific, Thailand and Japan, the game's story revolves around a group of adventurers and their quest to defeat a rampaging monster known as Sin. The player character is Tidus, a star athlete in the fictional sport of blitzball, who finds himself in the world Spira after his home city of Zanarkand is destroyed by Sin. Shortly after arriving to Spira, Tidus joins the summoner Yuna on her pilgrimage to destroy Sin.

Development of "Final Fantasy X" began in 1999, with a budget of more than US$32.3 million (US$ million in dollars) and a team of more than 100 people. The game was the first in the main series not entirely scored by Nobuo Uematsu; Masashi Hamauzu and Junya Nakano were signed as Uematsu's fellow composers. "Final Fantasy X" was both a critical and commercial success, selling over 8 million units worldwide on PlayStation 2. It is widely considered to be one of the greatest video games of all time. On March 3, 2003, it was followed by "Final Fantasy X-2", making it the first "Final Fantasy" game to have a direct game sequel.

Like previous games in the series, "Final Fantasy X" is presented in a third-person perspective, with players directly navigating the main character, Tidus, around the world to interact with objects and people. Unlike previous games, however, the world and town maps have been fully integrated, with terrain outside of cities rendered to scale. As Tidus explores the world, he randomly encounters enemies. When an enemy is encountered, the environment switches to a turn-based battle area where characters and enemies await their turn to attack.

The gameplay of "Final Fantasy X" differs from that of previous "Final Fantasy" games in its lack of a top-down perspective world map. Earlier games featured a miniature representation of the expansive areas between towns and other distinct locations, used for long-distance traveling. In "Final Fantasy X", almost all the locations are essentially continuous and never fade out to a world map. Regional connections are mostly linear, forming a single path through the game's locations, though an airship becomes available late in the game, giving the player the ability to navigate Spira faster. Like previous games in the series, "Final Fantasy X" features numerous minigames, most notably the fictional underwater sport "blitzball".

"Final Fantasy X" introduces the "Conditional Turn-Based Battle" system in place of the series' traditional "Active Time Battle" system first used in "Final Fantasy IV". Whereas the ATB concept features real-time elements, the CTB system is a turn-based format that pauses the battle during each of the player's turns. Thus, the CTB design allows the player to select an action without time pressure. A graphical timeline along the upper-right side of the screen details who will be receiving turns next, and how various actions taken will affect the subsequent order of turns. The ordering of turns can be affected by a number of spells, items, and abilities that inflict status effects upon the controlled characters or the enemies. The player can control up to three characters in battle, though a swapping system allows the player to replace them with a character outside the active party at any time. "Limit Breaks", highly damaging special attacks, reappear in "Final Fantasy X" as "Overdrives". In this new incarnation of the feature, most of the techniques are interactive, requiring button inputs to increase their effectiveness. While initially the Overdrives can be used when the character receives a significant amount of damage, the player is able to modify the requirements to unlock them.

"Final Fantasy X" introduces an overhaul of the summoning system employed in previous games of the series. Whereas in previous titles a summoned creature would arrive, perform one action, and then depart, the "Aeons" of "Final Fantasy X" arrive and entirely replace the battle party, fighting in their place until either the aeon wins the battle, is defeated itself, or is dismissed by the player. Aeons have their own statistics, commands, special attacks, spells, and Overdrives. The player acquires five aeons over the course of the game through the completion of Cloister of Trials puzzles, but three additional aeons can be obtained by completing various side-quests.

As with previous titles in the series, players have the opportunity to develop and improve their characters by defeating enemies and acquiring items, though the traditional experience point system is replaced by a new system called the "Sphere Grid". Instead of characters gaining pre-determined statistic bonuses for their attributes after leveling up, each character gains "Sphere Levels" after collecting enough Ability Points (AP). Sphere Levels allow players to move around the Sphere Grid, a pre-determined grid of interconnected nodes consisting of various statistic and ability bonuses. "Spheres" are applied to these nodes, unlocking its function for the selected character.

The Sphere Grid system also allows players to fully customize characters in contrast to their intended battle roles, such as turning the White Mage-roled Yuna into a physical powerhouse and the swordsman Auron into a healer. The "International" and PAL versions of the game include an optional "Expert" version of the Sphere Grid; in these versions, all of the characters start in the middle of the grid and may follow whichever path the player chooses. As a trade-off, the Expert grid has fewer nodes in total, thus decreasing the total statistic upgrades available during the game.

Blitzball is a minigame that requires strategy and tactics. The underwater sport is played in a large, hovering sphere of water surrounded by a larger audience of onlookers. The player controls one character at a time as they swim through the sphere performing passes, tackles, and attempts to score. The gameplay is similar to that of the main game in the way that the controlled character moves through the area until they encounter an enemy. In this case, the enemy is a member of the opposing team. Status effects are also implemented in the minigame as each player can learn techniques that are equivalent to abilities in the main game.

Blitzball is first introduced in the beginning of the game during one of the early cinematic sequences in which Tidus, the main character who is described as a star blitzball player, is part of an intense game. It is the only minigame that plays a role in the overall plot line as it is not only a main part of Tidus's character, but it's also in the first scene where the game's main antagonist, Sin is shown. Unlike with the other minigames, playing blitzball is mandatory near the beginning of the game, but it is later optional.

"Final Fantasy X" is set in the fictional world of Spira, consisting of one large landmass divided into three subcontinents, surrounded by small tropical islands. It features diverse climates, ranging from the tropical Besaid and Kilika islands, to the temperate Mi'ihen region, to the frigid Macalania and Mt. Gagazet areas. Although predominantly populated by humans, Spira features a variety of races. Among them are the Al Bhed, a technologically advanced but disenfranchised sub-group of humans with distinctive green eyes and unique language. The Guado, which are less human in appearance, with elongated fingers and other arboreal features. Still less human are the lion-like Ronso and the frog-like Hypello. A subset of Spira's sentient races are the "unsent", the strong-willed spirits of the dead that remain in corporeal form. In Spira, the dead who are not sent to the Farplane by a summoner come to envy the living and transform into "fiends", the monsters that are encountered throughout the game; however, unsent with strong attachments to the world of the living may retain their human form. Other fauna in Spira, aside from those drawn from real animals, such as cats, dogs, birds, and butterflies, include the gigantic, amphibious shoopufs (which are similar to elephants); and the emu-like chocobo, which appears in most "Final Fantasy" games. Spira is very different from the mainly European-style worlds found in previous "Final Fantasy" games, being much more closely modeled on Southeast Asia, most notably with respect to vegetation, topography, architecture, and names.

There are seven main playable characters in "Final Fantasy X", starting with Tidus, a cheerful young teenager and a star blitzball player from Zanarkand, who seeks a way home after an encounter with Sin transported him to Spira. To do so, he joins Yuna, a summoner on a journey to obtain the Final Aeon and defeat the enormous whale-like "Sin". Journeying with them are: Kimahri Ronso, a young warrior of the Ronso tribe who watched over Yuna during her childhood; Wakka, a blitzball player whose younger brother was killed by Sin; and Lulu, a stoic black mage close to Yuna and Wakka. During the journey, they are joined by Auron, a former warrior monk, who worked with both Tidus' and Yuna's fathers to defeat Sin 10 years prior; and Rikku, Yuna's cousin, a perky Al Bhed girl and the first friendly person Tidus meets upon arriving in Spira.

Tidus waits with his allies outside the ruins of an ancient city. He narrates the events that led to the present, spanning most of the game's storyline. It begins in his home city, the high-tech metropolis of Zanarkand, where he is a renowned blitzball player and son of the famous blitzball star, Jecht. During a blitzball tournament, the city is attacked by an immense creature that Auron, a man not originally from Zanarkand, calls "Sin". Sin destroys Zanarkand and takes Tidus and Auron to the world of Spira.

Upon arriving in Spira, Tidus is rescued by Al Bhed salvagers, who speak a language that is foreign to Tidus. One of them, Rikku, speaks the same language as Tidus and reveals that Sin destroyed Zanarkand 1,000 years ago. After Sin attacks again, Tidus is separated from the divers and drifts to the tropical island of Besaid, where he meets Wakka, captain of the local blitzball team, and shows off his blitzball skills. Wakka introduces Tidus to Yuna, a young summoner about to go on a pilgrimage to obtain the Final Aeon and defeat Sin with her guardians Lulu, a mage of black magic, and Kimahri, a member of the Ronso tribe. Meanwhile, Tidus joins to help Wakka in the upcoming blitzball tournament to find a way back home. The party travels across Spira to gather aeons, defending against attacks by Sin and its "offspring" called Sinspawn. After the tournament, they are joined by Auron, who convinces Tidus to become Yuna's guardian. He reveals to Tidus that Yuna's father, Lord Braska; Tidus's father, Jecht; and himself made the same pilgrimage to defeat Sin ten years ago. Tidus thought his father had died at sea ten years earlier. Following another attack from Sin, they are joined by Rikku, later revealed to be Yuna's cousin.

When the party arrives in the city of Guadosalam, the leader of the Guado, Seymour Guado, proposes to Yuna, saying that it will ease Spira's sorrow. At Macalania Temple, the group discovers a message from the spirit of Seymour's father, Lord Jyscal; he declares that he was killed by his own son, who now aims to destroy Spira. The group reunites with Yuna and kills Seymour in battle; soon afterward, Sin attacks, separating Yuna and sending the others to Bikanel Island. While searching for Yuna at the island's Al Bhed settlement, Tidus has an emotional breakdown when he learns that summoners die after summoning the Final Aeon, leading to his desire to find a way to defeat Sin while keeping Yuna alive. The group finds Yuna in Bevelle, where she is being forced to marry the unsent Seymour. They crash the wedding and escape with Yuna. The group heads toward the ruins of Zanarkand, seen in the introduction of the game.

Shortly before arriving, Tidus learns that he, Jecht, and the Zanarkand they hail from are summoned entities akin to aeons based on the original Zanarkand and its people. Long ago, the original Zanarkand battled Bevelle in a machina war, in which the former was defeated. Zanarkand's survivors became "fayth" so that they could use their memories of Zanarkand to create a new city in their image, removed from the reality of Spira. One thousand years after its creation, the fayth have become exhausted sustaining the "Dream Zanarkand", but are unable to stop due to Sin's influence.

Once they reach Zanarkand, Yunalesca—the first summoner to defeat Sin and unsent ever since—tells the group that the Final Aeon is created from the fayth of one close to the summoner. After defeating Sin, the Final Aeon kills the summoner and transforms into a new Sin, which has caused its cycle of rebirth to continue. Yuna decides against using the Final Aeon, due to the futile sacrifices it carries and the fact that Sin would still be reborn. Disappointed by their resolution, Yunalesca tries to kill Tidus' group, but she is defeated and vanishes, ending hope of ever attaining the Final Aeon. After the fight, the group learns that Yu Yevon, a summoner who lost his humanity and mind, is behind Sin's cycle of rebirth. This leads the group to infiltrate Sin's body to battle Seymour, and Jecht's imprisoned spirit. With Sin's host defeated, Tidus' group vanquishes Yu Yevon. Sin's cycle of rebirth ends, and the spirits of Spira's fayth are freed from their imprisonment. Auron, who had been revealed to be unsent, is sent to the Farplane. Dream Zanarkand and Tidus disappear, now that the freed fayth stopped the summoning. Afterward, in a speech to the citizens of Spira, Yuna resolves to help rebuild their world now that it is free of Sin. In a post-credits scene, Tidus awakens under water. He then swims towards the ocean surface, and the screen fades to white.

"Final Fantasy X"s development began in 1999, costing approximately ¥4 billion (approximately US$ million in dollars) with a crew of over 100 people, most of whom worked on previous games in the series. Executive producer Hironobu Sakaguchi has stated that although he had concerns about the transition from 2D to 3D backgrounds, the voice acting, and the transition to real-time story-telling, the success of the "Final Fantasy" series can be attributed to constantly challenging the development team to try new things. Producer Yoshinori Kitase was also the chief director of "Final Fantasy X", while the direction of events, maps and battles was split up between Motomu Toriyama, Takayoshi Nakazato and Toshiro Tsuchida, respectively. The development of the script for the game took three to four months, with the same amount of time dedicated to the voice recording afterwards. Kazushige Nojima collaborated with Daisuke Watanabe, Toriyama and Kitase on writing the scenario for "Final Fantasy X". Nojima was particularly concerned with establishing a connection in the relationship between player and main character. Thus, he penned the story such that the player's progress through the world and growing knowledge about it is reflected in Tidus' own understanding and narration.

Character designer Tetsuya Nomura has identified the South Pacific, Thailand and Japan as major influences on the cultural and geographic design of Spira, particularly concerning the geographic location of the southern Besaid and Kilika islands. He has also said that Spira deviates from the worlds of past "Final Fantasy" games in the level of detail incorporated, something he has expressed to have made a conscious effort to maintain during the design process. Kitase felt that if the setting went back to a medieval European fantasy, it would not seem to help the development team advance. While he was thinking of different world environments, Nojima suggested a fantasy world that incorporated Asian elements. Sub-character chief designer Fumi Nakashima's focus was to ensure that characters from different regions and cultures bore distinctive characteristics in their clothing styles, so that they could be quickly and easily identified as members of their respective sub-groups. For example, she has said that the masks and goggles of the Al Bhed give the group a "strange and eccentric" appearance, while the attire of the Ronso lend to them being able to easily engage in battle. Tidus' relationship with his father Jecht was based "stories throughout the ages, such as the ancient Greek legends." This would eventually reveal the key of Sin's weakness and eventual defeat.

"Final Fantasy X" features innovations in the rendering of characters' facial expressions, achieved through motion capture and skeletal animation technology. This technology allowed animators to create realistic lip movements, which were then programmed to match the speech of the game's voice actors. Nojima has revealed that the inclusion of voice acting enabled him to express emotion more powerfully than before, and he was therefore able to keep the storyline simple. He also said that the presence of voice actors led him to make various changes to the script, in order to match the voice actors' personalities with the characters they were portraying. The inclusion of voice, however, led to difficulties. With the game's cutscenes already programmed around the Japanese voice work, the English localization team faced the difficulty of establishing English-oriented dialogue and the obstacle of incorporating this modified wording with the rhythm and timing of the characters' lip movements. Localization specialist Alexander O. Smith noted that they had to keep the localized sound file within the duration of the original Japanese, as longer files would cause the game to crash. He described the process of fitting natural-sounding English speech into the game as "something akin to writing four or five movies' worth of dialogue entirely in haiku form [and] of course the actors had to act, and act well, within those restraints."

The game was initially going to feature online elements, offered through Square's PlayOnline service. The features, however, were dropped during production, and online gaming would not become part of the "Final Fantasy" series until "Final Fantasy XI". Map director Nakazato wanted to implement a world map concept with a more realistic approach than that of the traditional "Final Fantasy" game, in line with the realism of the game's 3D backgrounds, as opposed to pre-rendered backgrounds. As a player of the games in the "Final Fantasy" series, battle director Tsuchida wanted to recreate elements he found interesting or entertaining, which eventually led to the removal of the "Active Time Battle" system, and instead, incorporated the strategy-focused "Conditional Turn-Based Battle" system. Originally, "Final Fantasy X" was going to feature wandering enemies visible on the field map, seamless transitions into battles, and the option for players to move around the landscape during enemy encounters. Battle art director Shintaro Takai has explained that it was his intention that battles in "Final Fantasy X" come across as a natural part of the story and not an independent element. However, due to hardware limitations, this idea was not used. Instead, a compromise was made, whereby some transitions from the field map to the battle map were made relatively seamless with the implementation of a motion blur effect that would happen at the end of an event scene. The desire for seamless transitions also led to the implementation of the new summoning system seen in the game. Kitase has explained that the purpose behind the Sphere Grid is to give players an interactive means of increasing their characters' attributes, such that they will be able to observe the development of those attributes firsthand. The developers experienced difficulty with the scene of Tidus and Yuna kissing, as they were not used to animating romance scenes. Visual Works director Kazuyuki Ikumori stated that this was due to the use of 3D models in the scene. Because of the negative response from female members of staff, the scene was remade multiple times.

"Final Fantasy X" marks the first time regular series composer Nobuo Uematsu has had any assistance in composing the score for a game in the main series. His fellow composers for "Final Fantasy X" were Masashi Hamauzu and Junya Nakano. They were chosen for the soundtrack based on their ability to create music that was different from Uematsu's style while still being able to work together. PlayOnline.com first revealed that the game's theme song was completed in November 2000. As Square still had not revealed who would sing the song, GameSpot personally asked Uematsu, who jokingly answered "It's going to be Rod Stewart."

The game features three songs with vocalized elements, including the J-pop ballad "Suteki da ne", which translates to "Isn't it Wonderful?". The lyrics were written by Kazushige Nojima, and the music was written by Uematsu. The song is performed by Japanese folk singer Rikki, whom the music team contacted while searching for a singer whose music reflected an Okinawan atmosphere. "Suteki da ne" is also sung in Japanese in the English version of "Final Fantasy X". Like "Eyes on Me" from "Final Fantasy VIII" and "Melodies of Life" from "Final Fantasy IX", an orchestrated version of "Suteki da ne" is used as part of the ending theme. The other songs with lyrics are the heavy metal opening theme, "Otherworld", sung in English by Bill Muir; and "Hymn of the Fayth", a recurring piece sung using Japanese syllabary.

The original soundtrack spanned 91 tracks on four discs. It was first released in Japan on August 1, 2001, by DigiCube, and was re-released on May 10, 2004, by Square Enix. In 2002, Tokyopop released a version of "Final Fantasy X Original Soundtrack" in North America entitled "Final Fantasy X Official Soundtrack", which contained 17 tracks from the original album on a single disc. Other related CDs include "feel/Go dream: Yuna & Tidus" which, released in Japan by DigiCube on October 11, 2001, featured tracks based on Tidus' and Yuna's characters. "Piano Collections Final Fantasy X", another collection of music from the game, and "Final Fantasy X Vocal Collection", a compilations of exclusive character dialogues and songs were both in Japan in 2002.

The Black Mages, a band led by Nobuo Uematsu that arranges music from "Final Fantasy" video games into a rock music style, have arranged three pieces from "Final Fantasy X". These are "Fight With Seymour" from their self-titled album, published in 2003, and "Otherworld" and "The Skies Above", both of which can be found on the album "", published in 2004. Uematsu continues to perform certain pieces in his "Dear Friends: Music from Final Fantasy" concert series. The music of "Final Fantasy X" has also appeared in various official concerts and live albums, such as "20020220 Music from Final Fantasy", a live recording of an orchestra performing music from the series including several pieces from the game. An odd note; the unreleased/promo CD-R (Instrumental) version of Madonna's "What It Feels Like For A Girl" done by Tracy Young was used in the blitzball sequences. Additionally, "Swing de Chocobo" was performed by the Royal Stockholm Philharmonic Orchestra for the "Distant Worlds – Music from Final Fantasy" concert tour, while "Zanarkand" was performed by the New Japan Philharmonic Orchestra in the "Tour de Japon: Music from Final Fantasy" concert series. Independent but officially licensed releases of "Final Fantasy X" music have been composed by such groups as Project Majestic Mix, which focuses on arranging video game music. Selections also appear on Japanese remix albums, called "dojin music", and on English remixing websites.

The Japanese version of "Final Fantasy X" included an additional disc entitled "The Other Side of Final Fantasy", which featured interviews, storyboards, and trailers for "Blue Wing Blitz", "Kingdom Hearts", and "", as well as the first footage of "Final Fantasy XI". An international version of the game was released in Japan as "Final Fantasy X International" in January 2002, and in PAL regions under its original title. It features content not available in the original NTSC releases, including battles with "Dark" versions of the game's aeons and an airship fight with the superboss "Penance". The Japanese release of "Final Fantasy X International" also includes "Eternal Calm", a 14-minute video clip bridging the story of "Final Fantasy X" with that of its sequel, "Final Fantasy X-2". The video clip was included in a bonus DVD for "Unlimited Saga Collector's Edition" under the name "Eternal Calm, Final Fantasy X-2: Prologue". It was first released in Europe on October 31, 2003, and featured English voice-overs.

The international and PAL versions include a bonus DVD called "Beyond Final Fantasy", a disc including interviews with the game's developers, and two of the game's English voice actors, James Arnold Taylor (Tidus) and Hedy Burress (Yuna). Also included are trailers for "Final Fantasy X" and "Kingdom Hearts", a concept and promotional art gallery for the game, and a music video of "Suteki da ne" performed by Rikki. In 2005, a compilation featuring "Final Fantasy X" and "Final Fantasy X-2" was released in Japan as "Final Fantasy X/X-2 Ultimate Box".

Square also produced various types of merchandise and several books, including "The Art of Final Fantasy X" and three "Ultimania" guides, a series of artbooks/strategy guides published by DigiCube in Japan. They feature original artwork from "Final Fantasy X", offer gameplay walkthroughs, expand upon many aspects of the game's storyline and feature several interviews with the game's designers. There are three books in the series: "Final Fantasy X Scenario Ultimania", "Final Fantasy X Battle Ultimania", and "Final Fantasy X Ultimania Ω". On December 18, 2012 the game was re-released as part of the "Final Fantasy 25th Anniversary Ultimate Box" release.

On September 13, 2011, Square Enix announced that "Final Fantasy X" would be re-released in high-definition for the PlayStation 3 and PlayStation Vita, in celebration of the game's 10-year anniversary. In January 2012, production of the game had started. Producer Yoshinori Kitase was once again involved in the production of the game, wishing to work on its quality. On February 18, 2013 the first footage of the PlayStation Vita version of "Final Fantasy X HD" was released, showing off HD models of Tidus, Yuna, Bahamut and Yojimbo. On March 19, it was confirmed that the PS3 version of the game would also include its sequel "X-2", and that it would be remastered in HD. The two HD remastered games for the PS3 were released under the title "Final Fantasy X/X-2 HD Remaster" on a single Blu-ray disc game, and was sold separately on game cartridges on Vita in Japan and sold together in North America, Europe & Australia as a set, with "FFX" being on a cartridge and "FFX-2" being included as a download voucher. Downloadable versions are available for both systems. Square Enix launched an official website for the two HD remastered titles in March 2013. The games contain all the content found in the "International" version, including "Eternal Calm" and "Last Mission". During the PlayStation China press conference that took place in Shanghai on December 11, 2014, Square Enix confirmed "Final Fantasy X/X-2 HD Remaster" would be released for the PlayStation 4 in Spring 2015. It was released in North America on May 12, Australia & Japan on May 14 and Europe on May 15, 2015. It included enhanced graphics in full HD (1080p), the option to switch to the original soundtrack and the ability to transfer save files from the PS3 & PS Vita versions. One year later on May 12, 2016, it was released for Microsoft Windows via Steam. It includes an auto-save feature, 5 game boosters, 3 parameter changes, the option to skip FMVs/cinematics, 4K resolution support, audio settings and graphic options.

Square expected the game to sell at least two million copies worldwide owing to the reduced PlayStation 2's fanbase, making it smaller than the last three released titles. However, within four days of its release in Japan, the game had sold over 1.4 million copies in pre-orders, which set a record for the fastest-selling console RPG. These figures exceeded the performances of "Final Fantasy VII" and "IX" in a comparable period, and "Final Fantasy X" became the first PlayStation 2 game to reach two million and four million sold copies. In October 2007, the game was listed as the 8th best-selling game for the PlayStation 2. "Final Fantasy X" sold over 2.26 million copies in Japan alone in 2001, and sold 6.6 million copies worldwide by January 2004. By July 2006, it had sold 2.3 million copies and earned $95 million in the United States (US$ million in ). "Next Generation" ranked it as the 11th highest-selling game launched for the PlayStation 2, Xbox or GameCube between January 2000 and July 2006 in that country. As of March 2013, the game had shipped over 8.5 million copies worldwide on PS2. As of 2017, the PS2 version of the game has sold over 8 million copies worldwide.

The "Ultimate Hits" bargain reissue of the game in September 2005 sold over 131,000 copies in Japan by the end of 2006. In October 2013, Square Enix announced "Final Fantasy X" and its sequel "Final Fantasy X-2" have together sold over 14 million copies worldwide on PlayStation 2.

"Final Fantasy X" received critical acclaim by the media. The Japanese video game magazine "Famitsu" and "Famitsu PS2" awarded the game a near-perfect 39/40 score. Another Japanese gaming magazine, "The Play Station", gave the game a score of 29/30. "Famitsu", "Famitsu PS2", and "The Play Station" expressed particularly favorable responses toward the game's storyline, graphics, and movies. The game maintains a 92 out of 100 on Metacritic. Producer Shinji Hashimoto stated that the overall reception to the game was "excellent", having received praise and awards from the media.

IGN's David Smith offered praise for the voice actors and the innovations in gameplay, particularly with the revised battle and summon systems, the option to change party members during battle, and the character development and inventory management systems. They also felt that the game's graphics had improved on its predecessors in every way possible, and that the game as a whole was "the best-looking game of the series [and] arguably the best-playing as well". Greg Kasavin of GameSpot praised the game's storyline, calling it surprisingly complex, its ending satisfying, and its avoidance of role-playing game clichés commendable with Tidus viewed as an appealing protagonist. He also lauded the music, feeling it was "diverse and well suited to the various scenes in the game". Similarly, "GamePro" described its character building system and battle system as "two of the best innovations in the series". The visuals of the game were commended by GameSpy's Raymond Padilla, who referred to them as "top-notch", as well as giving praise to the character models, backgrounds, cutscenes, and animations. The voice casting was praised by Game Revolution who noted most of them were "above average" and called the music "rich".

"Edge" rated the game considerably lower, criticizing many aspects of the game for being tedious and uninnovative and describing the dialogue as "nauseating", particularly panning Tidus. Andrew Reiner of "Game Informer" criticized the game's linearity and that players were no longer able to travel the world by chocobo or control the airship. Eurogamer's Tom Bramwell noted that the game's puzzle segments were "depressing" and "superfluous", and that although the Sphere Grid was "a nice touch", it took up too much of the game. The linearity of the game was positively commented on by "GamePro" who stated that a player would not be required to participate in side-quests or the mini-game to reach the game's conclusion, finding some of them unappealing. Game Revolution complained that cutscenes could not be skipped, some even being too long.

"Final Fantasy X" received the Best Game Award from the Japan Game Awards for 2001–2002. In GameSpot's "Best and Worst Awards" from 2001, it came seventh in the category "Top 10 Video Games of the Year". Readers of "Famitsu" magazine voted it the best game of all time in early 2006. "Final Fantasy X" came in fifth on IGN's "Top 25 PS2 Games of All Time" list in 2007 and sixth in "The Top 10 Best Looking PS2 Games of All Time". In a similar list by GameSpy, the game took the 21st place. 1UP.com listed its revelation during the ending as the third-biggest video game spoiler, while IGN ranked the ending as the fifth best pre-rendered cutscene. In a Reader's Choice made in 2006 by IGN, it ranked as the 60th-best video game. It was also named one of the 20 essential Japanese role-playing games by Gamasutra. It also placed 43rd in "Game Informer"s list of "The Top 200 Games of All Time". In 2004, "Final Fantasy X" was listed as one of the best games ever made by GameFAQs, while in November 2005 it was voted as the 12th "Best Game Ever". In a general overview of the series, both GamesRadar and IGN listed "Final Fantasy X" as the fourth best game. At the sixth annual Interactive Achievement Awards in 2003, it was nominated for "Outstanding Achievement in Animation" and "Console Role-Playing Game of the Year". At the end of 2007, it was named the ninth best-selling RPG by "Guinness World Records". Readers from GameFaqs also voted it as Game of the Year during 2001. In 2008, readers of "Dengeki" magazine voted it the second best game ever made. It was voted first place in "Famitsu"s and "Dengeki"s polls of most tear-inducing games of all time. Both Tidus and Yuna have been popular characters in games in general due to their personalities and their romantic relationship.

Due to its commercial and critical success, Square Enix released a direct sequel to "Final Fantasy X" in 2003, titled "Final Fantasy X-2". The sequel is set two years after the conclusion of "Final Fantasy X", establishing new conflicts and dilemmas and resolving loose ends left by the original game. Although the sequel did not sell as well as the original, 5.4 million units versus over 8 million units, it can still be considered a commercial success. As a result of the title's popularity, Yoshinori Kitase and Kazushige Nojima decided to establish a plot-related connection between "Final Fantasy X" and "Final Fantasy VII", another well-received" Final Fantasy" game. In 2013, after the release of the HD Remaster, Nojima stated that he would like to see a second sequel to "X", and if there were demand for it, it could happen. The minigame of blitzball has made it into other games, such as "Final Fantasy X-2", and was mentioned as a possibility for "".

The advancements in portraying realistic emotions achieved with "Final Fantasy X" through voice-overs and detailed facial expressions have since become a staple of the series, with "Final Fantasy X-2" and other subsequent titles (such as "", "Final Fantasy XII", "XIII" and its sequels, and "XV") also featuring this development. Traversing real-time 3D environments instead of an overworld map has also become a standard of the series. "Final Fantasy X" can be considered a pioneer in 3-D RPG maps.

According to Square Enix producer Shinji Hashimoto, cosplays of the characters have been popular. Takeo Kujiraoka, director from "Dissidia Final Fantasy NT", regarded "Final Fantasy X" as his favorite game from the franchise based on its emotional impact on the players as well as the multiple amount of playable content that surpasses 100 hours. Kujiraoka noted that the staff received multiple requests by fans to include Tidus' and Yuna's "Will" look as an alternative design but Nomura said it was not possible as the company would first need to develop "Final Fantasy X-3".



</doc>
<doc id="53424" url="https://en.wikipedia.org/wiki?curid=53424" title="Hector Berlioz">
Hector Berlioz

Louis-Hector Berlioz (; ; 11 December 1803 – 8 March 1869) was a French Romantic composer. His output includes orchestral works such as the "Symphonie fantastique" and "Harold in Italy", choral pieces including the Requiem and "L'enfance du Christ", his three operas "Benvenuto Cellini", "Les Troyens" and "Béatrice et Bénédict", and works of hybrid genres such as the "dramatic symphony" "Roméo et Juliette" and the "dramatic legend" "La damnation de Faust".

The elder son of a provincial doctor, Berlioz was expected to follow his father into medicine, and he attended a Parisian medical college before defying his family by taking up music as a profession. His independence of mind and refusal to follow traditional rules and formulas put him at odds with the conservative musical establishment of Paris. He briefly moderated his style sufficiently to win France's premier music prize, the Prix de Rome, in 1830 but he learned little from the academics of the Paris Conservatoire. Opinion was divided for many years between those who thought him an original genius and those who viewed his music as lacking in form and coherence.

At age 22 Berlioz fell in love with the Irish Shakespearean actress Harriet Smithson, and he pursued her obsessively until she finally accepted him seven years later. Their marriage was happy at first but eventually foundered. Harriet inspired his first major success, the "Symphonie fantastique", in which an idealised depiction of her occurs throughout.

Berlioz completed three operas, the first of which, "Benvenuto Cellini", was an outright failure. The second, the huge epic "Les Troyens" (The Trojans), was so large in scale that it was never staged in its entirety during his lifetime. His last opera, "Béatrice et Bénédict"based on Shakespeare's comedy "Much Ado About Nothing"was a success at its premiere but did not enter the regular operatic repertoire. Meeting only occasional success in France as a composer, Berlioz increasingly turned to conducting, in which he gained an international reputation. He was highly regarded in Germany, Britain and Russia both as a composer and as a conductor. To supplement his earnings he wrote musical journalism throughout much of his career; some of it has been preserved in book form, including his "Treatise on Instrumentation" (1844), which was influential in the 19th and 20th centuries. Berlioz died in Paris at the age of 65.

Berlioz was born on 11 December 1803, the eldest child of Louis Berlioz (1776–1848), a physician, and his wife, Marie-Antoinette Joséphine, "née" Marmion (1784–1838). His birthplace was the family home in the commune of La Côte-Saint-André in the département of Isère, in south-eastern France. His parents had five more children, three of whom died in infancy; their surviving daughters, Nanci and Adèle, remained close to Berlioz throughout their lives.

Berlioz's father, a respected local figure, was a progressively-minded doctor credited as the first European to practise and write about acupuncture. He was an agnostic with a liberal outlook; his wife was a strict Roman Catholic of less flexible views. After briefly attending a local school when he was about ten, Berlioz was educated at home by his father. He recalled in his "Mémoires" that he enjoyed geography, especially books about travel, to which his mind would sometimes wander when he was supposed to be studying Latin; the classics nonetheless made an impression on him, and he was moved to tears by Virgil's account of the tragedy of Dido and Aeneas. Later he studied philosophy, rhetoric, and – because his father planned a medical career for him – anatomy.

Music did not feature prominently in the young Berlioz's education. His father gave him basic instruction on the flageolet, and he later took flute and guitar lessons with local teachers. He never studied the piano, and throughout his life played haltingly at best. He later contended that this was an advantage because it "saved me from the tyranny of keyboard habits, so dangerous to thought, and from the lure of conventional harmonies".

At the age of twelve Berlioz fell in love for the first time. The object of his affections was an eighteen-year-old neighbour, Estelle Dubœuf. He was teased for what was seen as a boyish crush, but something of his early passion for Estelle endured all his life. He poured some of his unrequited feelings into his early attempts at composition. Trying to master harmony, he read Rameau's "Traité de l'harmonie", which proved incomprehensible to a novice, but Charles-Simon Catel's simpler treatise on the subject made it clearer to him. He wrote several chamber works as a youth, subsequently destroying the manuscripts, but one theme that remained in his mind reappeared later as the A-flat second subject of the overture to "Les francs-juges".

In March 1821 Berlioz passed the baccalauréat examination at the University of Grenoble – it is not certain whether at the first or second attempt – and in late September, aged seventeen, he moved to Paris. At his father's insistence he enrolled at the School of Medicine of the University of Paris. He had to fight hard to overcome his revulsion at dissecting bodies, but in deference to his father's wishes, he forced himself to continue his medical studies.
The horrors of the medical college were mitigated thanks to an ample allowance from his father, which enabled him to take full advantage of the cultural, and particularly musical, life of Paris. Music did not at that time enjoy the prestige of literature in French culture, but Paris nonetheless possessed two major opera houses and the country's most important music library. Berlioz took advantage of them all. Within days of arriving in Paris he went to the Opéra, and although the piece on offer was by a minor composer, the staging and the magnificent orchestral playing enchanted him. He went to other works at the Opéra and the Opéra-Comique; at the former, three weeks after his arrival, he saw Gluck's "Iphigénie en Tauride", which thrilled him. He was particularly inspired by Gluck's use of the orchestra to carry the drama along. A later performance of the same work at the Opéra convinced him that his vocation was to be a composer.

The dominance of Italian opera in Paris, against which Berlioz later campaigned, was still in the future, and at the opera houses he heard and absorbed the works of Étienne Méhul and François-Adrien Boieldieu, other operas written in the French style by foreign composers, particularly Gaspare Spontini, and above all five operas by Gluck. He began to visit the Paris Conservatoire library in between his medical studies, seeking out scores of Gluck's operas and making copies of parts of them. By the end of 1822 he felt that his attempts to learn composition needed to be augmented with formal tuition, and he approached Jean-François Le Sueur, director of the Royal Chapel and professor at the Conservatoire, who accepted him as a private pupil.

In August 1823 Berlioz made the first of many contributions to the musical press: a letter to the journal "Le Corsaire" defending French opera against the incursions of its Italian rival. He contended that all of Rossini's operas put together could not stand comparison with even a few bars of those of Gluck, Spontini or Le Sueur. By now he had composed several works including "Estelle et Némorin" and "Le Passage de la mer Rouge" (The Crossing of the Red Sea) – both since lost.

In 1824 Berlioz graduated from medical school, after which he abandoned medicine, to the strong disapproval of his parents. His father suggested law as an alternative profession and refused to countenance music as a career. He reduced and sometimes withheld his son's allowance, and Berlioz went through some years of financial hardship.

In 1824 Berlioz composed a "Messe solennelle". It was performed twice, after which he suppressed the score, which was thought lost until a copy was discovered in 1991. During 1825 and 1826 he wrote his first opera, "Les francs-juges", which was not performed and survives only in fragments, the best known of which is the overture. In later works he reused parts of the score, such as the "March of the Guards", which he incorporated four years later in the "Symphonie fantastique" as the "March to the Scaffold".
In August 1826 Berlioz was admitted as a student to the Conservatoire, studying composition under Le Sueur and counterpoint and fugue with Anton Reicha. In the same year he made the first of four attempts to win France's premier music prize, the Prix de Rome, and was eliminated in the first round. The following year, to earn some money, he joined the chorus at the Théâtre des Nouveautés. He competed again for the Prix de Rome, submitting the first of his Prix cantatas, "La mort d'Orphée", in July. Later that year he attended productions of Shakespeare's "Hamlet" and "Romeo and Juliet" at the Théâtre de l'Odéon given by Charles Kemble's touring company. Although at the time Berlioz spoke hardly any English, he was overwhelmed by the plays – the start of a lifelong passion for Shakespeare. He also conceived a passion for Kemble's leading lady, Harriet Smithson – his biographer Hugh Macdonald calls it "emotional derangement" – and obsessively pursued her, without success, for several years. She refused even to meet him.

The first concert of Berlioz's music took place in May 1828, when his friend Nathan Bloc conducted the premieres of the overtures "Les francs-juges" and "Waverley" and other works. The hall was far from full, and Berlioz lost money. Nevertheless, he was greatly encouraged by the vociferous approval of his performers, and the applause from musicians in the audience, including his Conservatoire professors, the directors of the Opéra and Opéra-Comique, and the composers Auber and Hérold.

Berlioz's fascination with Shakespeare's plays prompted him to start learning English during 1828, so that he could read them in the original. At around the same time he encountered two further creative inspirations: Beethoven and Goethe. He heard Beethoven's third, fifth and seventh symphonies performed at the Conservatoire, and read Goethe's "Faust" in Gérard de Nerval's translation. Beethoven became both an ideal and an obstacle for Berlioz – an inspiring predecessor but a daunting one. Goethe's work was the basis of "Huit scènes de Faust" (Berlioz's Opus 1), premiered the following year and reworked and expanded much later as "La damnation de Faust".

Berlioz was largely apolitical, and neither supported nor opposed the July Revolution of 1830, but when it broke out he found himself in the middle of it. He recorded events in his "Mémoires":

The cantata was "La mort de Sardanapale", with which he won the Prix de Rome. His entry the previous year, "Cléopâtre", had attracted disapproval from the judges because to highly conservative musicians it "betrayed dangerous tendencies", and for his 1830 offering he carefully modified his natural style to meet official approval. During the same year he wrote the "Symphonie fantastique" and became engaged to be married.
By now recoiling from his obsession with Smithson, Berlioz fell in love with a nineteen-year-old pianist, Marie ("Camille") Moke. His feelings were reciprocated, and the couple planned to be married. In December Berlioz organised a concert at which the "Symphonie fantastique" was premiered. Protracted applause followed the performance, and the press reviews expressed both the shock and the pleasure the work had given. Berlioz's biographer David Cairns calls the concert a landmark not only in the composer's career but in the evolution of the modern orchestra. Franz Liszt was among those attending the concert; this was the beginning of a long friendship. Liszt later transcribed the entire "Symphonie fantastique" for piano to enable more people to hear it.

Shortly after the concert Berlioz set off for Italy: under the terms of the Prix de Rome, winners studied for two years at the Villa Medici, the French Academy in Rome. Within three weeks of his arrival he went absent without leave: he had learnt that Marie had broken off their engagement and was to marry an older and richer suitor, Camille Pleyel, the heir to the Pleyel piano manufacturing company. Berlioz made an elaborate plan to kill them both (and her mother, known to him as "l'hippopotame"), and acquired poisons, pistols and a disguise for the purpose. By the time he reached Nice on his journey to Paris he thought better of the scheme, abandoned the idea of revenge, and successfully sought permission to return to the Villa Medici. He stayed for a few weeks in Nice and wrote his "King Lear" overture. On the way back to Rome he began work on a piece for narrator, solo voices, chorus and orchestra, "Le retour à la vie" (The Return to Life, later renamed "Lélio"), a sequel to the "Symphonie fantastique".
Berlioz took little pleasure in his time in Rome. His colleagues at the Villa Medici, under their benevolent principal Horace Vernet, made him welcome, and he enjoyed his meetings with Felix Mendelssohn, who was visiting the city, but he found Rome distasteful: "the most stupid and prosaic city I know; it is no place for anyone with head or heart." Nonetheless, Italy had an important influence on his development. He visited many parts of it during his residency in Rome. Macdonald comments that after his time there, Berlioz had "a new colour and glow in his music ... sensuous and vivacious" – derived not from Italian painting, in which he was uninterested, or Italian music, which he despised, but from "the scenery and the sun, and from his acute sense of locale". Macdonald identifies "Harold in Italy", "Benvenuto Cellini" and "Roméo et Juliette" as the most obvious expressions of his response to Italy, and adds that "Les Troyens" and "Béatrice et Bénédict" "reflect the warmth and stillness of the Mediterranean, as well as its vivacity and force". Berlioz himself wrote that "Harold in Italy" drew on "the poetic memories formed from my wanderings in Abruzzi".

Vernet agreed to Berlioz's request to be allowed to leave the Villa Medici before the end of his two-year term. Heeding Vernet's advice that it would be prudent to delay his return to Paris, where the Conservatoire authorities might be less indulgent about his premature ending of his studies, he made a leisurely journey back, detouring via La Côte-Saint-André to see his family. He left Rome in May 1832 and arrived in Paris in November.

On 9 December 1832 Berlioz presented a concert of his works at the Conservatoire. The programme included the overture of "Les francs-juges", the "Symphonie fantastique" – extensively revised since its premiere – and "Le retour à la vie", in which Bocage, a popular actor, declaimed the monologues. Through a third party, Berlioz had sent an invitation to Harriet Smithson, who accepted, and was dazzled by the celebrities in the audience. Among the musicians present were Liszt, Frédéric Chopin and Niccolò Paganini; writers included Alexandre Dumas, Théophile Gautier, Heinrich Heine, Victor Hugo and George Sand. The concert was such a success that the programme was repeated within the month, but the more immediate consequence was that Berlioz and Smithson finally met.

By 1832 Smithson's career was in decline. She presented a ruinously unsuccessful season, first at the Théâtre-Italien and then at lesser venues, and by March 1833 she was deep in debt. Biographers differ about whether and to what extent Smithson's receptiveness to Berlioz's wooing was motivated by financial considerations; but she accepted him, and in the face of strong opposition from both their families they were married at the British Embassy in Paris on 3 October 1833. The couple lived first in Paris, and later in Montmartre (then still a village). On 14 August 1834 their only child, Louis-Clément-Thomas, was born. The first few years of the marriage were happy, although it eventually foundered. Harriet continued to yearn for a career but, as her biographer Peter Raby comments, she never learned to speak French fluently, which seriously limited both her professional and her social life.
Paganini, known chiefly as a violinist, had acquired a Stradivarius viola, which he wanted to play in public if he could find the right music. Greatly impressed by the "Symphonie fantastique", he asked Berlioz to write him a suitable piece. Berlioz told him that he could not write a brilliantly virtuoso work, and began composing what he called a symphony with viola obbligato, "Harold in Italy". As he foresaw, Paganini found the solo part too reticent – "There's not enough for me to do here; I should be playing all the time" – and the violist at the premiere in November 1834 was Chrétien Urhan.

Until the end of 1835 Berlioz had a modest stipend as a laureate of the Prix de Rome. His earnings from composing were neither substantial nor regular, and he supplemented them by writing music criticism for the Parisian press. Macdonald comments that this was activity "at which he excelled but which he abhorred". He wrote for "L'Europe littéraire" (1833), "Le rénovateur" (1833–1835), and from 1834 for the "Gazette musicale" and the "Journal des débats". He was the first, but not the last, prominent French composer to double as a reviewer: among his successors were Fauré, Messager, Dukas and Debussy. Although he complained – both privately and sometimes in his articles – that his time would be better spent writing music than in writing music criticism, he was able to indulge himself in attacking his bêtes noires and extolling his enthusiasms. The former included musical pedants, coloratura writing and singing, viola players who were merely incompetent violinists, inane libretti, and baroque counterpoint. He extravagantly praised Beethoven's symphonies, and Gluck's and Weber's operas, and scrupulously refrained from promoting his own compositions. His journalism consisted mainly of music criticism, some of which he collected and published, such as "Evenings in the Orchestra" (1854), but also more technical articles, such as those that formed the basis of his "Treatise on Instrumentation" (1844). Despite his complaints, Berlioz continued writing music criticism for most of his life, long after he had any financial need to do so.

Berlioz secured a commission from the French government for his Requiem – the "Grande messe des morts" – first performed at Les Invalides in December 1837. A second government commission followed – the "Grande symphonie funèbre et triomphale" in 1840. Neither work brought him much money or artistic fame at the time, but the Requiem held a special place in his affections: "If I were threatened with the destruction of the whole of my works save one, I would crave mercy for the "Messe des morts"".
One of Berlioz's main aims in the 1830s was "battering down the doors of the Opéra". In Paris at this period, the musical success that mattered was in the opera house and not the concert hall. Robert Schumann commented, "To the French, music by itself means nothing". Berlioz worked on his opera "Benvenuto Cellini" from 1834 until 1837, continually distracted by his increasing activities as a critic and as a promoter of his own symphonic concerts. The Berlioz scholar D. Kern Holoman comments that Berlioz rightly regarded "Benvenuto Cellini" as a work of exceptional exuberance and verve, deserving a better reception than it received. Holoman adds that the piece was of "surpassing technical difficulty", and that the singers were not especially co-operative. A weak libretto and unsatisfactory staging exacerbated the poor reception. The opera had only four complete performances, three in September 1838 and one in January 1839. Berlioz said that the failure of the piece meant that the doors of the Opéra were closed to him for the rest of his career – which they were, except for a commission to arrange a Weber score in 1841.

Shortly after the failure of the opera, Berlioz had a great success as composer-conductor of a concert at which "Harold in Italy" was given again. This time Paganini was present in the audience; he came on to the platform at the end and knelt in homage to Berlioz and kissed his hand. A few days later Berlioz was astonished to receive a cheque from him for 20,000 francs. Paganini's gift enabled Berlioz to pay off Harriet's and his own debts, give up music criticism for the time being, and concentrate on composition. He wrote the "dramatic symphony" "Roméo et Juliette" for voices, chorus and orchestra. It was premiered in November 1839 and was so well received that Berlioz and his huge instrumental and vocal forces gave two further performances in rapid succession. Among the audiences was the young Wagner, who was overwhelmed by its revelation of the possibilities of musical poetry, and who later drew on it when composing "Tristan und Isolde".

At the close of the decade Berlioz achieved official recognition in the form of appointment as deputy librarian of the Conservatoire and as an officer of the Legion of Honour. The former was an undemanding post, but not highly paid, and Berlioz remained in need of a reliable income to allow him the leisure for composition.

The "Symphonie funèbre et triomphale", marking the tenth anniversary of the 1830 Revolution, was performed in the open air under the direction of the composer in July 1840. The following year the Opéra commissioned Berlioz to adapt Weber's "Der Freischütz" to meet the house's rigid requirements: he wrote recitatives to replace the spoken dialogue and orchestrated Weber's "Invitation to the Dance" to provide the obligatory ballet music. In the same year he completed settings of six poems by his friend Théophile Gautier, which formed the song cycle "Les nuits d'été" (with piano accompaniment, later orchestrated). He also worked on a projected opera, "La nonne sanglante" (The Bloody Nun), to a libretto by Eugène Scribe, but made little progress. In November 1841 he began publishing a series of sixteen articles in the "Revue et gazette musicale" giving his views about orchestration; they were the basis of his "Treatise on Instrumentation", published in 1843.

During the 1840s Berlioz spent much of his time making music outside France. He struggled to make money from his concerts in Paris, and learning of the large sums made by promoters from performances of his music in other countries, he resolved to try conducting abroad. He began in Brussels, giving two concerts in September 1842. An extensive German tour followed: in 1842 and 1843 he gave concerts in twelve German cities. His reception was enthusiastic. The German public was better disposed than the French to his innovative compositions, and his conducting was seen as highly impressive. During the tour he had enjoyable meetings with Mendelssohn and Schumann in Leipzig, Wagner in Dresden and Meyerbeer in Berlin.
By this time Berlioz's marriage was failing. Harriet resented his celebrity and her own eclipse, and as Raby puts it, "possessiveness turned to suspicion and jealousy as Berlioz became involved with the singer Marie Recio". Harriet's health deteriorated, and she took to drinking heavily. Her suspicion about Recio was well founded: the latter became Berlioz's mistress in 1841 and accompanied him on his German tour.

Berlioz returned to Paris in mid-1843. During the following year he wrote two of his most popular short works, the overtures "Le carnaval romain" (reusing music from "Benvenuto Cellini") and "Le corsaire" (originally called "La tour de Nice"). Towards the end of the year he and Harriet separated. Berlioz maintained two households: Harriet remained in Montmartre and he moved in with Recio at her flat in central Paris. His son Louis was sent to a boarding school in Rouen.

Foreign tours featured prominently in Berlioz's life during the 1840s and 1850s. Not only were they highly rewarding both artistically and financially, but he did not have to grapple with the administrative problems of promoting concerts in Paris. Macdonald comments:
Berlioz's major work from the decade was "La damnation de Faust". He presented it in Paris in December 1846, but it played to half-empty houses, despite excellent reviews, some from critics not usually well disposed to his music. The highly romantic subject was out of step with the times, and one sympathetic reviewer observed that there was an unbridgeable gap between the composer's conception of art and that of the Paris public. The failure of the piece left Berlioz heavily in debt; he restored his finances the following year with the first of two highly remunerative trips to Russia. His other foreign tours during the rest of the 1840s included Austria, Hungary, Bohemia and Germany. After those came the first of his five visits to England; it lasted for more than seven months (November 1847 to July 1848). His reception in London was enthusiastic, but the visit was not a financial success because of mismanagement by his impresario, the conductor Louis-Antoine Jullien.

Soon after Berlioz's return to Paris in mid-September 1848, Harriet suffered a series of strokes, which left her almost paralysed. She needed constant nursing, which he paid for. When in Paris he visited her continually, sometimes twice a day.

After the failure of "La damnation de Faust", Berlioz spent less time on composition during the next eight years. He wrote a Te Deum, completed in 1849 but not published until 1855, and some short pieces. His most substantial work between "The Damnation" and his epic "Les Troyens" (1856–1858) was a "sacred trilogy", "L'enfance du Christ" (Christ's Childhood), which he began in 1850. In 1851 he was at the Great Exhibition in London as a member of an international committee judging musical instruments. He returned to London in 1852 and 1853, conducting his own works and others'. He enjoyed consistent success there, with the exception of a revival of "Benvenuto Cellini" at Covent Garden which was withdrawn after one performance. The opera was presented in Leipzig in 1852 in a revised version prepared by Liszt with Berlioz's approval and was moderately successful. In the early years of the decade Berlioz made numerous appearances in Germany as a conductor.

In 1854 Harriet died. Both Berlioz and their son Louis had been with her shortly before her death. During the year Berlioz completed the composition of "L'enfance du Christ", worked on his book of memoirs, and married Marie Recio, which, he explained to his son, he felt it his duty to do after living with her for so many years. At the end of the year the first performance of "L'enfance du Christ" was warmly received, to his surprise. He spent much of the next year in conducting and writing prose.

During Berlioz's German tour in 1856, Liszt and his companion, Carolyne zu Sayn-Wittgenstein, encouraged Berlioz's tentative conception of an opera based on the "Aeneid". Having first completed the orchestration of his 1841 song cycle "Les nuits d'été", he began work on "Les Troyens" – The Trojans – writing his own libretto based on Virgil's epic. He worked on it, in between his conducting commitments, for two years. In 1858 he was elected to the Institut de France, an honour he had long sought, though he played down the importance he attached to it. In the same year he completed "Les Troyens". He then spent five years trying to have it staged.

In June 1862 Berlioz's wife died suddenly, aged 48. She was survived by her mother, to whom Berlioz was devoted, and who looked after him for the rest of his life.

"Les Troyens" – a five-act, five-hour opera – was on too large a scale to be acceptable to the management of the Opéra, and Berlioz's efforts to have it staged there failed. The only way he could find of seeing the work produced was to divide it into two parts: "The Fall of Troy" and "The Trojans at Carthage". The latter, consisting of the final three acts of the original, was presented at the Théâtre‐Lyrique, Paris, in November 1863, but even that truncated version was further truncated: during the run of 22 performances, number after number was cut. The experience demoralised Berlioz, who wrote no more music after this.

Berlioz did not seek a revival of "Les Troyens" and none took place for nearly 30 years. He sold the publishing rights for a large sum, and his last years were financially comfortable; he was able to give up his work as a critic, but he lapsed into depression. As well as losing both his wives, he had lost both his sisters, and he became morbidly aware of death as many of his friends and other contemporaries died. He and his son had grown deeply attached to each other, but Louis was a captain in the merchant navy, and was more often than not away from home. Berlioz's physical health was not good, and he was often in pain from an intestinal complaint, possibly Crohn's disease.

After the death of his second wife, Berlioz had two romantic interludes. During 1862 he met – probably in the Montmartre Cemetery – a young woman less than half his age, whose first name was Amélie and whose second, possibly married, name is not recorded. Almost nothing is known of their relationship, which lasted for less than a year. After they ceased to meet, Amélie died, aged only 26. Berlioz was unaware of it until he came across her grave six months later. Cairns hypothesises that the shock of her death prompted him to seek out his first love, Estelle, now a widow aged 67. He called on her in September 1864; she received him kindly, and he visited her in three successive summers; he wrote to her nearly every month for the rest of his life.

In 1867 Berlioz received the news that his son had died in Havana of yellow fever. Macdonald suggests that Berlioz may have sought distraction from his grief by going ahead with a planned series of concerts in St Petersburg and Moscow, but far from rejuvenating him, the trip sapped his remaining strength. The concerts were successful, and Berlioz received a warm response from the new generation of Russian composers and the general public, but he returned to Paris visibly unwell. He went to Nice to recuperate in the Mediterranean climate, but fell on rocks by the shore, possibly because of a stroke, and had to return to Paris, where he convalesced for several months. In August 1868, he felt able to travel briefly to Grenoble to judge a choral festival. After arriving back in Paris he gradually grew weaker and died at his house in the Rue de Calais on 8 March 1869, at the age of 65. He was buried in Montmartre Cemetery with his two wives, who were exhumed and re-buried next to him.

In his 1983 book "The Musical Language of Berlioz", Julian Rushton asks "where Berlioz comes in the history of musical forms and what is his progeny". Rushton's answers to these questions are "nowhere" and "none". He cites well-known studies of musical history in which Berlioz is mentioned only in passing or not at all, and suggests that this is partly because Berlioz had no models among his predecessors and was a model to none of his successors. "In his works, as in his life, Berlioz was a lone wolf". Forty years earlier, Sir Thomas Beecham, a lifelong proponent of Berlioz's music, commented similarly, writing that although, for example, Mozart was a greater composer, his music drew on the works of his predecessors, whereas Berlioz's works were all wholly original: "the "Symphonie fantastique" or "La damnation de Faust" broke upon the world like some unaccountable effort of spontaneous generation which had dispensed with the machinery of normal parentage".

Rushton suggests that "Berlioz's way is neither architectural nor developmental, but illustrative". He judges this to be part of a continuing French musical aesthetic, favouring a "decorative" – rather than the German "architectural" – approach to composition. Abstraction and discursiveness are alien to this tradition, and in operas, and to a large extent in orchestral music, there is little continuous development; instead self-contained numbers or sections are preferred.

Berlioz's compositional techniques have been strongly criticised and equally strongly defended. It is common ground for critics and defenders that his approach to harmony and musical structure conforms to no established rules; his detractors ascribe this to ignorance, and his proponents to independent-minded adventurousness. His approach to rhythm caused perplexity to conservatively-inclined contemporaries; he hated the "phrase carrée" – the unvaried four- or eight-bar phrase – and introduced new varieties of rhythm to his music. He explained his practice in an 1837 article: accenting weak beats at the expense of the strong, alternating triple and duple groups of notes and using unexpected rhythmic themes independent of the main melody. Macdonald writes that Berlioz was a natural melodist, but that his rhythmic sense led him away from regular phrase lengths; he "spoke naturally in a kind of flexible musical prose, with surprise and contour important elements".

Berlioz's approach to harmony and counterpoint was idiosyncratic, and has provoked adverse criticism. Pierre Boulez commented, "There are awkward harmonies in Berlioz that make one scream". In Rushton's analysis, most of Berlioz's melodies have "clear tonal and harmonic implications" but the composer sometimes chose not to harmonise accordingly. Rushton observes that Berlioz's preference for irregular rhythm subverts conventional harmony: "Classic and romantic melody usually implies harmonic motion of some consistency and smoothness; Berlioz's aspiration to musical prose tends to resist such consistency." The pianist and musical analyst Charles Rosen has written that Berlioz often sets the climax of his melodies in relief with the most emphatic chord a triad in root position, and often a tonic chord where the melody leads the listener to expect a dominant. He gives as an example the second phrase of the main theme – the "idée fixe" – of the "Symphonie fantastique", "famous for its shock to classical sensibilities", in which the melody implies a dominant at its climax resolved by a tonic, but in which Berlioz anticipates the resolution by putting a tonic under the climactic note.

Even among those unsympathetic to his music, few deny that Berlioz was a master of orchestration. Richard Strauss wrote that Berlioz invented the modern orchestra. Some of those who recognise Berlioz's mastery of orchestration nonetheless dislike a few of his more extreme effects. The pedal point for trombones in the "Hostias" section of the Requiem is often cited; some musicians such as Gordon Jacob have found the effect unpleasant. Macdonald has questioned Berlioz's fondness for divided cellos and basses in dense, low chords, but he emphasises that such contentious points are rare compared with "the felicities and masterstrokes" abounding in the scores. Berlioz took instruments hitherto used for special purposes and introduced them into his regular orchestra: Macdonald mentions the harp, the cor anglais, the bass clarinet and the valve trumpet. Among the characteristic touches in Berlioz's orchestration singled out by Macdonald are the wind "chattering on repeated notes" for brilliance, or being used to add "sombre colour" to Romeo's arrival at the Capulets' vault, and the "Choeur d'ombres" in "Lélio". Of Berlioz's brass he writes:
Berlioz wrote four large-scale works he called symphonies, but his conception of the genre differed greatly from the classical pattern of the German tradition. With rare exceptions, such as Beethoven's Ninth, a symphony was taken to be a large‐scale wholly orchestral work, usually in four movements, using sonata form in the first movement and sometimes in others. Some pictorial touches were included in symphonies by Beethoven, Mendelssohn and others, but the symphony was not customarily used to recount a narrative.
All four of Berlioz's symphonies differ from the contemporary norm. The first, the "Symphonie fantastique" (1830), is purely orchestral, and the opening movement is broadly in sonata form, but the work tells a story, graphically and specifically. The recurring "idée fixe" theme is the composer's idealised (and in the last movement caricatured) portrait of Harriet Smithson. Schumann wrote of the work that despite its apparent formlessness, "there is an inherent symmetrical order corresponding to the great dimensions of the work, and this besides the inner connexions of thought", and in the 20th century Constant Lambert wrote, "Formally speaking it is among the finest of 19th-century symphonies". The work has always been among Berlioz's most popular.

"Harold in Italy", despite its subtitle "Symphony in four parts with viola principal", is described by the musicologist Mark Evan Bonds as a work traditionally seen as lacking any direct historical antecedent, "a hybrid of symphony and concerto that owes little or nothing to the earlier, lighter genre of the symphonie concertante". In the 20th century critical opinion varied about the work, even among those well-disposed to Berlioz. Felix Weingartner, an early 20th century champion of the composer, wrote in 1904 that it did not reach the level of the "Symphonie fantastique"; fifty years later Edward Sackville-West and Desmond Shawe-Taylor found it "romantic and picturesque ... Berlioz at his best". In the 21st century Bonds ranks it among the greatest works of its kind in the 19th century.

The "Dramatic Symphony" with chorus, "Roméo et Juliette" (1839), is still further from the traditional symphonic model. The episodes of Shakespeare's drama are represented in orchestral music, interspersed with expository and narrative sections for voices. Among Berlioz's admirers the work divides opinion. Weingartner called it "a style-less mixture of different forms; not quite oratorio, not quite opera, not quite symphony – fragments of all three, and nothing perfect". Countering accusations of lack of unity in this and other Berlioz works Emmanuel Chabrier replied in a single emphatic word. Cairns regards the work as symphonic, albeit "a bold extension" of the genre, but he notes that other Berliozians including Wilfrid Mellers view it as "a curious, not entirely convincing compromise between symphonic and operatic techniques". Rushton comments that "pronounced unity" is not among the virtues of the work, but he argues that to close one's mind on that account is to miss all that the music has to give.

The last of the four symphonies is the "Symphonie funebre et triomphale", for giant brass and woodwind band (1840), with string parts added later, together with optional chorus. The structure is more conventional than the instrumentation: the first movement is in sonata form, but there are only two other movements, and Berlioz did not adhere to the traditional relationship between the various keys of the piece. Wagner called the symphony "popular in the most ideal sense ... every urchin in a blue blouse would thoroughly understand it".

None of Berlioz's three completed operas were written to commission, and theatre managers were not enthusiastic about staging them. Cairns writes that unlike Meyerbeer, who was rich, influential, and deferred to by opera managements, Berlioz was "an opera composer on sufferance, one who composed on borrowed time paid for with money that was not his but lent by a wealthy friend".

The three operas contrast strongly with one another. The first, "Benvenuto Cellini" (1838), inspired by the memoirs of the Florentine sculptor, is an opera semiseria, seldom staged until the 21st century, when there have been signs of a revival in its fortunes, with its first production at the Metropolitan Opera (2003) and a co-production by the English National Opera and the Opéra national de Paris (2014), but it remains the least often produced of the three operas. In 2008, the music critic Michael Quinn called it "an opera overflowing in every way, with musical gold bursting from each curve and crevice ... a score of continually stupendous brilliance and invention" but agreed with the general view of the libretto: "incoherent ... episodic, too epic to be comedy, too ironic for tragedy". Berlioz welcomed Liszt's help in revising the work, streamlining the confusing plot; for his other two operas he wrote his own libretti.

The epic "Les Troyens" (1858) is described by the musical scholar James Haar as "incontestably Berlioz's masterpiece", a view shared by many other writers. Berlioz based the text on Virgil's "Aeneid", depicting the fall of Troy and subsequent travels of the hero. Holoman describes the poetry of the libretto as old fashioned for its day, but effective and at times beautiful. The opera consists of a series of self-contained numbers, but they form a continuous narrative, with the orchestra playing a vital part in expounding and commenting on the action. Although the work plays for five hours (including intervals) it is no longer the normal practice to present it across two evenings. "Les Troyens", in Holoman's view, embodies the composer's artistic creed: the union of music and poetry holds "incomparably greater power than either art alone".

The last of Berlioz's operas is the Shakespearean comedy "Béatrice et Bénédict" (1862), written, the composer said, as a relaxation after his efforts with "Les Troyens". He described it as "a caprice written with the point of a needle". His libretto, based on "Much Ado About Nothing", omits Shakespeare's darker sub-plots and replaces the clowns Dogberry and Verges with an invention of his own, the tiresome and pompous music master Somarone. The action focuses on the sparring between the two leading characters, but the score contains some gentler music, such as the nocturne-duet "nuit paisible et sereine", the beauty of which, Cairns suggests, matches or surpasses the love music in "Roméo" or "Les Troyens". Cairns writes that "Béatrice et Bénédict" "has wit and grace and lightness of touch. It accepts life as it is. The opera is a divertissement, not a grand statement".

"La damnation de Faust", although not written for the theatre, is sometimes staged as an opera.

Berlioz gained a reputation, only partly justified, for liking gigantic orchestral and choral forces. In France there was a tradition of open-air performance, dating from the Revolution, calling for larger ensembles than were needed in the concert hall. Among the generation of French composers ahead of him, Cherubini, Méhul, Gossec and Berlioz's teacher Le Sueur all wrote for huge forces on occasion, and in the Requiem and to a lesser degree the Te Deum Berlioz follows them, in his own manner. The Requiem calls for sixteen timpani, quadruple woodwind and twelve horns, but the moments when the full orchestral sound is unleashed are few – the Dies irae is one such – and most of the Requiem is notable for its restraint. The orchestra does not play at all in the "Quaerens me" section, and what Cairns calls "the apocalyptic armoury" is reserved for special moments of colour and emphasis: "its purpose is not merely spectacular but architectural, to clarify the musical structure and open up multiple perspectives."

What Macdonald calls Berlioz's monumental manner is more prominent in the Te Deum, composed in 1849 and first heard in 1855, when it was given in connection with the Exposition Universelle. By that time the composer had added to its two choruses a part for massed children's voices, inspired by hearing a choir of 6,500 children singing in St Paul's Cathedral during his London trip in 1851. A cantata for double chorus and large orchestra in honour of Napoleon III, "L'impériale", described by Berlioz as "en style énorme", was played several times at the 1855 exhibition, but has subsequently remained a rarity.

"La damnation de Faust", though conceived as a work for the concert hall, did not achieve success in France until it was staged as an opera long after the composer's death. Within a year of Raoul Gunsbourg's production of the piece at Monte Carlo in 1893 the work was presented as an opera in Italy, Germany, Britain, Russia and the US. The many elements of the work vary from the robust "Hungarian March" near the beginning to the delicate "Dance of the Sylphs", the frenetic "Ride to the Abyss", Méphistophélès' suave and seductive "Song of the Devil", and Brander's "Song of a Rat", a requiem for a dead rodent.

"L'enfance du Christ" (1850–1854) follows the pattern of "La damnation de Faust" in mixing dramatic action and philosophic reflection. Berlioz, after a brief youthful religious spell, was a lifelong agnostic, but he was not hostile to the Roman Catholic church, and Macdonald calls the "serenely contemplative" end of the work "the nearest Berlioz ever came to a devoutly Christian mode of expression".

Berlioz wrote songs throughout his career, but not prolifically. His best-known work in the genre is the song cycle "Les nuits d'été", a group of six songs, originally for voice and piano but now usually heard in its later orchestrated form. He suppressed some of his early songs, and his last publication, in 1865, was the "33 Mélodies", collecting into one volume all his songs that he chose to preserve. Some of them, such as "Hélène" and "Sara la baigneuse", exist in versions for four voices with accompaniment, and there are others for two or three voices. Berlioz later orchestrated some of the songs originally written with piano accompaniment, and some, such as "Zaïde" and "Le chasseur danois" were written with alternative piano or orchestral parts. "La captive", to words by Victor Hugo, exists in six different versions. In its final version (1849) it was described by the Berlioz scholar Tom S. Wotton as like "a miniature symphonic poem". The first version, written at the Villa Medici, had been in fairly regular rhythm, but for his revision Berlioz made the strophic outline less clear-cut, and added optional orchestral parts for the last stanza, which brings the song to a quiet close.

The songs remain on the whole among the least known of Berlioz's works, and John Warrack suggests that Schumann identified why this might be so: the shape of the melodies is, as usual with Berlioz, not straightforward, and to those used to the regular four-bar phrases of French (or German) song this is an obstacle to appreciation. Warrack also comments that the piano parts, though not lacking in harmonic interest, are discernibly written by a non-pianist. Despite that, Warrack considers up to a dozen songs from the "33 Mélodies" well worth exploring – "Among them are some masterpieces."

Berlioz's literary output was considerable and mostly consists of music criticism. Some was collected and published in book form. His "Treatise on Instrumentation" (1834) began as a series of articles and remained a standard work on orchestration throughout the 19th century; when Richard Strauss was commissioned to revise it in 1905 he added new material but did not change Berlioz's original text. The revised form remained widely used well into the 20th century; a new English translation was published in 1948.

Other selections from Berlioz's press columns were published in "Les soirées de l'orchestre" (Nights with the Orchestra, 1852), "Les grotesques de la musique" (1859) and "À travers chants" (Through Songs, 1862). His "Mémoires" were published posthumously in 1870. Macdonald comments that there are few facets of musical practice of the time untouched in Berlioz's "feuilletons". He professed to dislike writing his press pieces, and they undoubtedly took up time that he would have preferred to spend writing music. His excellence as a witty and perceptive critic may have worked to his disadvantage in another way: he became so well known to the French public in that capacity that his stature as a composer became correspondingly more difficult to establish.

The first biography of Berlioz, by Eugène de Mirecourt, was published during the composer's lifetime. Holoman lists six other French biographies of the composer published in the four decades after his death. Of those who wrote for and against Berlioz's music in the late 19th and early 20th centuries, among the most outspoken were musical amateurs such as the lawyer and diarist George Templeton Strong, who called the composer's music variously "flatulent", "rubbish", and "the work of a tipsy chimpanzee", and, in the pro-Berlioz camp, the poet and journalist Walter J. Turner, who wrote what Cairns calls "exaggerated eulogies". Like Strong, Turner was, in the words of the music critic Charles Reid, "unhampered by any excess of technical knowledge".

Serious studies of Berlioz in the 20th century began with Adolphe Boschot's "L'histoire d'un romantique" (three volumes, 1906–1913). His successors were Tom S. Wotton, author of a 1935 biography, and Julien Tiersot, who wrote numerous scholarly articles on Berlioz and began the collection and editing of the composer's letters, a process eventually completed in 2016, eighty years after Tiersot's death. In the early 1950s the best-known Berlioz scholar was Jacques Barzun, a protégé of Wotton, and, like him, strongly hostile to many of Boschot's conclusions, which they saw as unfairly critical of the composer. Barzun's study was published in 1950. He was accused at the time by the musicologist Winton Dean of being excessively partisan, and refusing to admit failings and unevenness in Berlioz's music; more recently he has been credited by the critic Nicholas Temperley with playing a major part in improving the climate of musical opinion towards Berlioz.

Since Barzun, the leading Berlioz scholars have included David Cairns, D. Kern Holoman, Hugh Macdonald and Julian Rushton. Cairns translated and edited Berlioz's "Mémoires" in 1969, and published a two-volume, 1500-page study of the composer (1989 and 1999), described in "Grove's Dictionary of Music and Musicians" as "one of the masterpieces of modern biography". Holoman was responsible for the publication in 1987 of the first thematic catalogue of Berlioz's works; two years later he published a single-volume biography of the composer. Macdonald was appointed in 1967 as the inaugural general editor of the New Berlioz Edition published by Bärenreiter; 26 volumes were issued between 1967 and 2006 under his editorship. He is also one of the editors of Berlioz's "Correspondance générale", and author of a 1978 study of Berlioz's orchestral music, and of the "Grove" article on the composer. Rushton has published two volumes of analyses of Berlioz's music (1983 and 2001). The critic Rosemary Wilson said of his work, "He has done more than any other writer to explain the uniqueness of Berlioz's musical style without losing a sense of wonder in its originality of musical expression."

Because few of Berlioz's works were often performed in the late-19th and early 20th centuries, widely accepted views of his music were based on hearsay rather than on the music itself. Orthodox opinion emphasised supposed technical defects in the music and ascribed to the composer characteristics that he did not possess. Debussy called him "a monster ... not a musician at all. He creates the illusion of music by means borrowed from literature and painting". In 1904, in the second edition of "Grove", Henry Hadow made this judgment:

By the 1950s the critical climate was changing, although in 1954 the fifth edition of "Grove" carried this verdict from Léon Vallas:

Cairns dismisses the article as "an astonishing anthology of all the nonsense that has ever been talked about [Berlioz]", but adds that by the 1960s it seemed a quaint survival from a vanished age. By 1963 Cairns, viewing Berlioz's greatness as firmly established, felt able to advise anyone writing on the subject, "Do not keep harping on the 'strangeness' of Berlioz's music; you will no longer carry the reader with you. And do not use phrases like 'genius without talent', 'a certain strain of amateurishness', 'curiously uneven': they have had their day."

One important reason for the steep rise in Berlioz's reputation and popularity is the introduction of the LP record after the Second World War. In 1950 Barzun made the point that although Berlioz was praised by his artistic peers, including Schumann, Wagner, Franck and Mussorgsky, the public had heard little of his music until recordings became widely available. Barzun maintained that many myths had grown up about the supposed quirkiness or ineptitude of the music – myths that were dispelled once the works were finally made available for all to hear. Neville Cardus made a similar point in 1955. As more and more Berlioz works became widely available on record, professional musicians and critics, and the musical public, were for the first time able to judge for themselves.

A milestone in the reappraisal of Berlioz's reputation came in 1957, when for the first time a professional opera company staged the original version of "The Trojans" in a single evening. It was at the Royal Opera House, Covent Garden; the work was sung in English with some minor cuts, but its importance was internationally recognised, and led to the world premiere staging of the work uncut and in French, at Covent Garden in 1969, marking the centenary of the composer's death.

In recent decades Berlioz has been widely regarded as a great composer, prone to lapses like any other. In 1999 the composer and critic Bayan Northcott wrote that the work of Cairns, Rushton, Sir Colin Davis and others retained "the embattled conviction of a cause". Nevertheless, Northcott was writing about Davis's "Berlioz Odyssey" of seventeen concerts of Berlioz's music, featuring all the major works, a prospect unimaginable in earlier decades of the century. Northcott concluded, "Berlioz still seems so immediate, so controversial, so ever-new".

All of Berlioz's major works and most of his minor ones have been commercially recorded. This is a comparatively recent development. In the mid-1950s the international record catalogues listed complete recordings of seven major works: the "Symphonie fantastique", "Symphonie funèbre et triomphale", "Harold in Italy", "Les nuits d'été," "Roméo et Juliette", the Requiem and the Te Deum, and various overtures. Excerpts from "Les Troyens" were available but there were no complete recordings of the operas.

Recordings conducted by Colin Davis are prominent in the Berlioz discography, some studio-made and others recorded live. The first was "L'enfance du Christ" in 1960 and the last the Requiem in 2012. In between there were five recordings of "Les nuits d'été", four each of "Béatrice et Bénédict", the "Symphonie fantastique" and "Roméo et Juliette", and three of "Harold in Italy", "Les Troyens", and "La damnation de Faust".

In addition to Davis's versions, "Les Troyens" has received studio recordings under Charles Dutoit and John Nelson; Nelson and Daniel Barenboim have recorded versions of "Béatrice et Bénédict", and Nelson and Roger Norrington have conducted "Benvenuto Cellini" for CD. Singers who have recorded "Les nuits d'été" include Victoria de los Ángeles, Leontyne Price, Janet Baker, Régine Crespin, Jessye Norman and Kiri Te Kanawa, and more recently, Karen Cargill and Susan Graham.

By far the most recorded of Berlioz's works is the "Symphonie fantastique". The discography of the British Hector Berlioz website lists 96 recordings, from the pioneering version by Gabriel Pierné and the Concerts Colonne in 1928 to those conducted by Beecham, Pierre Monteux, Charles Munch, Herbert von Karajan and Otto Klemperer to more recent versions including those of Boulez, Marc Minkowski, Yannick Nézet-Séguin and François-Xavier Roth.


Works

Writings


</doc>
<doc id="53878" url="https://en.wikipedia.org/wiki?curid=53878" title="Betelgeuse">
Betelgeuse

Betelgeuse, also designated α Orionis (Latinised to Alpha Orionis, abbreviated Alpha Ori, α Ori), is on average the ninth-brightest star in the night sky and second-brightest in the constellation of Orion. It is a distinctly reddish, semiregular variable star whose apparent magnitude varies between 0.0 and 1.3, the widest range of any first-magnitude star. Betelgeuse is one of three stars that make up the Winter Triangle asterism, and it marks the center of the Winter Hexagon. If the human eye could view all wavelengths of radiation, Betelgeuse would be the brightest star in the night sky.

Classified as a red supergiant of spectral type M1-2, the star is one of the largest stars visible to the naked eye. If Betelgeuse were at the center of the Solar System, its surface would extend past the asteroid belt, completely engulfing the orbits of Mercury, Venus, Earth, Mars, and possibly Jupiter. However, there are several other red supergiants in the Milky Way that could be larger, such as Mu Cephei and VY Canis Majoris. Calculations of its mass range from slightly under ten to a little over twenty times that of the Sun. It is calculated to be 640 light-years away, yielding an absolute magnitude of about −6. Less than 10 million years old, Betelgeuse has evolved rapidly because of its high mass. Having been ejected from its birthplace in the Orion OB1 Association—which includes the stars in Orion's Belt—this runaway star has been observed moving through the interstellar medium at a speed of 30 km/s, creating a bow shock over four light-years wide. Betelgeuse is in a late stage of stellar evolution, and it is expected to explode as a supernova within the next million years.

In 1920, Betelgeuse became the first extrasolar star to have the angular size of its photosphere measured. Subsequent studies have reported an angular diameter (apparent size) ranging from 0.042 to 0.056 arcseconds, with the differences ascribed to the non-sphericity, limb darkening, pulsations, and varying appearance at different wavelengths. It is also surrounded by a complex, asymmetric envelope roughly 250 times the size of the star, caused by mass loss from the star itself. The angular diameter of Betelgeuse is only exceeded by R Doradus and the Sun.

"α Orionis" (Latinised to "Alpha Orionis") is the star's Bayer designation. The traditional name "Betelgeuse" is derived from the Arabic ', meaning "the underarm of Orion", or ', meaning "the hand of Orion" (see below). In 2016, the International Astronomical Union organized a Working Group on Star Names (WGSN) to catalog and standardize proper names for stars. The WGSN's first bulletin of July 2016 included a table of the first two batches of names approved by the WGSN, which included "Betelgeuse" for this star. It is now so entered in the IAU Catalog of Star Names.

Betelgeuse and its red coloration have been noted since antiquity; the classical astronomer Ptolemy described its color as ὑπόκιρρος ("hypókirrhos"), a term that was later described by a translator of Ulugh Beg's "Zij-i Sultani" as "rubedo", Latin for "ruddiness". In the nineteenth century, before modern systems of stellar classification, Angelo Secchi included Betelgeuse as one of the prototypes for his Class III (orange to red) stars. By contrast, three centuries before Ptolemy, Chinese astronomers observed Betelgeuse as having a yellow coloration; if accurate, such an observation could suggest the star was in a yellow supergiant phase around the beginning of the Christian era, a possibility given current research into the complex circumstellar environment of these stars.

The variation in Betelgeuse's brightness was first described in 1836 by Sir John Herschel, when he published his observations in "Outlines of Astronomy". From 1836 to 1840, he noticed significant changes in magnitude when Betelgeuse outshone Rigel in October 1837 and again in November 1839. A 10-year quiescent period followed; then in 1849, Herschel noted another short cycle of variability, which peaked in 1852. Later observers recorded unusually high maxima with an interval of years, but only small variations from 1957 to 1967. The records of the American Association of Variable Star Observers (AAVSO) show a maximum brightness of 0.2 in 1933 and 1942, and a minimum of 1.2, observed in 1927 and 1941. This variability in brightness may explain why Johann Bayer, with the publication of his "Uranometria" in 1603, designated the star "alpha" as it probably rivaled the usually brighter Rigel ("beta"). From Arctic latitudes, Betelgeuse's red colour and higher location in the sky than Rigel meant the Inuit regarded it as brighter, and one local name was "Ulluriajjuaq" "large star".

In 1920, Albert Michelson and Francis Pease mounted a 6-meter interferometer on the front of the 2.5-meter telescope at Mount Wilson Observatory. Helped by John Anderson, the trio measured the angular diameter of Betelgeuse at 0.047", a figure which resulted in a diameter of 3.84 × 10 km (2.58 AU) based on the parallax value of 0.018". However, limb darkening and measurement errors resulted in uncertainty about the accuracy of these measurements.

The 1950s and 1960s saw two developments that would affect stellar convection theory in red supergiants: the Stratoscope projects and the 1958 publication of "Structure and Evolution of the Stars", principally the work of Martin Schwarzschild and his colleague at Princeton University, Richard Härm. This book disseminated ideas on how to apply computer technologies to create stellar models, while the Stratoscope projects, by taking balloon-borne telescopes above the Earth's turbulence, produced some of the finest images of solar granules and sunspots ever seen, thus confirming the existence of convection in the solar atmosphere.

Astronomers in the 1970s saw some major advances in astronomical imaging technology beginning with Antoine Labeyrie's invention of speckle interferometry, a process that significantly reduced the blurring effect caused by astronomical seeing. It increased the optical resolution of ground-based telescopes, allowing for more precise measurements of Betelgeuse's photosphere. With improvements in infrared telescopy atop Mount Wilson, Mount Locke and Mauna Kea in Hawaii, astrophysicists began peering into the complex circumstellar shells surrounding the supergiant, causing them to suspect the presence of huge gas bubbles resulting from convection. But it was not until the late 1980s and early 1990s, when Betelgeuse became a regular target for aperture masking interferometry, that breakthroughs occurred in visible-light and infrared imaging. Pioneered by John E. Baldwin and colleagues of the Cavendish Astrophysics Group, the new technique employed a small mask with several holes in the telescope pupil plane, converting the aperture into an ad-hoc interferometric array. The technique contributed some of the most accurate measurements of Betelgeuse while revealing bright spots on the star's photosphere. These were the first optical and infrared images of a stellar disk other than the Sun, taken first from ground-based interferometers and later from higher-resolution observations of the COAST telescope. The "bright patches" or "hotspots" observed with these instruments appeared to corroborate a theory put forth by Schwarzschild decades earlier of massive convection cells dominating the stellar surface.

In 1995, the Hubble Space Telescope's Faint Object Camera captured an ultraviolet image with a resolution superior to that obtained by ground-based interferometers—the first conventional-telescope image (or "direct-image" in NASA terminology) of the disk of another star. Because ultraviolet light is absorbed by the Earth's atmosphere, observations at these wavelengths are best performed by space telescopes. Like earlier pictures, this image contained a bright patch indicating a region in the southwestern quadrant hotter than the stellar surface. Subsequent ultraviolet spectra taken with the Goddard High Resolution Spectrograph suggested that the hot spot was one of Betelgeuse's poles of rotation. This would give the rotational axis an inclination of about 20° to the direction of Earth, and a position angle from celestial North of about 55°.
In a study published in December 2000, the star's diameter was measured with the Infrared Spatial Interferometer (ISI) at mid-infrared wavelengths producing a limb-darkened estimate of 55.2 ± 0.5 milliarcseconds (mas)—a figure entirely consistent with Michelson's findings eighty years earlier. At the time of its publication, the estimated parallax from the Hipparcos mission was 7.63 ± 1.64 mas, yielding an estimated radius for Betelgeuse of 3.6 AU. However, an infrared interferometric study published in 2009 announced that the star had shrunk by 15% since 1993 at an increasing rate without a significant diminution in magnitude. Subsequent observations suggest that the apparent contraction may be due to shell activity in the star's extended atmosphere.

In addition to the star's diameter, questions have arisen about the complex dynamics of Betelgeuse's extended atmosphere. The mass that makes up galaxies is recycled as stars are formed and destroyed, and red supergiants are major contributors, yet the process by which mass is lost remains a mystery. With advances in interferometric methodologies, astronomers may be close to resolving this conundrum. In July 2009, images released by the European Southern Observatory, taken by the ground-based Very Large Telescope Interferometer (VLTI), showed a vast plume of gas extending 30 AU from the star into the surrounding atmosphere. This mass ejection was equal to the distance between the Sun and Neptune and is one of multiple events occurring in Betelgeuse's surrounding atmosphere. Astronomers have identified at least six shells surrounding Betelgeuse. Solving the mystery of mass loss in the late stages of a star's evolution may reveal those factors that precipitate the explosive deaths of these stellar giants.

In the night sky, Betelgeuse is easy to spot with the naked eye owing to its distinctive orange-red color. In the Northern Hemisphere, beginning in January of each year, it can be seen rising in the east just after sunset. By mid-September to mid-March (best in mid-December), it is visible to virtually every inhabited region of the globe, except for a few research stations in Antarctica at latitudes south of 82°. In May (moderate northern latitudes) or June (southern latitudes), the red supergiant can be seen briefly on the western horizon after sunset, reappearing again a few months later on the eastern horizon before sunrise. In the intermediate period (June–July) it is invisible to the naked eye (visible only with a telescope in daylight), unless around midday (when the Sun is below horizon) on Antarctic regions between 70° and 80° south latitude.

Betelgeuse is a variable star whose brightness ranges between 0.0 and 1.3. There are periods when it will surpass Rigel to become the sixth brightest star, and occasionally it will be even brighter than Capella. At its faintest Betelgeuse can fall behind Deneb and Beta Crucis, themselves both slightly variable, to be the 20th-brightest star.

Betelgeuse has a color index (B–V) of 1.85—a figure which points to its advanced "redness". The photosphere has an extended atmosphere, which displays strong lines of emission rather than absorption, a phenomenon that occurs when a star is surrounded by a thick gaseous envelope (rather than ionized). This extended gaseous atmosphere has been observed moving away from and towards Betelgeuse, depending on radial velocity fluctuations in the photosphere. Betelgeuse is the brightest near-infrared source in the sky with a J band magnitude of −2.99. As a result, only about 13% of the star's radiant energy is emitted in the form of visible light. If human eyes were sensitive to radiation at all wavelengths, Betelgeuse would appear as the brightest star in the night sky.

Various catalogues list up to nine faint visual companions to Betelgeuse. They are at distances of about one to four arc-minutes and all are fainter than 10th magnitude.

Betelgeuse is generally considered to be a single isolated star and a runaway star, not currently associated with any cluster or star-forming region, although its birthplace is unclear.

Two spectroscopic companions have been proposed to the red supergiant star. Analysis of polarization data from 1968 through 1983 indicated a close companion with a periodic orbit of about 2.1 years. Using speckle interferometry, the team concluded that the closer of the two companions was located at (~9 AU) from the main star with a position angle (PA) of 273 degrees, an orbit that would potentially place it within the star's chromosphere. The more distant companion was estimated at (~77 AU) with a PA of 278 degrees. Further studies have found no evidence for these companions or have actively refuted their existence, but the possibility of a close companion contributing to the overall flux has never been fully ruled out. High-resolution interferometry of Betelgeuse and its vicinity, far beyond the technology of the 1980s and '90s, have not detected any companions.

Parallax is the apparent change of the position of an object, measured in seconds of arc, caused by the change of position of the observer of that object. As the Earth orbits the Sun, every star is seen to shift by a fraction of an arc second, which measure, combined with the baseline provided by the Earth's orbit gives the distance to that star. Since the first successful parallax measurement by Friedrich Bessel in 1838, astronomers have been puzzled by Betelgeuse's apparent distance. Knowledge of the star's distance improves the accuracy of other stellar parameters, such as luminosity that, when combined with an angular diameter, can be used to calculate the physical radius and effective temperature; luminosity and isotopic abundances can also be used to estimate the stellar age and mass.

In 1920, when the first interferometric studies were performed on the star's diameter, the assumed parallax was 0.0180 arcseconds. This equated to a distance of 56 parsecs (pc) or roughly 180 light-years (ly), producing not only an inaccurate radius for the star but every other stellar characteristic. Since then, there has been ongoing work to measure the distance of Betelgeuse, with proposed distances as high as 400 pc or about .

Before the publication of the Hipparcos Catalogue (1997), there were two conflicting parallax measurements for Betelgeuse. The first, in 1991, gave a parallax of π = 9.8 ± 4.7 mas, yielding a distance of roughly 102 pc or 330 ly. The second was the Hipparcos Input Catalogue (1993) with a trigonometric parallax of π = 5 ± 4 mas, a distance of 200 pc or 650 ly. Given this uncertainty, researchers were adopting a wide range of distance estimates, leading to significant variances in the calculation of the star's attributes.

The results from the Hipparcos mission were released in 1997. The measured parallax of Betelgeuse was π = 7.63 ± 1.64 mas, which equated to a distance of 131 pc or roughly 430 ly, and had a smaller reported error than previous measurements. However, later evaluation of the Hipparcos parallax measurements for variable stars like Betelgeuse found that the uncertainty of these measurements had been underestimated. In 2007, an improved figure of π =  was calculated, hence a much tighter error factor yielding a distance of roughly or .

In 2008, using the Very Large Array (VLA), produced a radio solution of π = , equalling a distance of or . As the researcher, Harper, points out: "The revised Hipparcos parallax leads to a larger distance () than the original; however, the astrometric solution still requires a significant cosmic noise of 2.4 mas. Given these results it is clear that the Hipparcos data still contain systematic errors of unknown origin." Although the radio data also have systematic errors, the Harper solution combines the datasets in the hope of mitigating such errors. An updated result from further observations with ALMA and e-Merlin gives a parallax of mas and a distance of pc. Further observations have resulted in a slightly revised parallax of .

Although the European Space Agency's current Gaia mission was not expected to produce good results for stars brighter than the approximately V=6 saturation limit of the mission's instruments, actual operation has shown good performance on objects to about magnitude +3. Forced observations of brighter stars mean that final results should be available for all bright stars and a parallax for Betelgeuse will be published an order of magnitude more accurate than currently available.

Betelgeuse is classified as a semiregular variable star, indicating that some periodicity is noticeable in the brightness changes, but amplitudes may vary, cycles may have different lengths, and there may be standstills or periods of irregularity. It is placed in subgroup SRc; these are pulsating red supergiants with amplitudes around one magnitude and periods from tens to hundreds of days.

Betelgeuse typically shows only small brightness changes near to magnitude +0.5, although at its extremes it can become as bright as magnitude 0.0 or as faint as magnitude +1.3. Betelgeuse is listed in the General Catalogue of Variable Stars with a possible period of 2,335 days. More detailed analyses have shown a main period near 400 days and a longer secondary period around 2,100 days.

Radial pulsations of red supergiants are well-modelled and show that periods of a few hundred days are typically due to fundamental and first overtone pulsation. Lines in the spectrum of Betelgeuse show doppler shifts indicating radial velocity changes corresponding, very roughly, to the brightness changes. This demonstrates the nature of the pulsations in size, although corresponding temperature and spectral variations are not clearly seen. Variations in the diameter of Betelgeuse have also been measured directly.

The source of the long secondary periods is unknown, but they certainly aren't due to radial pulsations. Interferometric observations of Betelgeuse have shown hotspots that are thought to be created by massive convection cells, a significant fraction of the diameter of the star and each emitting 5–10% of the total light of the star. One theory to explain long secondary periods is that they are caused by the evolution of such cells combined with the rotation of the star. Other theories include close binary interactions, chromospheric magnetic activity influencing mass loss, or non-radial pulsations such as g-modes.

In addition to the discrete dominant periods, small-amplitude stochastic variations are seen. It is proposed that this is due to granulation, similar to the same effect on the sun but on a much larger scale. Aboriginal people from the Great Victoria Desert of South Australia observed the variability of Betelgeuse and incorporated it into their oral traditions as Nyeeruna (Orion). Nyeeruna generates fire-magic in his right hand (Betelgeuse) to gain access to the Yugarilya sisters of the Pleiades, but is prevented from doing so by the eldest sister Kambugudha (Hyades), who kicks sand into his face, causing his fire-magic to dissipate in his humiliation. This is described in the oral tradition as a cyclic process, with Nyeeruna's right hand brightening and fading over time.

On 13 December 1920, Betelgeuse became the first star outside the Solar System to have the angular size of its photosphere measured. Although interferometry was still in its infancy, the experiment proved a success. The researchers, using a uniform disk model, determined that Betelgeuse had a diameter of 0.047 arcseconds, although the stellar disk was likely 17% larger due to the limb darkening, resulting in an estimate for its angular diameter of about 0.055". Since then, other studies have produced angular diameters that range from 0.042 to 0.069 arcseconds. Combining these data with historical distance estimates of 180 to 815 ly yields a projected radius of the stellar disk of anywhere from 1.2 to 8.9 AU. Using the Solar System for comparison, the orbit of Mars is about 1.5 AU, Ceres in the asteroid belt 2.7 AU, Jupiter 5.5 AU—so, assuming Betelgeuse occupying the place of the Sun, its photosphere might extend beyond the Jovian orbit, not quite reaching Saturn at 9.5 AU.
The precise diameter has been hard to define for several reasons:


To overcome these challenges, researchers have employed various solutions. Astronomical interferometry, first conceived by Hippolyte Fizeau in 1868, was the seminal concept that has enabled major improvements in modern telescopy and led to the creation of the Michelson interferometer in the 1880s, and the first successful measurement of Betelgeuse. Just as human depth perception increases when two eyes instead of one perceive an object, Fizeau proposed the observation of stars through two apertures instead of one to obtain interferences that would furnish information on the star's spatial intensity distribution. The science evolved quickly and multiple-aperture interferometers are now used to capture speckled images, which are synthesized using Fourier analysis to produce a portrait of high resolution. It was this methodology that identified the hotspots on Betelgeuse in the 1990s. Other technological breakthroughs include adaptive optics, space observatories like Hipparcos, Hubble and Spitzer, and the Astronomical Multi-BEam Recombiner (AMBER), which combines the beams of three telescopes simultaneously, allowing researchers to achieve milliarcsecond spatial resolution.

Which part of the electromagnetic spectrum—the visible, near-infrared (NIR) or mid-infrared (MIR)—produces the most accurate angular measurement is still debated. In 1996, Betelgeuse was shown to have a uniform disk of 56.6 ± 1.0 mas. In 2000, the SSL team produced another measure of 54.7 ± 0.3 mas, ignoring any possible contribution from hotspots, which are less noticeable in the mid-infrared. Also included was a theoretical allowance for limb darkening, yielding a diameter of 55.2 ± 0.5 mas. The earlier estimate equates to a radius of roughly 5.6 AU or , assuming the 2008 Harper distance of 197.0 ± 45 pc, a figure roughly the size of the Jovian orbit of 5.5 AU, published in 2009 in "Astronomy" Magazine and a year later in NASA's Astronomy Picture of the Day.

A team of astronomers working in the near-infrared announced in 2004, that the more accurate photospheric measurement was 43.33 ± 0.04 mas. The study also put forth an explanation as to why varying wavelengths from the visible to mid-infrared produce different diameters: the star is seen through a thick, warm extended atmosphere. At short wavelengths (the visible spectrum) the atmosphere scatters light, thus slightly increasing the star's diameter. At near-infrared wavelengths (K and L bands), the scattering is negligible, so the classical photosphere can be directly seen; in the mid-infrared the scattering increases once more, causing the thermal emission of the warm atmosphere to increase the apparent diameter. 

Studies with the IOTA and VLTI published in 2009 brought strong support to Perrin's analysis and yielded diameters ranging from 42.57 to 44.28 mas with comparatively insignificant margins of error. In 2011, a third estimate in the near-infrared corroborating the 2009 numbers, this time showing a limb-darkened disk diameter of 42.49 ± 0.06 mas. Consequently, if one combines the smaller Hipparcos distance from van Leeuwen of 152 ± 20 pc with Perrin's angular measurement of 43.33 mas, a near-infrared photospheric estimate would equate to about 3.4 AU or . A 2014 paper derives an angular diameter of 42.28 mas (equivalent to a 41.01 mas uniform disc) using H and K band observations made with the VLTI AMBER instrument.

Central to this discussion, it was announced in 2009, that the radius of Betelgeuse had shrunk from 1993 to 2009 by 15%, with the 2008 angular measurement equal to 47.0 mas, not too far from Perrin's estimate. Unlike most earlier papers, this study encompassed a 15-year period at one specific wavelength. Earlier studies have typically lasted one to two years by comparison and have explored multiple wavelengths, often yielding vastly different results. The diminution in Betelgeuse's apparent size equates to a range of values between 56.0 ± 0.1 mas seen in 1993 to 47.0 ± 0.1 mas seen in 2008—a contraction of almost 0.9 AU in 15 years. What is not fully known is whether this observation is evidence of a rhythmic expansion and contraction of the star's photosphere as astronomers have theorized, and if so, what the periodic cycle might be, although Townes suggested that if a cycle does exist, it is probably a few decades long. Other possible explanations are photospheric protrusions due to convection or a star that is not spherical but asymmetric causing the "appearance" of expansion and contraction as the star rotates on its axis.

The debate about differences between measurements in the mid-infrared, which suggest a possible expansion and contraction of the star, and the near-infrared, which advocates a relatively constant photospheric diameter, remains to be resolved. In a paper published in 2012, the Berkeley team reported that their measurements were "dominated by the behavior of cool, optically thick material above the stellar photosphere," indicating that the apparent expansion and contraction may be due to activity in the star's outer shells and not the photosphere itself. This conclusion, if further corroborated, would suggest an average angular diameter for Betelgeuse closer to Perrin's estimate at 43.33 arcseconds, hence a stellar radius of about 3.4 AU () assuming the shorter Hipparcos distance of 498 ± 73 ly in lieu of Harper's estimate at 643 ± 146 ly. The Gaia spacecraft may clarify assumptions presently used in calculating the size of Betelgeuse's stellar disk.

Once considered as having the largest angular diameter of any star in the sky after the Sun, Betelgeuse lost that distinction in 1997 when a group of astronomers measured R Doradus with a diameter of 57.0 ± 0.5 mas, although R Doradus, being much closer to Earth at about 200 ly, has a linear diameter roughly one-third that of Betelgeuse.

The generally reported radii of large cool stars are Rosseland radii, defined as the radius of the photosphere at a specific optical depth of two thirds. This corresponds to the radius calculated from the effective temperature and bolometric luminosity. The Rosseland radius differs from directly measured radii, but there are widely used conversion factors depending on the wavelength used for the angular measurements. For example, a measured angular diameter of 55.6 mas corresponds to a Rosseland mean diameter of 56.2 mas. The Rosseland radius derived from angular measurements of the star's photosphere rather than an extended envelope is .

Betelgeuse is a very large, luminous but cool star classified as an M1-2 Ia-ab red supergiant. The letter "M" in this designation means that it is a red star belonging to the M spectral class and therefore has a relatively low photospheric temperature; the "Ia-ab" suffix luminosity class indicates that it is an intermediate-luminosity supergiant, with properties partway between a normal supergiant and a luminous supergiant. Since 1943, the spectrum of Betelgeuse has served as one of the stable anchor points by which other stars are classified.

Uncertainty in the star's surface temperature, diameter, and distance make it difficult to achieve a precise measurement of Betelgeuse's luminosity, but research from 2012 quotes a luminosity of around , assuming a distance of 200 pc. Studies since 2001 report effective temperatures ranging from 3,250 to 3,690 K. Values outside this range have previously been reported, and much of the variation is believed to be real, due to pulsations in the atmosphere. The star is also a slow rotator and the most recent velocity recorded was 5 km/s—much slower than Antares which has a rotational velocity of 20 km/s. The rotation period depends on Betelgeuse's size and orientation to Earth, but it has been calculated to take 8.4 years to turn on its axis.

In 2004, astronomers using computer simulations speculated that even if Betelgeuse is not rotating it might exhibit large-scale magnetic activity in its extended atmosphere, a factor where even moderately strong fields could have a meaningful influence over the star's dust, wind and mass-loss properties. A series of spectropolarimetric observations obtained in 2010 with the Bernard Lyot Telescope at Pic du Midi Observatory revealed the presence of a weak magnetic field at the surface of Betelgeuse, suggesting that the giant convective motions of supergiant stars are able to trigger the onset of a small-scale dynamo effect.

Betelgeuse has no known orbital companions, so its mass cannot be calculated by that direct method. Modern mass estimates from theoretical modelling have produced values of , with values of – from older studies. It has been calculated that Betelgeuse began its life as a star of , based on a solar luminosity of 90,000–150,000. A novel method of determining the supergiant's mass was proposed in 2011, arguing for a current stellar mass of with an upper limit of 16.6 and lower of , based on observations of the star's intensity profile from narrow H-band interferometry and using a photospheric measurement of roughly 4.3 AU or 955 R. Model fitting to evolutionary tracks give a current mass of , from an initial mass of .

The kinematics of Betelgeuse are complex. The age of Class M supergiants with an initial mass of is roughly 10 million years. Starting from its present position and motion a projection back in time would place Betelgeuse around 290 parsecs farther from the galactic plane—an implausible location, as there is no star formation region there. Moreover, Betelgeuse's projected pathway does not appear to intersect with the 25 Ori subassociation or the far younger Orion Nebula Cluster (ONC, also known as Ori OB1d), particularly since Very Long Baseline Array astrometry yields a distance from Betelgeuse to the ONC of between 389 and 414 parsecs. Consequently, it is likely that Betelgeuse has not always had its current motion through space but has changed course at one time or another, possibly the result of a nearby stellar explosion. An observation by the Herschel Space Observatory in January 2013 revealed that the star's winds are crashing against the surrounding interstellar medium.

The most likely star-formation scenario for Betelgeuse is that it is a runaway star from the Orion OB1 Association. Originally a member of a high-mass multiple system within Ori OB1a, Betelgeuse was probably formed about 10–12 million years ago, but has evolved rapidly due to its high mass.

Like many young stars in Orion whose mass is greater than , Betelgeuse will use its fuel quickly and not live long. On the Hertzsprung–Russell diagram, Betelgeuse has moved off the main sequence and has swelled and cooled to become a red supergiant. Although young, Betelgeuse has exhausted the hydrogen in its core, causing the core to contract under the force of gravity into a hotter and denser state. As a result, it has begun to fuse helium into carbon and oxygen and has ignited a hydrogen shell outside the core. The hydrogen-burning shell and the contracting core cause the outer envelope to expand and cool. Its mass is such that the star will eventually fuse higher elements through neon, magnesium, and silicon all the way to iron, at which point it will collapse and explode, probably as a type II supernova.

In the late phase of stellar evolution, massive stars like Betelgeuse exhibit high rates of mass loss, possibly as much as 1  every 10,000 years, resulting in a complex circumstellar environment that is constantly in flux. In a 2009 paper, stellar mass loss was cited as the "key to understanding the evolution of the universe from the earliest cosmological times to the current epoch, and of planet formation and the formation of life itself". However, the physical mechanism is not well understood. When Schwarzschild first proposed his theory of huge convection cells, he argued it was the likely cause of mass loss in evolved supergiants like Betelgeuse. Recent work has corroborated this hypothesis, yet there are still uncertainties about the structure of their convection, the mechanism of their mass loss, the way dust forms in their extended atmosphere, and the conditions which precipitate their dramatic finale as a type II supernova. In 2001, Graham Harper estimated a stellar wind at 0.03  every 10,000 years, but research since 2009 has provided evidence of episodic mass loss making any total figure for Betelgeuse uncertain. Current observations suggest that a star like Betelgeuse may spend a portion of its lifetime as a red supergiant, but then cross back across the H-R diagram, pass once again through a brief yellow supergiant phase and then explode as a blue supergiant or Wolf-Rayet star. 

Astronomers may be close to solving this mystery. They noticed a large plume of gas extending at least six times its stellar radius indicating that Betelgeuse is not shedding matter evenly in all directions. The plume's presence implies that the spherical symmetry of the star's photosphere, often observed in the infrared, is "not" preserved in its close environment. Asymmetries on the stellar disk had been reported at different wavelengths. However, due to the refined capabilities of the NACO adaptive optics on the VLT, these asymmetries have come into focus. The two mechanisms that could cause such asymmetrical mass loss, were large-scale convection cells or polar mass loss, possibly due to rotation. Probing deeper with ESO's AMBER, gas in the supergiant's extended atmosphere has been observed vigorously moving up and down, creating bubbles as large as the supergiant itself, leading his team to conclude that such stellar upheaval is behind the massive plume ejection observed by Kervella.

In addition to the photosphere, six other components of Betelgeuse's atmosphere have now been identified. They are a molecular environment otherwise known as the MOLsphere, a gaseous envelope, a chromosphere, a dust environment and two outer shells (S1 and S2) composed of carbon monoxide (CO). Some of these elements are known to be asymmetric while others overlap.

At about 0.45 stellar radii (~2–3 AU) above the photosphere, there may lie a molecular layer known as the MOLsphere or molecular environment. Studies show it to be composed of water vapor and carbon monoxide with an effective temperature of about . Water vapor had been originally detected in the supergiant's spectrum in the 1960s with the two Stratoscope projects but had been ignored for decades. The MOLsphere may also contain SiO and AlO—molecules which could explain the formation of dust particles.
The asymmetric gaseous envelope, another cooler region, extends for several radii (~10–40 AU) from the photosphere. It is enriched in oxygen and especially in nitrogen relative to carbon. These composition anomalies are likely caused by contamination by CNO-processed material from the inside of Betelgeuse.

Radio-telescope images taken in 1998 confirm that Betelgeuse has a highly complex atmosphere, with a temperature of , similar to that recorded on the star's surface but much lower than surrounding gas in the same region. The VLA images also show this lower-temperature gas progressively cools as it extends outward. Although unexpected, it turns out to be the most abundant constituent of Betelgeuse's atmosphere. "This alters our basic understanding of red-supergiant star atmospheres", explained Jeremy Lim, the team's leader. "Instead of the star's atmosphere expanding uniformly due to gas heated to high temperatures near its surface, it now appears that several giant convection cells propel gas from the star's surface into its atmosphere." This is the same region in which Kervella's 2009 finding of a bright plume, possibly containing carbon and nitrogen and extending at least six photospheric radii in the southwest direction of the star, is believed to exist.

The chromosphere was directly imaged by the Faint Object Camera on board the Hubble Space Telescope in ultraviolet wavelengths. The images also revealed a bright area in the southwest quadrant of the disk. The average radius of the chromosphere in 1996 was about 2.2 times the optical disk (~10 AU) and was reported to have a temperature no higher than . However, in 2004 observations with the STIS, Hubble's high-precision spectrometer, pointed to the existence of warm chromospheric plasma at least one arcsecond away from the star. At a distance of 197 pc, the size of the chromosphere could be up to 200 AU. The observations have conclusively demonstrated that the warm chromospheric plasma spatially overlaps and co-exists with cool gas in Betelgeuse's gaseous envelope as well as with the dust in its circumstellar dust shells (see below).

The first claim of a dust shell surrounding Betelgeuse was put forth in 1977 when it was noted that dust shells around mature stars often emit large amounts of radiation in excess of the photospheric contribution. Using heterodyne interferometry, it was concluded that the red supergiant emits most of its excess radiation from positions beyond 12 stellar radii or roughly the distance of the Kuiper belt at 50 to 60 AU, which depends on the assumed stellar radius. Since then, there have been studies done of this dust envelope at varying wavelengths yielding decidedly different results. Studies from the 1990s have estimated the inner radius of the dust shell anywhere from 0.5 to 1.0 arcseconds, or 100 to 200 AU. These studies point out that the dust environment surrounding Betelgeuse is not static. In 1994, it was reported that Betelgeuse undergoes sporadic decades long dust production, followed by inactivity. In 1997, significant changes in the dust shell's morphology in one year were noted, suggesting that the shell is asymmetrically illuminated by a stellar radiation field strongly affected by the existence of photospheric hotspots. The 1984 report of a giant asymmetric dust shell 1 pc (206,265 AU) has not been corroborated by recent studies, although another published the same year said that three dust shells were found extending four light-years from one side of the decaying star, suggesting that Betelgeuse sheds its outer layers as it moves.

Although the exact size of the two outer CO shells remains elusive, preliminary estimates suggest that one shell extends from about 1.5 to 4.0 arcseconds and the other expands as far as 7.0 arcseconds. Assuming the Jovian orbit of 5.5 AU as the star radius, the inner shell would extend roughly 50 to 150 stellar radii (~300 to 800 AU) with the outer one as far as 250 stellar radii (~1,400 AU). The Sun's heliopause is estimated at about 100 AU, so the size of this outer shell would be almost fourteen times the size of the Solar System.

Betelgeuse is travelling supersonically through the interstellar medium at a speed of 30 km/second (i.e. ~6.3 AU/year) creating a bow shock. The shock is not created by the star, but by its powerful stellar wind as it ejects vast amounts of gas into the interstellar medium at a speed of 17 km/s, heating the material surrounding the star, thereby making it visible in infrared light. Because Betelgeuse is so bright, it was only in 1997 that the bow shock was first imaged. The cometary structure is estimated to be at least one parsec wide, assuming a distance of 643 light-years.

Hydrodynamic simulations of the bow shock made in 2012 indicate that it is very young—less than 30,000 years old—suggesting two possibilities: that Betelgeuse moved into a region of the interstellar medium with different properties only recently or that Betelgeuse has undergone a significant transformation producing a changed stellar wind. A 2012 paper, proposed that this phenomenon was caused by Betelgeuse transitioning from a blue supergiant (BSG) to a red supergiant (RSG). There is evidence that in the late evolutionary stage of a star like Betelgeuse, such stars "may undergo rapid transitions from red to blue and vice versa on the Hertzsprung-Russell diagram, with accompanying rapid changes to their stellar winds and bow shocks." Moreover, if future research bears out this hypothesis, Betelgeuse may prove to have traveled close to 200,000 AU as a red supergiant scattering as much as along its trajectory.

Betelgeuse is a red supergiant that has evolved from an O-type main sequence star. Its core will eventually collapse, producing a supernova explosion and leaving behind a compact remnant. The details depend on the exact initial mass and other physical properties of that main sequence star.

The initial mass of Betelgeuse can only be estimated by testing different stellar evolutionary models to match its current observed properties. The unknowns of both the models and the current properties mean that there is considerable uncertainty in Betelgeuse's initial appearance, but its mass is usually estimated to have been in the range of , with modern models finding values of . Its chemical makeup can be reasonably assumed to have been around 70% hydrogen, 28% helium, and 2.4% heavy elements, slightly more metal-rich than the Sun but otherwise similar. The initial rotation rate is more uncertain, but models with slow to moderate initial rotation rates produce the best matches to Betelgeuse's current properties. That main sequence version of Betelgeuse would have been a hot luminous star with a spectral type such as O9V.

A star would take between 11.5 and 15 million years to reach the red supergiant stage, with more rapidly-rotating stars taking the longest. Rapidly-rotating stars take only 9.3 million years to reach the red supergiant stage, while stars with slow rotation take only 8.1 million years. These form the best estimates of Betelgeuse's current age, as the time since its zero age main sequence stage, is estimated to be 8.0–8.5 million years as a star with no rotation.

Betelgeuse's time spent as a red supergiant can be estimated by comparing mass loss rates to the observed circumstellar material, as well as the abundances of heavy elements at the surface. Estimates range from 20,000 years to a maximum of 140,000 years. Betelgeuse appears to undergo short periods of heavy mass loss and is a runaway star moving rapidly through space, so comparisons of its current mass loss to the total lost mass are difficult. The surface of Betelgeuse shows enhancement of nitrogen, relatively low levels of carbon, and a high proportion of C relative to C, all indicative of a star that has experienced the first dredge-up. However, the first dredge-up occurs soon after a star reaches the red supergiant phase and so this only means that Betelgeuse has been a red supergiant for at least a few thousand years. The best prediction is that Betelgeuse has already spent around 40,000 years as a red supergiant, having left the main sequence perhaps one million years ago.

The current mass can be estimated from evolutionary models from the initial mass and the expected mass lost so far. For Betelgeuse, the total mass lost is predicted to be no more than about , giving a current mass of , considerably higher than estimated by other means such as pulsational properties or limb-darkening models.

All stars more massive than about are expected to end their lives when their core collapses, typically producing a supernova explosion. Up to about , a type II-P supernova is always produced from the red supergiant stage. More massive stars can lose mass quickly enough that they evolve towards higher temperatures before their cores can collapse, particularly for rotating stars and models with especially high mass loss rates. These stars can produce type II-L or type IIb supernovae from yellow or blue supergiants, or type Ib/c supernovae from Wolf-Rayet stars. Models of rotating stars predict a peculiar type II supernova similar to SN 1987A from a blue supergiant progenitor. On the other hand, non-rotating models predict a type II-P supernova from a red supergiant progenitor.

The time until Betelgeuse explodes depends on the predicted initial conditions and on the estimate of the time already spent as a red supergiant. The total lifetime from the start of the red supergiant phase to core collapse varies from about 300,000 years for a rotating star, 550,000 years for a rotating star, and up to a million years for a non-rotating star. Given the estimated time since Betelgeuse became a red supergiant, estimates of its remaining lifetime range from a "best guess" of under 100,000 years for a non-rotating model to far longer for rotating models or lower-mass stars. Betelgeuse's suspected birthplace in the Orion OB1 Association is the location of several previous supernovae. It is believed that runaway stars may be caused by supernovae, and there is strong evidence that OB stars μ Columbae, AE Aurigae and 53 Arietis all originated from such explosions in Ori OB1 2.2, 2.7 and 4.9 million years ago.

A typical type II-P supernova emits of neutrinos and produces an explosion with a kinetic energy of . As seen from Earth, it would have a peak apparent magnitude of about −12.4. It may outshine the full moon and would be easily visible in daylight. This type of supernova would remain at roughly constant brightness for 2–3 months before rapidly dimming. The visible light is produced mainly by the radioactive decay of cobalt, and maintains its brightness due to the increasing transparency of the cooling hydrogen ejected by the supernova.

Due to misunderstandings caused by the 2009 publication of the star's 15% contraction, apparently of its outer atmosphere, Betelgeuse has frequently been the subject of scare stories and rumors suggesting that it will explode within a year, leading to exaggerated claims about the consequences of such an event. The timing and prevalence of these rumors have been linked to broader misconceptions of astronomy, particularly to doomsday predictions relating to the Mayan calendar. Betelgeuse is not likely to produce a gamma-ray burst and is not close enough for its x-rays, ultraviolet radiation, or ejected material to cause significant effects on Earth.

Following Betelgeuse's supernova, a small dense remnant will be left behind, either a neutron star or black hole. This is predicted to be a neutron star of approximately .

Betelgeuse has been known as "Betelgeux", and in German "Beteigeuze" (according to Bode). "Betelgeux" and "Betelgeuze" were used until the early 20th century, when the spelling "Betelgeuse" became universal. Consensus on its pronunciation is weak and is as varied as its spellings:


Betelgeuse is often mistranslated as "armpit of the central one". In his 1899 work "Star-Names and Their Meanings", American amateur naturalist Richard Hinckley Allen stated the derivation was from the "", which he claimed degenerated into a number of forms including "Bed Elgueze", "Beit Algueze", "Bet El-gueze", "Beteigeuze" and more, to the forms "Betelgeuse", "Betelguese", "Betelgueze" and "Betelgeux". The star was named "Beldengeuze" in the "Alfonsine Tables", and Italian Jesuit priest and astronomer Giovanni Battista Riccioli had called it "Bectelgeuze" or "Bedalgeuze".

Paul Kunitzsch, Professor of Arabic Studies at the University of Munich, refuted Allen's derivation and instead proposed that the full name is a corruption of the Arabic "" meaning "the Hand of "al-Jauzā<nowiki>'"</nowiki>", "i.e.", Orion.
European mistransliteration into medieval Latin led to the first character "y" (ﻴ, with two dots underneath) being misread as a "b" (ﺒ, with only one dot underneath).
During the Renaissance, the star's name was written as ' ("house of Orion") or ', incorrectly thought to mean "armpit of Orion" (a true translation of "armpit" would be , transliterated as ")". This led to the modern rendering as "Betelgeuse". Other writers have since accepted Kunitzsch's explanation.

The last part of the name, "-elgeuse", comes from the Arabic ', a historical Arabic name of the constellation Orion, a feminine name in old Arabian legend, and of uncertain meaning. Because ', the root of ', means "middle", ' roughly means "the Central One". The modern Arabic name for Orion is ' ("the Giant"), although the use of ' in the name of the star has continued. The 17th-century English translator Edmund Chilmead gave it the name "Ied Algeuze" ("Orion's Hand"), from Christmannus. Other Arabic names recorded include ' ("the Right Hand"), ' ("the Arm"), and ' ("the Shoulder"), all appended to "of the giant", as '.

Other names for Betelgeuse included the Persian ' "the Arm", and Coptic ' "an Armlet". "" was its Sanskrit name, as part of a Hindu understanding of the constellation as a running antelope or stag. In traditional Chinese astronomy, Betelgeuse was known as (", the Fourth Star of the constellation of Three Stars") as the Chinese constellation originally referred to the three stars in the girdle of Orion. This constellation was ultimately expanded to ten stars, but the earlier name stuck. In Japan, the Taira, or Heike, clan adopted Betelgeuse and its red color as its symbol, calling the star "Heike-boshi", (), while the Minamoto, or Genji, clan had chosen Rigel and its white color. The two powerful families fought a legendary war in Japanese history, the stars seen as facing each other off and only kept apart by the Belt.

In Tahitian lore, Betelgeuse was one of the pillars propping up the sky, known as "Anâ-varu", the pillar to sit by. It was also called "Ta'urua-nui-o-Mere" "Great festivity in parental yearnings". A Hawaiian term for it was "Kaulua-koko" "brilliant red star". The Lacandon people of Central America knew it as "chäk tulix" "red butterfly".

Astronomy writer Robert Burnham Jr. proposed the term "padparadaschah" which denotes a rare orange sapphire in India, for the star.

With the history of astronomy intimately associated with mythology and astrology before the scientific revolution, the red star, like the planet Mars that derives its name from a Roman war god, has been closely associated with the martial archetype of conquest for millennia, and by extension, the motif of death and rebirth. Other cultures have produced different myths. Stephen R. Wilk has proposed the constellation of Orion could have represented the Greek mythological figure Pelops, who had an artificial shoulder of ivory made for him, with Betelgeuse as the shoulder, its color reminiscent of the reddish yellow sheen of ivory.

In the Americas, Betelgeuse signifies a severed limb of a man-figure (Orion)—the Taulipang of Brazil know the constellation as Zililkawai, a hero whose leg was cut off by his wife, with the variable light from Betelgeuse linked to the severing of the limb. Similarly, the Lakota people of North America see it as a chief whose arm has been severed. The Wardaman people of northern Australia knew the star as "Ya-jungin" "Owl Eyes Flicking", its variable light signifying its intermittent watching of ceremonies led by the Red Kangaroo Leader Rigel. In South African mythology, Betelgeuse was perceived as a lion casting a predatory gaze toward the three zebras represented by Orion's Belt.

A Sanskrit name for Betelgeuse is ārdrā "the moist one", eponymous of the "Ardra" lunar mansion in Hindu astrology. The Rigvedic God of storms Rudra presided over the star; this association was linked by 19th-century star enthusiast to Orion's stormy nature. The constellations in Macedonian folklore represented agricultural items and animals, reflecting their village way of life. To them, Betelgeuse was "Orach" "the ploughman", alongside the rest of Orion which depicted a plough with oxen. The rising of Betelgeuse at around 3 a.m. in late summer and autumn signified the time for village men to go to the fields and plough. To the Inuit, the appearance of Betelgeuse and Bellatrix high in the southern sky after sunset marked the beginning of spring and lengthening days in late February and early March. The two stars were known as "Akuttujuuk" "those (two) placed far apart", referring to the distance between them, mainly to people from North Baffin Island and Melville Peninsula.

The opposed locations of Orion and Scorpius, with their corresponding bright red variable stars Betelgeuse and Antares, were noted by ancient cultures around the world. The setting of Orion and rising of Scorpius signify the death of Orion by the scorpion. In China they signify brothers and rivals Shen and Shang. The Batak of Sumatra marked their New Year with the first new moon after the sinking of Orion's Belt below the horizon, at which point Betelgeuse remained "like the tail of a rooster". The positions of Betelgeuse and Antares at opposite ends of the celestial sky were considered significant and their constellations were seen as a pair of scorpions. Scorpion days marked as nights that both constellations could be seen.

As one of the brightest and best-known stars, Betelgeuse has featured in many works of fiction. The star's unusual name inspired the title of the 1988 film "Beetlejuice", and script writer Michael McDowell was impressed by how many people made the connection. In the popular science fiction series "The Hitchhiker's Guide to the Galaxy" by Douglas Adams, Ford Prefect was from "a small planet somewhere in the vicinity of Betelgeuse."

Two American navy ships were named after the star, both of them World War II vessels, the launched in 1939 and launched in 1944. In 1979, a French supertanker named "Betelgeuse" was moored off Whiddy Island discharging oil when it exploded, killing 50 people in one of the worst disasters in Ireland's history.

The Dave Matthews Band song "Black and Blue Bird" references the star. The Philip Larkin poem "The North Ship", found in the collection of the same name, references the star in the section titled "Above 80° N", which reads:" 'A woman has ten claws,' /
Sang the drunken boatswain; /
Farther than Betelgeuse, /
More brilliant than Orion /
Or the planets Venus and Mars, /
The star flames on the ocean; /
'A woman has ten claws,' /
Sang the drunken boatswain."Humbert Wolfe wrote a poem about Betelgeuse, which was set to music by Gustav Holst.

This table provides a non-exhaustive list of angular measurements conducted since 1920. Also included is a column providing a current range of radii for each study based on Betelgeuse's most recent distance estimate (Harper "et al.") of 197 ± 45 pc.



</doc>
<doc id="54410" url="https://en.wikipedia.org/wiki?curid=54410" title="Triceratops">
Triceratops

Triceratops is a genus of herbivorous ceratopsid dinosaur that first appeared during the late Maastrichtian stage of the late Cretaceous period, about 68 million years ago (mya) in what is now North America. It is one of the last known non-avian dinosaur genera, and became extinct in the Cretaceous–Paleogene extinction event 66 million years ago. The name "Triceratops", which literally means "three-horned face", is derived from the Ancient Greek words τρί- ("tri-") meaning "three", κέρας ("kéras") meaning "horn", and ὤψ ("ōps") meaning "face".

It has been documented by numerous remains collected since the genus was first described in 1889, including at least one complete individual skeleton. Paleontologist John Scannella observed: "It is hard to walk out into the Hell Creek Formation and not stumble upon a "Triceratops" weathering out of a hillside." Forty-seven complete or partial skulls were discovered in just that area from 2000 to 2010. Specimens representing life stages from hatchling to adult have been found. As the archetypal ceratopsid, "Triceratops" is one of the most popular dinosaurs, and has been featured in film, postal stamps, and many other types of media.

Bearing a large bony frill and three horns on its large four-legged body, and possessing similarities with the modern rhinoceros, "Triceratops" is one of the most recognizable of all dinosaurs and the best known ceratopsid. It shared the landscape with and was probably preyed upon by "Tyrannosaurus", though it is less certain that the two did battle in the manner often depicted in traditional museum displays and popular images. The functions of the frills and three distinctive facial horns on its head have long inspired debate. Traditionally, these have been viewed as defensive weapons against predators. More recent theories, noting the presence of blood vessels in the skull bones of ceratopsids, find it more probable that these features were primarily used in identification, courtship and dominance displays, much like the antlers and horns of modern reindeer, mountain goats, or rhinoceros beetles. The theory would find additional support if "Torosaurus" were found to be the mature form of "Triceratops", as this would mean the frill also developed holes (fenestrae) as individuals reached maturity, rendering the structure more useful for display than defense.

The exact placement of the genus "Triceratops" within the ceratopsid group has been debated by paleontologists. Two species, "T. horridus" and "T. prorsus", are considered valid, although many other species have been named. Research published in 2010 suggested that the contemporaneous "Torosaurus", a ceratopsid long regarded as a separate genus, represents "Triceratops" in its mature form. The view was immediately disputed and examination of more fossil evidence is expected to settle the debate.

Individual "Triceratops" are estimated to have reached about 7.9 to 9.0 m (25.9–29.5 ft) in length, in height, and 6.1–12.0 tonnes (13,000–26,000 lb) in weight. The most distinctive feature is their large skull, among the largest of all land animals. The largest known skull (specimen MWC 7584, formerly BYU 12183) is estimated to have been in length when complete, and could reach almost a third of the length of the entire animal. A specimen of "T. horridus" named Kelsey measured long with a skull, stood about tall, and was estimated by the Black Hills institute to weight nearly . A "Triceratops" long has been estimated by Gregory S. Paul to have massed . It bore a single horn on the snout, above the nostrils, and a pair of horns approximately long, with one above each eye. In 2010, paleontologists revealed a fossil (named "Yoshi's Trike," MOR 3027) with horn cores, housed and displayed at the Museum of the Rockies in Montana. To the rear of the skull was a relatively short, bony frill, adorned with epoccipitals in some specimens. Most other ceratopsids had large fenestrae in their frills, while those of "Triceratops" were noticeably solid. "T. horridus" can be distinguished from "T. prorsus" by having a shallower snout.

"Triceratops" species possessed a sturdy build, with strong limbs, short hands with three hooves each, and short feet with four hooves each. Although certainly quadrupedal, the posture of these dinosaurs has long been the subject of some debate. Originally, it was believed that the front legs of the animal had to be sprawling at angles from the thorax in order to better bear the weight of the head. This stance can be seen in paintings by Charles Knight and Rudolph Zallinger. Ichnological evidence in the form of trackways from horned dinosaurs and recent reconstructions of skeletons (both physical and digital) seem to show that "Triceratops" and other ceratopsids maintained an upright stance during normal locomotion, with the elbows flexed and slightly bowed out, in an intermediate state between fully upright and fully sprawling (as in the modern rhinoceros).

The hands and forearms of "Triceratops" retained a fairly primitive structure compared to other quadrupedal dinosaurs such as thyreophorans and many sauropods. In those two groups, the forelimbs of quadrupedal species were usually rotated so that the hands faced forward with palms backward ("pronated") as the animals walked. "Triceratops", like other ceratopsians and the related quadrupedal ornithopods, walked with most of their fingers pointing out and away from the body, the primitive condition for dinosaurs also retained by bipedal forms like the theropods. In "Triceratops", the weight of the body was carried by only the first three fingers of the hand, while digits 4 and 5 were vestigial and lacked claws or hooves. The phalangeal formula is 2-3-4-3-1, meaning that the innermost finger of the forelimb has two bones, the next has three, etc.

The first named specimen now attributed to "Triceratops" is a pair of brow horns attached to a skull roof, found near Denver, Colorado in the spring of 1887. This specimen was sent to American paleontologist Othniel Charles Marsh, who believed that the formation from which it came dated from the Pliocene, and that the bones belonged to a particularly large and unusual bison, which he named "Bison alticornis". He realized that there were horned dinosaurs by the next year, which saw his publication of the genus "Ceratops" from fragmentary remains, but he still believed "B. alticornis" to be a Pliocene mammal. It took a third and much more complete skull to change his mind. The specimen, collected in 1888 by John Bell Hatcher from the Lance Formation of Wyoming, was initially described as another species of "Ceratops". After reflection, Marsh changed his mind and gave it the generic name "Triceratops", accepting his "Bison alticornis" as another species of "Ceratops" (it would later be added to "Triceratops"). The sturdy nature of the animal's skull has ensured that many examples have been preserved as fossils, allowing variations between species and individuals to be studied. "Triceratops" remains have subsequently been found in the American states of Montana and South Dakota (in addition to Colorado and Wyoming), and in the provinces of Saskatchewan and Alberta, Canada.

An earlier specimen, also recovered from the Lance Formation, was named "Agathaumas sylvestris" by Edward Drinker Cope in 1872. Originally identified as a hadrosaur, this specimen consists only of post-cranial remains and is only provisionally considered an example of "Triceratops".

Within the first decades after "Triceratops" was described, various skulls were collected, which varied to a lesser or greater degree from the original "Triceratops", named "T. horridus" by Marsh (from the Latin "horridus"; "rough, rugose", suggesting the roughened texture of those bones belonging to the type specimen, later identified as an aged individual). This variation is unsurprising, given that "Triceratops" skulls are large three-dimensional objects, coming from individuals of different ages and both sexes, and which were subjected to different amounts and directions of pressure during fossilization. Discoverers would name these as separate species (listed below), and came up with several phylogenetic schemes for how they were related to each other.

In the first attempt to understand the many species, Lull found two groups, although he did not say how he distinguished them: one composed of "T. horridus", "T. prorsus", and "T. brevicornus"; the other of "T. elatus" and "T. calicornis". Two species ("T. serratus" and "T. flabellatus") stood apart from these groups. By 1933, and his revision of the landmark 1907 Hatcher-Marsh-Lull monograph of all known ceratopsians, he retained his two groups and two unaffiliated species, with a third lineage of "T. obtusus" and "T. hatcheri" that was characterized by a very small nasal horn. "T. horridus"-"T. prorsus"-"T. brevicornus" was now thought to be the most conservative lineage, with an increase in skull size and a decrease in nasal horn size, and "T. elatus"-"T. calicornis" was defined by large brow horns and small nasal horn. C. M. Sternberg made one modification, adding "T. eurycephalus" and suggesting that it linked the second and third lineages closer together than they were to the "T. horridus" lineage. This pattern was followed until the major studies of the 1980s and 1990s.

With time, the idea that the differing skulls might be representative of individual variation within one (or two) species gained popularity. In 1986, Ostrom and Wellnhofer published a paper in which they proposed that there was only one species, "Triceratops horridus". Part of their rationale was that generally there are only one or two species of any large animal in a region (modern examples being the elephant and the giraffe in modern Africa). To their findings, Lehman added the old Lull-Sternberg lineages combined with maturity and sexual dimorphism, suggesting that the "T. horridus"-"T. prorsus"-"T. brevicornus" lineage was composed of females, the "T.calicornis"-"T.elatus" lineage was made up of males, and the "T. obtusus"-"T. hatcheri" lineage was of pathologic old males. His reasoning was that males had taller, more erect horns and larger skulls, and females had smaller skulls with shorter, forward-facing horns.

These findings were contested a few years later by Catherine Forster, who reanalyzed "Triceratops" material more comprehensively and concluded that the remains fell into two species, "T. horridus" and "T. prorsus", although the distinctive skull of "T." (""Nedoceratops"") "hatcheri" differed enough to warrant a separate genus. She found that "T. horridus" and several other species belonged together, and "T. prorsus" and "T. brevicornus" stood alone, and since there were many more specimens in the first group, she suggested that this meant the two groups were two species. It is still possible to interpret the differences as representing a single species with sexual dimorphism.

In 2009, John Scannella and Denver Fowler supported the separation of "T. prorsus" and "T. horridus", and noted that the two species are also separated stratigraphically within the Hell Creek Formation, indicating that they did not live together at the same time.


Some of the following species are synonyms, as indicated in parentheses ("="T. horridus"" or "="T. prorsus""). All the others are considered "nomina dubia" ("dubious names") because they are based on remains too poor or incomplete to be distinguished from pre-existing "Triceratops" species.


"Triceratops" is the best known genus of the Ceratopsidae, a family of large, mostly North American horned dinosaurs. The exact location of "Triceratops" among the ceratopsians has been debated over the years. Confusion stemmed mainly from the combination of short, solid frills (similar to that of Centrosaurinae), and the long brow horns (more akin to Chasmosaurinae). In the first overview of horned dinosaurs, R. S. Lull hypothesized two lineages, one of "Monoclonius" and "Centrosaurus" leading to "Triceratops", the other with "Ceratops" and "Torosaurus", making "Triceratops" a centrosaurine as the group is understood today. Later revisions supported this view, formally describing the first, short-frilled group as Centrosaurinae (including "Triceratops"), and the second, long-frilled group as Chasmosaurinae.

In 1949, Charles M. Sternberg was the first to question this and proposed instead that "Triceratops" was more closely related to "Arrhinoceratops" and "Chasmosaurus" based on skull and horn features, making "Triceratops" a ceratopsine (chasmosaurine of his usage) genus. He was largely ignored, with John Ostrom, and later David Norman both placing "Triceratops" within Centrosaurinae.
Subsequent discoveries and analyses upheld Sternberg's view on the position of "Triceratops", with Lehman defining both subfamilies in 1990 and diagnosing "Triceratops" as ceratopsine (chasmosaurine of his usage) on the basis of several morphological features. In fact, it fits well into the ceratopsine subfamily, apart from its one feature of a shortened frill. Further research by Peter Dodson, including a 1990 cladistic analysis and a 1993 study using RFTRA (resistant-fit theta-rho analysis), a morphometric technique which systematically measures similarities in skull shape, reinforces "Triceratops" placement in the ceratopsine subfamily.

The cladogram below follows Longrich (2014), who named a new species of "Pentaceratops", and included nearly all species of chasmosaurine.
For many years after its discovery, the evolutionary origins of "Triceratops" remained largely obscure. In 1922, the newly discovered "Protoceratops" was seen as its ancestor by Henry Fairfield Osborn, but many decades passed before additional findings came to light. Recent years have been fruitful for the discovery of several antecedents of "Triceratops". "Zuniceratops", the earliest known ceratopsian with brow horns, was described in the late 1990s, and "Yinlong", the first known Jurassic ceratopsian, in 2005.

These new finds have been vital in illustrating the origins of horned dinosaurs in general, suggesting an Asian origin in the Jurassic, and the appearance of truly horned ceratopsians by the beginning of the late Cretaceous in North America. As "Triceratops" is increasingly shown to be a member of the long-frilled subfamily Ceratopsinae, a likely ancestor may have resembled "Chasmosaurus", which thrived some 5 million years earlier.

In phylogenetic taxonomy, the genus "Triceratops" has been used as a reference point in the definition of Dinosauria; dinosaurs have been designated as all descendants of the most recent common ancestor of "Triceratops" and Neornithes (i.e. modern birds). Furthermore, the bird-hipped dinosaurs, Ornithischia, have all been designated dinosaurs with a more recent common ancestor to "Triceratops" than modern birds.

Although "Triceratops" are commonly portrayed as herding animals, there is currently little evidence that they lived in herds. While several other genera of horned dinosaurs are known from bonebeds preserving bones from two to hundreds or thousands of individuals, to date there is only one documented bonebed dominated by "Triceratops" bones: a site in southeastern Montana with the remains of three juveniles. It may be significant that only juveniles were present. In 2012, a group of three "Triceratops" in relatively complete condition, each of varying sizes from a full-grown adult to a small juvenile, were found in Wyoming, near Newcastle. The remains are currently under excavation by paleontologist Peter Larson and a team from the Black Hills Institute. It is believed that the animals were traveling as a family unit, but it remains unknown if the group consists of a mated pair and their offspring, or two females and a juvenile they were caring for. The remains also show signs of predation or scavenging from "Tyrannosaurus", particularly on the largest specimen, with the bones of the front limbs showing breakage and puncture wounds from "Tyrannosaurus" teeth.

For many years, "Triceratops" finds were known only from solitary individuals. These remains are very common; for example, Bruce Erickson, a paleontologist of the Science Museum of Minnesota, has reported having seen 200 specimens of "T. prorsus" in the Hell Creek Formation of Montana. Similarly, Barnum Brown claimed to have seen over 500 skulls in the field. Because "Triceratops" teeth, horn fragments, frill fragments, and other skull fragments are such abundant fossils in the Lancian faunal stage of the late Maastrichtian (late Cretaceous, 66 mya) Period of western North America, it is regarded as among the dominant herbivores of the time, if not the most dominant herbivore. In 1986, Robert Bakker estimated it as making up 5/6ths of the large dinosaur fauna at the end of the Cretaceous. Unlike most animals, skull fossils are far more common than postcranial bones for "Triceratops", suggesting that the skull had an unusually high preservation potential.

"Triceratops" were herbivorous, and because of their low head, their primary food was probably low growth, although they may have been able to knock down taller plants with their horns, beak, and bulk. The jaws were tipped with a deep, narrow beak, believed to have been better at grasping and plucking than biting.

"Triceratops" teeth were arranged in groups called batteries, of 36 to 40 tooth columns in each side of each jaw, with 3 to 5 stacked teeth per column, depending on the size of the animal. This gives a range of 432 to 800 teeth, of which only a fraction were in use at any given time (tooth replacement was continuous throughout the life of the animal). They functioned by shearing in a vertical to near-vertical orientation. The great size and numerous teeth of "Triceratops" suggests that they ate large volumes of fibrous plant material, with some researchers suggesting palms and cycads, and others suggesting ferns, which then grew in prairies.

There has been much speculation over the functions of "Triceratops" head adornments. The two main theories have revolved around use in combat and in courtship display, with the latter now thought to be the most likely primary function.

Early on, Lull postulated that the frills may have served as anchor points for the jaw muscles to aid chewing by allowing increased size and thus power for the muscles. This has been put forward by other authors over the years, but later studies do not find evidence of large muscle attachments on the frill bones.

"Triceratops" were long thought to have used their horns and frills in combat with predators such as "Tyrannosaurus", the idea being discussed first by Charles H. Sternberg in 1917 and 70 years later by Robert Bakker. There is evidence that "Tyrannosaurus" did have aggressive head-on encounters with "Triceratops", based on partially healed tyrannosaur tooth marks on a "Triceratops" brow horn and squamosal; the bitten horn is also broken, with new bone growth after the break. Which animal was the aggressor is not known. Since the "Triceratops" wounds healed, it is most likely that the "Triceratops" survived the encounter. "Tyrannosaurus" is also known to have fed on "Triceratops", as shown by a heavily tooth-scored "Triceratops" ilium and sacrum.
In addition to combat with predators using horns, "Triceratops" are popularly shown engaging each other in combat with horns locked. While studies show that such activity would be feasible, if unlike that of present-day horned animals, there is disagreement about whether they did so. Although pitting, holes, lesions, and other damage on "Triceratops" skulls (and the skulls of other ceratopsids) are often attributed to horn damage in combat, a 2006 study finds no evidence for horn thrust injuries causing these forms of damage (for example, there is no evidence of infection or healing). Instead, non-pathological bone resorption, or unknown bone diseases, are suggested as causes. A newer study compared incidence rates of skull lesions and periosteal reaction in "Triceratops" and "Centrosaurus" and showed that these were consistent with "Triceratops" using its horns in combat and the frill being adapted as a protective structure, while lower pathology rates in "Centrosaurus" may indicate visual rather than physical use of cranial ornamentation, or a form of combat focused on the body rather than the head. The frequency of injury was found to be 14% in "Triceratops". The researchers also concluded that the damage found on the specimens in the study was often too localized to be caused by bone disease. Histological examination reveals that the frill of "Triceratops" is composed of fibrolamellar bone which contains fibroblasts that play a critical role in wound healing, and are capable of rapidly depositing bone during remodeling.
One skull was found with a hole in the jugal bone, apparently a puncture wound sustained while the animal was alive, as indicated by signs of healing. The hole has a diameter close to that of the distal end of a "Triceratops" horn. This, and other apparent healed wounds in the skulls of ceratopsians, has been cited as evidence of non-fatal intraspecific competition in these dinosaurs.

The large frill also may have helped to increase body area to regulate body temperature. A similar theory has been proposed regarding the plates of "Stegosaurus", although this use alone would not account for the bizarre and extravagant variation seen in different members of the Ceratopsidae, which would rather support the sexual display theory.

The theory that frills functioned as a sexual display was first proposed by Davitashvili in 1961 and has gained increasing acceptance since. Evidence that visual display was important, either in courtship or other social behavior, can be seen in the horned dinosaurs differing markedly in their adornments, making each species highly distinctive. Also, modern living creatures with such displays of horns and adornments use them similarly. A 2006 study of the smallest "Triceratops" skull, ascertained to be a juvenile, shows the frill and horns developed at a very early age, predating sexual development and thus probably important for visual communication and species recognition in general. The use of the exaggerated structures to enable dinosaurs to recognize their own species has been questioned, as no such function exists for such structures in modern species.

In 2006, the first extensive ontogenetic study of "Triceratops" was published in the journal Proceedings of the Royal Society. The study, by John R. Horner and Mark Goodwin, found that individuals of "Triceratops" could be divided into four general ontogenetic groups, babies, juveniles, subadults, and adults. With a total number of 28 skulls studied, the youngest was only long. 10 of the 28 skulls could be placed in order in a growth series with one representing each age. Each of the four growth stages were found to have identifying features. Multiple ontogenetic trends were discovered, including the size reduction of the epoccipitals, development and reorientation of postorbital horns, and hollowing out of the horns.

"Torosaurus" is a ceratopsid genus first identified from a pair of skulls in 1891, two years after the identification of "Triceratops". The genus "Torosaurus" resembles "Triceratops" in geological age, distribution, anatomy and size and it has been recognised as a close relative. Its distinguishing features are an elongated skull and the presence of two fenestrae, or holes, in the frill. Paleontologists investigating dinosaur ontogeny (growth and development of individuals over the life span) in the Hell Creek Formation, Montana, US, have recently presented evidence that the two represent a single genus.

John Scannella, in a paper presented in Bristol, UK at the conference of the Society of Vertebrate Paleontology (25 September 2009) reclassified "Torosaurus" as especially mature "Triceratops" individuals, perhaps representing a single sex. Jack Horner, Scannella's mentor at Bozeman Campus, Montana State University, noted that ceratopsian skulls consist of metaplastic bone. A characteristic of metaplastic bone is that it lengthens and shortens over time, extending and resorbing to form new shapes. Significant variety is seen even in those skulls already identified as "Triceratops", Horner said, "where the horn orientation is backwards in juveniles and forward in adults". Approximately 50% of all subadult "Triceratops" skulls have two thin areas in the frill that correspond with the placement of "holes" in "Torosaurus" skulls, suggesting that holes developed to offset the weight that would otherwise have been added as maturing "Triceratops" individuals grew longer frills. A paper describing these findings in detail was published in July 2010 by Scannella and Horner. It formally argues that "Torosaurus" and the similar contemporary "Nedoceratops" are synonymous with "Triceratops".

The assertion ignited debate. Andrew Farke had in 2006 stressed that, apart from the frill, no systematic differences could be found between "Torosaurus" and "Triceratops". He nevertheless disputed Scannella's conclusion by arguing in 2011 that the proposed morphological changes required to "age" a "Triceratops" into a "Torosaurus" would be without precedent among ceratopsids. Such changes would include the growth of additional epoccipitals, reversion of bone texture from an adult to immature type and back to adult again, and growth of frill holes at a later stage than usual. A study by Nicholas Longrich and Daniel Field analyzed 35 specimens of both "Triceratops" and "Torosaurus". The authors concluded that "Triceratops" individuals too old to be considered immature forms are represented in the fossil record, as are "Torosaurus" individuals too young to be considered fully mature adults. The synonymy of "Triceratops" and "Torosaurus" cannot be supported, they said, without more convincing intermediate forms than Scannella and Horner initially produced. Scannella's "Triceratops" specimen with a hole on its frill, they argued, could represent a diseased or malformed individual rather than a transitional stage between an immature "Triceratops" and mature "Torosaurus" form.

Opinion has varied on the validity of a separate genus for "Nedoceratops". John Scannella and Jack Horner regarded it as an intermediate growth stage between "Triceratops" and "Torosaurus". Andrew Farke, in his 2011 redescription of the only known skull, concluded that it was an aged individual of its own valid taxon, "Nedoceratops hatcheri". Nicholas Longrich and Daniel Fields also did not consider it a transition between "Torosaurus" and "Triceratops", suggesting that the frill holes were pathological.

As described above, John Scannella had argued in 2010 that "Nedoceratops" should be considered a synonym of "Triceratops". Andrew Farke (2011) maintained that it represents a valid distinct genus. Nick Longrich agreed with Scannella about "Nedoceratops" and made a further suggestion: that the recently described "Ojoceratops" was likewise a synonym. The fossils, he argued, are indistinguishable from the "T. horridus" specimens that were previously attributed to the defunct species "T. serratus".

Longrich observed that another newly described genus, "Tatankaceratops", displayed a strange mix of characteristics already found in adult and juvenile "Triceratops". Rather than representing a distinct genus, "Tatankaceratops" could as easily represent a dwarf "Triceratops" or a "Triceratops" individual with a developmental disorder that caused it to stop growing prematurely.

"Triceratops" lived during the Late Cretaceous of North America, its fossils coming from the Evanston Formation, Scollard Formation, Laramie Formation, Lance Formation, Denver Formation, and Hell Creek Formation. These fossil formations date back to the time of the Cretaceous-Paleogene Extinction Event, and have been dated to 66 ± 0.07 million years ago. Many animals and plants have been found in these formations, but mostly from the Lance Formation and Hell Creek Formation. "Triceratops" was one of the last ceratopsian genera to appear before the end of the Mesozoic. The related "Torosaurus", and the more distantly related diminutive "Leptoceratops", were also present, though their remains have been rarely encountered. 

Theropods from these formations include genera of tyrannosaurids, ornithomimids, troodontids, avialans, caenagnathids, and dromaeosaurids. "Acheroraptor" and "Dakotaraptor" are dromaeosaurids from the Hell Creek Formation. Indeterminate dromaeosaurs are known from other fossil formations. Common teeth previously referred to "Dromaeosaurus" and "Saurornitholestes" later were considered to be "Acheroraptor". The tyrannosaurids from the formation are "Nanotyrannus" and "Tyrannosaurus", although the former might be a junior synonym of the latter. Among ornithomimids are the genera "Struthiomimus" as well as "Ornithomimus", although an undescribed animal named "Orcomimus" could be from the formation. Troodontids are only represented by "Pectinodon" and "Paronychodon" in the Hell Creek Formation; with a possible species of "Troodon" from the Lance Formation. One species of coelurosaur is known from Hell Creek and similar formations by a single species, "Richardoestesia". Only three oviraptorosaurs are from the Hell Creek Formation, "Anzu", "Leptorhynchos" and a giant species of caenagnathid, very similar to "Gigantoraptor", from South Dakota. However, only fossilized foot prints were discovered. The avialans known from the formation are "Avisaurus", multiple species of "Brodavis", and several other species of hesperornithoforms, as well as several species of true birds including "Cimolopteryx".
Ornithischians are abundant in the Scollard Lance, Laramie, Lance, Denver, and Hell Creek Formation. The main groups of ornithischians are ankylosaurians, ornithopods, ceratopsians, and pachycephalosaurians. Three ankylosaurians are known, "Ankylosaurus", "Denversaurus", and possibly a species of "Edmontonia" or an undescribed genus. Multiple genera of ceratopsians are known from the formation other than "Triceratops", the leptoceratopsid "Leptoceratops", and the chasmosaurine ceratopsids "Torosaurus","Nedoceratops" and "Tatankaceratops". Ornithopods are common in the Hell Creek Formation, and are known from several species of the ornithopod "Thescelosaurus", and the hadrosaurids "Edmontosaurus", and a possible species of "Parasaurolophus". Several pachycephalosaurians have been found in the Hell Creek Formation and in similar formations. Among them are the derived pachycephalosaurids ""Stygimoloch"", ""Dracorex"", "Pachycephalosaurus", "Sphaerotholus", and an undescribed specimen from North Dakota. The first two might be junior synonyms of "Pachycephalosaurus".

Mammals are plentiful in the Hell Creek Formation. Groups represented include multituberculates, metatherians, and eutherians. The multituberculates represented include "Paracimexomys", the cimolomyids "Paressonodon", "Meniscoessus", "Essonodon", "Cimolomys", "Cimolodon", and "Cimexomys"; and the neoplagiaulacids "Mesodma", and "Neoplagiaulax". The alphadontids "Alphadon", "Protalphodon", and "Turgidodon", pediomyids "Pediomys", "Protolambda", and "Leptalestes", the stagodontid "Didelphodon", the deltatheridiid "Nanocuris", the herpetotheriid "Nortedelphys", and the glasbiid "Glasbius" all represent metatherians of the Hell Creek Formation. A few eutherians are known, being represented by "Alostera", "Protungulatum", the cimolestids "Cimolestes" and "Batodon", the gypsonictopsid "Gypsonictops", and the possible nyctitheriid "Paranyctoides".

"Triceratops" is the official state fossil of South Dakota, and the official state dinosaur of Wyoming.
In 1942, Charles R. Knight painted a mural incorporating a confrontation between the two dinosaurs in the Field Museum of Natural History for the National Geographic Society, establishing them as enemies in the popular imagination. Paleontologist Bob Bakker said of the imagined rivalry between "Tyrannosaurus" and "Triceratops", "No matchup between predator and prey has ever been more dramatic. It's somehow fitting that those two massive antagonists lived out their co-evolutionary belligerence through the very last days of the very last epoch of the Age of Dinosaurs."



</doc>
<doc id="55021" url="https://en.wikipedia.org/wiki?curid=55021" title="Josquin des Prez">
Josquin des Prez

Josquin des Prez (;  – 27 August 1521), often referred to simply as Josquin, was a French composer of the Renaissance. His original name is sometimes given as Josquin Lebloitte and his later name is given under a wide variety of spellings in French, Italian, and Latin, including and . His motet "Illibata Dei virgo nutrix" includes an acrostic of his name, where he spelled it "Josquin des Prez". He was the most famous European composer between Guillaume Dufay and Giovanni Pierluigi da Palestrina, and is usually considered to be the central figure of the Franco-Flemish School. Josquin is widely considered by music scholars to be the first master of the high Renaissance style of polyphonic vocal music that was emerging during his lifetime.

During the 16th century, Josquin gradually acquired the reputation as the greatest composer of the age, his mastery of technique and expression universally imitated and admired. Writers as diverse as Baldassare Castiglione and Martin Luther wrote about his reputation and fame; theorists such as Heinrich Glarean and Gioseffo Zarlino held his style as that best representing perfection. He was so admired that many anonymous compositions were attributed to him by copyists, probably to increase their sales. More than 370 works are attributed to him; it was only after the advent of modern analytical scholarship that some of these attributions were challenged, and revealed as mistaken, on the basis of stylistic features and manuscript evidence. Yet in spite of Josquin's colossal reputation, which endured until the beginning of the Baroque era and was revived in the 20th century, his biography is shadowy, and virtually nothing is known about his personality. The only surviving work which may be in his own hand is a graffito on the wall of the Sistine Chapel, and only one contemporary mention of his character is known, in a letter to Duke Ercole I of Ferrara. The lives of dozens of less revered Renaissance composers are better documented than that of Josquin.

Josquin wrote both sacred and secular music, and in all of the significant vocal forms of the age, including masses, motets, chansons and frottole. During the 16th century, he was praised for both his supreme melodic gift and his use of ingenious technical devices. In modern times, scholars have attempted to ascertain the basic details of his biography, and have tried to define the key characteristics of his style to correct misattributions, a task that has proved difficult, as Josquin liked to solve compositional problems in different ways in successive compositions—sometimes he wrote in an austere style devoid of ornamentation, and at other times he wrote music requiring considerable virtuosity. Heinrich Glarean wrote in 1547 that Josquin was not only a "magnificent virtuoso" (the Latin can be translated also as "show-off") but capable of being a "mocker", using satire effectively. While the focus of scholarship in recent years has been to remove music from the "Josquin canon" (including some of his most famous pieces) and to reattribute it to his contemporaries, the remaining music represents some of the most famous and enduring of the Renaissance.

Little is known for certain of Josquin's early life. Much is inferential and speculative, though numerous clues have emerged from his works and the writings of contemporary composers, theorists, and writers of the next several generations. Josquin was born in the area controlled by the Dukes of Burgundy, and was possibly born either in Hainaut (modern-day Belgium), or immediately across the border in modern-day France, since several times in his life he was classified legally as a Frenchman (for instance, when he made his will). Josquin was long mistaken for a man with a similar name, Josquin de Kessalia, born around the year 1440, who sang in Milan from 1459 to 1474, dying in 1498. More recent scholarship has shown that Josquin des Prez was born around 1450 or a few years later, and did not go to Italy until the early 1480s.

Around 1466, perhaps on the death of his father, Josquin was named by his uncle and aunt, Gille Lebloitte dit Desprez and Jacque Banestonne, as their heir. Their will gives Josquin's actual surname as Lebloitte. According to Matthews and Merkley, "des Prez" was an alternative name.

According to an account by Claude Hémeré, a friend and librarian of Cardinal Richelieu whose evidence dates as late as 1633, and who used the records of the collegiate church of Saint-Quentin, Josquin became a choirboy with his friend and colleague the Franco Flemish composer Jean Mouton at Saint-Quentin's royal church, probably around 1460. Doubt has been cast on the accuracy of Hémeré's account, however. Josquin may have studied counterpoint under Ockeghem, whom he greatly admired throughout his life: this is suggested both by the testimony of Gioseffo Zarlino and Lodovico Zacconi, writing later in the 16th century, and by Josquin's eloquent lament on the death of Ockeghem in 1497, "Nymphes des bois/Requiem aeternam", based on the poem by Jean Molinet. All records from Saint-Quentin were destroyed in 1669; however the collegiate chapel there was a center of music-making for the entire area, and in addition was an important center of royal patronage. Both Jean Mouton and Loyset Compère were buried there and it is certainly possible that Josquin acquired his later connections with the French royal chapel through early experiences at Saint-Quentin.

The first definite record of his employment is dated 19 April 1477, and it shows that he was a singer at the chapel of René, Duke of Anjou, in Aix-en-Provence. He remained there at least until 1478. No certain records of his movements exist for the period from March 1478 until 1483, but if he remained in the employ of René he would have transferred to Paris in 1481 along with the rest of the chapel. One of Josquin's early motets, "Misericordias Domini in aeternum cantabo", suggests a direct connection with Louis XI, who was king during this time. In 1483 Josquin returned to Condé to claim his inheritance from his aunt and uncle, who may have been killed by the army of Louis XI in May 1478, when they besieged the town, locked the population into the church, and burned them alive.

The period from 1480 to 1482 has puzzled biographers; contradictory evidence exists suggesting either that Josquin was still in France, or was already in the service of the Sforza family, specifically with Ascanio Sforza, who had been banished from Milan and resided temporarily in Ferrara or Naples. Residence in Ferrara in the early 1480s could explain the "Missa Hercules dux Ferrariae", composed for Ercole d'Este, but which stylistically does not fit with the usual date of 1503–04 when Josquin was known to be in Ferrara. Alternatively it has been suggested that Josquin spent some of that time in Hungary, based on a mid-16th-century Roman document describing the Hungarian court in those years, and including Josquin as one of the musicians present.

In either 1483 or 1484, Josquin is known to have been in the service of the Sforza family in Milan. While in their employ, he made one or more trips to Rome, and possibly also to Paris; while in Milan he made the acquaintance of Franchinus Gaffurius, who was "maestro di cappella" of the cathedral there. He was in Milan again in 1489, after a possible period of travel; but he left that year.

From 1489 to 1495, Josquin was a member of the papal choir, first under Pope Innocent VIII, and later under the Borgia pope Alexander VI. He may have gone there as part of a singer exchange with Gaspar van Weerbeke, who went back to Milan at the same time. While there, he may have been the one who carved his name into the wall of the Sistine Chapel; a "JOSQUINJ" was recently revealed by workers restoring the chapel. Since it was traditional for singers to carve their names into the walls, and hundreds of names were inscribed there during the period from the 15th to the 18th centuries, it is considered highly likely that the graffiti is by Josquin—and if so, it would be his only surviving autograph.

Josquin's mature style evolved during this period; as in Milan he had absorbed the influence of light Italian secular music, in Rome he refined his techniques of sacred music. Several of his motets have been dated to the years he spent at the papal chapel.

Around 1498, Josquin most likely re-entered the service of the Sforza family, on the evidence of a pair of letters between the Gonzaga and Sforza families. He probably did not stay in Milan long, for in 1499 Louis XII captured Milan in his invasion of northern Italy and imprisoned Josquin's former employers. Around this time Josquin most likely returned to France, although documented details of his career around the turn of the 16th century are lacking. Prior to departing Italy he most likely wrote one of his most famous secular compositions, the frottola "El grillo" (the Cricket), as well as "In te Domine speravi" ("I have placed my hope in you, Lord"), based on Psalm 30. The latter composition may have been a veiled reference to the religious reformer Girolamo Savonarola, who had been burned at the stake in Florence in 1498, and for whom Josquin seems to have had a special reverence; the text was the Dominican friar's favorite psalm, a meditation on which he left incomplete in prison prior to his execution.

Some of Josquin's compositions, such as the instrumental "Vive le roy", have been tentatively dated to the period around 1500 when he was in France. A motet, "Memor esto verbi tui servo tuo" ("Remember thy promise unto thy servant"), was, according to Heinrich Glarean writing in the "Dodecachordon" of 1547, composed as a gentle reminder to the king to keep his promise of a benefice to Josquin, which he had forgotten to keep. According to Glarean's story, it worked: the court applauded, and the king gave Josquin his benefice. Upon receiving it, Josquin reportedly wrote a motet on the text "Benefecisti servo tuo, Domine" ("Lord, thou hast dealt graciously with thy servant") to show his gratitude to the king.

Josquin probably remained in the service of Louis XII until 1503, when Duke Ercole I of Ferrara hired him for the chapel there. One of the rare mentions of Josquin's personality survives from this time. Prior to hiring Josquin, one of Duke Ercole's assistants recommended that he hire Heinrich Isaac instead, since Isaac was easier to get along with, more companionable, was more willing to compose on demand, and would cost significantly less (120 ducats vs. 200). Ercole, however, chose Josquin.

While in Ferrara, Josquin wrote some of his most famous compositions, including the austere, Savonarola-influenced "Miserere", which became one of the most widely distributed motets of the 16th century; the utterly contrasting, virtuoso motet "Virgo salutiferi"; and possibly the "Missa Hercules Dux Ferrariae", which is written on a "cantus firmus" derived from the musical letters in the Duke's name, a technique known as "soggetto cavato".

Josquin did not stay in Ferrara long. An outbreak of the plague in the summer of 1503 prompted the evacuation of the Duke and his family, as well as two-thirds of the citizens, and Josquin left by April of the next year, possibly also to escape the plague. His replacement, Jacob Obrecht, died of the plague in the summer of 1505, to be replaced by Antoine Brumel in 1506, who stayed until the disbanding of the chapel in 1510.

Josquin went directly from Ferrara to his home region of Condé-sur-l'Escaut, southeast of Lille on the present-day border between Belgium and France, becoming provost of the collegiate church of Notre-Dame on 3 May 1504, a large musical establishment that he headed for the rest of his life. While the chapter at Bourges Cathedral asked him to become master of the choirboys there in 1508, it is not known how he responded, and there is no record of his having been employed there; most scholars presume he remained in Condé. In 1509, he held concurrently provost and choir master offices at Saint Quentin collegiate church.

During the last two decades of his life, Josquin's fame spread abroad along with his music. The newly developed technology of printing made wide dissemination of his music possible, and Josquin was the favorite of the first printers: one of Petrucci's first publications, and the earliest surviving print of music by a single composer, was a book of Josquin's masses which he printed in Venice in 1502. This publication was successful enough that Petrucci published two further volumes of Josquin's masses, in 1504 and 1514, and reissued them several times.

On his death-bed, Josquin asked that he be listed on the rolls as a foreigner, so that his property would not pass to the Lords and Ladies of Condé. This bit of evidence has been used to show that he was French by birth. Additionally, he left an endowment for the performance of his late motet, "Pater noster", at all general processions in the town when they passed in front of his house, stopping to place a wafer on the marketplace altar to the Holy Virgin. "Pater noster" may have been his last work.

Josquin lived during a transitional stage in music history. Musical styles were changing rapidly, in part owing to the movement of musicians between different regions of Europe. Many northern musicians moved to Italy, the heart of the Renaissance, attracted by the Italian nobility's patronage of the arts; while in Italy, these composers were influenced by the native Italian styles, and often brought those ideas with them back to their homelands. The sinuous musical lines of the Ockeghem generation, the contrapuntal complexity of the Netherlanders, and the homophonic textures of the Italian lauda and secular music began to merge into a unified style; indeed Josquin was to be the leading figure in this musical process, which eventually resulted in the formation of an international musical language, of which the most famous composers included Palestrina and Lassus.

Josquin likely learned his craft in his home region in the North, in France, and then in Italy when he went to Milan and Rome. His early sacred works emulate the contrapuntal complexity and ornamented, melismatic lines of Ockeghem and his contemporaries, but at the same time he was learning his contrapuntal technique he was acquiring an Italianate idiom for his secular music: after all, he was surrounded by Italian popular music in Milan. By the end of his long creative career, which spanned approximately 50 productive years, he had developed a simplified style in which each voice of a polyphonic composition exhibited free and smooth motion, and close attention was paid to clear setting of text as well as clear alignment of text with musical motifs. While other composers were influential on the development of Josquin's style, especially in the late 15th century, he himself became the most influential composer in Europe, especially after the development of music printing, which was concurrent with the years of his maturity and peak output. This event made his influence even more decisive than it might otherwise have been.

Many "modern" musical compositional practices were being born in the era around 1500. Josquin made extensive use of "motivic cells" in his compositions, short, easily recognizable melodic fragments which passed from voice to voice in a contrapuntal texture, giving it an inner unity. This is a basic organizational principle in music which has been practiced continuously from approximately 1500 until the present day.

Josquin wrote in all of the important forms current at the time, including masses, motets, chansons, and frottole. He even contributed to the development of a new form, the motet-chanson, of which he left at least three examples. In addition, some of his pieces were probably intended for instrumental performance.

Each area of his output can be further subdivided by form or by hypothetical period of composition. Since dating Josquin's compositions is particularly problematic, with scholarly consensus only achieved on a minority of works, discussion here is by type.

Josquin wrote towards the end of the period in which the mass was the predominant form of sacred composition in Europe. The mass, as it had developed through the 15th century, was a long, multi-section form, with opportunities for large-scale structure and organization not possible in the other forms such as the motet. Josquin wrote some of the most famous examples of the genre, most using some kind of cyclic organization.

He wrote masses using the following general techniques, although there is considerable overlap between techniques in individual compositions:

Most of these techniques, particularly paraphrase and parody, became standardized during the first half of the 16th century; Josquin was very much a pioneer, and what was perceived by later observers as the mixing of these techniques was actually the process by which they were created.

Prior to Josquin's mature period, the most common technique for writing masses was the cantus firmus, a technique which had been in use already for most of the 15th century. It was the technique that Josquin used earliest in his career, with the "Missa L'ami Baudichon", possibly his first mass. This mass is based on a secular—indeed ribald—tune similar to "Three Blind Mice". That basing a mass on such a source was an accepted procedure is evident from the existence of the mass in Sistine Chapel part-books copied during the papacy of Julius II (1503 to 1513).

Josquin's most famous cantus-firmus masses are the two based on the L'homme armé tune, which was the favorite tune for mass composition of the entire Renaissance. The earlier of the two, "Missa L'homme armé super voces musicales", is a technical tour-de-force on the tune, containing numerous mensuration canons and contrapuntal display. It was by far the most famous of all his masses. The second, "Missa L'homme armé sexti toni", is a "fantasia on the theme of the armed man." While based on a cantus firmus, it is also a paraphrase mass, for fragments of the tune appear in all voices. Technically it is almost restrained, compared to the other "L'homme armé" mass, until the closing Agnus Dei, which contains a complex canonic structure including a rare retrograde canon, around which other voices are woven.

The paraphrase technique differs from the cantus-firmus technique in that the source material, though it still consists of a monophonic original, is embellished, often with ornaments. As in the cantus-firmus technique, the source tune may appear in many voices of the mass.

Several of Josquin's masses feature the paraphrase technique, and they include some of his most famous work including the great "Missa Gaudeamus". The relatively early "Missa Ave maris stella", which probably dates from his years in the Sistine Chapel choir, paraphrases the Marian antiphon of the same name; it is also one of his shortest masses. The late "Missa de Beata Virgine" paraphrases plainchants in praise of the Virgin Mary; it is a Lady Mass, a votive mass for Saturday performance, and was his most popular mass in the 16th century.

By far the most famous of Josquin's masses using the technique, and one of the most famous mass settings of the entire era, was the "Missa pange lingua", based on the hymn by Thomas Aquinas for the Vespers of Corpus Christi. It was probably the last mass that Josquin composed. This mass is an extended fantasia on the tune, using the melody in all voices and in all parts of the mass, in elaborate and ever-changing polyphony. One of the high points of the mass is the "et incarnatus est" section of the Credo, where the texture becomes homophonic, and the tune appears in the topmost voice; here the portion which would normally set "Sing, O my tongue, of the mystery of the divine body" is instead given the words "And he became incarnate by the Holy Ghost from the Virgin Mary, and was made man."

In parody masses, the source material was not a single line, but an entire texture, often of a popular song. Several works by Josquin fall loosely into this category, including the "Missa Fortuna desperata", based on the three-voice song "Fortuna desperata" (possibly by Antoine Busnois); the "Missa Malheur me bat" (based on a chanson variously ascribed to Obrecht, Ockeghem, or, most likely, Abertijne Malcourt); and the "Missa Mater Patris", based on a three-voice motet by Antoine Brumel. The "Missa Mater Patris" is probably the first true parody mass to be composed, for it no longer contains any hint of a cantus firmus. Parody technique was to become the most usual means of mass composition for the remainder of the 16th century, although the mass gradually fell out of favor as the motet grew in esteem.

The earliest known mass by any composer using this method of composition—the "soggetto cavato"—is the "Missa Hercules Dux Ferrariae", which Josquin probably wrote in the early 1480s for the powerful Ercole I, Duke of Ferrara. The notes of the cantus firmus are drawn from the musical syllables of the Duke's name in the following way: "Ercole, Duke of Ferrara" in Latin is "Hercules Dux Ferrarie". Taking the solmization syllables with the same vowels gives: (in modern nomenclature: ). Another mass using this technique is the "Missa La sol fa re mi", based on the musical syllables contained in "Lascia fare mi" ("let me do it"). The story, as told by Glareanus in 1547, was that an unknown aristocrat used to order suitors away with this phrase, and Josquin immediately wrote an "exceedingly elegant" mass on it as a jab at him.

Canonic masses came into increasing prominence in the latter part of the 15th century. Early examples include Ockeghem's famous "Missa prolationum", consisting entirely of mensuration canons, the "Missa L'homme armé" of Guillaume Faugues, whose cantus firmus is presented in canon at the descending fifth, the "Missa" ["Ad fugam"] of Marbrianus de Orto, based on freely composed canons at the fifth between superius and tenor, and the two great canonic masses of Josquin, the "Missa Ad fugam" and "Missa Sine nomine". Josquin makes use of canon in the Osanna and Agnus Dei III of the "Missa L'homme armé sexti toni", throughout the "Missa Sine nomine" and "Missa Ad fugam", and in the final three movements of the "Missa De beata virgine". The "Missa L'homme armé super voces musicales" incorporates mensuration canons in the Kyrie, Benedictus, and Agnus Dei II.

Josquin's motet style varied from almost strictly homophonic settings with block chords and syllabic text declamation to highly ornate contrapuntal fantasias, to the psalm settings which combined these extremes with the addition of rhetorical figures and text-painting that foreshadowed the later development of the madrigal. He wrote many of his motets for four voices, an ensemble size which had become the compositional norm around 1500, and he was also a considerable innovator in writing motets for five and six voices. No motets of more than six voices have been reliably attributed to Josquin.

Almost all of Josquin's motets use some kind of compositional constraint on the process; they are not freely composed. Some of them use a cantus firmus as a unifying device; some are canonic; some use a motto which repeats throughout; some use several of these methods. The motets that use canon can be roughly divided into two groups: those in which the canon is plainly designed to be heard and appreciated as such, and another group in which a canon is present, but almost impossible to hear, and seemingly written to be appreciated by the eye, and by connoisseurs.

Josquin frequently used imitation, especially paired imitation, in writing his motets, with sections akin to fugal expositions occurring on successive lines of the text he was setting. An example is his setting of "Dominus regnavit" (Psalm 93), for four voices; each of the lines of the psalm begins with a voice singing a new tune alone, quickly followed by entries of other three voices in imitation.

In writing polyphonic settings of psalms, Josquin was a pioneer, and psalm settings form a large proportion of the motets of his later years. Few composers prior to Josquin had written polyphonic psalm settings. Some of Josquin's settings include the famous "Miserere", written in Ferrara in 1503 or 1504 and most likely inspired by the recent execution of the reformist monk Girolamo Savonarola, "Memor esto verbi tui", based on Psalm 119, and two settings of "De profundis" (Psalm 130), both of which are often considered to be among his most significant accomplishments.

In the domain of secular music, Josquin left numerous French chansons, for from three to six voices, as well as a handful of Italian secular songs known as frottole, as well as some pieces which were probably intended for instrumental performance. Problems of attribution are even more acute with the chansons than they are with other portions of his output: while about 70 three and four-voice chansons were published under his name during his lifetime, only six of the more than thirty five- and six-voice chansons attributed to him were circulated under his name during the same time. Many of the attributions added after his death are considered to be unreliable, and much work has been done in the last decades of the 20th century to correct attributions on stylistic grounds.

Josquin's earliest chansons were probably composed in northern Europe, under the influence of composers such as Ockeghem and Busnois. Unlike them, however, he never adhered strictly to the conventions of the "formes fixes"—the rigid and complex repetition patterns of the rondeau, virelai, and ballade—instead he often wrote his early chansons in strict imitation, a feature they shared with many of his sacred works. He was one of the first composers of chansons to make all voices equal parts of the texture; and many of his chansons contain points of imitation, in the manner of motets. However he did use melodic repetition, especially where the lines of text rhymed, and many of his chansons had a lighter texture, as well as a faster tempo, than his motets.

Inside of his chansons, he often used a cantus firmus, sometimes a popular song whose origin can no longer be traced, as in "Si j'avoye Marion". Other times he used a tune originally associated with a separate text; and still other times he freely composed an entire song, using no apparent external source material. Another technique he sometimes used was to take a popular song and write it as a canon with itself, in two inner voices, and write new melodic material above and around it, to a new text: he used this technique in one of his most famous chansons, "Faulte d'argent" ("The problem with money"), a song sung by a man who wakes in bed with a prostitute, broke and unable to pay her.

Some of his chansons were doubtless designed to be performed instrumentally. That Petrucci published many of them without text is strong evidence of this; additionally, some of the pieces (for example, the fanfare-like "Vive le roy") contain writing more idiomatic for instruments than voices.

Josquin's most famous chansons circulated widely in Europe. Some of the better known include his lament on the death of Ockeghem, "Nymphes des bois/Requiem aeternam"; "Mille regretz" (the attribution of which has recently been questioned); "Plus nulz regretz"; and "Je me complains".

In addition to his French chansons, he wrote at least three pieces in the manner of the Italian frottola, a popular Italian song form which he would have encountered during his years in Milan. These songs include "Scaramella", "El grillo", and "In te domine speravi". They are even simpler in texture than his French chansons, being almost uniformly syllabic and homophonic, and they remain among the most frequently sung portions of his output.

While in Milan, Josquin wrote several examples of a new type of piece developed by the composers there, the motet-chanson. These compositions were texturally very similar to 15th century chansons in the "formes fixes" mold, except that unlike those completely secular works, they contained a chant-derived Latin cantus-firmus in the lowest of the three voices. The other voices, in French, sang a secular text which had either a symbolic relationship to the sacred Latin text, or commented on it. Josquin's three known motet-chansons, "Que vous madame/In pace", "A la mort/Monstra te esse matrem", and "Fortune destrange plummaige/Pauper sum ego", are similar stylistically to those by the other composers of the Milan chapel, such as Loyset Compère and Alexander Agricola.

Josquin's fame lasted throughout the 16th century, and indeed increased for several decades after his death. Zarlino, writing in the 1580s, was still using examples from Josquin in his treatises on composition; and Josquin's fame was only eclipsed after the beginning of the Baroque era, with the decline of the pre-tonal polyphonic style. During the 18th and 19th centuries Josquin's fame was overshadowed by later Roman School composer Palestrina, whose music was seen as the summit of polyphonic refinement, and codified into a system of composition by theorists such as Johann Fux; however, during the 20th century, Josquin's reputation has grown steadily, to the point where scholars again consider him "the greatest and most successful composer of the age." According to Richard Sherr, writing in the introduction to the "Josquin Companion", addressing specifically the shrinking of Josquin's canon due to correction of misattributions, "Josquin will survive because his best music really is as magnificent as everybody has always said it was."

Since the 1950s Josquin's reputation has been boosted by the increasing availability of recordings, of which there are many, and the rise of ensembles specializing in the performance of 16th century vocal music, many of which place Josquin's output at the heart of their repertoire.

The difficulties in compiling a works list for Josquin cannot be overstated. Because of his immense prestige in the early sixteenth century, many scribes and publishers did not resist the temptation of attributing anonymous or otherwise spurious works to Josquin. The German editor Georg Forster summed up the situation admirably in 1540 when he wrote, "I remember a certain eminent man saying that, now that Josquin is dead, he is putting out more works than when he was alive." Thus, the authenticity of many of the works listed below is disputed on stylistic grounds or problems with sources or both. This thorny issue has been taken up vigorously in the now nearly complete "New Josquin Edition" (NJE).


Doubtful works:








</doc>
<doc id="55115" url="https://en.wikipedia.org/wiki?curid=55115" title="Cabbage">
Cabbage

Cabbage or headed cabbage (comprising several cultivars of "Brassica oleracea") is a leafy green, red (purple), or white (pale green) biennial plant grown as an annual vegetable crop for its dense-leaved heads. It is descended from the wild cabbage, "B. oleracea" var. "oleracea", and belongs to the "cole crops", meaning it is closely related to broccoli and cauliflower (var. "botrytis"); Brussels sprouts (var. "gemmifera"); and savoy cabbage (var. "sabauda"). "Brassica rapa" is commonly named Chinese, celery or napa cabbage and has many of the same uses. Cabbage is high in nutritional value.

Cabbage heads generally range from , and can be green, purple or white. Smooth-leafed, firm-headed green cabbages are the most common. Smooth-leafed purple cabbages and crinkle-leafed savoy cabbages of both colors are rarer. It is a multi-layered vegetable. Under conditions of long sunny days, such as those found at high northern latitudes in summer, cabbages can grow quite large. , the heaviest cabbage was . 

Cabbage was most likely domesticated somewhere in Europe before 1000 BC, although savoys were not developed until the 16th century AD. By the Middle Ages, cabbage had become a prominent part of European cuisine. Cabbage heads are generally picked during the first year of the plant's life cycle, but plants intended for seed are allowed to grow a second year and must be kept separate from other cole crops to prevent cross-pollination. Cabbage is prone to several nutrient deficiencies, as well as to multiple pests, and bacterial and fungal diseases.

Cabbages are prepared many different ways for eating; they can be pickled, fermented (for dishes such as sauerkraut), steamed, stewed, sautéed, braised, or eaten raw. Cabbage is a good source of vitamin K, vitamin C and dietary fiber. The Food and Agriculture Organization of the United Nations (FAO) reported that world production of cabbage and other brassicas for 2014 was 71.8 million metric tonnes, with China accounting for 47% of the world total.

Cabbage ("Brassica oleracea" or "B. oleracea" var. "capitata", var. "tuba", var. "sabauda" or var. "acephala") is a member of the genus "Brassica" and the mustard family, Brassicaceae. Several other cruciferous vegetables (sometimes known as "cole crops") are considered cultivars of "B. oleracea", including broccoli, collard greens, brussels sprouts, kohlrabi and sprouting broccoli. All of these developed from the wild cabbage "B. oleracea" var. "oleracea", also called colewort or field cabbage. This original species evolved over thousands of years into those seen today, as selection resulted in cultivars having different characteristics, such as large heads for cabbage, large leaves for kale and thick stems with flower buds for broccoli.

The varietal epithet "capitata" is derived from the Latin word for "having a head". "B. oleracea" and its derivatives have hundreds of common names throughout the world.

"Cabbage" was originally used to refer to multiple forms of "B. oleracea", including those with loose or non-existent heads. A related species, "Brassica rapa", is commonly named Chinese, napa or celery cabbage, and has many of the same uses. It is also a part of common names for several unrelated species. These include cabbage bark or cabbage tree (a member of the genus "Andira") and cabbage palms, which include several genera of palms such as "Mauritia", "Roystonea oleracea", "Acrocomia" and "Euterpe oenocarpus".

The original family name of brassicas was "Cruciferae", which derived from the flower petal pattern thought by medieval Europeans to resemble a crucifix. The word "brassica" derives from "bresic", a Celtic word for cabbage. Many European and Asiatic names for cabbage are derived from the Celto-Slavic root "cap" or "kap", meaning "head". The late Middle English word "cabbage" derives from the word "caboche" ("head"), from the Picard dialect of Old French. This in turn is a variant of the Old French "caboce". Through the centuries, "cabbage" and its derivatives have been used as slang for numerous items, occupations and activities. Cash and tobacco have both been described by the slang "cabbage", while "cabbage-head" means a fool or stupid person and "cabbaged" means to be exhausted or, vulgarly, in a vegetative state.

Cabbage seedlings have a thin taproot and cordate (heart-shaped) cotyledon. The first leaves produced are ovate (egg-shaped) with a lobed petiole. Plants are tall in their first year at the mature vegetative stage, and tall when flowering in the second year. Heads average between , with fast-growing, earlier-maturing varieties producing smaller heads. Most cabbages have thick, alternating leaves, with margins that range from wavy or lobed to highly dissected; some varieties have a waxy bloom on the leaves. Plants have root systems that are fibrous and shallow. About 90 percent of the root mass is in the upper of soil; some lateral roots can penetrate up to deep.

The inflorescence is an unbranched and indeterminate terminal raceme measuring tall, with flowers that are yellow or white. Each flower has four petals set in a perpendicular pattern, as well as four sepals, six stamens, and a superior ovary that is two-celled and contains a single stigma and style. Two of the six stamens have shorter filaments. The fruit is a silique that opens at maturity through dehiscence to reveal brown or black seeds that are small and round in shape. Self-pollination is impossible, and plants are cross-pollinated by insects. The initial leaves form a rosette shape comprising 7 to 15 leaves, each measuring by ; after this, leaves with shorter petioles develop and heads form through the leaves cupping inward.

Many shapes, colors and leaf textures are found in various cultivated varieties of cabbage. Leaf types are generally divided between crinkled-leaf, loose-head savoys and smooth-leaf firm-head cabbages, while the color spectrum includes white and a range of greens and purples. Oblate, round and pointed shapes are found.

Cabbage has been selectively bred for head weight and morphological characteristics, frost hardiness, fast growth and storage ability. The appearance of the cabbage head has been given importance in selective breeding, with varieties being chosen for shape, color, firmness and other physical characteristics. Breeding objectives are now focused on increasing resistance to various insects and diseases and improving the nutritional content of cabbage. Scientific research into the genetic modification of "B. oleracea" crops, including cabbage, has included European Union and United States explorations of greater insect and herbicide resistance.

Although cabbage has an extensive history, it is difficult to trace its exact origins owing to the many varieties of leafy greens classified as "brassicas". The wild ancestor of cabbage, "Brassica oleracea", originally found in Britain and continental Europe, is tolerant of salt but not encroachment by other plants and consequently inhabits rocky cliffs in cool damp coastal habitats, retaining water and nutrients in its slightly thickened, turgid leaves. According to the triangle of U theory of the evolution and relationships between "Brassica" species, "B. oleracea" and other closely related kale vegetables (cabbages, kale, broccoli, Brussels sprouts, and cauliflower) represent one of three ancestral lines from which all other brassicas originated.

Cabbage was probably domesticated later in history than Near Eastern crops such as lentils and summer wheat. Because of the wide range of crops developed from the wild "B. oleracea", multiple broadly contemporaneous domestications of cabbage may have occurred throughout Europe. Nonheading cabbages and kale were probably the first to be domesticated, before 1000 BC, perhaps by the Celts of central and western Europe. Recent linguistic and genetic evidence enforces a Mediterranean origin of cultivated brassicas.

Unidentified brassicas were part of the highly conservative unchanging Mesopotamian garden repertory.

It is believed that the ancient Egyptians did not cultivate cabbage, which is not native to the Nile valley, though a word "shaw't" in Papyrus Harris of the time of Ramesses III, has been interpreted as "cabbage". Ptolemaic Egyptians knew the cole crops as "gramb", under the influence of Greek "krambe", which had been a familiar plant to the Macedonian antecedents of the Ptolemies. By early Roman times, Egyptian artisans and children were eating cabbage and turnips among a wide variety of other vegetables and pulses.

The ancient Greeks had some varieties of cabbage, as mentioned by Theophrastus, although whether they were more closely related to today's cabbage or to one of the other "Brassica" crops is unknown. The headed cabbage variety was known to the Greeks as "krambe" and to the Romans as "brassica" or "olus"; the open, leafy variety (kale) was known in Greek as "raphanos" and in Latin as "caulis".

Chrysippus of Cnidos wrote a treatise on cabbage, which Pliny knew, but it has not survived. The Greeks were convinced that cabbages and grapevines were inimical, and that cabbage planted too near the vine would impart its unwelcome odor to the grapes; this Mediterranean sense of antipathy survives today.

"Brassica" was considered by some Romans a table luxury, although Lucullus considered it unfit for the senatorial table. The more traditionalist Cato the Elder, espousing a simple Republican life, ate his cabbage cooked or raw and dressed with vinegar; he said it surpassed all other vegetables, and approvingly distinguished three varieties; he also gave directions for its medicinal use, which extended to the cabbage-eater's urine, in which infants might be rinsed. Pliny the Elder listed seven varieties, including Pompeii cabbage, Cumae cabbage and Sabellian cabbage. According to Pliny, the Pompeii cabbage, which could not stand cold, is "taller, and has a thick stock near the root, but grows thicker between the leaves, these being scantier and narrower, but their tenderness is a valuable quality". The Pompeii cabbage was also mentioned by Columella in "De Re Rustica". Apicius gives several recipes for "cauliculi", tender cabbage shoots. The Greeks and Romans claimed medicinal usages for their cabbage varieties that included relief from gout, headaches and the symptoms of poisonous mushroom ingestion. The antipathy towards the vine made it seem that eating cabbage would enable one to avoid drunkenness. Cabbage continued to figure in the "materia medica" of antiquity as well as at table: in the first century AD Dioscorides mentions two kinds of coleworts with medical uses, the cultivated and the wild, and his opinions continued to be paraphrased in herbals right through the 17th century.

At the end of Antiquity cabbage is mentioned in "De observatione ciborum" ("On the Observance of Foods") of Anthimus, a Greek doctor at the court of Theodoric the Great, and cabbage appears among vegetables directed to be cultivated in the "Capitulare de villis", composed c. 771-800 that guided the governance of the royal estates of Charlemagne.

In Britain, the Anglo-Saxons cultivated "cawel". When round-headed cabbages appeared in 14th-century England they were called "cabaches" and "caboches", words drawn from Old French and applied at first to refer to the ball of unopened leaves, the contemporaneous recipe that commences "Take cabbages and quarter them, and seethe them in good broth", also suggests the tightly headed cabbage.
Manuscript illuminations show the prominence of cabbage in the cuisine of the High Middle Ages, and cabbage seeds feature among the seed list of purchases for the use of King John II of France when captive in England in 1360, but cabbages were also a familiar staple of the poor: in the lean year of 1420 the "Bourgeois of Paris" noted that "poor people ate no bread, nothing but cabbages and turnips and such dishes, without any bread or salt". French naturalist Jean Ruel made what is considered the first explicit mention of head cabbage in his 1536 botanical treatise "De Natura Stirpium", referring to it as "capucos coles" ("head-coles"). Sir Anthony Ashley, 1st Baronet, did not disdain to have a cabbage at the foot of his monument in Wimborne St Giles.

In Istanbul, Sultan Selim III penned a tongue-in-cheek ode to cabbage: without cabbage, the halva feast was not complete. Cabbages spread from Europe into Mesopotamia and Egypt as a winter vegetable, and later followed trade routes throughout Asia and the Americas. The absence of Sanskrit or other ancient Eastern language names for cabbage suggests that it was introduced to South Asia relatively recently. In India, cabbage was one of several vegetable crops introduced by colonizing traders from Portugal, who established trade routes from the 14th to 17th centuries. Carl Peter Thunberg reported that cabbage was not yet known in Japan in 1775.

Many cabbage varieties—including some still commonly grown—were introduced in Germany, France, and the Low Countries. During the 16th century, German gardeners developed the savoy cabbage. During the 17th and 18th centuries, cabbage was a food staple in such countries as Germany, England, Ireland and Russia, and pickled cabbage was frequently eaten. Sauerkraut was used by Dutch, Scandinavian and German sailors to prevent scurvy during long ship voyages.

Jacques Cartier first brought cabbage to the Americas in 1541–42, and it was probably planted by the early English colonists, despite the lack of written evidence of its existence there until the mid-17th century. By the 18th century, it was commonly planted by both colonists and native American Indians. Cabbage seeds traveled to Australia in 1788 with the First Fleet, and were planted the same year on Norfolk Island. It became a favorite vegetable of Australians by the 1830s and was frequently seen at the Sydney Markets.

There are several "Guinness Book of World Records" entries related to cabbage. These include the heaviest cabbage, at , heaviest red cabbage, at , longest cabbage roll, at , and the largest cabbage dish, at . In 2012, Scott Robb of Palmer, Alaska, broke the world record for heaviest cabbage at .

Cabbage is generally grown for its densely leaved heads, produced during the first year of its biennial cycle. Plants perform best when grown in well-drained soil in a location that receives full sun. Different varieties prefer different soil types, ranging from lighter sand to heavier clay, but all prefer fertile ground with a pH between 6.0 and 6.8. For optimal growth, there must be adequate levels of nitrogen in the soil, especially during the early head formation stage, and sufficient phosphorus and potassium during the early stages of expansion of the outer leaves. Temperatures between prompt the best growth, and extended periods of higher or lower temperatures may result in premature bolting (flowering). Flowering induced by periods of low temperatures (a process called vernalization) only occurs if the plant is past the juvenile period. The transition from a juvenile to adult state happens when the stem diameter is about . Vernalization allows the plant to grow to an adequate size before flowering. In certain climates, cabbage can be planted at the beginning of the cold period and survive until a later warm period without being induced to flower, a practice that was common in the eastern US.
Plants are generally started in protected locations early in the growing season before being transplanted outside, although some are seeded directly into the ground from which they will be harvested. Seedlings typically emerge in about 4–6 days from seeds planted deep at a soil temperature between . Growers normally place plants apart. Closer spacing reduces the resources available to each plant (especially the amount of light) and increases the time taken to reach maturity. Some varieties of cabbage have been developed for ornamental use; these are generally called "flowering cabbage". They do not produce heads and feature purple or green outer leaves surrounding an inner grouping of smaller leaves in white, red, or pink. Early varieties of cabbage take about 70 days from planting to reach maturity, while late varieties take about 120 days. Cabbages are mature when they are firm and solid to the touch. They are harvested by cutting the stalk just below the bottom leaves with a blade. The outer leaves are trimmed, and any diseased, damaged, or necrotic leaves are removed. Delays in harvest can result in the head splitting as a result of expansion of the inner leaves and continued stem growth. Factors that contribute to reduced head weight include: growth in the compacted soils that result from no-till farming practices, drought, waterlogging, insect and disease incidence, and shading and nutrient stress caused by weeds.

When being grown for seed, cabbages must be isolated from other "B. oleracea" subspecies, including the wild varieties, by to prevent cross-pollination. Other "Brassica" species, such as "B. rapa", "B. juncea", "B. nigra", "B. napus" and "Raphanus sativus", do not readily cross-pollinate.

There are several cultivar groups of cabbage, each including many cultivars:

Some sources only delineate three cultivars: savoy, red and white, with spring greens and green cabbage being subsumed into the latter.

Due to its high level of nutrient requirements, cabbage is prone to nutrient deficiencies, including boron, calcium, phosphorus and potassium. There are several physiological disorders that can affect the postharvest appearance of cabbage. Internal tip burn occurs when the margins of inside leaves turn brown, but the outer leaves look normal. Necrotic spot is where there are oval sunken spots a few millimeters across that are often grouped around the midrib. In pepper spot, tiny black spots occur on the areas between the veins, which can increase during storage.

Fungal diseases include wirestem, which causes weak or dying transplants; "Fusarium" yellows, which result in stunted and twisted plants with yellow leaves; and blackleg (see "Leptosphaeria maculans"), which leads to sunken areas on stems and gray-brown spotted leaves. The fungi "Alternaria brassicae" and "A. brassicicola" cause dark leaf spots in affected plants. They are both seedborne and airborne, and typically propagate from spores in infected plant debris left on the soil surface for up to twelve weeks after harvest. "Rhizoctonia solani" causes the post-emergence disease wirestem, resulting in killed seedlings ("damping-off"), root rot or stunted growth and smaller heads.

One of the most common bacterial diseases to affect cabbage is black rot, caused by "Xanthomonas campestris", which causes chlorotic and necrotic lesions that start at the leaf margins, and wilting of plants. Clubroot, caused by the soilborne slime mold-like organism "Plasmodiophora brassicae", results in swollen, club-like roots. Downy mildew, a parasitic disease caused by the oomycete "Peronospora parasitica", produces pale leaves with white, brownish or olive mildew on the lower leaf surfaces; this is often confused with the fungal disease powdery mildew.

Pests include root-knot nematodes and cabbage maggots, which produce stunted and wilted plants with yellow leaves; aphids, which induce stunted plants with curled and yellow leaves; harlequin bugs, which cause white and yellow leaves; thrips, which lead to leaves with white-bronze spots; striped flea beetles, which riddle leaves with small holes; and caterpillars, which leave behind large, ragged holes in leaves. The caterpillar stage of the "small cabbage white butterfly" ("Pieris rapae"), commonly known in the United States as the "imported cabbage worm", is a major cabbage pest in most countries. The large white butterfly ("Pieris brassicae") is prevalent in eastern European countries. The diamondback moth ("Plutella xylostella") and the cabbage moth ("Mamestra brassicae") thrive in the higher summer temperatures of continental Europe, where they cause considerable damage to cabbage crops. The cabbage looper ("Trichoplusia ni") is infamous in North America for its voracious appetite and for producing frass that contaminates plants. In India, the diamondback moth has caused losses up to 90 percent in crops that were not treated with insecticide. Destructive soil insects include the cabbage root fly ("Delia radicum") and the cabbage maggot ("Hylemya brassicae"), whose larvae can burrow into the part of plant consumed by humans.

Planting near other members of the cabbage family, or where these plants have been placed in previous years, can prompt the spread of pests and disease. Excessive water and excessive heat can also cause cultivation problems.

In 2014, global production of cabbages (combined with other brassicas) was 71.8 million tonnes, led by China with 47% of the world total (table). Other major producers were India, Russia, and South Korea.

Cabbages sold for market are generally smaller, and different varieties are used for those sold immediately upon harvest and those stored before sale. Those used for processing, especially sauerkraut, are larger and have a lower percentage of water. Both hand and mechanical harvesting are used, with hand-harvesting generally used for cabbages destined for market sales. In commercial-scale operations, hand-harvested cabbages are trimmed, sorted, and packed directly in the field to increase efficiency. Vacuum cooling rapidly refrigerates the vegetable, allowing for earlier shipping and a fresher product. Cabbage can be stored the longest at with a humidity of 90–100 percent; these conditions will result in up to six months of longevity. When stored under less ideal conditions, cabbage can still last up to four months.

Cabbage consumption varies widely around the world: Russia has the highest annual per capita consumption at , followed by Belgium at , the Netherlands at , and Spain at . Americans consume annually per capita.

Cabbage is prepared and consumed in many ways. The simplest options include eating the vegetable raw or steaming it, though many cuisines pickle, stew, sautée or braise cabbage. Pickling is one of the most popular ways of preserving cabbage, creating dishes such as sauerkraut and kimchi, although kimchi is more often made from Chinese cabbage ("B. rapa"). Savoy cabbages are usually used in salads, while smooth-leaf types are utilized for both market sales and processing. Bean curd and cabbage is a staple of Chinese cooking, while the British dish bubble and squeak is made primarily with leftover potato and boiled cabbage and eaten with cold meat. In Poland, cabbage is one of the main food crops, and it features prominently in Polish cuisine. It is frequently eaten, either cooked or as sauerkraut, as a side dish or as an ingredient in such dishes as bigos (cabbage, sauerkraut, meat, and wild mushrooms, among other ingredients) gołąbki (stuffed cabbage) and pierogi (filled dumplings). Other eastern European countries, such as Hungary and Romania, also have traditional dishes that feature cabbage as a main ingredient. In India and Ethiopia, cabbage is often included in spicy salads and braises. In the United States, cabbage is used primarily for the production of coleslaw, followed by market use and sauerkraut production.

The characteristic flavor of cabbage is caused by glucosinolates, a class of sulfur-containing glucosides. Although found throughout the plant, these compounds are concentrated in the highest quantities in the seeds; lesser quantities are found in young vegetative tissue, and they decrease as the tissue ages. Cooked cabbage is often criticized for its pungent, unpleasant odor and taste. These develop when cabbage is overcooked and hydrogen sulfide gas is produced.

Cabbage is a rich source of vitamin C and vitamin K, containing 44% and 72%, respectively, of the Daily Value (DV) per 100-gram amount (right table of USDA nutrient values). Cabbage is also a moderate source (10–19% DV) of vitamin B6 and folate, with no other nutrients having significant content per 100-gram serving (table).

Basic research on cabbage phytochemicals is ongoing to discern if certain cabbage compounds may affect health or have anti-disease effects. Such compounds include sulforaphane and other glucosinolates which may stimulate the production of detoxifying enzymes during metabolism. Studies suggest that cruciferous vegetables, including cabbage, may have protective effects against colon cancer. Cabbage is a source of indole-3-carbinol, a chemical under basic research for its possible properties.

In addition to its usual purpose as an edible vegetable, cabbage has been used historically as a medicinal herb for a variety of purported health benefits. For example, the Ancient Greeks recommended consuming the vegetable as a laxative, and used cabbage juice as an antidote for mushroom poisoning, for eye salves, and for liniments used to help bruises heal. In "De Agri Cultura" ("On Agriculture"), Cato the Elder suggested that women could prevent diseases by bathing in urine obtained from those who had frequently eaten cabbage. The ancient Roman nobleman Pliny the Elder described both culinary and medicinal properties of the vegetable, recommending it for drunkenness—both preventatively to counter the effects of alcohol and to cure hangovers. Similarly, the Ancient Egyptians ate cooked cabbage at the beginning of meals to reduce the intoxicating effects of wine. This traditional usage persisted in European literature until the mid-20th century.

The cooling properties of the leaves were used in Britain as a treatment for trench foot in World War I, and as compresses for ulcers and breast abscesses. Accumulated scientific evidence corroborates that cabbage leaf treatment can reduce the pain and hardness of engorged breasts, and increase the duration of breast feeding. Other medicinal uses recorded in European folk medicine include treatments for rheumatism, sore throat, hoarseness, colic, and melancholy. In the United States, cabbage has been used as a hangover cure, to treat abscesses, to prevent sunstroke, or to cool body parts affected by fevers. The leaves have also been used to soothe sore feet and, when tied around a child's neck, to relieve croup. Both mashed cabbage and cabbage juice have been used in poultices to remove boils and treat warts, pneumonia, appendicitis, and ulcers.

Excessive consumption of cabbage may lead to increased intestinal gas which causes bloating and flatulence due to the trisaccharide raffinose, which the human small intestine cannot digest.

Cabbage has been linked to outbreaks of some food-borne illnesses, including "Listeria monocytogenes" and "Clostridium botulinum". The latter toxin has been traced to pre-made, packaged coleslaw mixes, while the spores were found on whole cabbages that were otherwise acceptable in appearance. "Shigella" species are able to survive in shredded cabbage. Two outbreaks of "E. coli" in the United States have been linked to cabbage consumption. Biological risk assessments have concluded that there is the potential for further outbreaks linked to uncooked cabbage, due to contamination at many stages of the growing, harvesting and packaging processes. Contaminants from water, humans, animals and soil have the potential to be transferred to cabbage, and from there to the end consumer.

Cabbage and other cruciferous vegetables contain small amounts of thiocyanate, a compound associated with goiter formation when iodine intake is deficient.




</doc>
<doc id="55791" url="https://en.wikipedia.org/wiki?curid=55791" title="Voting Rights Act of 1965">
Voting Rights Act of 1965

The Voting Rights Act of 1965 is a landmark piece of federal legislation in the United States that prohibits racial discrimination in voting. It was signed into law by President Lyndon B. Johnson during the height of the Civil Rights Movement on August 6, 1965, and Congress later amended the Act five times to expand its protections. Designed to enforce the voting rights guaranteed by the Fourteenth and Fifteenth Amendments to the United States Constitution, the Act secured the right to vote for racial minorities throughout the country, especially in the South. According to the U.S. Department of Justice, the Act is considered to be the most effective piece of federal civil rights legislation ever enacted in the country.

The Act contains numerous provisions that regulate elections. The Act's "general provisions" provide nationwide protections for voting rights. Section 2 is a general provision that prohibits every state and local government from imposing any voting law that results in discrimination against racial or language minorities. Other general provisions specifically outlaw literacy tests and similar devices that were historically used to disenfranchise racial minorities.

The Act also contains "special provisions" that apply to only certain jurisdictions. A core special provision is the Section 5 preclearance requirement, which prohibits certain jurisdictions from implementing any change affecting voting without receiving preapproval from the U.S. Attorney General or the U.S. District Court for D.C. that the change does not discriminate against protected minorities. Another special provision requires jurisdictions containing significant language minority populations to provide bilingual ballots and other election materials.

Section 5 and most other special provisions apply to jurisdictions encompassed by the "coverage formula" prescribed in Section 4(b). The coverage formula was originally designed to encompass jurisdictions that engaged in egregious voting discrimination in 1965, and Congress updated the formula in 1970 and 1975. In "Shelby County v. Holder" (2013), the U.S. Supreme Court struck down the coverage formula as unconstitutional, reasoning that it was no longer responsive to current conditions. The Court did not strike down Section 5, but without a coverage formula, Section 5 is unenforceable.

As initially ratified, the United States Constitution granted each state complete discretion to determine voter qualifications for its residents. After the Civil War, the three Reconstruction Amendments were ratified and limited this discretion. The Thirteenth Amendment (1865) prohibits slavery; the Fourteenth Amendment (1868) grants citizenship to anyone "born or naturalized in the United States" and guarantees every person due process and equal protection rights; and the Fifteenth Amendment (1870) provides that "[t]he right of citizens of the United States to vote shall not be denied or abridged by the United States or by any State on account of race, color, or previous condition of servitude." These Amendments also empower Congress to enforce their provisions through "appropriate legislation".

To enforce the Reconstruction Amendments, Congress passed the Enforcement Acts in the 1870s. The Acts criminalized the obstruction of a citizen's voting rights and provided for federal supervision of the electoral process, including voter registration. However, in 1875 the Supreme Court struck down parts of the legislation as unconstitutional in "United States v. Cruikshank" and "United States v. Reese". After the Reconstruction Era ended in 1877, enforcement of these laws became erratic, and in 1894, Congress repealed most of their provisions.

Southern states generally sought to disenfranchise racial minorities during and after Reconstruction. From 1868 to 1888, electoral fraud and violence throughout the South suppressed the African-American vote. From 1888 to 1908, Southern states legalized disenfranchisement by enacting Jim Crow laws; they amended their constitutions and passed legislation to impose various voting restrictions, including literacy tests, poll taxes, property-ownership requirements, moral character tests, requirements that voter registration applicants interpret particular documents, and grandfather clauses that allowed otherwise-ineligible persons to vote if their grandfathers voted (which excluded many African Americans whose grandfathers had been slaves or otherwise ineligible). During this period, the Supreme Court generally upheld efforts to discriminate against racial minorities. In "Giles v. Harris" (1903), the Court held that irrespective of the Fifteenth Amendment, the judiciary did not have the remedial power to force states to register racial minorities to vote.

In the 1950s, the Civil Rights Movement increased pressure on the federal government to protect the voting rights of racial minorities. In 1957, Congress passed the first civil rights legislation since Reconstruction: the Civil Rights Act of 1957. This legislation authorized the Attorney General to sue for injunctive relief on behalf of persons whose Fifteenth Amendment rights were denied, created the Civil Rights Division within the Department of Justice to enforce civil rights through litigation, and created the Commission on Civil Rights to investigate voting rights deprivations. Further protections were enacted in the Civil Rights Act of 1960, which allowed federal courts to appoint referees to conduct voter registration in jurisdictions that engaged in voting discrimination against racial minorities.

Although these acts helped empower courts to remedy violations of federal voting rights, strict legal standards made it difficult for the Department of Justice to successfully pursue litigation. For example, to win a discrimination lawsuit against a state that maintained a literacy test, the Department needed to prove that the rejected voter-registration applications of racial minorities were comparable to the accepted applications of whites. This involved comparing thousands of applications in each of the state's counties in a process that could last months. The Department's efforts were further hampered by resistance from local election officials, who would claim to have misplaced the voter registration records of racial minorities, remove registered racial minorities from the electoral rolls, and resign so that voter registration ceased. Moreover, the Department often needed to appeal lawsuits several times before the judiciary provided relief because many federal district court judges opposed racial minority suffrage. Thus, between 1957 and 1964, the African-American voter registration rate in the South increased only marginally even though the Department litigated 71 voting rights lawsuits.

Congress responded to rampant discrimination against racial minorities in public accommodations and government services by passing the Civil Rights Act of 1964. The Act included some voting rights protections; it required registrars to equally administer literacy tests in writing to each voter and to accept applications that contained minor errors, and it created a rebuttable presumption that persons with a sixth-grade education were sufficiently literate to vote. However, despite lobbying from civil rights leaders, the Act did not prohibit most forms of voting discrimination. President Lyndon B. Johnson recognized this, and shortly after the 1964 elections in which Democrats gained overwhelming majorities in both chambers of Congress, he privately instructed Attorney General Nicholas Katzenbach to draft "the goddamndest, toughest voting rights act that you can". However, Johnson did not publicly push for the legislation at the time; his advisers warned him of political costs for vigorously pursuing a voting rights bill so soon after Congress had passed the Civil Rights Act of 1964, and Johnson was concerned that championing voting rights would endanger his Great Society reforms by angering Southern Democrats in Congress.

Following the 1964 elections, civil rights organizations such as the Southern Christian Leadership Conference (SCLC) and the Student Nonviolent Coordinating Committee (SNCC) pushed for federal action to protect the voting rights of racial minorities. Their efforts culminated in protests in Alabama, particularly in the city of Selma, where County Sheriff Jim Clark's police force violently resisted African-American voter registration efforts. Speaking about the voting rights push in Selma, James Forman of SNCC said:
Our strategy, as usual, was to force the U.S. government to intervene in case there were arrests—and if they did not intervene, that inaction would once again prove the government was not on our side and thus intensify the development of a mass consciousness among blacks. Our slogan for this drive was "One Man, One Vote."

In January 1965, Martin Luther King, Jr., James Bevel, and other civil rights leaders organized several demonstrations in Selma that led to violent clashes with police. These marches received national media coverage and drew attention to the issue of voting rights. King and other demonstrators were arrested during a march on February 1 for violating an anti-parade ordinance; this inspired similar marches in the following days, causing hundreds more to be arrested. On February 4, civil rights leader Malcolm X gave a militant speech in Selma in which he said that many African Americans did not support King's nonviolent approach; he later privately said that he wanted to frighten whites into supporting King. The next day, King was released and a letter he wrote addressing voting rights, "Letter From A Selma Jail", appeared in "The New York Times". 

With the nation paying increasing attention to Selma and voting rights, President Johnson reversed his decision to delay voting rights legislation, and on February 6, he announced he would send a proposal to Congress. However, he did not reveal the proposal's content or when it would come before Congress.

On February 18 in Marion, Alabama, state troopers violently broke up a nighttime voting-rights march during which officer James Bonard Fowler shot and killed young African-American protester Jimmie Lee Jackson, who was unarmed and protecting his mother. Spurred by this event, and at the initiation of Bevel, on March 7 SCLC and SNCC began the Selma to Montgomery marches in which Selma residents proceeded to march to Alabama's capital, Montgomery, to highlight voting rights issues and present Governor George Wallace with their grievances. On the first march, demonstrators were stopped by state and county police on horseback at the Edmund Pettus Bridge near Selma. The police shot tear gas into the crowd and trampled protesters. Televised footage of the scene, which became known as "Bloody Sunday", generated outrage across the country.

In the wake of the events in Selma, President Johnson, addressing a televised joint session of Congress on March 15, called on legislators to enact expansive voting rights legislation. He concluded his speech with the words "we shall overcome", a major anthem of the Civil Rights Movement. The Voting Rights Act of 1965 was introduced in Congress two days later while civil rights leaders, now under the protection of federal troops, led a march of 25,000 people from Selma to Montgomery.

The Voting Rights Act of 1965 was introduced in Congress on March 17, 1965 as S. 1564, and it was jointly sponsored by Senate Majority Leader Mike Mansfield (D-MT) and Senate Minority Leader Everett Dirksen (R-IL), both of whom had worked with Attorney General Katzenbach to draft the bill's language. Although Democrats held two-thirds of the seats in both chambers of Congress after the 1964 Senate elections, Johnson worried that Southern Democrats would filibuster the legislation, as they had opposed other civil rights efforts. He enlisted Dirksen to help gain Republican support. Dirksen did not originally intend to support voting rights legislation so soon after supporting the Civil Rights Act of 1964, but he expressed willingness to accept "revolutionary" legislation after learning about the police violence against marchers in Selma on Bloody Sunday. Given Dirksen's key role in helping Katzenbach draft the legislation, it became known informally as the "Dirksenbach" bill. After Mansfield and Dirksen introduced the bill, 64 additional Senators agreed to cosponsor it, with a total 46 Democratic and 20 Republican cosponsors.

The bill contained several special provisions that targeted certain state and local governments: a "coverage formula" that determined which jurisdictions were subject to the Act's other special provisions ("covered jurisdictions"); a "preclearance" requirement that prohibited covered jurisdictions from implementing changes to their voting procedures without first receiving approval from the U.S. Attorney General or the U.S. District Court for D.C. that the changes were not discriminatory; and the suspension of "tests or devices", such as literacy tests, in covered jurisdictions. The bill also authorized the assignment of federal examiners to register voters, and of federal observers to monitor elections, to covered jurisdictions that were found to have engaged in egregious discrimination. The bill set these special provisions to expire after five years.

The scope of the coverage formula was a matter of contentious Congressional debate. The coverage formula reached a jurisdiction if: (1) the jurisdiction maintained a "test or device" on November 1, 1964, and (2) less than 50% of the jurisdiction's voting-age residents either were registered to vote on November 1, 1964 or cast a ballot in the November 1964 presidential election. This formula reached few jurisdictions outside the Deep South. To appease legislators who felt that the bill unfairly targeted Southern jurisdictions, the bill included a general prohibition on racial discrimination in voting that applied nationwide. The bill also included provisions allowing a covered jurisdiction to "bail out" of coverage by proving in federal court that it had not used a "test or device" for a discriminatory purpose or with a discriminatory effect during the 5 years preceding its bailout request. Additionally, the bill included a "bail in" provision under which federal courts could subject discriminatory non-covered jurisdictions to remedies contained in the special provisions.

The bill was first considered by the Senate Judiciary Committee, whose chair, Senator James Eastland (D-MS), opposed the legislation with several other Southern Senators on the committee. To prevent the bill from dying in committee, Mansfield proposed a motion to require the Judiciary Committee to report the bill out of committee by April 9, which the Senate overwhelmingly passed by a vote of 67 to 13. During the committee's consideration of the bill, Senator Ted Kennedy (D-MA) led an effort to amend the bill to prohibit poll taxes. Although the Twenty-fourth Amendment—which banned the use of poll taxes in federal elections— was ratified a year earlier, Johnson's administration and the bill's sponsors did not include a provision in the voting rights bill banning poll taxes in "state" elections because they feared courts would strike down the legislation as unconstitutional. Additionally, by excluding poll taxes from the definition of "tests or devices", the coverage formula did not reach Texas or Arkansas, mitigating opposition from those two states' influential congressional delegations. Nonetheless, with the support of liberal committee members, Kennedy's amendment to prohibit poll taxes passed by a 9-4 vote. In response, Dirksen offered an amendment that exempted from the coverage formula any state that had at least 60% of its eligible residents registered to vote or that had a voter turnout that surpassed the national average in the preceding presidential election. This amendment, which effectively exempted all states from coverage except Mississippi, passed during a committee meeting in which three liberal members were absent. Dirksen offered to drop the amendment if the poll tax ban were removed. Ultimately, the bill was reported out of committee on April 9 by a 12-4 vote without a recommendation.

On April 22, the full Senate started debating the bill. Dirksen spoke first on the bill's behalf, saying that "legislation is needed if the unequivocal mandate of the Fifteenth Amendment ... is to be enforced and made effective, and if the Declaration of Independence is to be made truly meaningful." Senator Strom Thurmond (R-SC) retorted that the bill would lead to "despotism and tyranny", and Senator Sam Ervin (D-NC) argued that the bill was unconstitutional because it deprived states of their right under to establish voter qualifications and because the bill's special provisions targeted only certain jurisdictions. On May 6, Ervin offered an amendment to abolish the coverage formula's automatic trigger and instead allow federal judges to appoint federal examiners to administer voter registration. This amendment overwhelmingly failed, with 42 Democrats and 22 Republicans voting against it. After lengthy debate, Ted Kennedy's amendment to prohibit poll taxes also failed 49-45 on May 11. However, the Senate agreed to include a provision authorizing the Attorney General to sue any jurisdiction, covered or non-covered, to challenge its use of poll taxes. An amendment offered by Senator Robert Kennedy (D-NY) to enfranchise English-illiterate citizens who had attained at least a sixth-grade education in a non-English-speaking school also passed by 48-19. Southern legislators offered a series of amendments to weaken the bill, all of which failed.

On May 25, the Senate voted for cloture by a 70-30 vote, thus overcoming the threat of filibuster and limiting further debate on the bill. On May 26, the Senate passed the bill by a 77-19 vote (Democrats 47-16, Republicans 30-2); only Senators representing Southern states voted against it.

Emanuel Celler (D-NY), Chair of the House Judiciary Committee, introduced the Voting Rights Act in the House of Representatives on March 19, 1965 as H.R. 6400. The House Judiciary Committee was the first committee to consider the bill. The committee's ranking Republican, William McCulloch (R-OH), generally supported expanding voting rights, but he opposed both the poll tax ban and the coverage formula, and he led opposition to the bill in committee. The committee eventually approved the bill on May 12, but it did not file its committee report until June 1. The bill included two amendments from subcommittee: a penalty for private persons who interfered with the right to vote, and a prohibition of all poll taxes. The poll tax prohibition gained Speaker of the House John McCormack's support. The bill was next considered by the Rules Committee, whose chair, Howard W. Smith (D-VA), opposed the bill and delayed its consideration until June 24, when Celler initiated proceedings to have the bill discharged from committee. Under pressure from the bill's proponents, Smith allowed the bill to be released a week later, and the full House started debating the bill on July 6.

To defeat the Voting Rights Act, McCulloch introduced an alternative bill, H.R. 7896. It would have allowed the Attorney General to appoint federal registrars after receiving 25 serious complaints of discrimination about a jurisdiction, and it would have imposed a nationwide ban on literacy tests for persons who could prove they attained a sixth-grade education. McCulloch's bill was co-sponsored by House Minority Leader Gerald Ford (R-MI) and supported by Southern Democrats as an alternative to the Voting Rights Act. The Johnson administration viewed H.R. 7896 as a serious threat to passing the Voting Rights Act. However, support for H.R. 7896 dissipated after William M. Tuck (D-VA) publicly said he preferred H.R. 7896 because the Voting Rights Act would legitimately ensure that African Americans could vote. His statement alienated most supporters of H.R. 7896, and the bill failed on the House floor by a 171-248 vote on July 9. Later that night, the House passed the Voting Rights Act by a 333-85 vote (Democrats 221-61, Republicans 112-24).

The chambers appointed a conference committee to resolve differences between the House and Senate versions of the bill. A major contention concerned the poll tax provisions; the Senate version allowed the Attorney General to sue states that used poll taxes to discriminate, while the House version outright banned all poll taxes. Initially, the committee members were stalemated. To help broker a compromise, Attorney General Katzenbach drafted legislative language explicitly asserting that poll taxes were unconstitutional and instructed the Department of Justice to sue the states that maintained poll taxes. To assuage concerns of liberal committee members that this provision was not strong enough, Katzenbach enlisted the help of Martin Luther King, Jr., who gave his support to the compromise. King's endorsement ended the stalemate, and on July 29, the conference committee reported its version out of committee. The House approved this conference report version of the bill on August 3 by a 328-74 vote (Democrats 217-54, Republicans 111-20), and the Senate passed it on August 4 by a 79-18 vote (Democrats 49-17, Republicans 30-1). On August 6, President Johnson signed the Act into law with King, Rosa Parks, John Lewis, and other civil rights leaders in attendance at the signing ceremony.

Congress enacted major amendments to the Act in 1970, 1975, 1982, 1992, and 2006. Each amendment coincided with an impending expiration of some or all of the Act's special provisions. Originally set to expire by 1970, Congress repeatedly reauthorized the special provisions in recognition of continuing voting discrimination. Congress extended the coverage formula and special provisions tied to it, such as the Section 5 preclearance requirement, for five years in 1970, seven years in 1975, and 25 years in both 1982 and 2006. In 1970 and 1975, Congress also expanded the reach of the coverage formula by supplementing it with new 1968 and 1972 trigger dates. Coverage was further enlarged in 1975 when Congress expanded the meaning of "tests or devices" to encompass any jurisdiction that provided English-only election information, such as ballots, if the jurisdiction had a single language minority group that constituted more than five percent of the jurisdiction's voting-age citizens. These expansions brought numerous jurisdictions into coverage, including many outside of the South. To ease the burdens of the reauthorized special provisions, Congress liberalized the bailout procedure in 1982 by allowing jurisdictions to escape coverage by complying with the Act and affirmatively acting to expand minority political participation.

In addition to reauthorizing the original special provisions and expanding coverage, Congress amended and added several other provisions to the Act. For instance, Congress expanded the original ban on "tests or devices" to apply nationwide in 1970, and in 1975, Congress made the ban permanent. Separately, in 1975 Congress expanded the Act's scope to protect language minorities from voting discrimination. Congress defined "language minority" to mean "persons who are American Indian, Asian American, Alaskan Natives or of Spanish heritage." Congress amended various provisions, such as the preclearance requirement and Section 2's general prohibition of discriminatory voting laws, to prohibit discrimination against language minorities. Congress also enacted a bilingual election requirement in Section 203, which requires election officials in certain jurisdictions with large numbers of English-illiterate language minorities to provide ballots and voting information in the language of the language minority group. Originally set to expire after 10 years, Congress reauthorized Section 203 in 1982 for seven years, expanded and reauthorized it in 1992 for 15 years, and reauthorized it in 2006 for 25 years. The bilingual election requirements have remained controversial, with proponents arguing that bilingual assistance is necessary to enable recently naturalized citizens to vote and opponents arguing that the bilingual election requirements constitute costly unfunded mandates.

Several of the amendments responded to judicial rulings with which Congress disagreed. In 1982, Congress amended the Act to overturn the Supreme Court case "Mobile v. Bolden" (1980), which held that the general prohibition of voting discrimination prescribed in Section 2 prohibited only "purposeful" discrimination. Congress responded by expanding Section 2 to explicitly ban any voting practice that had a discriminatory "effect", regardless of whether the practice was enacted or operated for a discriminatory purpose. The creation of this "results test" shifted the majority of vote dilution litigation brought under the Act from preclearance lawsuits to Section 2 lawsuits. In 2006, Congress amended the Act to overturn two Supreme Court cases: "Reno v. Bossier Parish School Board" (2000), which interpreted the Section 5 preclearance requirement to prohibit only voting changes that were enacted or maintained for a "retrogressive" discriminatory purpose instead of any discriminatory purpose, and "Georgia v. Ashcroft" (2003), which established a broader test for determining whether a redistricting plan had an impermissible effect under Section 5 than assessing only whether a minority group could elect its preferred candidates. In 2014, the Voting Rights Amendments Act was introduced in Congress to create a new coverage formula and amend various other provisions in response to the Supreme Court case "Shelby County v. Holder" (2013), which struck down the coverage formula as unconstitutional. It was referred to the Constitution and Civil Justice congressional subcommittee on February 11, 2015, but no action was taken on it, and it expired.

The Act contains two types of provisions: "general provisions", which apply nationwide, and "special provisions", which apply to only certain states and local governments. Most provisions are designed to protect the voting rights of racial and language minorities. The term "language minority" means "persons who are American Indian, Asian American, Alaskan Natives or of Spanish heritage." The Act's provisions have been colored by numerous judicial interpretations and congressional amendments.

Section 2 prohibits any jurisdiction from implementing a "voting qualification or prerequisite to voting, or standard, practice, or procedure ... in a manner which results in a denial or abridgement of the right ... to vote on account of race," color, or language minority status. The Supreme Court has allowed private plaintiffs to sue to enforce this prohibition. In "Mobile v. Bolden" (1980), the Supreme Court held that as originally enacted in 1965, Section 2 simply restated the Fifteenth Amendment and thus prohibited only those voting laws that were "intentionally" enacted or maintained for a discriminatory purpose. In 1982, Congress amended Section 2 to create a "results" test, which prohibits any voting law that has a discriminatory effect irrespective of whether the law was intentionally enacted or maintained for a discriminatory purpose. The 1982 amendments provided that the results test does not guarantee protected minorities a right to proportional representation.

When determining whether a jurisdiction's election law violates this general prohibition, courts have relied on factors enumerated in the Senate Judiciary Committee report associated with the 1982 amendments ("Senate Factors"), including:
The report indicates not all or a majority of these factors need to exist for an electoral device to result in discrimination, and it also indicates that this list is not exhaustive, allowing courts to consider additional evidence at their discretion.

Section 2 prohibits two types of discrimination: "vote denial", in which a person is denied the opportunity to cast a ballot or to have their vote properly counted, and "vote dilution", in which the strength or effectiveness of a person's vote is diminished. Most Section 2 litigation has concerned vote dilution, especially claims that a jurisdiction's redistricting plan or use of at-large/multimember elections prevents minority voters from casting sufficient votes to elect their preferred candidates. An at-large election can dilute the votes cast by minority voters by allowing a cohesive majority group to win every legislative seat in the jurisdiction. Redistricting plans can be gerrymandered to dilute votes cast by minorities by "packing" high numbers of minority voters into a small number of districts or "cracking" minority groups by placing small numbers of minority voters into a large number of districts.

In "Thornburg v. Gingles" (1986), the Supreme Court used the term "vote dilution through submergence" to describe claims that a jurisdiction's use of an at-large/multimember election system or gerrymandered redistricting plan diluted minority votes, and it established a legal framework for assessing such claims under Section 2. Under the "Gingles" test, plaintiffs must show the existence of three preconditions:
The first precondition is known as the "compactness" requirement and concerns whether a majority-minority district can be created. The second and third preconditions are collectively known as the "racially polarized voting" or "racial bloc voting" requirement, and they concern whether the voting patterns of the different racial groups are different from each other. If a plaintiff proves these preconditions exist, then the plaintiff must additionally show, using the remaining Senate Factors and other evidence, that under the "totality of the circumstances", the jurisdiction's redistricting plan or use of at-large or multimember elections diminishes the ability of the minority group to elect candidates of its choice.

Subsequent litigation further defined the contours of "vote dilution through submergence" claims. In "Bartlett v. Strickland" (2009), the Supreme Court held that the first "Gingles" precondition can be satisfied "only" if a district can be drawn in which the minority group comprises a majority of voting-age citizens. This means that plaintiffs cannot succeed on a submergence claim in jurisdictions where the size of the minority group, despite not being large enough to comprise a majority in a district, is large enough for its members to elect their preferred candidates with the help of "crossover" votes from some members of the majority group. In contrast, the Supreme Court has not addressed whether different protected minority groups can be aggregated to satisfy the "Gingles" preconditions as a coalition, and lower courts have split on the issue.

The Supreme Court provided additional guidance on the "totality of the circumstances" test in "Johnson v. De Grandy" (1994). The Court emphasized that the existence of the three "Gingles" preconditions may be insufficient to prove liability for vote dilution through submergence if other factors weigh against such a determination, especially in lawsuits challenging redistricting plans. In particular, the Court held that even where the three "Gingles" preconditions are satisfied, a jurisdiction is unlikely to be liable for vote dilution if its redistricting plan contains a number of majority-minority districts that is proportional to the minority group's population. The decision thus clarified that Section 2 does not require jurisdictions to maximize the number of majority-minority districts. The opinion also distinguished the proportionality of majority-minority districts, which allows minorities to have a proportional "opportunity" to elect their candidates of choice, from the proportionality of election "results", which Section 2 explicitly does not guarantee to minorities.

An issue regarding the third "Gingles" precondition remains unresolved. In "Gingles", the Supreme Court split as to whether plaintiffs must prove that the majority racial group votes as a bloc specifically because its members are motivated to vote based on racial considerations and not other considerations that may overlap with race, such as party affiliation. A plurality of justices said that requiring such proof would violate Congress's intent to make Section 2 a "results" test, but Justice White maintained that the proof was necessary to show that an electoral scheme results in "racial" discrimination. Since "Gingles", lower courts have split on the issue.

Although most Section 2 litigation has involved claims of vote dilution through submergence, courts also have addressed other types of vote dilution under this provision. In "Holder v. Hall" (1994), the Supreme Court held that claims that minority votes are diluted by the small size of a governing body, such as a one-person county commission, may not be brought under Section 2. A plurality of the Court reasoned that no uniform, non-dilutive "benchmark" size for a governing body exists, making relief under Section 2 impossible. Another type of vote dilution may result from a jurisdiction's requirement that a candidate be elected by a majority vote. A majority-vote requirement may cause a minority group's candidate of choice, who would have won the election with a simple plurality of votes, to lose after a majority of voters unite behind another candidate in a runoff election. The Supreme Court has not addressed whether such claims may be brought under Section 2, and lower courts have reached different conclusions on the issue.

In addition to claims of vote dilution, courts have considered vote denial claims brought under Section 2. The Supreme Court, in "Richardson v. Ramirez" (1974), held that felony disenfranchisement laws cannot violate Section 2 because, among other reasons, Section 2 of the Fourteenth Amendment permits such laws. A federal district court in Mississippi held that a "dual registration" system that requires a person to register to vote separately for state elections and local elections may violate Section 2 if the system has a racially disparate impact in light of the Senate Factors. Starting in 2013, lower federal courts began to consider various challenges to voter ID laws brought under Section 2.

The Act contains several specific prohibitions on conduct that may interfere with a person's ability to cast an effective vote. One of these prohibitions is prescribed in Section 201, which prohibits any jurisdiction from requiring a person to comply with any "test or device" to register to vote or cast a ballot. The term "test or device" is defined as literacy tests, educational or knowledge requirements, proof of good moral character, and requirements that a person be vouched for when voting. Before the Act's enactment, these devices were the primary tools used by jurisdictions to prevent racial minorities from voting. Originally, the Act suspended tests or devices temporarily in jurisdictions covered by the Section 4(b) coverage formula, but Congress subsequently expanded the prohibition to the entire country and made it permanent. Relatedly, Section 202 prohibits jurisdictions from imposing any "durational residency requirement" that requires persons to have lived in the jurisdiction for more than 30 days before being eligible to vote in a presidential election.

Several further protections for voters are contained in Section 11. Section 11(a) prohibits any person acting under color of law from refusing or failing to allow a qualified person to vote or to count a qualified voter's ballot. Similarly, Section 11(b) prohibits any person from intimidating, harassing, or coercing another person for voting or attempting to vote. Two provisions in Section 11 address voter fraud: Section 11(c) prohibits people from knowingly submitting a false voter registration application to vote in a federal election, and Section 11(e) prohibits voting twice in a federal election.

Finally, under Section 208, a jurisdiction may not prevent anyone who is English-illiterate or has a disability from being accompanied into the ballot box by an assistant of the person's choice. The only exceptions are that the assistant may not be an agent of the person's employer or union.

Section 3(c) contains a "bail-in" or "pocket trigger" process by which jurisdictions that fall outside the coverage formula of Section 4(b) may become subject to preclearance. Under this provision, if a jurisdiction has racially discriminated against voters in violation of the Fourteenth or Fifteenth Amendments, a court may order the jurisdiction to have future changes to its election laws preapproved by the federal government. Because courts have interpreted the Fourteenth and Fifteenth Amendments to prohibit only intentional discrimination, a court may bail in a jurisdiction only if the plaintiff proves that the jurisdiction enacted or operated a voting practice to purposely discriminate.

Section 3(c) contains its own preclearance language and differs from Section 5 preclearance in several ways. Unlike Section 5 preclearance, which applies to a covered jurisdiction until such time as the jurisdiction may bail out of coverage under Section 4(a), bailed-in jurisdictions remain subject to preclearance for as long as the court orders. Moreover, the court may require the jurisdiction to preclear only particular types of voting changes. For example, the bail-in of New Mexico in 1984 applied for 10 years and required preclearance of only redistricting plans. This differs from Section 5 preclearance, which requires a covered jurisdiction to preclear all of its voting changes.

During the Act's early history, Section 3(c) was little used; no jurisdictions were bailed in until 1975. Between 1975 and 2013, 18 jurisdictions were bailed in, including 16 local governments and the states of Arkansas and New Mexico. Although the Supreme Court held the Section 4(b) coverage formula unconstitutional in "Shelby County v. Holder" (2013), it did not hold Section 3(c) unconstitutional. Therefore, jurisdictions may continue to be bailed-in and subjected to Section 3(c) preclearance. In the months following "Shelby County", courts began to consider requests by the Attorney General and other plaintiffs to bail in the states of Texas and North Carolina, and in January 2014 a federal court bailed in Evergreen, Alabama.

A more narrow bail-in process pertaining to federal observer certification is prescribed in Section 3(a). Under this provision, a federal court may certify a non-covered jurisdiction to receive federal observers if the court determines that the jurisdiction violated the voting rights guaranteed by the Fourteenth or Fifteenth Amendments. Jurisdictions certified to receive federal observers under Section 3(a) are not subject to preclearance.

Section 4(b) contains a "coverage formula" that determines which states and local governments may be subjected to the Act's other special provisions (except for the Section 203(c) bilingual election requirements, which fall under a different formula). Congress intended for the coverage formula to encompass the most pervasively discriminatory jurisdictions. A jurisdiction is covered by the formula if:


As originally enacted, the coverage formula contained only November 1964 triggering dates; subsequent revisions to the law supplemented it with the additional triggering dates of November 1968 and November 1972, which brought more jurisdictions into coverage. For purposes of the coverage formula, the term "test or device" includes the same four devices prohibited nationally by Section 201—literacy tests, educational or knowledge requirements, proof of good moral character, and requirements that a person be vouched for when voting—and one further device defined in Section 4(f)(3): in jurisdictions where more than 5% of the citizen voting age population are members of a single language minority group, any practice or requirement by which registration or election materials are provided only in English. The types of jurisdictions that the coverage formula applies to include states and "political subdivisions" of states. Section 14(c)(2) defines "political subdivision" to mean any county, parish, or "other subdivision of a State which conducts registration for voting."

The 1965 coverage formula included the whole of Alabama, Alaska, Georgia, Louisiana, Mississippi, South Carolina, and Virginia; and some subdivisions (usually counties) in Arizona, Hawaii, Idaho, and North Carolina. The 1968 coverage resulted in the partial coverage of Alaska, Arizona, California, Connecticut, Idaho, Maine, Massachusetts, New Hampshire, New York, and Wyoming. Connecticut, Idaho, Maine, Massachusetts, and Wyoming filed successful "bailout" lawsuits, as also provided by section 4. The 1972 coverage formula covered the whole of Alaska, Arizona, and Texas, and parts of California, Florida, Michigan, New York, North Carolina, and South Dakota. The special provisions of the Act were due to expire in 1970, and Congress renewed them for another five years. In 1975, the Act's special provisions were extended for another seven years. In 1982, the coverage formula was extended again, this time for 25 years, but no changes were made to the coverage, and in 2006, the coverage formula was again extended for 25 years.

Throughout its history, the coverage formula remained controversial because it singled out certain jurisdictions, most of which were in the Deep South. In "Shelby County v. Holder" (2013), the Supreme Court declared the coverage formula unconstitutional because the criteria used were outdated and thus violated principles of equal state sovereignty and federalism. The other special provisions that are dependent on the coverage formula, such as the Section 5 preclearance requirement, remain valid law. However, without a valid coverage formula, these provisions are unenforceable.

Section 5 requires that covered jurisdictions receive federal approval, known as "preclearance", before implementing changes to their election laws. A covered jurisdiction has the burden of proving that the change does not have the purpose or effect of discriminating on the basis of race or language minority status; if the jurisdiction fails to meet this burden, the federal government will deny preclearance and the jurisdiction's change will not go into effect. The Supreme Court broadly interpreted Section 5's scope in "Allen v. State Board of Election" (1969), holding that any change in a jurisdiction's voting practices, even if minor, must be submitted for preclearance. The Court also held that if a jurisdiction fails to have its voting change precleared, private plaintiffs may sue the jurisdiction in the plaintiff's local district court before a three-judge panel. In these Section 5 "enforcement actions", a court considers whether the jurisdiction made a covered voting change, and if so, whether the change has been precleared. If the jurisdiction improperly failed to obtain preclearance, then the court will order the jurisdiction to obtain preclearance before implementing the change. However, the court may not consider the merits of whether the change should be approved.

Jurisdictions may seek preclearance through either an "administrative preclearance" process or a "judicial preclearance" process. If a jurisdiction seeks administrative preclearance, the Attorney General will consider whether the proposed change has a discriminatory purpose or effect. After the jurisdiction submits the proposed change, the Attorney General has 60 days to interpose an objection to it. The 60-day period may be extended an additional 60 days if the jurisdiction later submits additional information. If the Attorney General interposes an objection, then the change is not precleared and may not be implemented. The Attorney General's decision is not subject to judicial review, but if the Attorney General interposes an objection, the jurisdiction may independently seek judicial preclearance, and the court may disregard the Attorney General's objection at its discretion. If a jurisdiction seeks judicial preclearance, it must file a declaratory judgment action against the Attorney General in the U.S. District Court for D.C. A three-judge panel will consider whether the voting change has a discriminatory purpose or effect, and the losing party may appeal directly to the Supreme Court. Private parties may intervene in judicial preclearance lawsuits.

In several cases, the Supreme Court has addressed the meaning of "discriminatory effect" and "discriminatory purpose" for Section 5 purposes. In "Beer v. United States" (1976), the Court held that for a voting change to have a prohibited discriminatory effect, it must result in "retrogression" (backsliding). Under this standard, a voting change that causes discrimination, but does not result in "more" discrimination than before the change was made, cannot be denied preclearance for having a discriminatory effect. For example, replacing a poll tax with an equally expensive voter registration fee is not a "retrogressive" change because it causes equal discrimination, not more. Relying on the Senate report for the Act, the Court reasoned that the retrogression standard was the correct interpretation of the term "discriminatory effect" because Section 5's purpose is " 'to insure that [the gains thus far achieved in minority political participation] shall not be destroyed through new [discriminatory] procedures' ". The retrogression standard applies irrespective of whether the voting change allegedly causes vote denial or vote dilution.

In 2003, the Supreme Court held in "Georgia v. Ashcroft" that courts should not determine that a new redistricting plan has a retrogressive effect solely because the plan decreases the number of minority-majority districts. The Court emphasized that judges should analyze various other factors under the "totality of the circumstances", such as whether the redistricting plan increases the number of "influence districts" in which a minority group is large enough to influence (but not decide) election outcomes. In 2006, Congress overturned this decision by amending Section 5 to explicitly state that "diminishing the ability [of a protected minority] to elect their preferred candidates of choice denies or abridges the right to vote within the meaning of" Section 5. Uncertainty remains as to what this language precisely means and how courts may interpret it.

Before 2000, the "discriminatory purpose" prong of Section 5 was understood to mean "any" discriminatory purpose, which is the same standard used to determine whether discrimination is unconstitutional. In "Reno v. Bossier Parish" ("Bossier Parish II") (2000), the Supreme Court extended the retrogression standard, holding that for a voting change to have a "discriminatory purpose" under Section 5, the change must have been implemented for a "retrogressive" purpose. Therefore, a voting change intended to discriminate against a protected minority was permissible under Section 5 so long as the change was not intended to increase existing discrimination. This change significantly reduced the number of instances in which preclearance was denied based on discriminatory purpose. In 2006, Congress overturned "Bossier Parish II" by amending Section 5 to explicitly define "purpose" to mean "any discriminatory purpose."

Until the 2006 amendments to the Act, Section 6 allowed the appointment of "federal examiners" to oversee certain jurisdictions' voter registration functions. Federal examiners could be assigned to a covered jurisdiction if the Attorney General certified that
Federal examiners had the authority to register voters, examine voter registration applications, and maintain voter rolls. The goal of the federal examiner provision was to prevent jurisdictions from denying protected minorities the right to vote by engaging in discriminatory behavior in the voter registration process, such as refusing to register qualified applicants, purging qualified voters from the voter rolls, and limiting the hours during which persons could register. Federal examiners were used extensively in the years following the Act's enactment, but their importance waned over time; 1983 was the last year that a federal examiner registered a person to vote. In 2006, Congress repealed the provision.

Under the Act's original framework, in any jurisdiction certified for federal examiners, the Attorney General could additionally require the appointment of "federal observers". By 2006, the federal examiner provision was used solely as a means to appoint federal observers. When Congress repealed the federal examiner provision in 2006, Congress amended Section 8 to allow for the assignment of federal observers to jurisdictions that satisfied the same certification criteria that had been used to appoint federal examiners.

Federal observers are tasked with observing poll worker and voter conduct at polling places during an election and observing election officials tabulate the ballots. The goal of the federal observer provision is to facilitate minority voter participation by deterring and documenting instances of discriminatory conduct in the election process, such as election officials denying qualified minority persons the right to cast a ballot, intimidation or harassment of voters on election day, or improper vote counting. Discriminatory conduct that federal observers document may also serve as evidence in subsequent enforcement lawsuits. Between 1965 and the Supreme Court's 2013 decision in "Shelby County v. Holder" to strike down the coverage formula, the Attorney General certified 153 local governments across 11 states. Because of time and resource constraints, federal observers are not assigned to every certified jurisdiction for every election. Separate provisions allow for a certified jurisdiction to "bail out" of its certification.

Under Section 4(a), a covered jurisdiction may seek exemption from coverage through a process called "bailout." To achieve an exemption, a covered jurisdiction must obtain a declaratory judgment from a three-judge panel of the District Court for D.C. that the jurisdiction is eligible to bail out. As originally enacted, a covered jurisdiction was eligible to bail out if it had not used a test or device with a discriminatory purpose or effect during the 5 years preceding its bailout request. Therefore, a jurisdiction that requested to bail out in 1967 would have needed to prove that it had not misused a test or device since at least 1962. Until 1970, this effectively required a covered jurisdiction to prove that it had not misused a test or device since before the Act was enacted five years earlier in 1965, making it impossible for many covered jurisdictions to bail out. However, Section 4(a) also prohibited covered jurisdictions from using tests or devices in any manner, discriminatory or otherwise; hence, under the original Act, a covered jurisdiction would become eligible for bailout in 1970 by simply complying with this requirement. But in the course of amending the Act in 1970 and 1975 to extend the special provisions, Congress also extended the period of time that a covered jurisdiction must not have misused a test or device to 10 years and then to 17 years, respectively. These extensions continued the effect of requiring jurisdictions to prove that they had not misused a test or device since before the Act's enactment in 1965.

In 1982, Congress amended Section 4(a) to make bailout easier to achieve in two ways. First, Congress provided that if a state is covered, local governments in that state may bail out even if the state is ineligible to bail out. Second, Congress liberalized the eligibility criteria by replacing the 17-year requirement with a new standard, allowing a covered jurisdiction to bail out by proving that in the 10 years preceding its bailout request:
Additionally, Congress required jurisdictions seeking bailout to produce evidence of minority registration and voting rates, including how these rates have changed over time and in comparison to the registration and voting rates of the majority. If the court determines that the covered jurisdiction is eligible for bailout, it will enter a declaratory judgment in the jurisdiction's favor. The court will retain jurisdiction for the following 10 years and may order the jurisdiction back into coverage if the jurisdiction subsequently engages in voting discrimination.

The 1982 amendment to the bailout eligibility standard went into effect on August 5, 1984. Between that date and 2013, 196 jurisdictions bailed out of coverage through 38 bailout actions; in each instance, the Attorney General consented to the bailout request. Between that date and 2009, all jurisdictions that bailed out were located in Virginia. In 2009, a municipal utility jurisdiction in Texas bailed out after the Supreme Court's opinion in "Northwest Austin Municipal Utility District No. 1 v. Holder" (2009), which held that local governments that do not register voters have the ability to bail out. After this ruling, jurisdictions succeeded in at least 20 bailout actions before the Supreme Court held in "Shelby County v. Holder" (2013) that the coverage formula was unconstitutional.

Separate provisions allow a covered jurisdiction that has been certified to receive federal observers to bail out of its certification alone. Under Section 13, the Attorney General may terminate the certification of a jurisdiction if 1) more than 50% of the jurisdiction's minority voting age population is registered to vote, and 2) there is no longer reasonable cause to believe that residents may experience voting discrimination. Alternatively, the District Court for D.C. may order the certification terminated.

Two provisions require certain jurisdictions to provide election materials to voters in multiple languages: Section 4(f)(4) and Section 203(c). A jurisdiction covered by either provision must provide all materials related to an election—such as voter registration materials, ballots, notices, and instructions—in the language of any applicable language minority group residing in the jurisdiction. Language minority groups protected by these provisions include Asian Americans, Hispanics, Native Americans, and Native Alaskans. Congress enacted the provisions to break down language barriers and combat pervasive language discrimination against the protected groups.

Section 4(f)(4) applies to any jurisdiction encompassed by the Section 4(b) coverage formula where more than 5% of the citizen voting age population are members of a single language minority group. Section 203(c) contains a formula that is separate from the Section 4(b) coverage formula, and therefore jurisdictions covered solely by 203(c) are not subject to the Act's other special provisions, such as preclearance. The Section 203(c) formula encompasses jurisdictions where the following conditions exist:

Section 203(b) defines "limited-English proficient" as being "unable to speak or understand English adequately enough
to participate in the electoral process". Determinations as to which jurisdictions satisfy the Section 203(c) criteria occur once a decade following completion of the decennial census; at these times, new jurisdictions may come into coverage while others may have their coverage terminated. Additionally, under Section 203(d), a jurisdiction may "bail out" of Section 203(c) coverage by proving in federal court that no language minority group within the jurisdiction has an English illiteracy rate that is higher than the national illiteracy rate. After the 2010 census, 150 jurisdictions across 25 states were covered under Section 203(c), including statewide coverage of California, Texas, and Florida.

After its enactment in 1965, the law immediately decreased racial discrimination in voting. The suspension of literacy tests and the assignments of federal examiners and observers allowed for high numbers of racial minorities to register to vote. Nearly 250,000 African Americans registered in 1965, one-third of whom were registered by federal examiners. In covered jurisdictions, less than one-third (29.3%) of the African American population was registered in 1965; by 1967, this number increased to more than half (52.1%), and a majority of African American residents became registered to vote in 9 of the 13 Southern states. Similar increases were seen in the number of African Americans elected to office: between 1965 and 1985, African Americans elected as state legislators in the 11 former Confederate states increased from 3 to 176. Nationwide, the number of African American elected officials increased from 1,469 in 1970 to 4,912 in 1980. By 2011, the number was approximately 10,500. Similarly, registration rates for language minority groups increased after Congress enacted the bilingual election requirements in 1975 and amended them in 1992. In 1973, the percent of Hispanics registered to vote was 34.9%; by 2006, that amount nearly doubled. The number of Asian Americans registered to vote in 1996 increased 58% by 2006.

After the Act's initial success in combating tactics designed to deny minorities access to the polls, the Act became predominately used as a tool to challenge racial vote dilution. Starting in the 1970s, the Attorney General commonly raised Section 5 objections to voting changes that decreased the effectiveness of racial minorities' votes, including discriminatory annexations, redistricting plans, and election methods such as at-large election systems, runoff election requirements, and prohibitions on bullet voting. In total, 81% (2,541) of preclearance objections made between 1965 and 2006 were based on vote dilution. Claims brought under Section 2 have also predominately concerned vote dilution. Between the 1982 creation of the Section 2 results test and 2006, at least 331 Section 2 lawsuits resulted in published judicial opinions. In the 1980s, 60% of Section 2 lawsuits challenged at-large election systems; in the 1990s, 37.2% challenged at-large election systems and 38.5% challenged redistricting plans. Overall, plaintiffs succeeded in 37.2% of the 331 lawsuits, and they were more likely to succeed in lawsuits brought against covered jurisdictions.

By enfranchising racial minorities, the Act facilitated a political realignment of the Democratic and Republican parties. Between 1890 and 1965, minority disenfranchisement allowed conservative Southern Democrats to dominate Southern politics. After Democratic President Lyndon B. Johnson signed the Act into law, newly enfranchised racial minorities began to vote for liberal Democratic candidates throughout the South, and Southern white conservatives began to switch their party registration from Democrat to Republican en masse. These dual trends caused the two parties to ideologically polarize, with the Democratic Party becoming more liberal and the Republican Party becoming more conservative. The trends also created competition between the two parties, which Republicans capitalized on by implementing the Southern strategy. Over the subsequent decades, the creation of majority-minority districts to remedy racial vote dilution claims also contributed to these developments. By packing liberal-leaning racial minorities into small numbers of majority-minority districts, large numbers of surrounding districts became more solidly white, conservative, and Republican. While this increased the elected representation of racial minorities as intended, it also decreased white Democratic representation and increased the representation of Republicans overall. By the mid-1990s, these trends culminated in a political realignment: the Democratic Party and the Republican Party became more ideologically polarized and defined as liberal and conservative parties, respectively; and both parties came to compete for electoral success in the South, with the Republican Party controlling most of Southern politics.

A 2016 study in the "American Journal of Political Science" found "that members of Congress who represented jurisdictions subject to the preclearance requirement were substantially more supportive of civil rights–related legislation than legislators who did not represent covered jurisdictions." A 2018 study in "The Journal of Politics" found that Section 5 of the 1965 Voting Rights Act "increased black voter registration by 14–19 percentage points, white registration by 10–13 percentage points, and overall voter turnout by 10–19 percentage points. Additional results for Democratic vote share suggest that some of this overall increase in turnout may have come from reactionary whites."

Early in the Act's enforcement history, the Supreme Court addressed the constitutionality of several provisions relating to voter qualifications and prerequisites to voting. In "Katzenbach v. Morgan" (1966), the Court upheld the constitutionality of Section 4(e). This section prohibits jurisdictions from administering literacy tests to citizens who attain a sixth-grade education in an American school in which the predominant language was Spanish, such as schools in Puerto Rico. Although the Court had earlier held in "Lassiter v. Northampton County Board of Elections" (1959) that literacy tests did not violate the Fourteenth Amendment, in "Morgan" the Court held that Congress could enforce Fourteenth Amendment rights—such as the right to vote—by prohibiting conduct it deemed to interfere with such rights, even if that conduct may not be independently unconstitutional. After Congress created a nationwide ban on all literacy tests and similar devices in 1970 by enacting Section 201, the Court upheld the ban as constitutional in "Oregon v. Mitchell" (1970).

Also in "Oregon v. Mitchell", the Supreme Court addressed the constitutionality of various other provisions relating to voter qualifications and prerequisites to voting. The Court upheld Section 202, which prohibits any state or local jurisdiction from requiring people to live in their borders for longer than 30 days before allowing them to vote in a presidential election. Additionally, the Court upheld the provision lowering the minimum voting age to 18 in federal elections, but it held that Congress exceeded its power by lowering the voting age to 18 in state elections; this precipitated the ratification of the Twenty-sixth Amendment the following year, which lowered the voting age in all elections to 18. The Court was deeply divided in "Oregon v. Mitchell", and a majority of justices did not agree on a rationale for the holding.

The constitutionality of Section 2, which contains a general prohibition on discriminatory voting laws, has not been definitively explained by the Supreme Court. As amended in 1982, Section 2 prohibits any voting practice that has a discriminatory effect, irrespective of whether the practice was enacted or is administered for the purpose of discriminating. This "results test" contrasts with the Fourteenth and Fifteenth Amendments, both of which directly prohibit only purposeful discrimination. Given this disparity, whether the Supreme Court would uphold the constitutionality of Section 2 as appropriate legislation passed to enforce the Fourteenth and Fifteenth Amendments, and under what rationale, remains unclear.

In "Mississippi Republican Executive Opinion v. Brooks" (1984), the Supreme Court summarily affirmed, without a written opinion, a lower court's decision that 1982 amendment to Section 2 is constitutional. Justice Rehnquist, joined by Chief Justice Burger, dissented from the opinion. Their reasoning was that the case presented complex constitutional issues that should have warranted a full hearing. In later cases, the Supreme Court is more likely to disregard one of its previous judgments that lacks a written opinion, but lower courts must respect the Supreme Court's unwritten summary affirmances as being as equally binding on them as Supreme Court judgments with written opinions. Partially due to "Brooks", the constitutionality of the Section 2 results test has since been unanimously upheld by lower courts.

The Supreme Court has upheld the constitutionality of the Section 5 preclearance requirement in three cases. The first case was "South Carolina v. Katzenbach" (1966), which was decided about five months after the Act's enactment. The Court held that Section 5 constituted a valid use of Congress's power to enforce the Fifteenth Amendment, reasoning that "exceptional circumstances" of pervasive racial discrimination, combined with the inadequacy of case-by-case litigation in ending that discrimination, justified the preclearance requirement. The Court also upheld the constitutionality of the 1965 coverage formula, saying that it was "rational in both practice and theory" and that the bailout provision provided adequate relief for jurisdictions that may not deserve coverage.

The Supreme Court again upheld the preclearance requirement in "City of Rome v. United States" (1980). The Court held that because Congress had explicit constitutional power to enforce the Reconstruction Amendments "by appropriate legislation", the Act did not violate principles of federalism. The Court also explicitly upheld the "discriminatory effect" prong of Section 5, stating that even though the Fifteenth Amendment directly prohibited only intentional discrimination, Congress could constitutionally prohibit unintentional discrimination to mitigate the risk that jurisdictions may engage in intentional discrimination. Finally, the Court upheld the 1975 extension of Section 5 because of the record of discrimination that continued to persist in the covered jurisdictions. The Court further suggested that the temporary nature of the special provisions was relevant to Section 5's constitutionality.

The final case in which the Supreme Court upheld Section 5 was "Lopez v. Monterey County" ("Lopez II") (1999). In "Lopez II", the Court reiterated its reasoning in "Katzenbach" and "Rome", and it upheld as constitutional the requirement that covered local governments obtain preclearance before implementing voting changes that their parent state required them to implement, even if the parent state was not itself a covered jurisdiction.

The 2006 extension of Section 5 was challenged before the Supreme Court in "Northwest Austin Municipal Utility District No. 1 v. Holder" (2009). The lawsuit was brought by a municipal water district in Texas that elected members to a water board. The District wished to move a voting location from a private home to a public school, but that change was subject to preclearance because Texas was a covered jurisdiction. The District did not register voters, and thus it did not appear to qualify as a "political subdivision" eligible to bail out of coverage. Although the Court indicated in dicta (a non-binding part of the court's opinion) that Section 5 presented difficult constitutional questions, it did not declare Section 5 unconstitutional; instead, it interpreted the law to allow any covered local government, including one that does not register voters, to obtain an exemption from preclearance if it meets the bailout requirements.

On November 9, 2012, the Supreme Court granted certiorari in the case of "Shelby County v. Holder" limited to the question of "whether Congress' decision in 2006 to reauthorize Section 5 of the Voting Rights Act under the pre-existing coverage formula of Section 4(b) ... exceeded its authority under the Fourteenth and Fifteenth Amendments, thus violating the Tenth Amendment and Article IV of the United States Constitution". On June 25, 2013, the Court struck down Section 4(b) as unconstitutional. The Court reasoned that the coverage formula violates the constitutional principles of "equal sovereignty of the states" and federalism because its disparate treatment of the states is "based on 40 year-old facts having no logical relationship to the present day", which makes the formula unresponsive to current needs. The Court did not strike down Section 5, but without Section 4(b), no jurisdiction may be subject to Section 5 preclearance unless Congress enacts a new coverage formula. After the decision, several states that were fully or partially covered—including Texas, Mississippi, North Carolina, and South Carolina—implemented laws that were previously denied preclearance. This prompted new legal challenges to these laws under other provisions unaffected by the Court's decision, such as Section 2.

While Section 2 and Section 5 prohibit jurisdictions from drawing electoral districts that dilute the votes of protected minorities, the Supreme Court has held that in some instances, the Equal Protection Clause of the Fourteenth Amendment prevents jurisdictions from drawing district lines to favor protected minorities. The Court first recognized the justiciability of affirmative "racial gerrymandering" claims in "Shaw v. Reno" (1993). In "Miller v. Johnson" (1995), the Court explained that a redistricting plan is constitutionally suspect if the jurisdiction used race as the "predominant factor" in determining how to draw district lines. For race to "predominate", the jurisdiction must prioritize racial considerations over traditional redistricting principles, which include "compactness, contiguity, [and] respect for political subdivisions or communities defined by actual shared interests." If a court concludes that racial considerations predominated, then the redistricting plan is considered "racially gerrymandered" and must be subjected to strict scrutiny, meaning that the redistricting plan will be upheld as constitutional only if it is narrowly tailored to advance a compelling state interest. In "Bush v. Vera" (1996), a plurality of the Supreme Court assumed that complying with Section 2 or Section 5 constituted compelling interests, and lower courts have allowed only these two interests to justify racial gerrymandering.





</doc>
